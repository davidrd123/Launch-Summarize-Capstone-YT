foreign my name is Cody Williams I'm here today with Martin Graham Marcos Avila and Muhammad Al Shanti and we're excited to tell you all about our experience building trellis I'll begin today's presentation with a brief introduction to Tyler a little set of foundation for its objectives by discussing some technical background and problems that inspired the project next Marcos will expand on how trellis solves these problems Martin will walk us through its architecture and Muhammad will dive into our engineering decisions finally Martin will wrap up with a description of our future work so what is trellis chalice is an open tour low configuration deployment pipeline for teams who developed serverless applications it minimizes the number of steps required for teams to start deploying their code to the cloud and its dashboard provides a centralized space for them to Monitor and manage multiple deployment environments today you'll learn about some challenges trade-offs and choices we face while building out this functionality to understand the utility of the trellis and the problems it's called for its users we first need to take a look at its technical background and the deployment related problems that inspired us to build it so deployment is at the heart of trellis but what's deployment fundamentally deployment is the process of moving application code from a developer's machine to infrastructure that serves it to end users traditionally software developers were tasked with writing code and a separate operations team was in charge of deploying it any friction between these teams meant more time taken for the operations team to discover problems and for the development team to respond with fixes to speed up release Cycles many organizations have adopted the philosophy of devops where development and operations roles are combined into a single team with devops less friction means faster application iterations shorter development cycles and quicker delivery of features to users as development teams work together to iterate on an application it's important to test each developer the code changes and validate that they're acceptable for integration into the applications code base with an interest in Rapid relief Cycles devops genes often turn to automation to accomplish code integration and this is known as continuous integration a fundamental step pertains to achieve continuous integration is committing all code changes to a shared repository in a version control system like GitHub a version control system is key to continuous integration because it acts as a single source of Truth to build a comprehensive artifact that can be deployed as full application foreign tegration is only part of the story once code contributions are compiled into a single artifact its functionality should be further validated to ensure that it's ready for end users continuous delivery is a devops practice that automatically deploys all code changes so teams can operate Monitor and test them for validation across multiple dimensions doing so effectively prepares an artifact for Relief to users deployment environments play a key role in continuous delivery a deployment environment is a discrete set of resources to which an application is deployed themes often use a number of development staging and production environments as part of their continuous delivery process let's take a look at them development environments let developers examine preliminary and potentially problematic code changes introducing this code to end users could expose them to Broken features or render the application completely unavailable to them but applications in development environments are not accessible to end users instead they're only used by Developers for trying out their code and making sure that it runs as a result development environments generally don't need the same resource capacity as Downstream environments where more intensive verification take place staging environments are like development and environments and that they don't serve end users either however they tend to use more robust resources because they're meant to replicate the environment that will serve end users this allows teams to see how the application will behave for users and it also provides a suitable environment for methods that stimulate real-world interaction with the application these can include things like UI testing load testing integration testing and more lastly production environments contain the infrastructure that does serve real users code that runs in this environment has generally been thoroughly vetted enough to be trusted for release the resources for this environment must have sufficient capacity to serve the application at scale so a production environment may contain many machines to handle the traffic logic execution and database queries required by the application but simply having multiple deployment environments is not enough to ensure that code is ready for release what's needed is a process where progressing the code through these environments all the way to production this process is referred to as a deployment pipeline the early stages of a deployment pipeline like the development environment can run basic tests to give early feedback on major issues while later stages like the staging environment provide an opportunity for more comprehensive testing and feature probing when developers are satisfied with how their application measures up in one environment they can promote it to the next environment and as code is promoted through a pipeline genes can develop confidence in its capacity to serve users when it comes to deployment pipelines teams have the option of manually deploying and promoting code through its various environments this could involve cloning code from a Version Control System injecting the application's environment variable running tests and entering a series of commands to deploy the code and while this is technically a valid way to make use of a deployment pipeline it's time consuming and error-prone as it requires developers to follow several steps and in the right order for every development deployment a more effective approach is to embrace continuous delivery and automate the operations duties related to deployment as a result teams can reap the benefit of a devops culture while still being able to focus most of their time and energy on development related tasks in automated deployment pipelines deployments are often initiated in response to changes to the source code in a team's Version Control System for instance engineer could develop new features on the main branch of their GitHub repository and configure their deployment pipeline so that any commits to the branch will be automatically deployed to a corresponding environment with continuous delivery scenes generally retain control over promoting code to production both sneaky bugs can't reach end users without explicit approval from a human but still they can benefit from automating the deployment steps that take place before and after that manual approval in general the bulk of the automated steps are handled by a build server its work generally involves obtaining code from Source control conducting tests on it running commands or scripts to deploy it and producing logs to indicate deployment status or convey any errors that may have occurred throughout the process deployment pipelines also commonly include a centralized dashboard that teams can use to interact with and collaborate on shared deployment Pipelines a dashboard can display build server logs provide manual controls over the pipeline offer ways to configure settings and generally introduces transparency to the work carried out by the build server so now that we have a general understanding of automated deployment pipelines Marcos will tell us a little about how they can be implemented thanks Cody depending on the infrastructure of the application being deployed several employment pipelines can be developed let's examine their implementation in three distinct application infrastructures the term on-prem refers to server Hardware that is fully managed and controlled by the developing organization maintaining on-prem infrastructure frequently necessitates considerable Capital investments in order to purchase the Harvard required to host application and maintain application performance for a three-tier application to be deployed on-prem the company would have to set up web application and database servers on its own Hardware a tool that on-prem deployment pipeline will deploy code to the environments for staging and production that are on these Prem machines the development environments on the other hand would probably be on the team's local machines which can host all three application tiers and mimic how they work in production for small teams that don't have the time or skills to take care of physical infrastructure it makes sense to use cloud hosting instead of on-prem Hosting Cloud hosting means deploying an app on machines that are managed by third party like Amazon web services this infrastructure lets teams run their code on Enterprise level machines without having to buy those machines a cloud hosted applications deployment pipeline works in the same way as the simple applications on-premise deployment the cloud provider lets developers choose the hardware specs of the cloud servers that are set up for their application because of this the servers that host the staging and production environments can be the same as those used in on-premise deployment method and developers can often continue to use local development environments to make changes to code before putting it into production serverless is a model that can be thought of as a subset of applications that run in the cloud Cloud providers don't host virtualized versions of web application and database servers instead they offer services that abstract the underlying servers serverless resources generally have characteristics such as Auto scaling resources scale up or down based on demand High availability redundant physical servers in several locations minimize downtime if some fail now pay for use on the most popular ones this is built based on compute time store data or data transfer which leads to not infrastructure management whatsoever serverless applications deployment pipelines differ between on-premise and cloud-hosted pipelines a serverless applications components are defined on the cloud providers platform rather than on separate machines thus although being in the same affirmal Cloud a serverless application deployment pipeline must retain The Logical separation of deployment environments and their resources another issue arises with development of environments for serverless applications the look of the developer environment is now very different from staging and production environments there are local mocking solutions for cloud environments but nothing beats actually running an app in the cloud to get a feel for how it will perform thus it is beneficial to the developer experience to use the staging or production-like environment for development thankfully the payaseko nature of serverless means that even if they share the same resources the development environments won't have to bear the same financial burden as the production environment that is open to public traffic but there are wide variety of options available for putting automated deployment pipelines into practice in-house Solutions can be developed using open source tools as an alternative in-house options while more affordable and flexible are typically more difficult to implement smaller teams don't have the Personnel to take on additional tasks of Designing and managing an entire deployment application and other class of options are third party or SAS deployment pipelines such as Travis Ci or Circle CI this pre-configured tools provide feature-rich platforms for developing testing and delivering projects in a variety of deployment scenarios of course this has costs extensive customization costs these Solutions demand a costly premium plans they're complicated workflows feature sets Employments may be tough to actually navigate there are products that Target smaller niches such as seed which is a SAS deployment pipeline for serverless apps compared to Jenkins and Travis CI it has simpler configuration because it supports fewer developer methods our applications and exclusively deploys applications to AWS supports applications developed on the serverless framework or the serverless stack framework and takes code from three of the most prominent Version Control Systems seats constraints make it unsuitable for many software development workflows but they allow it to abstract most of the complexity for its Target users it supports numerous app environments requires no build spec files no deployment scripts and has a simple web-based dashboard for deployment management and monitoring some teams will need a solution like seed that is opinionated low config and inexpensive but allows the user full control over the pipeline's infrastructure and data we built trellis to address this need now that we understand where trellis fits in we can start speaking about trellis itself trellis is a low config continuous delivery deployment pipeline for teams who develop serverless applications on AWS it simplifies automated the deployment pipeline setup for teams that wish to focus on feature development rather than operations trellis users get a low config deployment Pipeline and data control as an open source self-hosted application trellis allows its customers to perform a variety of actions needed of a deployment pipeline it allows the users for example to connect from GitHub when a user logs into trellis they can create new applications and choose it a GitHub repository that has the source code for each application by default the development environment is linked to the main branch of the selected repository this makes it easy to deploy changes to a cloud environment users can set up other git branches to give each deployment environment the source code they want when code is committed to an application's GitHub repository trellis automatically starts deploying the new code to any environment that is configured to deploy that branch if users want to deploy a commit that occurred before Trellis was configured users can manually deploy the most recent commit for any branch unit tests can give you confidence that code is doing what you want it to do if tests are thorough and the code passes them developers can have confidence that things are working the way they should and trellis users can turn on or off unit testing for each deployment environment this lets them see the output of the npm run test command every time a deployment happens they can also set up a different command run tests if they need to with the Press of a button trailers users can promote code to the next environment if its developers are confident in its performance trellis prevents automatic deployments in production since they can impact the company by exposing end users to bugs errors and infrastructure issues manual promotion to production is the only way to deploy an application to end users the development team may want to roll back an environment to a trusted application version after deploying problematic code if a mist-typed database query crashes an application users should be provided an older version all deployment environments have a trellis rollback button now this button lets users to redeploy or roll back to a previous application version Charlie's users May deploy their code then decided requires further development and should not Advance along the pipeline in such instances trellis provides a tear down option for uninstalling all Cloud resources connected with a deployed environment trellis validates code modifications and puts them in environments for testing and pipeline promotion to production a build server a dashboard and a back-end performed this task trellis build server deploys to Target environments from an isolated machine and allocates unique Cloud resources for each environment users May request teardowns and rollbacks for all deployment environments from this build server these jobs are executed on isolated machines to assure reliability trellis's dashboard interface lets customers control deployment pipelines it lets users trigger the deployment pipeline manual steps use the skin customized as deployment environments manage the pipelines features and see the deployment status the back end connects to the front end and build server it also maintains user logins environment specific settings GitHub repository data deployment environment status and build server output its API initiates the build server updates the database and sends data to the dashboard and build server now with that said I'll hand it over to Martin to address the specific architecture of trellis components take it away Martin thanks Marcos now that we understand trellis's functionality and its high level components let's dive into the architecture of trellis Carlos's Target users are teams that deploy serverless applications to AWS to better serve these users we decided to design trellis itself as a serverless application hosted on AWS using the user Zone account our Target users would be able to adopt trellis into their existing workflows and leverage their existing serverless expertise additionally trellis's usage patterns are a good fit for a serverless application builds are intermittent but may occur concurrently so Auto scaling is desirable furthermore there will be a large periods of time for which trellis will sit dormant and so paper use is a good model for trellis the decision to design trellis itself as a serverless application guided our architecture choices regarding its components before examining trellis's build server we need to discuss how resources the resources that compose a serverless application are defined process build process is built atop an infrastructure as code tool known as the serverless stack framework or SST which itself wraps one of Amazon's infrastructure's code tools called the cloud development kit IAC tools enable developers to specify resources using a familiar programming language as opposed to manually deploying resources individually on the AWS dashboard when using IEC the source code itself contains instructions for what resources to deploy and how they should be connected which adds consistency to the deployment process IAC tools also offer commands for deploying the application and the cloud resources it relies on trellis executes a build command provided by SST to deploy the user's applications to AWS as defined in the application source code these IAC deployment commands must be executed by a computer deployment pipelines traditionally use a dedicated build server to run the build process and deploy users applications trellis as a serverless application does not have a dedicated long-standing build server though for the rest of this talk we will continue to refer to this component as a build server trellis's build server is implemented as an AWS ECS fargate task which offers on-demand Computing via containers containers are Standalone executable software packages that include an application and all the dependencies it needs to run each time a fargate task is invoked a new container is spun up which is based on an image that is stored by trellis in the AWS elastic container registry this container is passed the information it needs to complete the build process such as which commit to deploy and information about the target deployment environment each container has access to a Sandbox file system that it can use to clone source code from git GitHub install application dependencies and execute the build commands that provision the user's application on AWS build server containers are scaled in and out according to incoming requests in our case each deployment to an environment invokes a new fargate task so that all deployments occur in a fresh isolated environment finally the containers are de-provisioned once they have completed their work or trust to deploy to AWS resources on behalf of its users it must have credentials with both the create and delete permissions for all AWS Services used within that application trellis stores these credentials using AWS Secrets manager which requires that the users of resources be given affirmative permission to access secrets build server containers use the AWS software development kit to programmatically access trellis Secrets stored in Secrets manager during the build process lastly logs of the build processor captured by AWS cloudwatch Amazon's log collection service this brings us to the back end the bulk of trellis's back end is composed of multiple Lambda functions AWS Lambda is Amazon's functions as a service offering Lambda functions are individually defined units of executable logic and when invoked the cloud provider runs just that function on their servers the infrastructure that runs each function is provisioned and deprovisioned according to incoming demand Amazon scales the number of functions responding to requests to match incoming traffic flow crucially there's no long-running web server to handle requests for trellis there are many Lambda functions for managing each of trellis's back-end responsibilities such as authenticating users through GitHub creating new applications retrieving those applications deployment environments promoting deployment environments even triggering automatic builds and GitHub notifies trellis of new commands and many others trellis uses AWS API Gateway as the front door for its backend AWS API Gateway is a service that makes it easy for developers to create and secure apis an API Gateway can accept incoming HTTP requests and has Lambda functions associated with each of its URL endpoints the back end also interacts with AWS cloudwatch to retrieve the logs produced by the build server lastly trellis needs to store data about the state of the deployment pipelines it is managing trellis stores this data in AWS dynamodb a serverless key value nosql database the information stored in the database includes databout users their status of applications and their deployment environments and other such metadata we built the trellis dashboard as a single page application in react which is populated by making calls to trellis's backend the dashboard code is stored in an AWS S3 bucket S3 is Amazon's object storage service static files such as our dashboard application can be retrieved directly from an S3 bucket our dashboard is served using AWS cloudfront Amazon's content delivery network using S3 and cloudfront in this way allowed us to avoid using a long-leading web server to serve the dashboard next Muhammad will explain in Greater detail some of the choices we faced when making trellis all right thank you Martin now that you have an understanding of what trellis does and its architecture let us dive into the engineering decisions and the trade-offs we made while building trellis first we're going to talk about the implementation of the build server so as Martin mentioned we wanted the entire trellis application to conform to the serverless model this means that compute resources are not left either but rather spun up on demand we consider two AWS services to implement our build server first was the AWS Lambda and the AWS forget task let's see how they compare with each other first because the builds are executed intermittently we wanted the build server to have a pay-as equal pricing fortunately both the land and the forget task provide this future now another future that we considered was Auto scaling with multiple applications each with multiple deployment environments it is entirely possible that simultaneous builds could occur again both the Lambda functions and the forget task provide out scaling capability now as I said earlier both Lambda functions and forget tasks are spun on demand so we looked at their startup time to ensure that it does not significantly affect the deployment process time now the Lambda function takes about five seconds to stop whereas the forget task takes up to one minute to start with this future we favored Lambda functions over the forget tasks the last item that we looked at was the time limit for each service now deploying a modest application like trellis took around 20 minutes to complete one constraint that Lambda had but the forget that task did not was the timeout Lambda functions time out after 15 minutes whereas forget task does not have a timeout it stays up until the task is complete we can accept the additional minute of startup time for the forget task but we cannot accept the 15 minutes timeout file under function and hence we decided to go with the forget test for our build server now the second engineering decision that we want to discuss is sending real-time deployment data to the dashboard the deployment data is mainly to inform the user of the state of the deployment of their application within trellis a deployment can be in one of five states either the deployment is deployed it's deploying it's getting removed it's removed or it ran into an error now as you can see from this picture the data that we sent to the user includes the overall deployment State the data it was deployed and the logs of the deployment the user can use these logs for various reasons for example if there is an error then users can use the logs to debug their application now before we get into it let's review the flow of a deployment request and trellis so the dashboard sends a request to an API Gateway that triggers a Lambda function which in turn starts the builds up the Lambda function responds to the dashboard as soon as the build server starts building and then shuts itself down meaning the Lambda function does not stay up for the duration of the deployment and hence it cannot send the deployment information to the dashboard now it gets even more complicated for automatic belts in this case GitHub sends a request to the API Gateway which triggers the Lambda function that then triggers an ecs4 gate container and hence there is no opening communication between the build server and the dashboard at all foreign we broke it down into two problems Ferris is to retrieve the deployment state from the build server and second is to send the retrieved information to the dashboard now before addressing how we retrieve the data from the build server let's review how that release build server works trellis issues several commands for various tasks some of these commands are cloning their repo downloading the necessary application packages executing optional unit tests executing optional pre-deployment Scripts and deploying to AWS now the deployment command is provided by SSD which is an IAC tool like Martin mentioned this command compiles the code down to Resource templates which are sent to AWS cloud formation cloud formation uses these templates to make API calls and provision the resources they find in them provisioning the resources may take over 20 minutes meaning a single deployment command may take over 20 minutes to complete now while the build server is executing the deployment command it makes frequent calls to cloud formation to get the deployment state of the various resources and it locks this information the build server also creates logs related to the other build process commands by capturing all these logs we can gain a comprehensive picture of the deployment status now we consider two different options to retrieving the deployment state from the build server the first option was to configure the build server to trigger a Lambda function after each command this Lambda function then stores the data in the database this meant that we would send deployment State updates to the database in patches only dispatching updates after executing each command as I mentioned earlier the deployment command may take over 20 minutes meaning that the user could be waiting for 20 minutes before receiving an update to their deployment we notice that this is not an ideal implementation and we did recognize that real-time updates would provide benefits to the user but we could have engineered a method to update the state from the build server directly but to avoid Reinventing the wheel we used AWS cloudwatch cloudwatch is an AWS tool that enables the connection of a specific AWS resource to allow group the log group is composed of multiple log streams each log stream is connected to a particular instance of the resource connected to the log group Whenever there is an update with analog group cloudwatch publishes analog event now other AWS resources can subscribe to the event and react to it occurred accordingly to use cloud words with enter us we connected the build server forget task with a log group to collect logs from the build server containers then we configured a Lambda function to subscribe to events from this Law Group when an event is triggered on the Law Group this Lambda is invoked and it writes the logs to the database enabling trellis to send them to the dashboard as they occur now that we have a way of retrieving the data from the build server we need a way to send the data to the dashboard to achieve this we consider two options who link and websockets pulling is easier to implement as it only requires configuring the dashboard to send HTTP requests at a regular enter however pooling can generate unnecessary requests and the polling interval can introduce a delay between the dashboard and the build server on the other hand warp sockets allow for bi-directional communication between a client and a server it does this by maintaining a persistent connection between the client and the server this way the server can send the data to the client as soon as it's available without the client making articles to implement websockets and trellis we use the AWS websocket API the websocket API relies on an API Gateway to persist open connections between the clients and any back-end AWS service let's review the overall process of the deployment logs so the build server sends logs to a cloud watch log Group which in turn triggers a Lambda function the Lambda function then uses the websocket connection to send the updates to the dashboard with this process users can view real-time logs of the deployment process from the dashboard as soon as they occur on the Builder now I would like to hand it back to Martin to speak about future improvements we would like to implement for Traders thanks Muhammad we've identified a few features we'd like to add to Travis we would like to introduce user roles and permissions per application this is primarily to limit the production environment control to certain developers while still allowing other team members to view the status of the deployment pipeline we also want to optimize deployment times we could for example avoid re-downloading the application dependencies for every deployment by caching the dependencies in a shared file system like AWS elastic file system or in an S3 bucket we could also optimize deployment Times by storing build artifacts so rollbacks don't require rebuilding you know older application version we think we could improve the developer experience by creating a CLI for developers to manage and monitor deployments from a local console and we'd like to allow users to connect trials to existing CI CD pipelines in cases where their serverless application is just one part of a larger application that concludes our presentation of trellis thank you all for joining we'd like to open the floor to any questions at this time please go ahead and use the question and answer feature and submit that I would be happy to answer uh it looks like I have uh one question that's coming from Travis and I want to go ahead and answer it says do you have any plans in the future to have a nist CBE vulnerability scanning to prevent deploying an application that has a known vulnerability that's a really interesting idea uh and I'll say that that had not occurred I think to any of us um so for those that don't know the nist CBE is a list of known vulnerabilities and so anytime they publish like oh this application had a bug or there was some kind of problem in it they're assigned a number and kept a record and so as a result you can kind of scan that list and verify that your deployments for instance don't have um you know aren't using applications that have known Bots and so it's a very popular thing to do in a lot of kind of security planning uh Travis has another question uh I noted that you're writing the logs to dynamodb why not put as a plain text to an S3 bucket assuming sensitive data has already been removed um so this is a question about where we're storing those logs um do you have any uh thoughts on this question yeah thank you Martha we did the thanks for bringing this up Travis we did consider using an S3 bucket but most of our data or all of our data was stored in dynamodb so if we were to start an nsa3 pocket we need the reference ID to be stored in the dynamodb and then use the reference ID to give use this reference ID to basically gather the data from the S3 pocket so it would introduce a delay it would be probably a short delay but it would regardless introduce a delay if we were to store the slogs in S3 pocket and with Dynamo DP like the logs are not the logs are taxed as you said so it does not take a lot of storage in the dynamodp itself hopefully that answered the question thanks uh we have another question here can we talk about how we integrated with GitHub uh repos uh did we have to write a GitHub app uh Cody I think this might be a question that you could tackle yeah so we considered a few options like having the users um create ease in their own GitHub repos uh in the end we did find it easier to just create a GitHub app uh and we include the instructions to install that GitHub app in the repo for those that aren't familiar with GitHub apps when you sort of think about it deploying the pipeline it needs to have a pretty large access to a repository be able to read the various branches on it and download code and because it needs to happen automatically you know we can't have the user logging in every time a build has to happen and so creating a GitHub app is a way to kind of establish a permanent kind of permission connection to the application that we need for trust to do its job uh one person here is asked what was the most difficult part of creating trellis uh there are probably a few different answers from each of us so I'll start uh I think going back to our decision to make trellis a serverless application you know we really started the process thinking from the user's standpoint you know what do they need and what do they want uh and how can we best design an application that that fits their use case and so you know we committed to making making trellis a serverless application but that came along with um difficulties a number of things that would have been significantly easier such as handling the logs if we just had kind of a containerized application that had all the components in one uh and so we sort of took on that extra complexity and it definitely made things a little more difficult um just getting everything to communicate and talk with each other but but I do think it was worth it was there any other answers to uh this question about the most difficult part of creating trellis okay the same like for me oh sorry Muhammad you can go ahead no go ahead all right um thing kind of thing Martin um using AWS it was essentially my first time doing anything am I big in the cloud um it was a major challenge for me to wrap my head around all the terms um so yeah along the same lines as Martin's answer for sure yeah I was just going to basically add to that like I think that decision designing as a serverless application it would help the users of trellis a lot but it did provide challenges for us and basically working with AWS choosing what choosing the services different services that we need to use for different parts of the application now that in itself was challenging like writing the code for it would probably easier than knowing which resources to use and which resources would best fit our use case um I'll just add extra to that was prior to working with AWS as a bit clueless and just getting your hands on is the best way to learn just anything in general even if things fail and another great thing was relying on teammates right one doesn't understand one thing you're the we hit that freedom to ask someone else right and each one of us contributed a little bit and so again like AWS best way to learn it is to use it I can definitely add uh one more thing that that I've reflected upon after building Trellis was uh you know I I spent a lot of years uh doing it for instance in which I was kind of just by myself you know every projects I wanted to implement it was on me to do it and so kind of working as a team to make something with a very different uh a very different experience uh had its own kind of set of challenges of maintaining kind of synchronization between us making sure we're all on the same page and agreeing agreeing on the various directions we wanted to go but it was a really really valuable experience uh I don't see uh any other questions yet but you've got just a little bit more if you want to throw a question in there in the meantime if you want to read more about trellis uh or find our links you can go ahead and visit trellis Dash deployment.github.io and you'll see a kind of Fuller version of the case study kind of getting into the details of trellis as well as how to get in touch with any of the members of the team uh it looks like we don't have any more questions so I think we're going to go ahead and wrap it up for today on behalf of Cody Marcos Muhammad and myself uh thanks for coming and everybody have a great day thank you everyone for attending 