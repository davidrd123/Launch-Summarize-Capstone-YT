Today, I am joined by Cody Williams, Martin Graham, Marcos Avila, and Mammad Al Shanti, and we are excited to share our experience building Trellis. To begin our presentation, I will provide a brief introduction to Trellis, setting the foundation for our objectives. I will then discuss the technical background and the problems that inspired this project. Marcos will expand on how Trellis solves these problems, followed by Martin, who will walk us through its architecture. Mammad will then dive into our engineering decisions, and finally, Martin will conclude with a description of our future work.

So, what exactly is Trellis? Trellis is an open-source, low-configuration deployment pipeline designed for teams developing serverless applications. Its main goal is to minimize the steps required for teams to deploy their code to the cloud. Additionally, Trellis provides a centralized dashboard to monitor and manage multiple deployment environments. Today, we will discuss the challenges, trade-offs, and choices we faced while building this functionality.

To understand the utility of Trellis and the problems it solves for its users, we first need to delve into its technical background and the deployment-related issues that inspired its development. At its core, deployment is the process of moving application code from a developer's machine to the infrastructure that serves it to end users.

Traditionally, software developers and operations teams worked separately, with developers focusing on writing code and operations teams handling the deployment. This division of labor often led to delays as operations teams needed time to discover and address problems found by developers. To overcome these challenges, many organizations have adopted the DevOps philosophy, merging development and operations roles into a single team. This collaboration minimizes friction, resulting in faster application iterations, shorter development cycles, and quicker delivery of features to users.

In DevOps, continuous integration plays a crucial role. It involves testing each developer's code changes to ensure they integrate smoothly into the application's codebase. To achieve continuous integration, all code changes are committed to a shared repository in a version control system such as GitHub. This version control system serves as a single source of truth to build a comprehensive artifact that can be deployed as a full application.

Continuous delivery, another DevOps practice, automatically deploys all code changes to allow teams to operate, monitor, and test them for validation across multiple dimensions. This practice ensures that the artifact is ready for release. Continuous delivery relies on deployment environments, which are discrete sets of resources to which an application is deployed. Development, staging, and production environments are common deployment environments used in the continuous delivery process.

Development environments are used by developers to try out their code and ensure it runs correctly. Staging environments, similar to development environments, replicate the environment that will serve end users. They provide an opportunity for comprehensive testing and real-world interaction with the application. Lastly, production environments are where the application is thoroughly vetted and contains the infrastructure to serve real users at scale.

Simply having multiple deployment environments is not enough to ensure that code is ready for release. A deployment pipeline, which is the process of progressing code through these environments all the way to production, is essential. Manual deployment and promotion of code through different environments is one approach, but it is time-consuming and error-prone.

A more effective approach is to embrace continuous delivery and automate the deployment steps. This allows teams to focus on development tasks while still benefiting from a DevOps culture. Automated deployment pipelines are often triggered by changes to the source code in a version control system. A build server handles most of the automated steps, such as obtaining code, running tests, and deploying the code. A centralized dashboard provides a way for teams to interact with and collaborate on the deployment pipeline.

Now that we have a general understanding of automated deployment pipelines, Marcos will explain how they can be implemented in various application infrastructures.

There are three distinct application infrastructures to consider: on-premise, cloud-hosted, and serverless. On-premise infrastructure refers to servers managed and controlled by the developing organization. Maintaining on-prem infrastructure requires significant capital investment to purchase and maintain the necessary hardware. In this infrastructure, a deployment pipeline deploys code to on-prem machines for staging and production environments, while development environments are typically hosted locally.

Cloud hosting, on the other hand, allows teams to deploy their application on third-party-managed machines, such as Amazon Web Services (AWS). This eliminates the need to purchase and maintain hardware, making it more suitable for small teams without the resources or skills to handle physical infrastructure. The deployment pipeline for a cloud-hosted application works similarly to the on-premise method, with staging and production environments using cloud servers and development environments still being hosted locally.

Serverless is a cloud-based model where applications run on abstracted services provided by cloud providers, eliminating the need for virtualized servers. Serverless applications' deployment pipelines require logical separation of deployment environments and their resources, even though they run on the same cloud infrastructure. Development environments in serverless applications may differ from staging and production environments, but using staging or production-like environments for development allows developers to get a better feel for performance. The pay-as-you-go nature of serverless applications means that development environments carry less financial burden compared to production environments, which are open to public traffic.

When it comes to implementing automated deployment pipelines, there are various options available. In-house solutions can be developed using open-source tools, which offer affordability and flexibility but are more challenging to implement. Small teams may not have the resources to design and manage an entire deployment application. Another option is to use third-party or Software-as-a-Service (SaaS) deployment pipelines, such as Travis CI or Circle CI. These pre-configured tools provide feature-rich platforms for developing, testing, and delivering projects in different deployment scenarios. However, customization can be costly, and complex workflows and feature sets may present navigational challenges.

There are also products that target specific niches, such as Seed, a SaaS deployment pipeline for serverless apps. Compared to Jenkins and Travis CI, Seed offers simpler configuration, supports fewer development methods, and exclusively deploys applications to AWS. It abstracts much of the complexity to simplify the deployment management and monitoring process. Although Seed may not fit every software development workflow, it caters to users seeking an opinionated, low-config, and inexpensive solution with full control over the pipeline's infrastructure and data.

With this need in mind, we built Trellis. Trellis is a low-config continuous delivery deployment pipeline designed specifically for teams developing serverless applications on AWS. Its main goal is to simplify the setup of an automated deployment pipeline, allowing teams to focus on feature development instead of operations. Trellis users can perform various actions within the application, such as connecting to GitHub and selecting the repository for each application. By default, the development environment is linked to the main branch of the selected repository, making it easy to deploy changes to the cloud. Users can also set up other git branches to configure each deployment environment.

Whenever code is committed to an application's GitHub repository, Trellis automatically starts deploying the new code to any configured environment. Users can manually deploy the most recent commit for any branch if they need to deploy a version from before Trellis was configured. Unit testing plays a crucial role in ensuring the code's correctness. Trellis allows users to turn on or off unit testing for each deployment environment, displaying the output of the test command during deployment. They can also configure a different command for running tests if necessary.

Trellis users have the ability to promote code to the next environment if they are confident in its performance. However, automatic deployments to production are prevented to avoid exposing end users to potential bugs or errors. Manual promotion is the only way to deploy an application to end users. Additionally, if problematic code causes issues, Trellis provides a rollback button in all deployment environments. Users can easily redeploy or roll back to a previous version of the application.

In some cases, code may require further development and should not advance along the pipeline. Trellis offers a tear-down option that uninstalls all cloud resources connected with a deployed environment. This allows users to remove a deployment if necessary. Trellis ensures that code modifications are validated and that the deployment pipeline is efficient and user-friendly.

As we have discussed, deployment pipelines play a vital role in the development and release of applications. Trellis simplifies the process for teams developing serverless applications by providing a low-config deployment pipeline with data control. By automating many of the deployment steps, Trellis allows development teams to focus on their core tasks while still benefiting from a DevOps culture. It offers a solution for teams seeking an affordable, opinionated, and controllable deployment pipeline. Our work on Trellis is ongoing, and we are continuously improving and adding new features to meet the evolving needs of our users.

In conclusion, Trellis is an open-source, self-hosted application that streamlines the deployment process for serverless applications on AWS. It allows teams to focus on feature development by automating many of the deployment steps and providing a centralized dashboard for monitoring and managing multiple environments. Trellis aims to simplify the setup of a deployment pipeline while offering data control and flexibility. With ongoing development, Trellis continues to provide a reliable solution for teams embarking on serverless application development journeys.

Thank you for joining us today as we presented Trellis, and we hope you have gained a valuable understanding of its capabilities, challenges, and benefits. Trellis, an open-source self-hosted application, is a low-config continuous delivery deployment pipeline designed for teams developing serverless applications on AWS. It simplifies the setup of automated deployment pipelines, enabling teams to focus on feature development rather than operations. Trellis offers users full control over the pipeline's infrastructure and data, making it an opinionated, low-config, and cost-effective solution. Now that we understand where Trellis fits in, let's dive into its functionality.

When users log into Trellis, they can create new applications and link them to their desired GitHub repositories containing the application's source code. By default, the development environment is linked to the main branch of the selected repository, allowing for easy deployment of changes to a cloud environment. Users can also set up other git branches to customize the source code for each deployment environment. Whenever code is committed to the repository, Trellis automatically deploys the new code to any configured deployment environments. Even if Trellis was configured after the commit, users can manually deploy the most recent commit for any branch.

Unit tests play a crucial role in ensuring code quality. Trellis allows users to turn on or off unit testing for each deployment environment. With the press of a button, users can see the output of the "npm run test" command every time a deployment happens. Additionally, users can set up a different command for running tests if needed.

To ensure the quality of production environments, Trellis prevents automatic deployments to production. Manual promotion is the only way to deploy an application to end users. This prevents potential bugs, errors, and infrastructure issues from impacting end users. Moreover, if the development team wants to roll back an environment to a trusted application version due to problematic code, Trellis provides a rollback button for all deployment environments.

The rollback button allows users to either redeploy or roll back to a previous application version. In case a deployed environment requires further development and should not advance along the pipeline, Trellis provides an option for uninstalling all cloud resources connected with that environment.

Trellis validates code modifications and puts them in environments for testing and promotion to production. To accomplish this, Trellis relies on a build server, a dashboard, and a backend. The build server deploys the code to target environments from an isolated machine and allocates unique cloud resources for each environment. It also handles teardowns and rollbacks for all deployment environments.

The dashboard interface connects to the backend and build server, allowing customers to control deployment pipelines. Users can trigger the deployment pipeline's manual steps, customize deployment environments, manage pipeline features, and monitor deployment statuses. The backend, on the other hand, maintains user logins, environment-specific settings, GitHub repository data, deployment environment statuses, and build server output. It acts as a bridge between the frontend and build server, initiating build server updates, updating the database, and sending data to the dashboard and build server.

Moving on to the architecture of Trellis, it was designed as a serverless application hosted on AWS. This decision was made to better serve teams deploying serverless applications to AWS. By leveraging their existing serverless expertise and incorporating Trellis into their workflows, teams can seamlessly adopt it. The intermittent nature of builds and the potential for concurrency made auto scaling and pay-as-you-go pricing important considerations.

To define the resources that make up a serverless application, Trellis utilizes the Serverless Stack Framework (SST), which is built on Amazon's Cloud Development Kit (CDK). This infrastructure-as-code approach allows developers to specify resources using a familiar programming language, ensuring consistency in the deployment process. Trellis uses SST's build command to deploy user applications to AWS as defined in the application source code.

For the build server, two AWS services were considered: AWS Lambda and AWS Fargate tasks. Both services provide pay-as-you-go pricing and auto scaling. However, the startup time of Lambda functions was longer compared to Fargate tasks, which was one reason why Fargate tasks were favored. Additionally, Fargate tasks do not have a timeout, which was a constraint with Lambda functions.

To securely store the credentials required for deploying AWS resources, Trellis uses AWS Secrets Manager. Build server containers access these stored credentials using the AWS Software Development Kit. Logs of the build process are captured by AWS CloudWatch, providing crucial information for debugging and monitoring.

The backend of Trellis mainly consists of multiple AWS Lambda functions. Each function corresponds to a specific backend responsibility, such as authentication, application management, environment promotion, and GitHub integration. AWS API Gateway serves as the front door for the backend, allowing developers to create and secure APIs. The backend also interacts with AWS CloudWatch to retrieve build server logs and stores deployment pipeline data in AWS DynamoDB, a serverless NoSQL database.

For the Trellis dashboard, it was implemented as a single-page application using React. The dashboard code is stored in an AWS S3 bucket, and the application is served through AWS CloudFront, Amazon's content delivery network. This approach eliminates the need for a long-running web server and ensures efficient and scalable serving of the dashboard.

Throughout the development of Trellis, multiple engineering decisions and trade-offs were made. One such decision was the choice between Lambda functions and Fargate tasks for the build server. The startup time, scalability, and timeout constraints made Fargate tasks the preferred option. Another decision involved how to retrieve real-time deployment data from the build server and send it to the dashboard. The use of AWS CloudFormation, CloudWatch, and Lambda functions facilitated the collection and dissemination of this information.

In summary, Trellis is a low-config continuous delivery deployment pipeline for teams developing serverless applications on AWS. Through a user-friendly web-based dashboard, it simplifies the setup and management of automated deployment pipelines, allowing users to focus on feature development. Trellis's serverless architecture, powered by AWS services, ensures scalability, reliability, and cost-effectiveness. By leveraging infrastructure-as-code and integrating with GitHub, Trellis provides users with full control over their deployment pipelines while maintaining simplicity and ease of use. The deployment status can be one of the following: deployed, deploying, removing, removed, or encountering an error. The data sent to the user includes the overall deployment state, deployment logs, and other relevant information. Users can use these logs to debug their applications in case of errors.

Before delving into the details, let's review the flow of a deployment request in Trellis. The process starts with the dashboard sending a request to an API Gateway, which triggers a Lambda function. The Lambda function initiates the build process and responds to the dashboard once the build server starts building. The Lambda function is then shut down, as it doesn't need to stay active throughout the entire deployment. This means that the Lambda function cannot directly send deployment information to the dashboard.

For automatic builds, the process becomes more complicated. GitHub sends a request to the API Gateway, which triggers the Lambda function. The Lambda function then triggers an ECS Fargate container, eliminating direct communication between the build server and the dashboard. To address this, we divided the problem into two parts: retrieving the deployment state from the build server and sending the retrieved information to the dashboard.

Let's first review how the build server works. Trellis issues various commands to the build server, including cloning the repository, downloading application packages, executing unit tests and pre-deployment scripts, and deploying to AWS. The deployment command is performed by SSD, an IAC tool that compiles the code into resource templates. These templates are then sent to AWS CloudFormation, which uses them to provision the required resources. The provisioning process can take over 20 minutes, making the deployment command itself time-consuming.

While the build server executes the deployment command, it makes frequent calls to AWS CloudFormation to obtain the deployment state of the resources and logs this information. The build server also creates logs related to other build process commands. By capturing these logs, we gather a comprehensive picture of the deployment status.

We considered two options for retrieving the deployment state from the build server. The first option was to configure the build server to trigger a Lambda function after each command, which would store the data in a database. However, this approach would result in the deployment state updates being sent to the database in patches, causing delays in updating the dashboard. Real-time updates were desirable, so we decided to leverage AWS CloudWatch instead.

AWS CloudWatch is a tool that allows the connection of a specific AWS resource to a log group. The log group consists of multiple log streams, each associated with a particular instance of the resource. When an update occurs in the log group, CloudWatch publishes a log event. Other AWS resources can subscribe to these events and react accordingly. In Trellis, we connected the build server task to a log group to collect logs from the build server containers. We then configured a Lambda function to subscribe to events from this log group. When an event is triggered, the Lambda function writes the logs to the database, enabling Trellis to send them to the dashboard in real-time.

Now that we have a way to retrieve data from the build server, we need to find a way to send this data to the dashboard. After considering different options, we narrowed it down to two possibilities: using HTTP polling or WebSocket. HTTP polling involves configuring the dashboard to send regular HTTP requests, but it can result in unnecessary requests and introduce delays between the dashboard and the build server. On the other hand, WebSockets allow for bi-directional communication between a client and a server, maintaining a persistent connection. This enables the server to send data to the client as soon as it's available. In Trellis, we leveraged the AWS WebSocket API, which relies on an API Gateway to establish and maintain open connections between clients and AWS services.

Let's summarize the deployment log process. The build server sends logs to a CloudWatch log group, which triggers a Lambda function. This Lambda function uses the WebSocket connection to send updates to the dashboard. As a result, users can view real-time logs of the deployment process from the dashboard as they occur on the build server.

Moving on to future improvements for Trellis, we have identified several features we would like to implement. First, we want to introduce user roles and permissions per application. This would allow for better control over the production environment, limiting access to certain developers while still enabling other team members to view the deployment pipeline status.

We also aim to optimize deployment times. One approach is to avoid re-downloading application dependencies for every deployment by caching them in a shared file system such as AWS Elastic File System or an S3 bucket. Additionally, storing build artifacts would allow for rollbacks without the need to rebuild previous versions of the application.

To enhance the developer experience, we plan to create a command-line interface (CLI) for developers to manage and monitor deployments from a local console. This would provide a more convenient and flexible way to interact with Trellis.

Lastly, we would like to enable users to connect Trellis to existing CI/CD pipelines when their serverless application is part of a larger application. This integration would allow for seamless coordination between different parts of the development process.

In conclusion, we have presented Trellis, a serverless deployment tool. We have discussed its architecture, including the retrieval of deployment state from the build server and the real-time transmission of updates to the dashboard using AWS CloudWatch and WebSockets. We have also highlighted some future improvements we plan to implement. Thank you for attending our presentation, and now we would be happy to answer any questions you may have. Before diving into the content of the Capstone project, let me highlight a few key points. Firstly, prior to working with AWS, we were relatively inexperienced and lacked knowledge in this area. However, we strongly believe that the best way to learn is by actually getting hands-on experience, even if things don't always go smoothly. Additionally, one of the greatest benefits we experienced during this project was the ability to rely on our teammates. When one of us didn't grasp a certain concept or faced difficulties, we had the freedom to ask for help from the others. This collaborative aspect allowed each of us to contribute in different ways, making the learning process more efficient.

Moving on to the Capstone project, one realization that came to me after building Trellis is the stark contrast between working as a team and working individually. In the past, I spent a significant number of years working on projects alone, where I was solely responsible for implementing everything. However, working as a team brought a completely different experience, along with its own set of challenges. It required us to maintain synchronization, ensuring that we were all on the same page and agreed on the direction we wanted to take. Despite the challenges, this collaborative effort proved to be an immensely valuable learning experience.

If anyone has any further questions, please feel free to ask. In the meantime, if you're interested in reading more about Trellis or accessing relevant links, you can visit trellis-deployment.github.io. There, you'll find a more comprehensive version of the case study, delving into the details of Trellis, as well as contact information for each team member.

As it seems there aren't any additional questions, we will conclude the session for today. On behalf of Cody, Marcos, Mammad, and myself, I'd like to express our gratitude for your attendance. We hope you have a wonderful day. Thank you, everyone.