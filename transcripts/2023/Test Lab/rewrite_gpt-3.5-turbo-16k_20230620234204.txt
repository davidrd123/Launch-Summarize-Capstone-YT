Our team has been working hard on developing Test Lab, a feature flag and ad testing platform. Test Lab provides feature management infrastructure for the creation of feature toggles, rollouts, and experiments. We are excited to share our project with you.

Let's start with an introduction to A/B testing. A/B testing is a process that allows for data-driven decisions based on real empirical observations. It involves creating variations of a design or feature, testing them with different users, and analyzing the data to make informed decisions.

A simple example of A/B testing is testing the effectiveness of a button on a website. You have a website with a button, but you're not getting the desired click count. So, you come up with an idea to add a visual element, like a picture of Ryan Reynolds pointing at the button. You introduce this variation to some users and observe their behavior. If more users click the button with the variation, you can roll out this change for all users.

Google famously used A/B testing to choose the best shade of blue for their advertisements. They tested 41 different shades of blue and observed user behavior to determine the most effective color. The result was an additional $200 million in ad revenue.

A/B testing can go beyond design changes. It can be used to test new features or functionality, changes to the back end or hardware, and the integration of third-party services. It allows for experimentation and optimization to increase desired user behaviors and, consequently, revenue.

There are three main approaches to implementing A/B testing: client-side, server-side, and content delivery networks (CDNs). 

Client-side implementation is the most common approach. It involves manipulating the Document Object Model (DOM) using JavaScript to serve different versions of a page to users. This approach is easy to use and allows for quick testing of changes. However, it can result in a brief flash of the original page before the test version is rendered.

Server-side implementation modifies the code on the server directly, allowing for greater access to user data and segmentation. This approach requires more development expertise but offers more flexibility and control over experiments.

CDNs can be used to route users to different versions of a site, but they lack access to user data, limiting the ability to curate user experiences. 

Implementing A/B testing comes with challenges. Consistent user assignment is crucial for collecting usable user behavior data. The degree of developer involvement and the ease of use of testing tools are important factors in the success of A/B testing. Data collection and analysis methods can vary, from using third-party services to in-house solutions. Lastly, statistical validation and proper experiment design are critical to obtaining meaningful results.

Now, let's dive into the existing solutions for A/B testing. There are numerous platforms available, ranging from specialized editors for aesthetic changes to comprehensive suites for companies with skilled development teams.

Some basic platforms offer APIs for creating and retrieving experiments through the command line. At the other end of the spectrum are platforms like Launch Darkly, which provide APIs, SDKs, administrative user interfaces, and support for data analytics services. These platforms are often costly and externally hosted, making them more suitable for large companies with existing data analysis capabilities.

Open-source self-hosted platforms, like Growth Book and Unleash, offer similar features but with the option for self-deployment. However, they may not provide robust data analytics capabilities, making them less suitable for clients without existing data collection.

Test Lab stands out by offering the ability to collect and store event data. Our platform is designed for clients who have not yet started data collection or want to host their own event data. We provide the flexibility of an open-source locally hosted platform without the need for massive analytics infrastructure. Integration of API calls into event listeners allows clients to track events and have full control over their event tracking.

Test Lab consists of four key components: an administrative user interface built with React, a Node.js Express back-end server, a PostgreSQL database for persisting data, and native SDKs for easy integration into server and client-side rendered applications.

One key implementation challenge we faced was ensuring that users are only enrolled in one experiment at a time. To address this, we developed a user block strategy that assigns users to specific experiments, making the results more meaningful and allowing for accurate analysis.

In conclusion, A/B testing is a powerful tool for data-driven decision making. Test Lab offers a unique solution, providing the flexibility of a locally hosted platform with the ability to collect event data. Our team has worked hard to create a user-friendly platform that addresses the challenges of A/B testing. In the following sections, my team members will provide a deeper dive into the specifics of our architecture, back-end functionality, and design decisions. We can enroll users in a given feature and use feature rollouts in our platform. Feature rollouts allow clients to enroll a customized percentage of users into a specific feature. However, neither feature toggles nor rollouts include event data collection. Now that we have an overview of B test platforms, let's dive into our implementation.

Our test lab architecture consists of four key components. First, we have an administrative user interface built with React. This interface enables clients to create, edit, delete, and view current, scheduled, and past features. It also provides a way to view experiment data. Secondly, we have a node Express backend server that exposes API endpoints. Clients can use these endpoints to create, retrieve, edit, and delete feature configurations. They can also collect and retrieve event data for analytics. Thirdly, we have a PostgreSQL database that stores data persistently. This includes feature configurations, user block allocations, and event data for analytics. Lastly, we provide native SDKs for Node.js, React, Ruby, and Python. These SDKs allow for easy integration of our platform into both server-side and client-side rendered applications.

Now, let's address a key implementation challenge: ensuring that users of our client site are only enrolled in one experiment at a time. If users were enrolled in multiple experiments concurrently, it would be impossible to determine whether any difference in user behavior was due to the evaluated variant or the combination of variants. To address this, we developed a user block strategy.

To explain briefly, we segment the available user pool into 20 chunks, each representing 5% of the user base. Each of these chunks, or user blocks, is permanently assigned to one experiment for the duration of that experiment. This ensures that users in each user block are only exposed to one experiment. Once an experiment concludes, the user blocks allocated to that experiment are released and can be reallocated to upcoming experiments.

Now let's discuss our administrative user interface. The admin UI is a CRUD (Create, Read, Update, Delete) application designed to make it easy for clients to manage their features. In addition to basic CRUD functionality, the admin UI includes features to ensure client success with experiment creation.

First, the admin UI prevents clients from overbooking experiment user blocks. It calculates the maximum percentage of users available for experiment enrollment during a selected period and prevents clients from scheduling experiments that require more user blocks than are available.

The admin UI also allows clients to pause features. This can be useful if a bug needs fixing or if there are external factors that might disrupt the experiment data. Pausing a feature temporarily stops exposing it to users.

Clients can also delete features entirely, although this action permanently removes all related data from the database. Care should be taken when deleting features, especially if there is valuable data associated with the feature, like in the case of the Ryan Reynolds experiment.

For upcoming features that are not currently ongoing, the admin UI offers flexibility in editing options. Clients can modify names, descriptions, start dates, end dates, and user enrollment percentages. They can also adjust the value, weight, and quantity of variants for experiments. However, to maintain data integrity for ongoing features, the admin UI limits clients' ability to make destructive changes. Once an experiment has begun, the start date cannot be altered, variants cannot be added or removed, and variant weights cannot be changed. However, clients can still modify the name, description, end date, and overall percentage of users exposed to the experiment. They can also adjust the variant values if needed.

The admin UI is designed with client success in mind. We aim to ensure smooth running of features and experiments, providing valid and useful data. For current and past experiments, the admin UI provides clients with the ability to view in-progress and final results of the collected data. Visualizing the data helps differentiate variant performance and guides clients towards making successful decisions when experiments conclude.

Now, let's discuss the test lab backend server. The backend server plays several key roles in the test lab application. First, it exposes API endpoints for creating, editing, deleting, and retrieving feature data. It also enables the creation and retrieval of user data for experiment analysis. Additionally, the backend server stores and retrieves event data from experiments.

The backend server ensures data integrity by validating the data sent to the test lab database. It also handles the assignment of user blocks to experiments, ensuring that no more than 100% of users are enrolled in an experiment at any given time. This prevents users from moving between experiments.

The test lab database is where persistent data is stored. It consists of several categories including feature configuration data for toggles, rollouts, and experiments. This data includes the feature's name, description, and the proportion of users enrolled. Variant configuration data for experiments is also stored, including variant names, values, and the proportion of users experiencing each variant.

User data is stored for users enrolled in experiments. This includes a unique user identifier, their IP address, and the variant they experience. Event data posted by experiments, including the time of the event, the user triggering the event, and the variant experienced, is also captured and stored. Lastly, the database keeps track of user block data, which represents the allocation of user blocks to experiments at a given time.

To deliver specific features to users of our client's application, we provide native SDKs for Node.js, React, Ruby, and Python. These SDKs help developers translate feature configurations into application code.

The SDKs enable three main functions. First, they retrieve up-to-date feature and user block information from the backend server. This is done through polling, where the server responds with new data only if there have been modifications since the last request.

Next, the SDKs use assignment logic to evaluate features for each user request. For toggles, it checks if the feature is active and if the current date is within the feature's start and end dates. For rollouts, it adds an extra step of checking if the user falls within the percentage of users assigned to the rollout. Lastly, for experiments, the SDKs use a hashing function to determine the user block assigned to the user. Based on the hash value, the correct variant is returned.

To illustrate this in code, we have a snippet from a simple Node.js Express app that uses the test lab SDK client. This code retrieves the value of a feature called "my experiment" for a user with a randomly generated ID. The developer can then include logic to render specific content based on the value of the experiment.

In summary, our test lab platform allows for the enrollment of users in features and offers feature rollouts. The administrative user interface provides a user-friendly way for clients to manage their features and experiments. The backend server ensures data integrity and handles user block assignment. The PostgreSQL database stores persistent data, and the native SDKs allow for easy integration into client applications. Feature evaluation is done through a combination of toggles, rollouts, and experiments. The SDKs retrieve feature configurations, evaluate features based on assignment logic, and deliver appropriate variants to users. Today, we will discuss the functionality of our code and how it retrieves specific variants for rendering. Let's start with the basics. We first check if an experiment is active and if it falls within the predefined start and end dates. However, the next step is different. We use a hashing function to hash the unique user ID, resulting in a value between 0 and 1. This value determines the user block assignment. As Chelsea explained earlier, user blocks define which blocks of users are assigned to a particular experiment.

Based on the hashed user ID value, we identify the user block and check if the experiment is active for this user. If it is not, we return false. If it is, we return the appropriate variant based on the hash value. For example, in this case, we would return variant one 30% of the time, variant two 30% of the time, and variant three 40% of the time.

Now, let's delve deeper into how this assignment logic works. User blocks are crucial for experiment evaluation. Once we determine that a user is enrolled in a specific experiment through their user block assignment, we need to determine which variant to serve. To do this, we rely on variant weights. In the example, we have three variants with weights of 30%, 30%, and 40%.

For instance, let's suppose the hashed value of our user ID is 0.54. This value places us in the 0.5 to 0.55 user block, which corresponds to experiment two in this example. But how do we decide which variants to return? Simple. We divide the user block into segments based on the variant weights. Values that map to the first 30% of the block receive variant one. Values that map to the next 30% receive variant two. Values that map to the last 40% receive variant three. In our example, a value of 0.54 falls in the last segment, resulting in variant three being served.

Moving on to the code implementation, let's see how the SDK renders the appropriate variant. In this snippet from a node Express app, we use the Test Lab SDK client to retrieve the value of "my experiment" for a user with a randomly generated user ID. The developer would then include logic to determine what to render based on the experiment value. In this case, if the value returned is "Ryan," we render Ryan Reynolds on our ice cream shop page. Otherwise, we display the default page with no image.

Now, let's discuss the implications of variant weights. If the weights are set at 50-50, we can expect approximately 50% of users enrolled in the experiment to see Ryan Reynolds, while the other 50% will miss out.

We are now going to shift our focus to the engineering decisions we made while developing Test Lab. Designing Test Lab involved making several interesting decisions and trade-offs. Let's explore some of these decisions, along with their rationale.

First, let's discuss the architecture of Test Lab. We opted for a self-hosted platform that utilizes a PostgreSQL database to store data. This approach allows for customization, complete control over data, and lower costs. While managed A/B testing platforms are easier to set up, we mitigated the complexity of a self-hosted solution by dockerizing Test Lab and providing clear documentation on how to deploy it using a Docker image.

By choosing a self-hosted and open-source application, we enable code customization to fit specific needs. Managed solutions do not provide access to the source code, limiting customization possibilities. Additionally, self-hosting gives us ownership of data, which can be crucial for compliance requirements or on-premise data storage. Moreover, self-hosting provides cost benefits, as there is no charge to download the software, and it allows for scalability as the company grows.

Next, let's discuss the database decision. We needed a database to store feature, user, and event data. We considered options like time-series databases, NoSQL databases, and SQL databases. While time-series databases could offer more granularity for event data, we deemed them overly complex for our needs since our analysis focuses on aggregated data. NoSQL databases are ideal for unstructured or changing data structures, but we chose PostgreSQL for its structured data requirements. PostgreSQL provides specific data types that work seamlessly with the SDK and admin UI. Additionally, it offers data consistency, reliability, and an extra layer of validation.

Moving on to API design, we aimed to make our API accessible without always relying on the admin UI. While this exposes more of the API interface, it also increases the risk of developers misusing the API. To mitigate this risk, we implemented strong validation measures in the front-end, preventing most user errors. We still believe the flexibility of the API outweighs the potential downsides of misuse. To support developers, we provide extensive API documentation.

Let's now shift our attention to the design choices we made while building Test Lab. We implemented context for memory representation, allowing users to experience the same variant across multiple requests and sessions. We opted for a flexible approach, where the default context consists of a randomly assigned UUID as the user ID. However, users can also change the context by using the update context method and providing a new value for the user ID. This flexibility allows users to choose the context that best fits their needs, whether it be a random value for features independent of user identity, a session identifier for session-dependent features, or an email/login for identity-dependent features. By considering these options, Test Lab offers versatility in how it handles user experiences.

Another notable decision we made was to limit users to only one experiment at a time. Allowing users to participate in multiple experiments simultaneously would require a more complex multivariate analysis and a larger sample size, increasing resources and time needed for the experiment. Additionally, it would increase the risk of false positives due to overfitting. To address these concerns, we employed user blocks, which allocate users to specific experiment groups. User ID hashing ensures that users remain in the same user block until their user ID changes. Once a user block is allocated to an experiment, it remains assigned to that experiment until its completion. User blocks enable multiple experiments to run concurrently while avoiding unnecessary complexity.

To keep the in-memory representation of feature configurations up to date, we opted for polling. While server-side events and websockets offer real-time updates, we didn't require immediate updates for our purpose. Polling, which updates data at predefined intervals, was a simpler, compatible, and efficient solution. It can be easily adjusted to suit different use cases, unlike the more complex server-side event or websocket options. Polling only responds with new data when the feature configuration changes, which minimizes unnecessary data transfer.

In addition to the overall functionality, we are proud to introduce our comprehensive test suite. We developed the test suite concurrently with the API to ensure that new code did not affect previous functionality. Our tests cover 90% of the code and are written using Jest and the supertest library. They focus on testing each API route individually, including setup and teardown to preserve data integrity. We confirm that the routes respond, accept and return data in the correct format. We also test error responses, default values, and expected behavior. Ensuring that the SDK receives a 304 status message when there is no new data is also part of our testing process.

Now, let's discuss some future work to improve Test Lab. One possibility is to explore more granular user block segmentation based on user feedback. We could add routes that allow users to choose from a range of block sizes when using the SDK or API. This would provide even more flexibility for experimentation. Additionally, we plan to implement server-sent events (SSE) to provide real-time updates on the status of features. An admin dashboard displaying experiment health metrics on current experiments is another feature we envision. This would make it easier for admins to monitor experiment performance.

Furthermore, we plan to incorporate statistical analysis into Test Lab. While we initially focus on providing visual representations and API access to raw data, we recognize the potential value in offering simple measures of statistical significance for commonly used experiment types. This could enhance the analysis capabilities for users.

Finally, our roadmap includes various other features that we are excited to develop. These enhancements will further enrich Test Lab's functionality and provide our users with more options and customization possibilities.

Now, let's move on to the Q&A segment. We invite our audience to ask any questions they may have, and Abby will read them out from the comments section. We utilized an SDK in our project for its simplicity, compatibility, and efficiency. Another key attribute of Tesla is our comprehensive test suite. We are proud to have developed the API alongside the test suite, ensuring that new code does not affect previous functionality. With a 90 percent coverage, the test suite is written in Jess and utilizes the supertest library to test the API routing. Our tests are structured as unit tests for each API route, including setup and tear down operations to prevent any impact on real data. We thoroughly test each API endpoint to confirm its response, data acceptance, and correct data format. This includes testing error handling to ensure that the correct status and message are returned. We also include tests that verify the acceptance of default values for certain attributes. Lastly, we made sure that the SDK receives a 304 status message when no new data is available.

Moving forward, we have several ideas to further enhance Test Lab. Currently, we segment users into 5 percent chunks based on feedback. However, we are open to changing this to smaller chunks based on user preferences. One option we considered is adding routes that allow users to choose from a range of block sizes when utilizing the SDK or API. Another feature we are excited to develop is the addition of a Server-Sent Event (SSE) for real-time updates on feature status. This will enable us to create a dashboard where admins can easily monitor and track experiment performance. Furthermore, we plan to incorporate statistical analysis into our system. Initially, we will provide visual representations of the data and API access to the raw data, allowing users to analyze it using their preferred metrics. However, we also plan to offer simple measures of statistical significance for commonly used types of experiments. Our roadmap includes a range of other functionalities that we are eager to develop.

Now, let's move on to the Q&A segment where we will address questions from the audience. Please feel free to leave your questions in the comments section, and Abby will read them out.

Question 1: "Does user blocking take into account active versus no longer active users? Do users get shuffled for blocking purposes? Assuming user IDs are assigned incrementally at the time of registration, would older users be more likely to be no longer active? If one experiment is assigned to the bottom 30 percent based on user ID, wouldn't they be disproportionately likely to contain non-active users?"

Our user blocking mechanism is designed to allocate users randomly. Initially, we use a default random UUID value for each user, ensuring an even distribution. We do not consider the activity status of users. However, if clients choose to utilize other identification methods, such as email logins or system identifiers, that would be a custom set-up on their end. In that case, the hashing algorithm will ensure randomness within the identified user group. Hence, the distribution remains fair, regardless of activity status. The concept of "old" or "new" users does not play a role in our user blocking process.

Question 2: "How does sectioning or blocking work for newly signed-up users? As this would affect the 100 percent NBER since the number of users in groups changes."

The user blocking process is independent of the number of users in a system. We use a hashing algorithm to assign a numerical value to each user, ensuring even distribution. The algorithm does not rely on the existing number of users, making the system scalable. As new users sign up, they will go through the same user blocking process, and their assignment to groups will follow the established distribution. Hence, the number of users in a specific group may change as users sign up, but the distribution remains consistent.

Question 3: "Are there any perceived scaling issues with your application as the user count increases?"

Our application is designed to handle scalability with ease. The user blocking mechanism, based on hashing algorithms, has been selected to ensure scalability, even for larger platforms. The computational resources required for scaling are ultimately dependent on our clients' infrastructure, as our product is self-hosted. Therefore, no major scaling issues are expected, as our architecture is designed to handle increasing user counts efficiently.

Finally, we have a question from Sergeant himself: "Why is Ryan so good looking?"

Well, Sergeant, that's an intriguing question! Unfortunately, even we at Test Lab cannot answer that. However, we appreciate the positive note to end on. Thank you all for attending our presentation. We hope you found it informative and enjoyable.