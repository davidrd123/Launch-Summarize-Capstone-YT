foreign our team has been working hard these past few months on test lab a feature flag on ad testing platform test lab provides feature management infrastructure for the creation of featured toggles rollouts and experiments we are so excited to share our project with you my name is Abby and I'm going to get us started as you can see on the map we're spread across the U.S in different regions I live on the Central Coast of California near San Luis Obispo Chelsea's up in Bellingham Washington Allison's in Harrisburg Pennsylvania and Sarah is near Salt Lake City Utah besides being an all-female team we're also United by a strong level of animals here's our plan for today I'll start with an introduction to a b testing as a whole and we'll highlight some of the key challenges that exist within the problem space Chelsea will take it over from there to describe the existing Solutions on the market and what is unique about test lab so give us an overview of test Labs architecture and she'll describe and demo some of the key features of our admin user interface that provides good user experience for the developer who designs a b tests then Allison will take over and dive into the back end she'll explain the architecture and functionality of the API the database and the sdks finally Sarah will wrapbox was a review and more detailed description of some of our key design decisions as well as our ideals for future work so what is a b testing well it's fairly straightforward to understand on the surface level let's walk through an example you have a website as seen here on the left with a button this is a super Magical Mystery button and you you want users to click it and not ask any questions however you're not getting the kind of Click count you need so you think of an idea a great idea you're going to give the people a reason that whip you present this variation to some users and as you correctly predict it when you add Ryan Reynolds pointing at the button more users push it you make the decision to roll this change out for the rest of users and now everyone is happy the key thing for a b testing is that it allows for data-driven decisions based on real empirical observations as another example Google famously tested 41 different shades of blue for the color of advertisements on their site they presented each color to about one percent of their users and they observed their behavior to end up choosing the best shade of a purplish blue it may seem like a small detail but it netted Google an additional 200 million dollars in ad Revenue when well done a b testing can increase desired user behaviors and therefore also increase Revenue it also reduces the risks of a detrimental design Choice based purely on a designer's preference rather than actual user Behavior the process of a b testing is very much like the scientific process the design team or developers form a hypothesis about how a proposed change will affect user Behavior then they work to create those variations uses are segmented so that each user experiences one of the two variations and then their behavior is observed and that data is collected as the experiment concludes the team then analyzes the data and makes a decision based on the variant that performs the best with the users a b testing can go beyond simple design changes to the user interface there are four main use cases the most obvious are user interface and design changes as described in the Ryan Reynolds example and the Google 41 shades of blue experiment the simplest form of this is called what you see is what you get where the tester can insert things or drag and drop elements onto the page without develop their involvement and the software itself generates the code next the developers may want to test out new features or functionality such as a search bar or a like feature third there are many changes to the back end or Hardware that an application can make which may change user behavior in an unanticipated way for example some applications have users who are loyal independent enough on their application to tolerate The increased latency of less cost than service finally an application may want to test out various third-party services or apis such as adding PayPal checkout there are three main approaches to implementing a b testing the first and most common implementation is on the client side which happens within the browser itself the same version of the page is always served and then the Dom is manipulated using JavaScript for users assigned to the test group this approach is easy for developers or even non-developers to make experiments and test ideas and there are a lot of existing services that perform this you can easily and quickly test many changes without investing a lot of developer time on the downside the users in the test group will always experience a brief flash of the original page before a render of the test version also sites that use libraries and Frameworks dependent on a virtual Dom such as reactant angular will have the undesired side effect of misalignment between the virtual Dom and the Donald cell the second implementation of baby testing is on the server side the code is modified on the server directly in the code base before being sent to the browser on the plus side this allows for greater access to user data so segmentation based on certain criterias possible these testing platforms are generally custom built which allows for more privacy and security server side testing is a more powerful testing option but the trade-off is that it requires more developer expertise and there are fewer third-party options finally content delivery networks can be used to Route users to completely separate paths this is easy to roll out a change to all users and it's ideal for static sites with none of the re-rendering concerns that we had on the client-side implementation however there is no access to user data so there's no real way to curate user experience since variant assignment is fandom in the worst case scenario you could also run them to Cache misses if one version is underutilized now we'll transition into some of the challenges that exist when trying to implement AV testing and then Chelsea will take over to compare some of the existing Solutions and describe where tests lab would do there are four main challenges when it comes to actually implementing maybe testing that I want to discuss first there's that of consistent user assignment it's essential that experiments can be designed so that the end users will always be directed to the same experiment or overall research periods this is not only necessary for good user experience but it's also required to have usable user Behavior data for collection the Second Challenge is to what degree will developers have to be involved in creating and managing the experiments if the tools are too hard to use that requires expensive developer time to implement tests that may not even see a significant return on investment however if the tools are too easy the trade-off is that maybe the tests won't be very robust or that the solution may not have as many options for full control and flexibility of use third we have data collection and Analysis how are we observing and analyzing user Behavior some tests and services require an existing third party to be used for analysis such as Google Analytics others do it all in-house again there are options and trade-offs here including data privacy and demand for computing resources finally there's the question of statistical validation and proper experiment design there are many factors involved in creating a good experiment and a b testing services May provide some safeguards to guarantee through a valid and proper design or they may not the key thing here is extracting the impact of a single variation by having clear-cut control variants and not presenting too many different features to a user at once you need to be able to clearly identify which change in the application caused the different user Behavior the second key thing is that there must be a significant enough amount of data to truly measure and understand the impact of changes if there's a large number of variants tested or the change in user behavior is small or the variants are not tested on enough users overall the data may not be meaningful enough to implement a change the rest of my team will describe how we handle these challenges at tests lab a little later first Chelsea will describe some of the existing Solutions on the market and begin to introduce test lab and where it fits in looks like I'm up I'm Chelsea and I'll be leading you on the exciting journey through some of the existing solutions for a b testing there's a wide range of a b testing platforms and while there is a market for specialized what you see is what you get editors like vwo which allow users without web development experience to perform limited testing of purely aesthetic changes our Focus was on platforms that offer a wider range of testing even though they do require developer expertise in other words you're going to need some nerds like us on board within the category of a B test Suites geared towards companies with a skilled development team there's a broad range of products the most basic of which are Bare Bones applications that expose apis for creating and retrieving experiments from the command line however platforms like that are this basic are not widely used at The Other Extreme are options like launch Darkly which offer not only apis but sdks for easy integration with client websites administrative user interfaces for managing experiments and that either provide or support existing data analytics services Platforms in this category are often costly and externally hosted hosted platforms like this can be a fine option for large companies with existing data collection and the financial resources to spare on third-party hosted Solutions however they're Out Of Reach for smaller companies without data analysis already in place and in need of a self-hosted option open source self-hosted platforms that fall more in the middle of the road are relatively sparse and tends to either fade away or experience rapid growth and become fully featured managed options as just discussed within the limited self-hosted category two key options are growth book and unleash both of which offer some of the same key features as the larger players but also feature open source deployment options these Solutions inspired some of the key features and functionality of test lab however neither of these platforms is well suited for a site that's not already equipped with a robust data analytics platform as you can see one key feature that sets test lab apart is our ability to collect and store event data our product is targeted towards clients that have not yet started data collection or who want or need the ability to host all of their own event data test lab provides our clients the flexibility of an open source locally hosted platform but without the burden of incorporating massive analytics infrastructure while we do require our clients to integrate API calls into their own event listeners in order to send data to the event database this is a trivial task and provides our clients total flexibility over their event tracking as well as a little job security for developers like us while our primary focus is experimentation test lab also offers feature toggles which automatically enroll 100 of users in a given feature and feature rollouts which allow our clients to enroll a custom percentage of users into a given feature however neither feature toggles or rollouts include event data collection now that we've gotten a sense of the overall landscape of a B test platforms let's move on to a basic overview of our implementation test lab architecture consists of four key components an administrative user interface built with react that facilitates creating editing deleting and viewing current scheduled and past features as well as viewing experiment data a node Express back-end server that exposes API endpoints for creating retrieving editing and deleting feature configurations as well as collecting and retrieving event data for Analytics a postgres database for persisting data including feature configuration user block allocation and event data for experiment Analytics and finally native sdks for node react Ruby and python for easy integration of our platform into both server and client-side rendered applications we will do a deep dive into each of these components but this is the general outline of test lab infrastructure before I begin discussing the specifics of our architecture I do need to address one key implementation challenge that we faced ensuring that users of our client site are only enrolled in one experiment at a time if users are enrolled in multiple experiments concurrently it would be impossible to determine whether any difference in tracked user Behavior was the result of the variant being evaluated or the combination of variants to which the user was exposed in other words if you are simultaneously running a test to see if a blue button made users more likely to click that button and another test to see if a photo of actor and heartthrob Ryan Reynolds next to a button made users more likely to click that button some users would see only the blue button some users would see only Ryan Reynolds and some users would see both and that would be chaos you would not be able to distinguish whether it was the button color or the photo or the combination of both that cause users to click more and we would have failed at our job of providing you with useful data this would not Empower our clients to make informed decisions even after the results were compiled in order to ensure user enrollment and experiments is mutually exclusive meaning if you're in the Ryan Reynolds test you only see Ryan Reynolds not the blue button and if you're in the blue button test you only see the blue button and not Ryan Reynolds we developed a user block strategy user blocks track assignment of users to experiments or the available pool of users is segmented into 20 chunks each representing five percent of the user base each of these chunks or user blocks is permanently assigned to exactly one experiment for the duration of that experiment ensuring that users in each user block are only exposed to one experiment as soon as an experiment concludes all user blocks that were allocated to that experiment are released and may be reallocated to upcoming experiments as needed Allison will walk through the logic of user blocks in more detail when she discusses our sdks but this should be enough of an explanation of user blocks to get the discussion of our administrative user interface going so let's do it at its core the administrative user interface or admin UI as the cool kids say is basically a crud application designed to let clients create read update and delete features with ease and a touch of Glamor at least as compared with command line API calls but the admin UI also features additional functionality to ensure client success with experiment creation among the additional functionality included in our admin UI is a tool to ensure that our clients do not overbook experiment user blocks as our clients select dates for their experiments the admin UI calculates the maximum percentage of users available for experiment enrollment during that period and prevents our clients from scheduling experiments that require more user blocks than are available additionally as our clients view their existing features they can also pause features perhaps they've discovered a pesky bug that needs fixing or they realize their computer is having their competitors are having a big sale that might disrupt their experiment data for a day or two our admin UI allows our clients the ability to pause and resume features with ease plans can also delete features entirely though this does result in permanent removal of all data related to that feature from the database so maybe not something you want to do with your Ryan Reynolds experiment we've also tailored our feature editing options to ensure maximum flexibility when editing upcoming features that are not currently ongoing including the ability to modify names descriptions start dates and dates and user enrollment percentages were relevant as well as the value weight and quantity of variance for experiments however in order to ensure that our clients do not make changes the dampness the Integrity of their data for currently running experiments our admin UI limits our clients ability to make destructive changes to ongoing features once an experiment has begun the start date may not be altered variants may not be added or removed and variant weights may not be changed clients still have the flexibility to alter the name description end date and overall percentage of users site-wide that are exposed to the experiment as well as the variant values because hey typos happen our admin UI is designed with client success in mind and we do our best to ensure all features and experiments run smoothly and provide valid useful data finally for current and past experiments our admin UI allows our clients to view in progress and final results respectively of the data collected from their experiment the visualization of data helps illuminate the difference between variant performance and guides our clients towards making successful decisions when experiments conclude that about sums up the admin UI I'll hand this off to Allison to discuss more of the test lab architecture thanks Chelsea let's kick it off with the Tesla back-end server which plays several key roles within the test lab application first it exposes API endpoints for creating editing deleting and retrieving feature data also creating and retrieving user data to be used during the experiment data analysis and finally storing and retrieving event data from experiments the test lab backend server also ensures the Integrity of data sent to the test lab database as Chelsea mentioned in the previous section the test lab backend server also assigns user blocks to each experiment to ensure that no more than 100 percent of users are enrolled in an experiment at any given time this also ensures that users do not move from one experiment to another finally the test lab backend server keeps track of the last time that the feature data was modified which as you'll see in a few slides supports the efficient retrieval of future data by the sdks it's important to note that developers have the option of using the admin UI and sdks to interact with the test lab back-end server or they can use the API directly and we do provide extensive API documentation to support that option persistent data is stored in the test lab postgres database the database stores several categories of data first future configuration data for toggle rollout and experiment features including a feature's name description and the proportion of users enrolled we also store variant configuration data for experiments including a variance name value and the proportion of users in a given experiment that will experience that variant user data is stored for users enrolled in experiments including a unique string identifying the user the user's IP address and the variant that the user experienced we capture and store event data posted by experiments including the time of the event the user triggering the event and the variant the user experienced when triggering the event and finally we keep track of user block data that represents which blocks of users are allocated to which experiments at a given point in time so how do we use this data to actually deliver specific features to a particular user of our client's application well test lab includes node react Ruby and python native sdks that help a developer translate the feature configurations made in the admin UI into their application code the sdks enable three main functions the first is retrieving features where the test lab SDK client retrieves up-to-date feature and user block information from the backend server and uses that in the Second Step which is evaluating features the test lab assignment logic is used to evaluate the feature for each request and finally we deliver a variant based on the feature value the appropriate variant is rendered for a specific user so let's walk through each of those three steps first retrieving features the sdks use polling to keep the feature configuration up to date the first time the SDK client makes a request to the backend server the server reaches out to the test lab database to retrieve the current feature and user block configuration and that data is returned to the server and then to the SDK client the SD SDK client then stores an in-memory representation of the current configuration of features and user blocks and uses that data to determine the value of features for a given user and we'll talk about that process in just a minute as I mentioned a few slides ago the test lab backend server keeps track of the last time any feature data was modified so the next time that the SDK client reaches out to the test lab backend server if there have been no modifications then the server just returns a 304 not modified status code and does not need to reach out to the database for new data the key takeaway is that new feature data is only returned if it has been modified since the last request you may wonder why we use this strategy rather than setting up server send events or web sockets and Sarah will talk about that a bit in the next section but first let's talk about evaluating features the sdks determine whether features are active and let's start with toggles and rollouts toggles are simple we check to see whether the feature is active or paused and we looked at the start and end date if the feature is active and the current date is within the start and end date of a toggle then the SDK returns true and the feature is active otherwise the SDK returns false rollouts are a lot like toggles but with an extra step we still check to make sure that the rollout is active and that we're within the start and end date range but rollouts are only active for a particular percentage of users so how do we determine whether a rollout is active for a user well both rollouts and experiments as we'll see on the next few slides rely on using a hashing function to convert a unique string to a uniformly distributed value between 0 and 1. in the case of a rollout the unique string is the user ID concatenated to the name of the rollout if the hash value of this string is less than the rollout percentage then a rollout is active for a given user let's take this a step further and see how feature evaluation works for the last feature category which is experiments we just saw that rollouts and toggles return to true or false in other words they're either on or off for a particular user however active experiments do not return true but instead return the particular variant that should be rendered let's walk through how this works we start with the same checks to ensure that an experiment is active and within the start and end dates but the next step is different we use our hashing function to Hash our unique user ID to a value between 0 and 1. and we use that value to determine which user block this user is assigned to you may recall from Chelsea's discussion the user blocks are our way of determining which blocks of users are assigned to a particular experiment So based on the hash value of the user ID we identify the user block and we check to see whether the experiment is active for this user if not we just return false but if so we return the appropriate variant based on that hash value in the example here we'd return variant one thirty percent of the time variant two thirty percent of the time and variant three forty percent of the time let's take a closer look at how that works as we just discussed user blocks are critical to experiment evaluation and once we've determined that a user is enrolled in a particular experiment based on the user block assignment we need to determine which variant to serve and we do this based on the variant weights in the example here there are three variants again with weights of thirty percent thirty percent and forty percent let's suppose the hashed value of our user ID is 0.54 we know that we're in the 0.5 to 0.55 user block which in this example corresponds to experiment two but how do we know which variants are returned well we break up the user block into segments based on the variant weight hashed values that map to the first 30 of the block would be served variant one values that map to the next 30 percent of the block would be serve variant two and values that map to the last 40 percent of the block would be served variant three in our example 0.54 would fall into the last segment and would be served variant three now that we've looked at how the SDK retrieves feature configurations and how it uses assignment logic to serve a particular variant let's see what that looks like in the code So based on the return value the appropriate variant is rendered in this example we have a snippet from a simple node Express app where we use the test lab SDK client to retrieve the value of my experiment for a user with a randomly generated user ID the developer would include Logic on what they want to render depending on the value of the experiment in this case if we return the value of Ryan we Render Ryan Reynolds on our ice cream shop page and otherwise we display the default page with no image if the variant weights are 50 50 then you would expect that about 50 percent of users that are enrolled in this experiment would see Ryan Reynolds and the other unfortunate 50 would miss out now that we've seen the process of using test lab from creating experiments to ultimately rendering specific variants Sarah's going to walk you through some of the engineering decisions we made while creating test lab thank you Allison designing test lab led to some interesting decision points I'm going to share some of those decisions the trade-offs we made and the logic the lead test lab to become what it is today the first set of decisions relate to the architecture of test lab we chose to have a self-hosted platform that stores data in a postgres SQL database and has an API that is easily accessible outside of the admin UI first let's discuss our decision to have test lab be self-hosted we mitigated the complexity of a self-hosted solution by doc Rising test lab and using a self-hosted solution allows the opportunity for customization complete control over data and a lower cost a majority of options on the market are managed a b testing with client-side rendering one advantage of managed platforms is that they can be easier to set up than a self-hosted platform however as the display documentation shows teslab has mitigated this difference instead of complexity by dockerizing our platform and providing clear documentation that explains how to deploy test lab using a Docker image now let's discuss the benefits of cell hosting a self-hosted app a self-hosted and open source application allows customization of code to fit your specific needs for example if the developer always expects features to last two months they can add code so the ad feature form always defaults to two months a managed solution does not provide access to the source code so this sort of customization would be impossible another benefit is owning your own data this can be important if you want to keep your data on premise or if you have data specific data retention or compliance requirements finally the self-hosted solution has a cost benefit there is no cost to download the software a self-hosted platform allows you to scale your platform as your company growth requires we minimize the shortcomings of a self-hosted platform by making installation as easy as possible and thereby gaining a solution that allows customization control over the data and cost benefits another major decision was which type of database to use we needed to store data about features users and events for example some options were a Time series database nosql or a SQL database the time series database would allow us to store event data with more granularity what we decided it was overly complex for our needs since the focus of analysis is on the aggregate of the data nosql databases are ideal for data that is not structured or could have a changing structure we chose postgres a SQL database because our data needs to be structured we rely on the relationship between a user and event data for a data visualization feature for example feature configurations require specific data types to work properly with the SDK and admin UI the rigidity of postgres postgres adds an additional layer of validation on top of the validation in an admin UI and the backing other benefits of a SQL database are data consistency and reliability the next decision relates to the API design while building our API we tried to make it so that using the admin UI is not required for the really big nerds this meant exposing more of the API interface than it is needed by the Admin UI one downside of this is greater potential for the developer to use the API incorrectly AKA nerds who think they're bigger than they really are apparently we have more validation present in the front end to prevent user errors from in the back end we felt that the flexibility of allowing the developer to interact with API outweigh the potential downsides of a developer using the API incorrectly we tried to mitigate this risk by providing extensive API documentation as you can see on the left we also made decisions related to the design of test lab we chose to implement context with flexibility limit users to one experiment at a time and polling for sdks to retrieve the current features configuration first let's look at how we chose to implement context context is the internal memory representation of the factors that influence which features and variants are active and can be used to allow a user to experience the same variant across multiple requests and across multiple sessions it is implemented within the SDK earlier Alison explained how the user ID is hashed to determine the active features and variants for that user when implementing context we chose a flexible approach that defaults to a random uuid assigned to the user ID in the context the context can also be changed using the update context method passing a new value for the user ID this new value can be any unique value you could keep it the default branded value or change it to a session identifier or an email for example the random value is ideal for features that are not dependent on user Identity or behavior such as a button color or Banner placement this ensures that the results are not affected by any systematic differences between users the session identifier is best for features that are dependent on user behavior during a single session such as the effectiveness of product recommendation this ensures user behaviors consistent across control and treatment groups using an email or login may be appropriate for features dependent on user Identity or behavior over time such as a personalized email marketing campaign this ensures that the user receives the same variant over a longer time frame and that the differences in user Behavior are from the feature and not user variability these are just some of the options you could Implement to utilize the flexibility in test Labs design next let's discuss why we chose to limit users to only one experiment at a time and how we implemented this feature using user blocks if users are allowed in multiple experiments at the same time then to determine the effects of each experiment would require a multivariate analysis to determine the effects of the given feature this is not only more complex but requires a larger sample size which would increase the time and resources needed for the experiment it would also create a higher risk of false positives in overfitting the data limiting users to one experiment avoids those risks and is more appropriate for a use case with a more modest user base we accomplish this with user blocks which as explained before by Chelsea and Allison are created by blocking the users into percentage groups the main goal of user blocks is to limit users to one experiment the hashing of user id2 user block is deterministic so each user ID remains in the same user block until the user ID changes also once a block of users is allocated to an experiment it remains allocated to that experiment until the experiment finishes this allows us to split the experiment across separated user blocks by dynamically assigning the experiment to any user blocks that are unassigned when the experiment begins when designing the user blocks we chose to use five percent groups which allows some granularity allowing up to 20 concurrent experiments having a coarser for granularity will make it more likely for experiments to reach statistical significance by having larger sample sizes designing user blocks to allocated experiments to fixed groups within the user base allowed us to have multiple experiments running at the same time while avoiding the unnecessary complexity of multivariate analysis now let's discuss our decision to use polling to keep an up-to-date in-memory representation of the feature configuration Allison described earlier how the SDK uses the feature configuration to determine which features are active we consider three options to keep it up to date polling the option we chose to implement server-side events and a websocket we chose polling for its Simplicity compatibility and efficiency websockets and SSE provide real-time updates to the data since we don't need to immediately update the features configuration we can use the simpler polling option that updates the data at predefined intervals the interval can be changed to better fit your use case websockets and SSE would require server-side configuration whereas polling is widely supported by all modern browsers so polling has better compatibility finally pooling is more efficient because the server only responds with new data when the feature configuration has changed instead of keeping the connection open like an SSE or websocket polling was a clear choice for updating the feature configuration with an SDK because it is simple compatible and efficient another attribute of Tesla that we are excited to share with you is the full test Suite we are proud to have developed the API concurrently with the test Suite which allowed us to ensure that new code did not impact previous functionality the test Suite has 90 percent coverage it is written in Jess and uses the supertest library to test the API routing our tests are written as unit tests to test each API route they include setup and tear down to avoid affecting real data as for each API endpoint includes testing to confirm it responds accepts data and returns data in the correct format this includes testing that errors return the correct status and message some entities should also accept using default values when creating or updating certain values we also include a test that ensures the API accepts using the default values finally we wanted to ensure that the SDK received a 304 status message when there is no new data now let's explore some ideas we had for future work to continue to improve test lab we chose to segment users into five percent chunks based on test lab user feedback we might can change this to smaller chunks one option we considered was adding routes that allowed the user to choose from a range of block sizes when using the SDK or API another feature that we are excited to develop is adding an SSE for real-time updates on the status of features we envision a dashboard where we display the experiment Health metric on current experiments making it easier for an admin to check the experiments are performing as desired another major feature that we are looking forward to developing for forward to exploring is adding some statistical analysis we chose to start with A visual representation of the data as well as providing API access to the raw data this provides flexibility in how the data is analyzed with your preferred metric however there may be some value in providing some simple measures of statistical significance for commonly used types of experiments in addition we believe it could be useful to allow users to convert the winning variant to a toggle or rule out our roadmap continues with other functionality we are excited to develop which leads to our next segment questions from the audience we are now opening the floor for questions and Abby will read those from the comments section okay all right so here in the comments section or I mean the question section we have a question from my does user blocking take into account active versus no longer active users do users get shuffled for blocking purposes assuming that user IDs are assigned incrementally at the time of registration I would also assume that older users are more likely to be no longer active if one experiment's assigned to say the bottom thirty percent based on user ID wouldn't they be unproportionally likely to contain non-active users maybe someone wants to take that that was a long one and then I'll read the second section I feel like it might be helpful if I go back to the user block slide so one of the decisions we talked about was how the flexibility and how we assigned it users and I think that you're thinking about users in a way that it's uh like a user signs up for an account and then they have a user login and they have um a email and password to log in and that's one way that you could assign a user to within the system however the default is using a random uuid so when we look at a user ID uh being in the bottom 10 percent were in the top 30 percent it is random for that random uuid plus the user ID is hashed from when we when we get that value we hash it from the value that you provide so for example if you are taking if you decide to utilize um their session the their session identifier or their email login which includes when they were signed up then it would that value would be hush so it's it's still assigned to a random value between zero and one um does it take into account if users or older users are no longer active so again that's the flexibility of it the original setup is using a random default value in which case when you go to the website then you would be assigned a value and you would be considered an active user because you're using it however if you are using uh email logins or system identifiers that's added functionality that you would have to set up on your end of it um and how would this affect the 100 number uh so uh you can you read the rest of the question sure also how does sectioning or blocking work for newly signed up users as this would affect the hundred percent number since the number of users in groups so I think maybe a point of confusion is that um we are not starting with a quantity of users um so essentially what we're doing is taking a value which can be a uuid or an email or whatever our clients select and um putting that value through a hashing algorithm and what a hashing algorithm does is it um basically randomly assigns a numerical value so um it's not looking at like okay you have 100 users today and we're going to take the first five users it's literally just an equation and statistically hashing algorithms are designed in such a way that the um the the outcomes the numbers that they produce should be evenly distributed um so it's really kind of irrelevant how many users you have there there's a point at which you might have so few users you know if you've only got 10 users you know you might not get really even distribution but um but we have selected hashing algorithms that are sort of designed for for the user base that we're targeting um and and so it's it's less about the the overall volume of numbers and more about kind of where this hashing algorithm assigns um that what value it assigns to that that input so I hope that answers your question and feel free you know to reach out to any of us individually um if you have further questions but I think we spent maybe a little bit of time answering this one already so um yeah thanks Chelsea uh we have one other great question here in the chat and then um we'll wrap up um are there any perceived scaling issues with your application as the user count increases and again I think that some of this goes back to that confusion on user count and how the user blocks work but can anyone answer that yeah I think I think it's the same answer basically um you know the the hashing algorithm you know essentially every every time a user visits our client site um you know whatever ID whether it's the ID that we generate or the ID that they've selected to use um will go through that hashing algorithm so you know there's not really any um anticipated scaling issues this this is actually um the hashing algorithm method of doing this is something that some of the the larger products that we looked into also use so it's not an uncommon solution even for really large platforms so there really shouldn't shouldn't be any scalability issues again unless you're looking at a site that you know literally has like 10 users in which case you know or maybe wouldn't be as effective the Middle East that's and the other so oh sorry go ahead sorry the other key piece of our solution is that it is South posted so ultimately the Computing resources are up to our clients who um you know who basically launches our our products on their talk through all right and then we have just one final question from Sergeant himself why is Ryan mantled so good looking well surgeon that's an excellent question definitely one for the ages um I don't think even we at test lab can answer that but um I do think that's a nice note to end things on so thank you all so much for attending our presentation and we hope that you enjoyed it foreign 