# Harold: An Open-Source Observability Solution

## ELI5 Summary:

Harold is like a doctor for your computer software. It keeps a close eye on how your software systems work and gives feedback, using the popular ELK stack - Elasticsearch, Logstash, and Kibana. This feedback includes detailed messages (logs), precise where-when-what data in a series of actions (traces), and numeric overall health stats (metrics). Harold makes it easy to place all this feedback in one place and understand your system's health and performance.

## Tools Used:

1. ELK (Elasticsearch, Logstash, Kibana) stack
2. AWS - Amazon Web Services
3. AWS CDK - Cloud Development Kit
4. Docker and ECS - AWS Elastic Container Service
5. AWS Elastic File System
6. AWS Cloud Map
7. AWS CloudWatch
8. AWS Lambda Function
9. Filebeat
10. Elastic APM agents
11. Fleet Server
12. Java, Python, JavaScript, TypeScript for AWS CDK

---

## What is Harold?

Harold is an open-source observability solution that simplifies the deployment of the ELK stack. The ELK stack is a popular set of tools used for monitoring the health and performance of systems.

---

## Observability

Observability allows understanding how a system is functioning based on its outputs and behaviors. The **three pillars** of observability are:

1. **Logs**: Detailed records of events or messages generated by a system. 
2. **Traces**: Analyzing a software system by collecting data about the different stages of a request as it moves through components or services.
3. **Metrics**: Numeric representation of data measured over time intervals, providing an overview of the system's health.

---

## Harold and Existing Observability Solutions

While there are existing commercial and open-source observability solutions, Harold attempts to bridge the advantages of both groups: it is an open-source solution that simplifies the deployment of the complex ELK stack, prioritizing ease of setup while maintaining user control over data and infrastructure.

---

## Harold: From Data Collection to Visualization

Harold does the following:

1. **Data Collection**: Using a collection agent installed on each component of the system.
2. **Data Processing & Transformation**: Processing the collected data and transforming it into a suitable format.
3. **Data Storage**: Saving the data in a central location, enabling data analysis.
4. **Data Visualization**: Finally, using a user-friendly UI to visualize the data.

---

## Harold's Infrastructure

Harold and its user applications reside within the same virtual private cloud (VPC) in AWS. It uses public and private subnets to host various services like Elasticsearch and Logstash clusters, APM and Fleet servers, and AWS resources like Cloud Map, Elastic File System, and CloudWatch.

---

## Harold's Implementation via AWS CDK

Harold is built using **AWS CDK**, **Docker**, and **ECS**. Key applications like Elasticsearch, Logstash, Kibana, and Fleet Server are installed using Docker containers via ECS. The provisioning of Harold uses Amazon's Cloud Development Kit (CDK), an infrastructure-as-code tool.

---

## The Herald Pipeline: Collection, Transformation, and Analysis

1. **Data Collection and Shipment**: Log data is continuously collected by Filebeat and sent to Logstash. Traces and metrics data are collected by the Elastic APM agent.
2. **Data Processing and Transformation**: Logstash processes data from Filebeat and applies filters for specific transformations. At the same time, the APM server processes data from the APM agent, transforming it into Elasticsearch documents.
3. **Data Storage**: Data from Logstash and the APM server is stored in Elasticsearch, enabling querying and visualization.
4. **Data Visualization**: Kibana visualizes and analyzes data stored in Elasticsearch via user-friendly dashboards and charts.

---

## Harold's Future Improvements

Currently planned improvements include:

- Implementing auto-scaling for Logstash cluster.
- Adding Kafka to minimize data loss.
- Incorporating AWS S3 cold storage for cost reduction and performance enhancement.
- Auto-scaling the Elasticsearch cluster based on storage and CPU utilization.