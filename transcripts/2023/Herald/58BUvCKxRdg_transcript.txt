foreign to our presentation we're excited to tell you about Harold so thanks for coming let's get started Harold is an observability solution that simplifies the deployment of the elk stack a popular set of tools used for monitoring the health and performance of software systems it allows software developers to conveniently collect and explore Telemetry data including logs traces and metrics through a single user-friendly interface yeah in this presentation we're going to start by examining what observability is why it's important and the challenges involved in implementing an observability solution we'll then look at existing Solutions in the observability space after that we will outline how Harold fits into this solution space before diving into an overview of what Harold is and what it does finally we will examine some of the design decisions and implementation challenges we faced in building Herald um observability is the ability to understand how a system is functioning based on its outputs and behaviors it uses three types of telemetry data often referred to as the three pillars of observability logs traces and metrics this data provides visibility into a software system and allows development teams to get to the root cause of various issues to improve performance below we take a closer look at each type of data the first type of data we'll look at is logs logs are records of events or messages a software application or system generates they are typically very detailed and provide information about a specific event or action within a software system information contained in logs includes time stamps message content severity level and other contextual information the second type of data we look at is traces tracing is a method of analyzing a software system by collecting data about the different stages of a request as it passes through various components or Services of the system it involves creating a trace that includes information about each step of the request a trace comprises one or more spans a span represents a specific piece of work performed by a specific service within the request path such as an HTTP request or a call to a database spans contain important information such as the start and end times of the work as well as any metadata that might be relevant to understanding the span the trace can help Engineers identify the different Services the request passes through and how they interact with each other by analyzing this Trace developers can determine where the requests spent the most time and which Services were involved this information can be used to identify performance bottlenecks optimize a system and improve the user experience and finally the third type of data we look at is metrics metrics are like vital signs for a software system they are a numeric representation of data measured over intervals of time they help developers understand the health of a system by studying performance goals and baselines metrics allow developers to track whether a system is meeting its targets and catch problems before they become critical logs traces and metrics work together to provide developers with observability of their software systems allowing them to diagnose issues and improve system performance to illustrate how developers can use logs traces and metrics together to diagnose a problem and help them fix it let's walk through an example suppose a developer is responsible for a web application that allows users to purchase items online upon checking some metrics related to the performance of the app the developer notices that the average response time and error rate for the app's checkout page have increased they decide to investigate further to see what might be causing the Slowdown based on these metrics the developer uses tracing to follow a request through the system and pinpoint where a potential error may be occurring they choose a recent transaction that experienced a slow response time and use a tracing tool to follow the request as it moved through the system the tracing tool shows that the request spent a significant amount of time in a particular service responsible for verifying the user's payment details the developer suspects there may be an issue with this service and that further investigation is required to track down the cause the developer decides to check the logs associated with a service in question in the logs the developer sees a large number of Errors related to the payment Gateway API being used by the service the logs also show that the payment Gateway API has recently changed its authentication method which may be causing the errors based on this information the developer updates the service to use the new authentication method for the payment Gateway API they deploy the updated service to production and monitor the application's metrics traces and logs to verify the issue has been resolved alone each pillar of observability provides valuable information but not a complete picture the metrics alerted the developer to the issue the tracing helped them pinpoint where the issue was occurring and the logs provided more detailed information about the root cause of the issue by having insight into all three developers get complete visibility of their systems health and performance but knowing that observability is important is one thing making software systems of observable is another how does a development team aggregate their Telemetry data into a single location in order to be visualized and analyzed the solution to this problem involves solving several smaller problems let's examine each of them the first problem that needs to be solved is how to collect the data typically this is accomplished with a collection agent installed on each component of the software system that is going to be observed this collection agent is responsible for collecting the data from the component and shipping it to Some central location but before the data is shipped to the central location there's another problem to solve at this point the data is still raw and unstructured it will need to be processed and transformed into a form or structure suitable for a particular analysis thus a data processor is needed before the data goes anywhere else the next problem concerns the central location there needs to be some data storage component that can be queried for the purpose of visualization and Analysis this data store should handle the continuous inflow of data and enable fast and efficient queries for real-time data analysis the final problem to be solved is how to visualize that data so it can be gleaned for Meaningful insights what is required is an intuitive and easy to use UI thus to achieve observability a solution is needed that performs the following four functions data collection and shipment data processing and transformation data storage and data visualization next we look at some of the existing Solutions out there that aim to solve the observability problem for development teams several companies offer various observability tools to satisfy most developers needs one of the benefits of using these observability tools is that they are typically easy to set up and feature Rich some of those extra features include infrastructure monitoring network monitoring and error tracking however some commercial Solutions may not be a good fit for software team teams concerned about data and infrastructure ownership in particular teens working with sensitive data or in highly regulated Industries may be wary of using third-party Solutions due to concerns about data privacy security and compliance these teams may prefer to keep complete control over their data and infrastructure including the ability to choose where and how it is stored and processed using a commercial solution may require them to relinquish some of this control commercial Solutions also come at a cost that may be too high for smaller companies additionally certain commercial Solutions May create vendor lock-in situations where a company becomes dependent on a particular Solutions products and ecosystem in addition to commercial observability Solutions various open source observability tools are available that can provide a cost-effective alternative for Developers these tools offer various features for collecting analyzing and visualizing log metric and tracing data while also providing flexibility in terms of data ownership and infrastructure they also don't lock organizations into a particular vendor while open source observability tools offer many advantages such as data ownership and cost savings deploying and managing these tools can be significantly more complex than commercial Solutions piecing together various observability Tools in order to collect process store and visualize Telemetry data can require substantial time and expertise this may not be feasible for smaller development teams the complexity involved in this process is subtracted away with the commercial Solutions that's where Harold comes in Harold Bridges the gap between commercial and open source Solutions it is an open source observability solution that abstracts away the complexity of setting up open source tools while offering development teams the ability to maintain data and infrastructure ownership further the only cost of using Herald is the cost of provisioning and using the AWS resources on which it is deployed Harold was built for growing applications that have reached a size with the ability to monitor their health and performance has become an issue the growing complexity of these apps requires their development team to consider a comprehensive observability solution what is needed is a solution comprised of a set of battle-tested observability tools Herald is built on the outstack comprised of elasticsearch log stash and Cabana the elk stack is a popular set of Open Source tools commonly used for log management and Analysis the full elk ecosystem also offers tools for trace and Metric data the combination of these tools provides complete observability solution combining other open source tools into a complete observability solution is also possible but managing these other tools necessitates developers becoming familiar with a separate documentation associated with each tool these docs may be sufficient for understanding how each tool Works in isolation but are less helpful when the tools are combined the elk stack on the other hand is managed by single organization elastic elastic provides unified documentation on the entire stack making it easier to troubleshoot issues that may arise between the various components of the stack there is also a strong support community on which developers can rely the comprehensiveness of its documentation across the stack the strong community of supporters and its popularity make them elk stack an ideal choice for developers just getting started with observability the elk stack is also battle tested as several commercial Solutions use it in their own offerings setting up the elk stack can be challenging particularly for developers new to observability the configuration details involved in setting up each component of the stack for complete observability can be overwhelming however however Herald simplifies the process by abstracting away the complexity providing an effortless deployment of the stack with just a few commands developers can quickly get started with Harold The Herald pipeline encompasses the main components required to achieve observability data collection and shipment data processing and transformation data storage and data visualization let's revisit the observability implementation challenges from earlier and introduce the components in Herald that are used to solve them the first challenge that needs to be solved in order to achieve observability is collecting data from the various components of software system and shipping them to Some central location Harold uses two separate tools for data collection and shipment filebeat for log data and the application performance monitoring agent for traces and metrics data filebeat is a collection agent designed for collecting and shipping log data its primary function is to continuously scan for new log data and send such data to logstash where it is processed and transformed filebeat is not part of the herald deployment but it is installed separately on the user's application servers after installation it must be configured to monitor specific log files and output the data to logstash for collecting and shipping traces and metrics data we have elastic APM agents APM agents are open source libraries that collect data generated by an application these agents are written in the same programming language as the application and can be easily installed like any other Library once installed the user then instruments instruments their code to allow the agents to collect tracing and metrics data the APM agents then ship the data to the APM server for processing Second Challenge that needs to be solved to achieve observability is data processing and transformation this component of the pipeline must be capable of processing data for particular analysis and transforming the data into a format that is accepted by the data storage component Harold uses two separate tools for data processing and transformation log stash for logs and the APM server for traces and metrics within the herald pipeline logstash is configured to ingest data from filebeat the user must configure logstash with an appropriate filter that enables a specific transformation of the ingested data to support a specific application use case for example a user may use the geoip filter to add information about the geographical location of IP addresses once the data is processed it is sent to elasticsearch for storage and indexing the APM server comprises two parts the elastic agent and the APM integration elastic agents are installed on the user's application servers to receive different data types such as metrics and traces from the APM agents the elastic agent can be updated with configurations enabling the collection of new or different data sources the configurations are implemented through agent policies the APM integration is one of those configurations that gets specified within an agent policy the elastic agent with the APM integration acts as the APM server which lives entirely on the user's application server the APM server accepts tracing and metrics data from an APM agent the APM server then processes the data which includes validating it and transforming it into elastic search documents before sending it on to elasticsearch the third challenge to be solved to achieve observability is the data storage component the data store is where data will be housed and made available for querying by the visualization component the elasticsearch is a distributed search and analytics engine and document store it stores complex data structures serialized as Json documents elasticsearch stores and indexes data in a way that enables near real-time searching it is a durable data store which means it can persist long-term data as needed within the herald pipeline elasticsearch receives data from logstash and the APM server it acts as a storage component that can be queried through kibana to be visualized the final challenge to be solved in order to achieve observability is the data visualization problem data sitting in the data store is only good if it can be visualized and analyzed Harold uses kibana as its data visualization component kibana is a powerful open source data visualization and exploration platform it provides a user-friendly interface for searching analyzing and visualizing large volumes of data in real time with kibana you can search observe and analyze your data and visualize your findings and charts gauges maps and graphs here is a sample of what it looks like to view logs in Cabana from here developers can search and analyze log data and here is a sample of what it looks like to view a trace in Cabana from here developers can view a trace for a particular request and finally here's a sample of what it looks like to view metrics in Cabana from here the developers can gain a better understanding of their overall health of their system before we transition to the next phase of our discussion let's briefly recap the key topics we've discussed so far first we covered why an organization may need an observability solution then we looked at how logs traces and metrics are used for active monitoring and debugging we then discussed the challenges with implementing an observability solution and where Harold fits into the landscape of existing Solutions finally we looked at the herald Pipeline and how Harold collects logs traces and metrics and turns them into usable observability data at this point we're going to transition to discussing how we built Harold as well as some of the implementation challenges we faced and the design decisions we made some of the topics we'll cover include building Herald with Amazon web services and Cloud development kit deploying containerized applications service Discovery within the herald application secure and communication with elasticsearch nodes creating a multi-node elasticsearch cluster Auto scaling the elasticsearch cluster and how each application fits into Harold's overall architecture but before we discuss the specific details related to building Herald let's take a look at Harold's overall architecture the first thing to note is that both Herald and users application reside within the same virtual private Cloud within AWS when deploying Harold existing vpcs within the user's account will be automatically displayed for the user to select which VPC within which to deploy Herald this ensures that the user's application can communicate with Harold without additional configuration looking at the center of the diagram we have a private subnet where most of Herald services are deployed we have two elasticsearch clusters one for master eligible nodes that are responsible for management of all elastic search nodes and one for auto scaling data and ingestion nodes we have a log stash cluster for processing log Data before it gets stored and indexed in elasticsearch and we have a fleet server cluster for enrolling and managing elastic agents note that both the log stash and Fleet server clusters have load balancers in order to distribute loads across the Clusters evenly on the right side of the diagram we have a public subnet where the Cabana application which requires a public IP address is deployed Cabana receives queries from a user and sends requests to the elasticsearch cluster here you can also see that we've listed a number of other AWS resources that are part of Herald's architecture cloud map for service Discovery elastic file system for a centralized volume storage for containerized applications cloudwatch for application logging and a Lambda function which we will discuss a bit further in the presentation and finally on the left side of the diagram we have a public subnet where the user's application resides applications shown here are not part of Herald's deployment but are necessary for instrumenting a user's application to send data into Herald two applications are installed on the user's architecture filebeat which sends log data to logstash so that the data can be processed for getting stored and indexed in elasticsearch and APM server which sends traces and metrics data to elasticsearch Harold was built and deployed using aws's Cloud development kit or cdk cdk is an infrastructure as code tool that allows a developer to provision AWS resources using the same code a developer uses to build an application for example JavaScript typescript python or Java all of Harold's applications elasticsearch log stash Cabana and Fleet server are installed using Docker containers as such Harold uses AWS in his elastic container service or ECS which is a fully managed container orchestration service designed to facilitate the deployment management and scaling of containerized applications ECS can deploy containers on aws's elastic compute cloud service or on its serverless application hosting service bargate the first two components in building Peril are the elasticsearch and Cabana applications as discussed earlier in the presentation elasticsearch is the database where data is stored and indexed while kibana provides the user interface that allows for querying in visualizing the data stored in elasticsearch as a front-end application kibana requires a public IP address and is placed in a public subnet while all elasticsearch is placed in a private subnet to ensure that it is inaccessible from outside of the herald virtual private cloud however in order to deploy kabonnet the bottom must know elasticsearch's IP address because a successful Cabana deployment requires confirming a successful connection to the elastic search service otherwise Cabana will generate an error and fail to run but when using Cloud development kit to deploy applications through UCS the IP address of the host server is unknown until after the deployment of all applications is complete so this presented a problem how can Cabana be configured to communicate with elasticsearch if elasticsearch's IP address cannot be ascertained until after deployment to deploy applications that must be configured to communicate with other services but where the IP address is unknown at the time of configuration we used aws's cloud map service to create a private DNS Network this gave us the ability to assign DNS host names to each service where each hostname would resolve to the IP address that eventually gets assigned to that service after deployment of the herald application now that Cabana knows where to locate elastic search through service Discovery there is still one more step that needs to be implemented in order for Cabana to make a successful connection to elasticsearch the communication must be secured using TLS in fact any Community any application that communicates with an elasticsearch instance must do so through a secure connection even if that application is another elasticsearch instance this ensures that all data stored in elasticsearch is only accessed by authorized applications to enable TLS a dedicated certificate Authority needs to be created using elasticsearches certificate generation utility this certificate Authority is then used to generate certificates for Cabana and each elastic search node which they then use to establish secure connections with each other One Challenge we encountered was that kibana and elastic search nodes need access to TLS certificates before they can be successfully deployed this meant that these certificates needed to be created as part of the overall deployment of payroll application but before deployment of the Cabana and elastic surge services to create these certificates a separate instance of elasticsearch was deployed whose sole purpose was to create these certificates using elasticsearch's certificate generation utility as well as to set some initial configuration variables in the initial elastic search node for any application that would need to communicate with elasticsearch certificates for that application were generated at this time as well and uploaded to a central elastic file system volume with the certificate volume now available and populated with the appropriate certificates any application that will communicate with elasticsearch will need to mount this volume to the docker container within which the application is run and finally once the certificates have been created in the initial elastic search node had been configured we use the Lambda function to shut down the elasticsearch instance that was used to create certificates this Lambda function was configured to listen for completion of the deployment of the elasticsearch cluster which ensures that the setup elasticsearch instance was not shut down prematurely now that the elasticsearch and kavana applications have been properly configured and deployed A new challenge is presented how can the elasticsearch service handle large fluctuations in the volume of telemetry data that's being generated when things are running well Telemetry data will be generated in a predictably steady stream however data generation can multiply significantly when problems arise a key feature of Herald is the ability to ingest and index data in real time and it must continue to do so even under heavy loads however it's unlikely that a single elasticsearch node can handle continuous High spikes and volume in order to handle large fluctuations in Telemetry data Harold distributes the incoming data stream across multiple elasticsearch nodes this offers a number of benefits multiple nodes are available to handle incoming requests whether queries from kibana or data ingestion from logs damaged and APM servers and data distribution and replication across multiple nodes to have multiple nodes of elasticsearch work together though we need to configure them to form a single cluster and to do this first we have to tell this nodes that we need them to form a multi-node cluster and second we have to tell them where to find each other Step One is relatively straightforward and for step two since we are using AWS cloud map for service discovery in the configuration file for each node we can just specify the DNS host names of other nodes so for example in the configuration file for es01 that is to the left of the screen which stands for elasticsearch node number one we just list the DNS host names of node 2 and 3. so now in our updated architecture diagram the nodes are now working together as a single image and now to communicate with this cluster Cabana would for example send a request to one of the elasticsearch nodes and then the elasticsearch cluster with this internal knowledge of node responsibilities and data distribution will determine how to process and respond to that request and now recall that the reason we are setting up multiple nodes of elastic search is because we want to be able to deal with Spike in Telemetry data generation and to further ensure that Harold is ready to deal with this Spike and Telemetry data generation we also have built-in Auto scaling for the elasticsearch cluster and this elasticsearch cluster that we just saw Will scale up when the CPU utilization for the cluster when the average CP utilization for the cluster is 60 or above to avoid data loss though this cluster only scales up and not down or as shown in this diagram here for auto scaling Herald keeps the three initial nodes as is and then it creates a separate Auto scaling group that starts with one node and then it scales up to meet increasing demands and with that we have successfully added elasticsearch and Cabana to Herald the next component we will add is logstash and you might recall that logs cache is used to transform and enrich logs so it also needs to handle the peak loads which implies that we're going to need multiple nodes of logstash um so in Herald we have two log stash nodes with the load balancer in front of them and one good thing with logstash is that these nodes are working independently so we don't need to worry about Service uh we wouldn't need to worry about no Discovery like we did with elasticsearch and with lockstash added here is updated architecture diagram we still have Cabana to the far right of the screen and then we have elastic search cluster just two is left and we see the log stash notes added in here again as we said previously we have two notes of log stash the load balancer in front of them and then to the far left file beats is installed on the application server it collects the logs and then sends them to your last Dash and then log stashed and enriches the data and then sends it to elasticsearch with loss Dash added we have now finished building our logging pipeline which is um upper portion of this pipeline that is shown here so this pipeline consists of filebeat log stash elasticsearch and Cabana we now need to build the tracing and Matrix Pipeline and for that we're going to need four new components and the first two components are the two different types of Agents we need the first Asian is an elastic APM agent and these are specialized agents which are designed to collect raw traces at Matrix data from an application and then the second type of agent is elastic agents which are kind of general purpose agents and we can upgrade their capability by adding more Integrations and this agents can also collect wide variety of data from different services the third component we need to talk about is a fleet server the fleet server provides us a centralized way to manage the elastic agents and then finally the APM server which is a four component its function is to validate the data that the APM agents are sending to it and then it transforms that data into elasticsearch documents before sending it to elasticsearch um there are two ways this four components that we just talked about can be set up for collecting traces traces and metrics um in the first architecture we have a centralized Leaf server and APM server and and in the second architecture we have a centralized Fleet server but with this architecture we have a distributed APM server so Harold uses architecture 2 which has a centralized please server and a distributed APM server so before we go into how this architecture looks like uh we just take a let's just talk briefly about what Fleet server and APM server actually is um at least for the fleece for the fleet server it is really an elastic agent with a special integration and we install this agent on an ec2 and that allows the ECT to act as a fleet server similarly for the APM server again it's an elastic Asian with any special integration and again we install this on an ec2 our application server uh and this elastic agent will then act as an APM server so all these four components that we just talked about the APM engine the elastic agent the APM server and Fleet server are combined to work as shown here so starting from the left we have the application server and in the application server we install the APM agent for a node.js server for example this would mean that you have to install an APM and npm package and inserting a middleware in your code the APM agent then it's going to collect the raw traces and Matrix data and then send that data to the APM server and this APM server we also installed in the application server takes that data from the APM agent and then validates it and then transforms it to elasticsearch documents before sending it to elasticsearch and in the middle part of the screen we have the fleet server and the APM server communicates with the flip server to enroll into a policy and this policy is essentially control the behavior of the elastic agents including the Integrations that this elastic agent is going to have and these policies are stored in elasticsearch so the fleet server will constantly check if these policies have changed and if they have then the fleet server is going to update all the elastic agents that that are enrolled in that policy so using this architecture um with a centralized Fleet server and distributed APM server has its advantages and disadvantages So speaking about the pearls first first this architecture is more resilient as there are more as there are more APM server nodes and second the number of APM server grows as a number of application server grows so the user does not have to worry about scaling the ATM server and then third there is a reduced cost there is a reduced cost and that is because data is collected and transformed locally and also the cost is reduced because we're utilizing users existing application server to act as APN server as well and last also there's reduced latency because data is collected and transformed locally versus in architecture one where data was first collected locally and then sent over to the application the APM server over the network in terms of disadvantages uh the first disadvantage is that it can be it can be harder to manage the growing number of APM server nodes which are distributed across the application servers and second it could be harder to skill uh in general because the APM server shares Resources with your application so incorporating Fleet server and APN server into our architecture diagram we can see here first the fleet servers which are in the middle uh toward the middle part of the screen they are used to manage the elastication in this case the only elastic agent it is managing is a elastic agent which is installed on the app server to act as APN server and then the elastic agent collects data from an APM agent and then sends its elasticsearch with Fleet server and APN server added we have finished building Herald so that was a lower part of this pipeline that is car that is currently shown here and as a recap as to the steps we took the to build Herald the first two components that we added are elastic storage on Cabana and while adding those components we had to solve a couple of implementation challenges and make some design decisions the first one was deploying elasticsearch on a private subnet and deploying Cabana on a public subnet the second issue the second thing was we had to solve the service Discovery issue using AWS cloud map and then lastly we had to encrypt all communication using TLS after adding elasticsearch and Cabana we talked about why we needed multiple north of elasticsearch and the reason we needed them was to deal with spikes and Telemetry data generation and we talked about configuring the elasticsearch nodes to form a single cluster so they work together to do that we had to we had to deal with no discovery and to further ensure that Herald is more uh is more capable of dealing with spice and Telemetry data generation we also built in Auto scaling for elasticsearch cluster next we talked about adding lockstash which is used to your interest logs and then lastly we talked about adding Fleet server and APM server as a reminder Herald uses architecture with distributed APM server and centralized Fleet server so billing Herald was a lot of fun and it involved solving a lot of challenges uh we strongly feel that we have successfully built a robust observability solution however there are still quite a there's still room for a lot of improvements um so for example the first feature we want to add in the future uh in the future is that we want to Auto scale the last dash cluster as well so it is better equipped to deal with spikes and Telemetry data generation second we want to add Kafka in front of logstash to minimize the risk of data loss third we want to implement AWS S3 cold storage and also install a mechanism for log rehydration doing so would lead to lower cost of data storage and enhanced elastic surge performance and lastly we want to Auto scale elasticsearch cluster based on storage and CPU utilization and not just based on CP utilization and with that our presentation is complete we want to thank everyone for listening to us today and if you're still here with us after this long presentation we want to now give you a chance to ask questions that you may have all right looks like we've got one question here [Music] um what was the hardest part about building Harold if you wanted to take that one kaushik sure I think the hardest part was definitely encrypting communication with TLS um because anytime we were adding new components we had to make sure that security settings were correctly configured and then we also had to make sure the certificates existed in the right place so that kind of slowed us down a lot and made made the project much more difficult we've got another question here says this solution appears to depend on Amazon Cloud infrastructure is that correct if so how difficult would it be to build similar Solutions within other Cloud providers um I could take a stab answering that um yeah thanks for the question and yes you're correct this was deployed using Amazon's Cloud infrastructure Amazon web services um and as far as the other question about how difficult would it be to build a similar solution with other Cloud providers it's a great question um and I don't know the answer to that just because we really only dealt with um AWS as opposed to you know something with um makes something on Google or Microsoft's Cloud infrastructure but one thing I can tell you is that it was very difficult to um deploy this because um I think the the elk stack is not necessarily uh meant to be deployed programmatically in a way that doesn't require any input from the user so it was rather challenging to do that so I think just speaking generally from our experience doing this using AWS I imagine that it would be rather challenging to do this on on any other platform which has less to do most likely with the platform that you're deploying on and more to do with the elk stack itself in just some of the challenges that are inherent in trying to piece everything together um automatically so I hope that answers the question a little bit and got another question um what necessitated using two separate pipelines one for logs and the other for traces and metrics why couldn't you have a single pipeline for all of that data um I'll take I'll I'll take this one um it's actually possible for the elastic agent and the with the APM integration are actually sorry with the elastic agent using other types of Integrations for it to also collect log data however the there is an issue that arises that the way it works with elastic is that you can't actually have two separate outputs so you either have to output everything to elasticsearch or output everything to log stash and since log stash is just for logs um we needed something for the traces and metrics data so that needed to go to elasticsearch and then um for the log data we wanted to have the use leverage the capabilities of log stash the data processing capabilities of logstash for processing logs so that's why we needed to build the two separate Pipelines okay we've got another question roughly what percentage of the time spent working on this project was spent on additional research to fill knowledge gaps not filled by LS material I can I can yeah sorry go on okay I'll I'll take this one um I don't know if I can say what percentage but uh I would say there was a lot of additional research I mean I would I would say that LS definitely gave us the fundamentals without without having the fundamentals that we had from launch school from The Core Curriculum and Capstone um I think this project would have been pretty much impossible but it required a lot of extra research in terms of we had to learn AWS cdk we had to learn about how the elk stack worked just in general um but yeah all of the launch School material gave us that Foundation to be able to do conduct that research so you know I guess that kind of answered that question um another question was observability an interesting space to build a Capstone project in were there any pros slash cons that stood out um I can take this one I think in general when we started we definitely had a strong interest and it's kind of hard to pinpoint why that insurance was there I mean the interest was just theirs when we discovered this solution space and the problem space I think the interns definitely grew as we started working through the project um and I think the reason I think one of the reason uh we wanted to build a solution in this space was that we found that first we found that setting up this elasticsearch log stash in Cabana was really difficult and then we found that a lot of commercial Solutions are kind of using that in their a lot of commercial Solutions are built using Alka stack but then if someone like me for example wants to have an automated way of just not offsetting those tools up like there's no current solution that existed so we kind of wanted to build a tool that we know that is good for uh good for the team now and we're hoping the solution can grow with the team as well so I think that kind of lets you uh using the specific solution we showed and also what kind of drove us in a way to build a solution in this space and we've got another question here how was the development workload distributed across the team members and What proportion of the time was spent doing cdk and config uration oh sorry go ahead oh yeah so I was just gonna say um yeah so we you know we kind of took a divide and conquer approach as far as Distributing workload um across the team as far as of course the time spent doing CK and config um uh I don't know that I can say What proportion of time it's definitely a lot um you know cdk Cloud development kit is the main API that we use to build Herald but cdk really is just sort of like a API wrapper around a thousand other smaller apis um so learning cdk really just took a large investment of time because of all of the different resources that we use to deploy Herald and all of the related apis that we had to learn in order to do that successfully so yeah um I couldn't give you a specific number but definitely a significant portion of the time was spent learning cdk and learning how to configure everything to work properly together thanks for the question um another question uh speaking of workload did you guys follow any particular collaboration ideology example agile um I guess I would say yeah like the closest uh I guess agile would be the one that best resembles what kind of how we organized our workflow um we always sort of we what we ended up doing uh this wasn't right away but later on in the process was we ended up setting up a Trello board and we'd organize what the specific tasks that we needed to do and we'd have some tasks that were on Deck other tasks that were in progress other tasks that were completed and then we'd have different members of the team assigned to these different tasks and and we'd be regularly meeting up to you know discuss any issues or problems we'd have with our work work and yeah that was sort of the the how we collaborated in in completing the project okay I guess if there's no further questions then we'll wrap up so I want to say thanks for everybody for coming out and thanks for all the questions 