foreign we're going to talk about canopy and our experience building an open source real-time monitoring framework designed specifically for cdns here's a roadmap for our presentation first we'll give some background on cdns and monitoring next we'll talk about the CDN monitoring space and the solutions that existed before we built canopy then we'll discuss what it took to build our prototype finally we'll wrap up by going over what we did to improve our prototype and arrive at our final architecture before we discuss how canopy works we need to review some basic concepts like cdns and monitoring and what can be gained from monitoring the CDN let's first take a look at content delivery Networks a Content delivery Network or CDN is a geographically distributed network of servers that stores cached versions of web content like HTML Pages images and videos at locations close to end users using cdns can result in two major benefits for web applications first cdns reduce latency two users requesting data from the same server may experience different response times depending on how far they are from that server cdns help to reduce that distance take these users for example instead of every request they make being served by the European origin server a majority of their requests are served by the edge location closest to them reducing the distance the data travels and providing a faster experience for the user the second major benefit that comes from using cdns is they can lower the load on a company's web servers otherwise known as origin servers a request fulfilled by the CDN is a request the origin server didn't have to address now cdns are widely used and companies are dependent on them to provide these benefits as well as a few others how can someone make sure that their CDN servers are in Tip-Top shape serving resources as efficiently as possible this is where monitoring comes in Google describes monitoring as collecting processing aggregating and displaying real-time quantitative data about a system by visualizing aggregated data it's much easier for developers and system administrators to recognize Trends and gain insight into the health of a system but in order to monitor a system we're going to need some data specifically Telemetry data Telemetry data is data that production systems emit to provide feedback about what's happening inside the system the three main Telemetry data types used in software engineering are logs metrics and traces we are concerned primarily with logs and metrics so what can logs tell us logs are timestamped records of events that occur within a software system this image shows a CDN log it has information about the date the event occurred it shows us an IP address it even tells us which HTTP method was used during this event this is where logs shine logs provide a ton of data about one single event when we want more information about our system as a whole and how it's changing over time we turn to metrics metrics are a numerical representation of data they're usually smaller in size than logs have fewer fields and measure a single aspect of the system being monitored metric data can be presented in a dashboard offering a comprehensive view of A System's overall health but what exactly goes on that dashboard which metric should we follow to get the best view into the health of our system Google answered these questions when they identified the four golden signals of monitoring latency errors traffic and saturation taken together these four signals serve as a guidepost for monitoring teams and provide developers with a well-rounded understanding of what is happening in production systems CDN log data can provide us with insights into each of these four signals so why go through the trouble of gathering these logs and metrics and keeping tabs on the four golden signals in the first place well although cdns currently handle an estimated 72 percent of all internet traffic there are in many ways a black box the physical infrastructure that makes up the CDN is operated by Third parties and is largely outside of our control therefore monitoring the logs generated from CDN traffic is one of the only ways to gain observability into this system let's see what it takes to crack open that black box and effectively monitor the CDN the first thing we'll need is a data pipeline data pipelines are systems that process and move data from a source to a destination a logging pipeline is a kind of data pipeline for log base Telemetry data Telemetry pipelines have three key stages emitting shipping and presentation let's look at each stage first is the emitting stage in the emitting stage data is accepted from production systems and prepared for shipment in the logging pipeline next is a shipping stage the shipping stage takes raw log data from the data source and moves it to storage in order to do this there are three necessary steps collection transformation and Storage in the collection step emitted data is gathered batched and prepared to be read next is a transformation step during transformation data is decoded and parsed to extract relevant information last comes a storage step where transform log data is aggregated and exported to a data store where it can be later accessed and queried the final stage in a logging pipeline is presentation this is where log data is queried and visualized through a user interface here users make sense of the data and derive insights for various purposes now I'll pass it off to Michael who will talk about the challenges involved in building a logging pipeline thanks Alex now that we have a better understanding of what a logging pipeline is let's take a look at the challenges associated with building one for CDN data building a pipeline for CDN log data has a unique core challenge cdns emit massive amounts of log data so how much data does this CDN emit exactly let's take a look when a user opens a web page their browser might issue many requests to the CDN why is this a single web page can consist of a variety of different assets such as images JavaScript HTML and CSS files their browser will issue requests to the CDN for all of these resources each of these requests will then hit the CDN layer this means that traffic from a relatively small number of viewers can add up to a large number of requests at the CDN because of this traffic to small or medium-sized web applications can end up generating millions or even billions of CDN logs this presents a challenge to engineering teams at these companies who need to hand handle all this data an example is love holidays an online travel agency based in the UK love holidays States online that they handle more than 30 gigabytes per day of CDM logs that does quite a lot for a medium-sized company to deal with for just one type of telemetry data the large flow of data has important implications for our for our project specifically it affects data ingestion the way we query and visualize data and the storage requirements for our pipeline ingesting CDN log data into a pipeline can be challenging internet traffic tends to be bursty similar to traffic at a major train station which might be busy during rush hour but nearly empty in the late evening the pipeline needs to be able to handle this varying flow without slowing down or backing up an equally important challenge is how to efficiently query and visualize CDN log data monitoring the CDN requires running analytic queries like data Aggregates where we perform mathematical operations such as sum count min max and average over a large portion of the data set this can be slow and expensive to run especially with such large data sets and we need to do all this in real time this poses a significant challenge aside from efficiently running analytic queries a storage solution for CDN logs has three major requirements first it should ideally be able to store and retrieve every log line the CDN emits for debugging and compliance this means we cannot sample the data or only store a small portion of it second it needs to support efficient compression to reduce the size of the data set third it should support fast insertions for the batch log data to review even a small number of users can generate many requests to the CDN resulting in huge log data sets these data sets are difficult to work with and pose various challenges to a logging pipeline however these technical challenges are not the only considerations teams need to keep in mind when choosing a monitoring solution different solutions offer different trade-offs that we need to consider so what options are available exactly let's take a look at existing Solutions available to teams that want to monitor the AWS cloudfront CDN and their various trade-offs these teams have a few options first they could use the cdn's native solution for cloudfront this would be the included reports and analytics page these Solutions are easy to use and don't require teams to send data to a third party but don't easily integrate with other observability data and in Amazon's case don't offer customizable dashboards they could also choose a SAS solution or software as a service like datadog or New Relic SAS Solutions have several advantages they are easy to use integrate with other observability data and feature a customizable dashboards however SAS Solutions are not ideal for teams that have strict data ownership requirements because it requires them to trust a third party with their data they can also be expensive finally they can choose to build a DIY solution DIY Solutions give teams full control over their data and can have robust features however they require substantial developer time and are not easy to use as teams we need to build and maintain their own pipeline for some teams both SAS Solutions and the AWS native solution may not work for their specific use case however they may also not want to devote substantial developer time to building a DIY solution this is where canopy fits in canopy Incorporated the ease of use of a third-party SAS solution with the data ownership and control associated with a DIY approach canopy's architecture is built using open source components that are configured within the team's own AWS account allowing them full control of their data canopy also features customizable real-time dashboards and fully automated deployment let's explore how to use canopy for your team's monitoring needs canopy is designed to be easy to use and require minimal configuration the pipeline can be deployed to AWS from the CLI in one command using canopy deploy canopy's grafana dashboards are used to monitor CDN data in real time and offer visualizations corresponding to the four golden signals this image shows the CDN logs overview dashboard dashboard which is your main landing page for monitoring your thought front distributions the top row shows the overall cash hit ratio as well as information related to errors and total requests there are two other grafana dashboards titled client information and performance which offer different visualizations focusing on client data and performance related metrics canopy also has an admin dashboard displayed here from the admin dashboard users can conveniently deploy and configure pipeline infrastructure as well as monitor the status of pipeline architecture after it has been deployed from the admin dashboard teams can also configure quick alerts with the click of a button quick alerts send email notifications to teams when certain thresholds are met corresponding to the golden signals now I'll hand it off to Jason who will walk us through canopy's core architecture thank you Michael when building a logging pipeline we knew that we needed an emitting stage a shipping stage and a presentation stage and so we settled to build canopy we started by mapping out the core components we needed for each stage of its pipeline during the emitting stage the CDN emits a continuous flow of logs as users make requests to the CDN the shipping stage has three necessary steps collection transformation and Storage for collection we needed a stream to collect and store logs in real time as they flow from the CDA for transformation we needed a log Transformer to transform those logs to a format appropriate for storage for storage we recognize a need for both a log shipper to buffer and batch the transform logs in the database where we insert and store the batch log data during the presentation stage the visualizer queries the data stored in the database and visualizes the results in charts graphs and tables in real time now after pinpointing these core components in a diagram The Next Step that we undertook was to build a working prototype based off of this architecture and it was during the course of prototyping that we encountered a number of challenges the first and most difficult challenge we faced was data storage what database do we choose for storing log data we considered various options elasticsearch a Time series database such as time scale and a columnar database such as clickhouse as Michael outlined earlier our use case has unique requirements due to the nature of CDN log data we wanted to be able to efficiently handle data Aggregates and we also did not want to sample the data set therefore we need to consider a couple of things the level of indexing in order to minimize storage needs and preventing sampling and we also need to consider the type of queries that each database optimizes for ultimately we decided on a columnar database more specifically clickhouse because it offered sparse indexing and it had a column oriented approach to processing data thus optimizing for aggregating multiple metrics over time now that we have chosen clickhouse as our database the next challenge was how do we move log data from the CDN to clickhouse in real time this represents the shipping stage and so we needed stream storage a log Transformer and a log shipper by default cloudfront real-time logs are delivered from the CDN to AWS Kinesis data streams a fully managed service for collecting and storing data in a stream after the logs are stored in the Stream we needed a way to deliver them to our log Transformer one option was to build an application that would read data from Kinesis data streams process the data and deliver it to the next destination however we decide to prioritize development speed and reliability we chose AWS Kinesis fire hose a fully managed service for data delivery it has a minimum buffer interval of 60 seconds and therefore provided us with a near real-time solution for delivering logs and any logs that fail are sent to S3 which is aws's cloud object storage offering finally we needed a log Transformer and log shipper one option was to build these components building a log Transformer and a log shipper in tandem would streamline our overall architecture ultimately we opted to use Vector an open source tool for aggregating transforming and routing observability data since Vector was out of the bus compatible with both fire hose and clickhouse it was a convenient choice to use as a data pipe between both of those components it could also do log transformation and so Vector could fulfill the dual function of both log Transformer and log shipper the next challenge we faced was figuring out the Nitty Gritty of data transformation how to transform cloudflop logs before loading the data into Data into the database on the left we can see that each cloudform log is emitted in the form of a plain text string and it doesn't include any predefined fields with Vector our log Transformer we can use its built-in parsing function to write a custom regex pattern this pattern converts the log to a structured Json object Maps field names to their corresponding values in the log and converts values to the appropriate data type whether it be Unix timestamp integer or float at this structure we have tackled a number of challenges data storage moving data in real time and data transformation after successfully doing so we had developed a working prototype on our local computer but there's one final challenge we wanted to address making it so that anyone could set up and start using canopy and do so as easily as possible and what that meant was deploying canopy to the cloud with data streams and fire hose we can create these resources directly with AWS for the other components which comprise the canopy backend vector clickhouse and or visualizer grafana we needed to figure out their deployment as our first objective we wanted to successfully be able to deploy the canopy backend locally for our local deployment we chose to use Docker as our solution we use Docker to containerize our back-end components and deploy them as containers on a host because data persistence and service Discovery is built in our containers would automatically communicate with each other over a private Network and the data in our database and dashboards in grafana would persist and it would never be lost Docker also integrates well with AWS and so once we got this setup working locally moving the back end to the cloud would be doable and simplified after successfully deploying the canopy backend locally The Next Step was deploying to the cloud and we consider two options Amazon ec2 a virtual private server service service in this case we ran a virtual private server otherwise known as an instance of ec2 and run our backend as Docker containers within the instance the other option was Amazon elastic container service with 4K ECS and fargate are fully managed services and would remove the need for manually managing and scaling our containers and infrastructure that they're hosted on ultimately for Simplicity purposes we opted for the ec2 instance deploying Docker containers on ec2 mimics deploying on a local computer configuring our containers that work with ECS and fargate for data persistence service Discovery as well as network security add substantial complexity and we decide to forego that complexity for creating a working prototype here we present our prototypes architecture starting from the top left then moving to the bottom right can it be automatically deployed for our users Kinesis data streams Kinesis fire hose vector clickhouse and grafana all on AWS infrastructure and with this prototype we had a working solution and its core components are compatible and play well with each other however during the course of our prototyping we came across areas where we felt we could improve upon how can it be could better suit its use case and so now I'll be passing out the Met where he will be discussing how we set out to optimize and refine our current working version of canopy thank you Jason before we discuss the steps we took to improve the core pipeline let's review canopy score objectives when we set out to build canopy two of our key goals were ease of use and real-time dashboard updates we wanted we wanted to create a solution that would be as easy to set up and use as possible we also wanted to deliver a true real-time experience meaning our dashboards would update instantaneously as log data stream did after building our prototype we identified limitations in our architecture to both of these goals our initial prototype relied on AWS fire hose to deliver data from Kinesis data streams to the like Transformer Vector over an encrypted https connection this approach exposed several limitations first https requirements limited ease of use by making configuration more complex specifically it forced our users to create a new domain or a subdomain generate their own TLS certificate file upload the certificate maintain certificate validity and update the DNS records with the IP address of our dynamically generated ec2 instance this place displaced a real burden on the user and made our solution more difficult to set up and use furthermore Firehouse buffers data for at least 60 seconds before streaming it to its destination resulting in unwanted latency latency is also introduced by the https connection between fire hose and vector a three-way handshake plus a TLS handshake means three additional round chips across the wire before log data transfers to vector this is in addition to a three-way handshake from Vector to clickhouse this introduced over a minute of delay before logs could be stored in the database and visualized in grafana this delay prevented canopy from being a true real-time experience to address these limitations we decided to build a custom like Transformer and shipper using AWS Lambda this approach allowed us to simplify the architecture eliminating the need for both vector and fire hose the Lambda function fulfills two critical roles first Lambda acts as a log Transformer replacing vector second Lambda acts as a lot as a log shipper replacing fire hose in addition to simplifying canopies architecture our Lambda based solution offered the following benefits first using Lambda gave us complete control over log buffering we can adjust the buffer to imitate firehose 60 second buffer or achieve a more real-time buffer as low as zero seconds depending on the needs of our users second the Lambda function allowed us to ship logs over HTTP or C or https to an external endpoint on AWS service making configuration much easier as well as providing flexibility and ease of integration for our dockerized backend stack third logs could now be passed and wrapped and routed directly to the clickhouse database from the Lambda function removing the additional latency and network hubs described earlier fourth when our Lambda code throws an error Lambda of course log that details exactly what happened which is convenient for debugging network and infrastructure faults fire hose does not provide these logs making debugging frustrating fifths the Lambda functions ability to process up to 10 000 records per invocation aligned well with canopies need to handle a massive number of logs surpassing fire whose maximum batch size of 500 records while building a custom Transformer shipper and shipper brought several advantages it also introduced new challenges particularly we needed to account for a familiar Network failures display consists metrics and archive failed logs for debugging and compliance in our prototype fire hose handled failed logs for us it routed any batches of logs that failed to be delivered to the database to an S3 bucket for storage because we had replaced fire hose in Atlanta we needed to implement similar functionality ourselves this means we needed the way to Route 5 logs to S3 however this presented a problem fed locks stored in S3 cannot be visualized in our definite dashboards which would result in inaccurate data and metrics for our users specifically in the event of ephemeral Network outages to address this issue we used Kinesis data streams as a buffer for fed logs when a batch of flies cannot be inserted into clickhouse our Lambda function throws an error rather than shut these logs these failed logs immediately to S3 the log data is put back into the stream when it where it can then be retried only after the data reaches a Max maximum retry limits with it then be sent to S3 this ensures that ephemeral Network outages would not result in inconsistent data in our grafana dashboards however not all errors are ephemeral so we still needed a way to move fed logs to S3 for persistent storage after a certain number of free tries to address this issue we created a separate pipeline for handling failed log data composed of a dead letter Q managed by AWS simple queue service and a Lambda function that pushes failed logs to S3 for persistent storage Genesis data streams pushes failed logs to the queue when the batch has failed after multiple retries the Lambda function then reads from the dead letter q and stores the FED logs and is three before clearing them from the queue asynchronously while this approach introduced added complexity it ensured that fed logs would be archived for debugging and compliance needs supporting our core our core use case this improved architecture offered our users a truer real-time monitoring experience and also simplified deployment while still accounting for Network failures with the core pipeline elements in place we turned our attention to quality of life improvements this included add-in support for monitoring multiple cloudfront distributions in parallel as well as creating an admin dashboard for pipeline management let's first discuss multiple multiple cloudfront distributions it is not uncommon for the for developers to have multiple Cloud front distributions in a single AWS account as distributions can have different settings for different domains and web applications to cater to these users we introduced native support for handling multiple parallel distributions within canopy when considering how to implement support for multiple cloudfront distributions we evaluated two main options parallelization and consolidation the first option was parallelization this meant parallelizing or duplicating pipeline components for each cloudfront distribution in the image we see each cloudfront distribution delivers logs to its own data stream and Lambda function but analyzing components could help our pipeline scale by adding extra capacity however we opted to parallelize as few components as possible and instead consolidate log data from different Cloud front distributions into the same pipeline this is because duplicating infrastructure can escalate cause and complexity ingestion logs for multiple distributions into a single Pipeline with simpler and more maintainable the other Improvement we made was building a custom admin dashboard we adopted a three-tier architecture with a presentation layer an application tier and a data tier where a react UI communicates when Express backend server and and an app server and sqlite for storage in addition to the features described earlier in the presentation the admin dashboard was designed to help monitor the canopy pipeline infrastructure it collects and presents metrics from clickhouse and grafana instances enabling users to monitor the health of their deployed pipeline infrastructure it also displays a list of configured cloudfront distributions the data of which is stored in an SQL Lite database this capability provides users with a seamless way to track and manage their various distributions with that we arrive at our final architecture the diagram outlines the flow of logs through canopy's login pipeline first one or more cloudfront CDN instances emit log data which is interested in our pipeline via iows Kinesis data skills on the left side of the pipeline the logs are then parsed and transformed by AWS Lambda and then shipped to clickhouse grafana visualizes the logs stored in clickhouse and sends email alerts to our users when metrics reach certain thresholds on the right side of the pipeline failed logs are processed by the FED locks Pipeline and stored in S3 in conclusion building canopy proved to be an interesting challenge it involved complex research as well as careful planning debugging and testing in the end we achieved our objective of building easy to use fully automated real-time login pipeline for cdl.data however there's always room for improvement and we hope to continue our work improve canopy in the future some of some of the features we would like to incorporate in future iterations of canopy include adding the ability for users to perform custom Transformations and enrichments on logged records improving the quick alerts feature making it more customizable and offering more options for notification settings and we'd also like to add adding configuration options for clickhouse and the ec2 instance and with that we open up our q a section thank you Matt so yeah as much just said we're going to start our q a session thank you all for attending um so we're going to give everybody a few minutes to write questions you can write questions in the Q a comment area um and in order to start off the discussion while we're waiting for your guys questions I'm going to ask Med what do you think was the most challenging aspect of building canopy thank you Michael um personally uh it was uh ews stocks and their inconsistencies and um yeah there were a lot of consistencies inconsistencies we found we were trying to build the when we were trying to automate the uh the deployed deployment pipeline for canopy um yeah I can go into further details but if if somebody's interested I can go in more details uh that's okay thank you Matt we we have our first question that's from Fred Durham and he asks let's start with an easy one how did you come up with the name canopy and actually I think I can answer this question so we thought for a pretty long time about a best name for our project um and canopy is meant to convey a few different things so canopy is as in a rainforest canopy is the interface between the cloud and the forest below it um and that seems like a fitting idea for a CDN monitoring pipeline uh which is sort of the interface between the cloud and your architecture but also um because it's high up in the trees you can observe the forest below and because we are a monitoring pipeline we also felt that that fit uh with with our use case so thank you for the question oh and we have another question so Anthony Alexander asks what made you choose this project over other project ideas in the context of showing off skills to potential employers uh does anybody want to to answer this question maybe Jason or Alex do you want to do you want to give it a shot yeah sure so in terms of the context of showing off skills of potential employers we feel like if we felt like this project actually covered a lot of different interesting areas um on the one hand we had built a admin dashboard and that involved web development and creating an application in typescript with the react front and express backend and a database as well it also involved having to figure out and stitching together a series of tools that will all be sort of compatible with each other at least in our initial prototype and as well with deployments you know the first step we took was to deploy with Docker containers and then actually we had actually initially tried the elastic container service with fargate option first and there was a substantial amount of complexity and we felt that at least a better First Step would be to do the ec2 instance and so it just we felt like it covered a lot of different aspects of development whether whether it be deployments or web developments as well as wrapping a framework together with different tools so thank you Jason we have another question from an anonymous attendee mysterious and the question is seems like the database was one of the bigger decisions can you go into more detail about your database decision um and yeah I think all of us can talk a little bit about this if we needed to but yeah I can definitely I can definitely answer this one so okay choosing a database was a really big decision for our project because we had a very data focused project um the whole idea of our project was to take this massive stream of log data and then process it efficiently and visualize it efficiently and we ended up choosing clickhouse uh which as we mentioned is a column or database for a couple of reasons but one of the most important reasons we considered was its ability to quickly run data aggregate queries uh clickhouse as a columnar database and because of the way it stores data tends to run data Aggregates more quickly and actually there's a couple of real companies in the CDN monitoring I don't know if it's fair to it's not a CDN monitoring space but a couple of real companies that are monitoring the CDN uh use clickhouse so uh Cloud cloudflare for example uh which is obviously a CDN company itself uses clickhouse in terms internally for CDN monitoring um so we were kind of inspired by real companies who had gone that direction I don't know if anybody wants to add any more info any more context to that but thank you for that question okay so we have another I we have another question which is what was it like working as a team uh I think we worked really well as the team uh do do any of you guys want to comment on that yeah sure um I'll agree with Alex I really enjoyed working with Alex Ned and Michael uh within the context of a team we had heard that other teams did there's a there was a high probability of having communication issues or conflict that for us actually it was pretty smooth and and I feel like all of us had a had a respect for other people's personalities we tend to be very introverted and I really liked how each of us sort of respected each other's space to work on their own but also yeah when we did need to work together either in pairs or you know to have discussions we did but the fact that we were able to also respect each other's space to either work on our own or we need to explore a particular path and we we just did not get offended at all when we would have our discussions you know we we were free to sort of express ourselves and say whatever we wanted and none of us would be offended by um each other or be threatened by another person's ideas and so that's that's at least for me that that's something I really really appreciated about about everyone on the team and Michael yeah I believe there's a question before that uh from Nabil oh okay um yeah do you want to read it but sure uh so this question from the bill says from the SWOT analysis perspective what would you consider the biggest external threat to canopies course service in other words what would possibly run the canopy useless uh could you elaborate more on what you mean by is what do you mean by the um abbreviation s-w-o-t and a bill so while we're waiting for that uh we have another another question so thank you guys for all the uh congratulations and great job by the way really appreciate it um I know we probably have some other Capstone uh grads and students so great job to you guys as well keep up the hard work um we have a question from show show cityhara uh who asks what part of the project was the most fun to work on uh does anybody want to say what was the most fun to them maybe Alex because Alex you haven't talked since the since the beginning you want to give this one a go what was the most fun to work on sure I'll I'll try them in here um yeah I would say I mean it's like corny but like I think all of it was was fun I think working with people you know people on the team and then uh our Mentor was was terrific and our uh launch School staff was was incredible so I think it's the most fun part for me was just collaborating with people and realizing that uh you know on a team you can do way more by than than you can do by yourself so Nabil elaborated that it's uh strengths weaknesses opportunities and threats um yeah I think they're I think it's an interesting question um what are the things that could uh bring down our pipeline potentially it's something that we thought about um and as we went into a little bit uh when Med was talking earlier one of the advantages of using Lambda that we talked about was actually that it made uh debugging easier because there were times where if log data isn't inserted correctly if the request isn't formatted correctly like things can go wrong in the logging pipeline um but one of the things that I think we could work on more in the future um is things like load testing um just putting our pipeline under more stress uh we tested it we actually built um a testing program we called it request monster and request monster basically just sends like many many different requests to hit the CDN um and then like the more requests request monster sends the more logs that generates and those logs hit our pipeline um and so we did test the pipeline under a certain amount of load but I think there's probably more optimization we could do we could probably test it under higher load to make sure that our visualizations are actually functioning um to the to the level that we hope them to I don't know if anybody else has any comments on that yeah I can add um a comment to nabil's question uh so when when we set out to build this project we we actually were more interested in the engineering uh challenge uh and and solving it and and creating the solution that uh actually uh provides uh real value to the marketplace and our project is an open source project it's not it wasn't built to be uh commercialize or um uh we didn't set out to like create a startup or anything like that we're more interested in the the engineering challenges within this problem domain and how we can we could tackle them in a way that hasn't been attacked before actually when we were doing research for the existing Solutions uh most if not all of the existing solutions they have a particular they rely on particular sets of uh architecture components like fire hose and as we mentioned in our presentation fire hose does have a lot of drawbacks when it comes to having real-time dashboard updates so in our solution we were we were able to have a more of a a true a truer real-time experience when it comes to the dashboard updates as opposed to existing Solutions in the market however again this is an open source project and we were more focused on the engineering side of of the problem domain as opposed to um commercializing it or or selling a service if that makes sense yeah hopefully that answers your question thank you Matt so we have a interesting question from Vincent uh in the webinar chats what are the new technologies or skills you had to acquire to successfully complete this project or did you work with the ones you already knew and were familiar with this is a great question um I think that we all were exposed to a lot of really interesting Technologies and skills in this project um none of us had ever worked with a just from the basic things like none of us had ever worked with a columnar database before we we had experience with a SQL um and in writing SQL queries uh with postgres um but working with columnar data databases is different um and there was a whole bunch of other things like none of us had a lot of experience with uh AWS specifically so uh like Jason and Med in particular put they learned a lot they did a lot of learning really quickly about AWS uh to do this project um I don't know if anybody has any comments on that okay and we have a question from Rodney who was our project Mentor so shout out to Rodney Rodney matambo he's great uh and Rodney asks now that you're at the end of Capstone how was the journey in retrospect does anybody want to take this question yeah sure I'll take that on uh Michael so it was it was long and tough but definitely worth it you know um it was cool to put this like huge challenge in in front of us when we decided what we wanted to do and uh it took like a lot of like determination to just kind of like chip away at it chip away at it and uh it was cool to see kind of like how the project changed you know like what what what uh Jason was talking about and what Matt was talking about and how the the the project kind of like transformed you know we we really saw those we really made those decisions like we said the the our our user is is gonna suffer if he has to you know maintain his https certificate and everything so um it was it was it was really cool to kind of like be a part of all of those those conversations and and see the the project uh transformed it was really quite the journey in retrospect it's amazing how far we've all come uh yeah totally totally it was a an incredible incredible experience we learned a lot uh we actually uh learned many Technologies uh from the beginning Capstone up to now up to the implementation of the Capstone projects like from from websockets this is uh Service Center events we how to deal with real-time data how to track real-time data um and managing um working with common databases like uh Michael mentioned uh it definitely was a a a worthwhile experience so Nabil has another question which I think is interesting which is are you aware of any services and uh in development or new tech that could replace canopy in the short term uh this is an interesting question because um we designed our project specifically to for the AWS Cloud front CDN kind of for this reason where uh there are actually a lot of existing Monitoring Solutions uh provided by like the CDN provider itself but AWS doesn't provide the same like robust monitoring services natively for plot front real-time blogs um and there's a lot of third-party SAS Solutions available that we've mentioned like datadog and do Relic but there there aren't as many of these robust open source automated deployment solutions for cloudform specifically um so like Med was saying uh we're mostly focusing on the engineering challenges of this product it's possible that a competitor could come along and knock us out so to say um but also it seems like a space that has some real real need uh for cloudfront specifically uh any further questions I hope I didn't miss any sorry if I missed any of these questions thank you all again oh okay Vincent has another one um and I think we can go until five by the way uh we can go until the end of the hour uh and Vincent Vincent asks what would what what would you what would you have done differently now that you've gone through all of it uh honestly for for me I'm not sure I would have done anything so differently uh I think I think the Capstone Staff first of all do a really great job organizing this program um obviously this project is only possible because we are part of the launch School Capstone program and all the instructors and organizers are phenomenal um and I think we all learned a lot along the way so if we were going to go back in time and do it again we would have to learn all these things all over again um so I can't think of anything I would have done differently although because Jason and Med spent so so much time wrestling with AWS docs maybe they would go back in time and not have to wrestle with those AWS Docs well at least for me I'm I'm pretty cool with it um you know like at the develop got to develop some of my um devops skills for this particular heirs in particular with the Cloud development kits and the SDK as well right the software development kit yes that one as well hopefully that answers your question there Vincent's hey well I think um I think maybe we can call it there yeah so thank you all for attending the presentation and thank you all for the great questions Billy there's a lot more questions than I thought they were going to be and they're all really good questions uh thank you guys so much um and hope you have a great weekend and the rest of your day thank you 