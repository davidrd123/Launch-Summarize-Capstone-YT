Welcome to our presentation of Seamless! We are a team of three: Ethan Weiner, Rhonda Young, and Jason White. Let's dive right in and discuss what Seamless is. Seamless is an open-source CI/CD tool that streamlines the deployment of containerized microservice applications. But before we delve into the details, here's a quick overview of our agenda.

In the first section, we'll cover the problem domain. We'll explain what deployment processes are, introduce CI/CD, discuss a crucial trade-off within the CI/CD world, and explore the specific challenges of CI/CD pipelines for microservices. We'll also look at existing CI/CD tools. In the second section, we'll introduce Seamless, take a closer look at its core infrastructure and the tasks it performs, and discuss how it enables users to monitor their pipeline. Lastly, we'll touch on how we built Seamless for performance and scalability.

Now, let's start with the concept of a deployment process. Essentially, a deployment process includes all the steps involved in getting new code from a developer's machine to a production environment. Most deployment processes follow four core stages: source, test, build, and deploy. The source stage involves identifying the code's source, usually through a version control system like GitHub. The test stage runs tests to ensure code quality and functionality. During the build stage, the source code and its dependencies are packaged into a deployable artifact. Finally, the deploy stage involves shipping the artifact to a production environment for end-user access.

Now that we've defined a deployment process, let's discuss the two types: manual and automated. In a manual deployment process, most steps are performed manually by developers, maintainers, or operation staff. On the other hand, automated deployments are triggered immediately upon updates and version control. Automated deployments offer advantages in terms of speed and reliability compared to manual deployments. With automation, code can be delivered to end users more quickly, and the use of a centralized build server ensures a consistent environment for tasks, maximizing reliability.

However, there is a trade-off between speed and reliability when it comes to automated deployments. An immediate switch to a fully automated pipeline may be risky for companies that are accustomed to manual deployments. To optimize safety, companies need to find a balance between automation and manual intervention. This is where CI/CD practices come into play. CI/CD stands for continuous integration and continuous delivery/deployment. It involves combining practices that suit a company's safety and velocity needs. Continuous integration focuses on regularly merging code into a central repository, with the option to automate or require manual review before merging. Continuous delivery extends this by preparing new builds for release, while continuous deployment goes a step further by immediately releasing the new version into production.

Moving on to microservices, we encounter some unique challenges with CI/CD pipelines. Unlike monolithic applications, microservices are distributed across a network and can be deployed independently. This introduces challenges in pipeline management. One approach is to use individual pipelines for each microservice, providing control but making maintenance difficult. Another option is to create a single reusable pipeline, where each microservice passes parameters for testing and triggering. This simplifies pipeline maintenance but may not suit large organizations with many microservices. Testing microservices also presents challenges due to their distributed nature. Unit testing focuses on individual service components, while integration testing involves testing multiple services together as a subsystem. Staging environments replicate production conditions, allowing the testing of the entire system. Balancing these various testing approaches is essential for ensuring reliable microservice deployments.

Now, let's take a look at existing CI/CD solutions and how Seamless compares. Open-source DIY tools like Jenkins and GitLab offer extensive pipeline customization, while commercial solutions like CodeFresh and CircleCI provide some customization capabilities along with ease of use. However, we identified a need for an open-source tool that is easy to use and specifically caters to smaller organizations managing CI/CD pipelines for multiple microservices. This led us to develop Seamless.

Seamless is a CI/CD pipeline designed for containerized microservices deployed to AWS's Elastic Container Service (ECS) Fargate. It offers a standard set of pipeline stages that are ready to use straight out of the box. Furthermore, it allows customization of the workflow. Seamless provides options for testing microservices in isolation and together, along with pipeline monitoring and notifications. Let's delve into the steps involved in installing, setting up, and running Seamless.

To get started, the user installs the Seamless NPM package globally and then runs "seamless init" followed by "seamless deploy" in the CLI to provision infrastructure on AWS. Once this is complete, the user can access the Seamless dashboard to set up a pipeline. The user specifies ECS cluster names for production and staging environments and proceeds to set up a service. Under triggers, the user defines the GitHub events that will initiate the pipeline. The user can also choose to automatically merge pull requests, use a staging environment, and auto-deploy to production. Additional options such as repository URLs and test commands can also be provided. After setting up a service, the user can monitor pipeline run progress on the dashboard. Details of each pipeline stage are displayed, with statuses updated in near real-time. The user can inspect each stage's logs for troubleshooting purposes. The Seamless dashboard enables users to monitor run status, runtimes, stage status, and logs.

During the development of Seamless, we encountered four primary challenges in building the core pipeline. First, we had to determine how to model and store pipeline data efficiently. Next, we needed to implement automatic triggering of the pipeline based on specific events, such as code updates. Managing pipeline tasks was another challenge. Lastly, we had to figure out the most efficient way to execute these tasks to ensure a seamless CI/CD experience.

In conclusion, Seamless is an open-source CI/CD solution tailored for containerized microservices deployed on AWS ECS Fargate. It offers a user-friendly and customizable pipeline workflow and addresses the challenges of pipeline management and testing in microservice architectures. While there are existing CI/CD solutions in the market, we aimed to create a tool specifically for organizations managing multiple microservices. Seamless provides an accessible and flexible solution that enhances speed and reliability in the deployment process. Seamless is an open-source tool that offers customization capabilities and is easy to use, similar to DIY tools. It provides reusable Pipelines. We recognized the need for a tool that is both open-source and user-friendly, which led us to develop Seamless. It is designed for companies with fewer employees who struggle with managing CI/CD pipelines for multiple microservices. These companies require an easy-to-use, open-source solution that allows them to reuse a single pipeline. 

Let's dive deeper into the features of Seamless. It is a CI/CD pipeline specifically designed for containerized microservices deployed on AWS Elastic Container Service (ECS) Fargate. Seamless uses a standard set of pipeline stages, making it user-friendly right out of the box. It also offers customization options for workflow. Users can test microservices individually or together and have access to pipeline monitoring and notifications. 

Now, let's explore how users can install, set up, and run Seamless. The first step is to globally install the Seamless npm package. Next, users can use the CLI to run "seamless init" and "seamless deploy" to provision infrastructure on AWS. Once this process is complete, users can access the Seamless dashboard and set up a pipeline. They need to provide the ECS cluster names for both production and staging environments. From there, they can continue to set up a service. Users have the option to connect a service to the pipeline and specify which GitHub events will trigger the pipeline. They can also decide whether to automatically merge pull requests, use a staging environment, or auto-deploy to production. Additional details, such as repository URL and test commands, can also be provided. 

Once a service is set up, users can monitor the progress of pipeline runs. They can stop or rerun a run at any time. The runtime, duration, and commit data are displayed for each run. Real-time updates on the statuses of pipeline stages allow users to track the completion of each stage and move forward accordingly. Users can inspect each stage's logs for monitoring purposes and troubleshooting if a stage fails. The Seamless dashboard provides a comprehensive overview of run status, runtimes, stage status, and logs. 

Now, let's address the challenges we faced when building the core pipeline. The first challenge was to determine how to model and store pipeline data. We chose to use a data model with several one-to-many relationships in a Postgres database. We also utilized the Prisma orm for schema updates. 

The second challenge was automating the pipeline trigger when specific events occur. To accomplish this, we implemented web hooks. GitHub sends a web hook to our backend server whenever there is a code update. The backend server processes the web hook, retrieves data associated with the service from the repository, and instructs the task manager to start the pipeline. 

Next, we needed a way to manage pipeline tasks, which led us to explore different options. Initially, we used a job queue, but it proved to be insufficient as the complexity of workflows increased. We needed a solution that could handle conditional logic and track the state of the entire pipeline. Eventually, we decided to implement the task manager as a state machine using AWS Step Functions. 

Finally, we needed a way to perform the pipeline tasks efficiently. Tasks in Seamless are JavaScript programs that use execca for running shell commands and the AWS SDK for interacting with infrastructure. We considered using either containers or virtual machines to run the tasks. After researching both options, we chose containers because of their lightweight nature and faster spin-up time. We used ECS on EC2 for executing the tasks, as it gave us the necessary control to run Docker commands inside Docker containers. 

To address the challenge of sharing data between tasks, we utilized a shared Docker volume on AWS Elastic File System (EFS). This allows tasks to access centrally stored data and facilitates parallel execution, enabling multiple pipelines to run simultaneously without conflicts. EFS was chosen over AWS Elastic Block Storage (EBS) due to its ability to mount to many instances and containers, providing scalability. 

With the core pipeline infrastructure in place, we implemented various tasks in Seamless. These tasks include code cloning, quality checks, unit tests, building Docker images, integration tests, and deploying to staging and production environments. Integration tests in Seamless are performed in an isolated test environment, ensuring that production is not impacted. The ability to roll back to previous versions is another crucial feature we implemented, allowing users to revert to stable versions in case of issues. Monitoring features, such as a dashboard, logging service, and notification service, were also integrated into Seamless to catch and report any issues before deployment. 

In summary, Seamless is an open-source and user-friendly CI/CD pipeline tool designed for containerized microservices deployed on AWS ECS Fargate. It offers standard pipeline stages, customization options, testing capabilities, and monitoring features. The installation and setup process involve globally installing the Seamless npm package, provisioning infrastructure on AWS, and configuring the pipeline through the Seamless dashboard. The core pipeline infrastructure addresses challenges related to data modeling, pipeline triggering, task management, and task execution. Seamless utilizes AWS services like Step Functions, ECS on EC2, and EFS to automate these processes efficiently. Tasks in Seamless are implemented as JavaScript programs that run in containers, allowing for scalability and faster spin-up time. The shared Docker volume on EFS facilitates data sharing among tasks, while the ability to roll back to previous versions ensures safety and stability. Monitoring features, including a dashboard, logging service, and notifications, help monitor the pipeline's progress and catch any issues. Overall, Seamless provides an easy-to-use and open-source solution for managing CI/CD pipelines for companies with multiple microservices. The goal of integration testing is to ensure that multiple microservices are working correctly together. There are two approaches to integration testing. The first approach involves testing service A against live production instances of services B and C. This approach is convenient and simulates real-world scenarios. However, it is also risky because any destructive calls made during testing could unintentionally impact the production system.

The second approach is to avoid production altogether by creating an isolated test environment where services A, B, and C are spun up. All testing is done in this environment. This approach adds complexity and requires additional resources, but it eliminates the risk of impacting production. Seamless uses this second approach.

To make this approach work, we utilize Docker Compose, which is a service for container management and networking. Users provide a configuration file that specifies the dependencies for service A, such as services B and C. Docker Compose pulls the necessary containers from a container registry. Once all the services are running, integration tests are executed. If the integration tests pass, the updated service is deployed.

In the event that issues are discovered in production, seamless provides a rollback feature. Rollbacks allow the user to revert to a previous stable version of the service. This is important for the safety of deployments. Each Docker image is tagged with a git commit hash, allowing seamless to retrieve a list of all deployed images. If a bug is found in production, the rollback button can be pressed, and the previous stable version is redeployed. The buggy version is then removed from the process. Rollbacks can be done for individual services without impacting others in the system.

In addition to rollbacks, seamless also includes pre-deployment testing and monitoring features. Any failed tests prevent code deployment to production. Monitoring features include a status update dashboard, a logging service, and a notification service. The dashboard provides real-time updates on the status of the pipeline, and the statuses are automatically updated to reflect success or failure. Users are notified of the updates via email, Slack, or PagerDuty. The logs, which are received from the backend using web sockets, are stored in a Redis cache. These logs are kept for 48 hours before they lose relevance.

To handle a high volume of pipelines, seamless is designed to scale. If users make frequent commits or add microservices, the infrastructure must be able to support multiple runs in parallel. To achieve this, seamless launches new instances of AWS Step Functions for each pipeline run. This allows for multiple runs to occur simultaneously. The backend server, which is a containerized Express server with Node.js, is also designed to scale. By using Fargate and a load balancer, the backend can automatically spin up additional containers as needed to handle the load. This ensures that the backend can handle high volumes and distribute traffic effectively.

With all the components in place, Ethan presents the final seamless architecture. Starting from the top left, when code is updated in GitHub, a webhook is sent to the backend, which is an Express server running on Fargate. The backend retrieves the pipeline data from a PostgreSQL database and sends it to the task manager, which is a state machine implemented using Step Functions. The state machine executes each of the pipeline tasks in separate containers using ECS on EC2. These containers share data using EFS. Finally, the updated service is deployed to staging and production Fargate clusters.

During the pipeline run, the task manager sends notifications through SNS, and the logs from the task containers are stored in a Redis cache. All status updates and logs are sent to the dashboard through the WebSocket API Gateway.

Seamless is currently a CI/CD pipeline designed specifically for node-based containerized microservices that deploy to ECS Fargate. However, the team has identified areas for improvement to support more use cases and functionality. These potential improvements include adding more testing options, supporting additional deployment options, adding support for additional runtimes, and caching dependencies between pipeline executions to speed up the pipeline.

In conclusion, the team has successfully developed Seamless, a CI/CD pipeline that integrates multiple microservices and provides testing, monitoring, and deployment functionalities. The team highlights their enjoyment of working on Seamless, especially the experience of writing infrastructure as code and the satisfaction of working with a dedicated and enthusiastic team. They also express their appreciation for the mentorship provided by Sir John and Rodney.

One question asked about the scaling capabilities of Fargate. The team explains that they used the AWS Cloud Development Kit (CDK) to provision their infrastructure. For the backend server, they used the higher-level construct called Application Load Balance Target Service from the AWS EC patterns library. This construct allows them to define the auto scale task count, which specifies the minimum and maximum number of containers. They also set metrics, such as requests per target, to trigger scaling. When the load increases, Fargate automatically spins up additional containers to handle the demand and distributes the traffic using the load balancer.

Another question asks about running a Docker container inside another container and how to configure the environment for the task container. The team explains that they explored two options. The first option is true Docker in Docker, where a parent container runs Docker daemon inside it. However, this approach requires running the parent container in privileged mode, which poses security risks. The second option they settled on is bind mounting, where the task container's Docker daemon is bound to the parent container's Docker daemon. This allows the task container to perform Docker operations using the parent container's Docker daemon without exposing the system to security vulnerabilities.

The team also discusses their individual highlights working on Seamless. They mention the satisfaction of writing infrastructure as code using the AWS CDK, the experience of working with a dedicated and enthusiastic team, and the sense of accomplishment from overcoming challenges, such as setting up webhooks and interacting with the GitHub API.

In the end, the team acknowledges that Seamless is specifically designed for their niche use case of node-based containerized microservices deployed on ECS Fargate. However, they express their plans to further improve Seamless to support more use cases and functionality in the future. They thank the audience for attending the presentation and offer to answer any further questions. During the Capstone project, we encountered a scenario where security was crucial. To achieve this, we needed to run the parent container in privilege mode, which grants full access to the host machine. However, after conducting further research and reading blogs by Docker developers, we discovered an alternative approach. If the real parent-child relationship is not necessary and we only need to run commands such as Docker build or Docker push, we can avoid exposing the system to potential security vulnerabilities.

Instead, we can use a method called "Bind Mount." Here's how it works: when the Docker engine runs on a virtual machine, it also runs a Docker Daemon or service. This daemon binds to the Unix socket on the host machine. Additionally, each container also has its own Docker Daemon and Unix socket. By binding the internal container's Unix socket to the parent container's Docker Unix socket, we establish a connection where the child container utilizes the parent's Docker statement to perform Docker operations. This transforms the relationship from a parent-child dynamic to a sibling dynamic. In this way, the task container can execute work externally rather than within itself.

Thank you, Jason, for your question. I hope this explanation clarifies the concept. Now, we have time for one more question before we conclude. An anonymous attendee asked a question regarding Team Surgeon or Team Rodney, stating that it was on behalf of a friend. This question refers to the well-known rivalry between Surgeon and Rodney in Launch School. Throughout our Capstone project, we have received invaluable assistance from both Surgeon in Data Structures and Algorithms, and Rodney, our team mentor. As Rodney has worked closely with us as our mentor, we consider ourselves part of the Rodney camp. We are grateful for the support and guidance from both Surgeon and Rodney.

Thanks to everyone for attending this session. We will now conclude the video.