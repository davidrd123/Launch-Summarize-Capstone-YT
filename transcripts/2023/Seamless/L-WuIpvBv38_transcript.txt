foreign so welcome to our presentation of seamless we are a team of three I'm Ethan weiner I'm here with my teammates Rhonda young and Jason White so first off what is seamless seamless is an open source CI CD tool that streamlines the deployment of containerized microservice applications but before we actually get into just what that means let's start off with an agenda so we'll split up the presentation into two sections in the problem domain section we'll talk about what deployment processes are and then we'll introduce cicd and discuss an important trade-off within the world of CI CD we'll also go into the specific challenges of cicd pipelines for microservices specifically and then we'll look into some existing cicd tools in the second section we'll introduce seamless taking a tour of its core infrastructure and looking at the tasks that it actually runs and how it enables users to monitor their pipeline finally we'll just talk a bit about how we built seamless for performance and scale but first off what is a deployment process a deployment process is all the steps involved in getting new code from a developer's machine to a production environment most deploy deployment processes tend to follow the same rough outline of stages so let's take a look at what we're calling the four core stages of a deployment process first off we have the source stage for any deployment the source of the code must be identified and usually it's a version control system like GitHub that's used to collect code from local developers in one central place and this is where the deployment process begins next the test stage runs tests on the code to ensure quality and functionality during the build stage the updated source code is packaged with its dependencies into a single Deployable artifact and finally during the deploy stage the built artifact was actually shipped off to a production environment accessible to end users so to recap there are four main stages of a deployment process Source test build and deploy now that we've talked about what a deployment process is let's talk about the two types of a deployment process manual and automated traditionally humans would manually perform Most states of a deployment process whether that may be running commands or navigating an interface now most deployment pipelines are automated to some degree and whether a deployment process is manual or automated can pretty much affect two major aspects of deployment the first is the speed with which new code can be delivered to end users the second is reliability which refers to the assurance that only high quality functional code is released to end users let's talk about speed first with manual deployments when a developer makes a change they often need to wait for another developer maintainer or operation staff member to actually go ahead and begin the manual deployment process on the other hand automated deployments are triggered immediately upon updates and Version Control and even once the deployment process starts mail deployment processes experience delays between steps themselves this effect is Amplified further if those different steps are being carried out by different teams that must coordinate to carry everything out in the correct sequence however in an automated employment process when one step finishes the next one usually starts immediately Okay so we've discussed how manual deployments are slower than automated ones now let's talk about reliability which means the confidence companies can have that the code they're releasing into production will actually work in production as we'll see in a moment manual deployments are usually less reliable than automated ones so first of all mail deployments can run into environmental inconsistency issues what this means is that a developer's local environment probably not going to be configured identically to the production environment to which code it will be deployed the environments could have say different operating systems maybe different dependencies installed with different versions different environment variables you get the idea as a result code functioning in a developer's local environment doesn't necessarily indicate that it will also function properly in production but with manual deployments developers or operation staff often push straight from their local machines to production automated deployment pipelines address this issue instead of humans running tasks and deploying from their own machines automated deployments uses a centralized build server to automatically run all tasks in a consistent environment on every deployment the build server is kind of like a gate if the Builder test fails on the build server the code won't make it to production another reason for these differences in reliability is that each step in a deployment process is really just a set of Road activities like running commands clicking through an interface or maybe adjusting configuration files build servers of course our machines which makes them better than humans at performing these kind of row activities in a consistent and reliable manner for example like a developer might forget to run a few tests but a build server assuming it's configured properly probably won't okay to recap manual deployments tend to be slower and less reliable than automated deployment Pipelines however as we'll see in a moment there are some caveats to that so we've talked about what deployment processes are and why the industry has shifted toward automated ones now let's talk a bit about a particular trade-off most automated deployment processes need to make we mentioned that automated deployments are safer and faster than manual ones but it's actually always as simple as that our company is used to a manual dilemma switching immediately to a fully automated pipeline can be risky for example if the company maybe a comprehensive test Suite so actually retaining some aspects of traditional manual deployments in an automated deployment pipeline can make them differ for some companies safe mean decreasing the likelihood of bugs entering production in order for a team to dial the degree of Automation in their deployment pipeline to their safety and velocity means there are a few key decisions they can make these decisions are closely related to a set of combined practices called cicd which stands for continuous integration and continuous delivery for deployment let's break these down a bit further and see how each can be used to achieve different goals in terms of safety and in terms of speed so continuous integration is the practice of regularly merging code into the main branch of a central repository after it's built and tested continuous integration is mainly focused on Version Control the goal being to quickly get developers code into the main branch now how continuous A team's continuous integration practices depends largely on their merging strategy teams can have workflows that either automate the merging of branches or pull requests say when they pass status effects or they can require a manual review process by a team member before merging although automating merging can speed up the pipeline it might introduce the risk of potentially merging code that hasn't really undergone adequate testing and review all right now let's talk about continuous delivery continuous delivery extends upon continuous integration by regularly taking the new build and promoting it to a staging environment where it's held for manual approval this effectively prepares the new version for release without actually going ahead and releasing it but with continuous deployment the new version isn't just prepared for at least is immediately released into production the main takeaway from the previous few sections is that companies can enhance safety of their cicd pipeline by mandating human code reviews before merging and also by using a staging environment alternatively they can enhance the velocity of their pipeline by automating merging and continuously deploying so far we've talked about deployment processes what cicd is and how cicd pipelines can be adapted to meet these different speed and safety goals now let's talk about how they can be adapted for two different application architectures monoliths and microservices but first of all let's actually Define the terms monolith and microservice a monolith is a single unit a single code base all of its components exist within a scope of one application a microservices architecture on the other hand consists of more independent Loosely coupled applications that are distributed across the network one significant way in which these two architectures differ is in how they are deployed for a monolith such as just one application that entire application must be restarted whenever it is deployed as a result a monolith always has to be deployed all at once but microservices on the other hand are split into multiple applications so they can be deployed as smaller independent units this also means that each microservice can be deployed on its own schedule it's independence of deployments is actually one of the prize characteristics of microservices however as Ryan will show in a second this exact feature of microservices actually introduces A new challenge when it comes to managing their CI CD Pipelines thanks Ethan two challenges we identified in our research of CI CD pipelines for microservices or the management of deployment pipelines and comprehensively testing microservices in the pipeline let's start with the challenge of pipeline management one way to deploy microservices is to attach an individual CI CD pipeline to each one this many pipeline approach gives teams full control over the pipeline and its steps it can be used by applications with independent microservices managed by autonomous teams however for organizations that are smaller or have more centralized management it can be a nightmare to maintain tens or hundreds of individual pipelines along with their configuration files sharing and reusing pipeline steps can eliminate redundancies across Pipelines but this approach has its own downsides we still have to manage multiple pipelines as well as the shared steps the two previous approaches assume that each microservice must have its own dedicated CI CD pipeline but what if we create a single reusable pipeline when starting a run each service passes its own parameters such as test commands and Trigger type adding a new microservice to the pipeline is simply a matter of providing these parameters this approach can make it simpler to build and maintain Pipelines another challenge with deploying microservices has to do with their unique testing requirements testing microservices can be challenging due to their distributed nature they need to be tested both for their own internal functionality and for how they interact with other microservices we will now look at three approaches used to test microservice applications unit testing integration testing and the use of staging environments unit testing involves testing components within a single service in the diagram service a is tested internally to verify that it works properly in isolation testing does not pass outside the boundary of service a integration testing involves making Network calls to test multiple multiple Services together as a single subsystem this verifies that when a service is updated it continues to interact properly with other services lastly staging environments replicate production conditions and include all the microservices in the system this can be used to test and inspect the whole system as if it were in production so to recap two micro service specific CI CD challenges are managing pipelines and testing them extensively at this point we have introduced deployment processes and CI CD as well as some of the difficulties that come when using CI CD with microservice applications next we will look at some of the existing CI CD Solutions available and how seamless compares to these open source DIY tools may be useful for organizations that require extensive pipeline customization tools like Jenkins and gitlab allow for pipeline reusability but implementing them can be challenging because they typically require knowledge of plugins and Integrations commercial Solutions like code fresh and circle CI may not always be as flexible as open source tools but do generally provide some customization capabilities they are usually easy to use and like DIY tools they offer reusable Pipelines after exploring the existing solution space we felt there was room for a tool that was both open source and easy to use with seamless we wanted to build a tool for a specific user companies with fewer employees that have difficulties managing CI CD pipelines for multiple microservices they are seeking an easy to use solution that is open source and provides the ability to reuse a single pipeline we will now take a closer look at seamless seamless is a CI CD pipeline designed for containerized microservices deployed to aws's elastic container service specifically ECS fargate seamless uses a standard set of pipeline stages which makes it easy to use right out of the box and it offers some customization of the workflow it provides options for testing microservices in isolation and together it also features pipeline monitoring and notifications in the following sections we will explore the steps a user takes to install set up and run seamless first the user installs the seamless mpm package globally next in the CLI the user runs seamless init followed by seamless deploy to provision infrastructure on AWS after this completes the user can access the seamless dashboard to set up a pipeline the user provides ECS cluster names for production and staging environments they can then continue to set up a service here the user can connect a service to the pipeline under triggers they can specify which GitHub events will start the pipeline in the next section they can decide whether to automatically merge pull requests use a staging environment or Auto deploy to production the user can also provide the repository URL test commands and other options after a service has been set up the user will be able to monitor pipeline run progress the pipeline run is in progress and the user can stop or rerun it at any time the Run header shows the runtime and duration as well as the get commit data beneath the Run header the details of all pipeline stages are displayed their statuses are updated in near real time so the user can know exactly when one stage completes and the next stage begins lastly the user can inspect each stage's logs in order to monitor its progress if a stage fails these logs can help with troubleshooting putting it all together the seamless dashboard enables the user to monitor run status and runtimes as well as stage status and logs next we will consider four primary challenges we face when building the core pipeline first we needed to determine how to model and store pipeline data next we investigated how to automatically trigger the pipeline when specific events occur then we had to find a way to manage pipeline tasks and finally we needed a way to figure out how to perform these tasks now we'll give a brief overview of the architecture we used to address these challenges and afterward we will look at them in more detail we started by creating the data model and selecting a suitable database then we set up a back-end server to handle requests and communicate with the database next we built the components to automatically trigger the pipeline when the user updates their code repository GitHub sends a web hook to our backend server the backend server processes the web hook and retrieves data for the service associated with the Repository with this data it then instructs the task manager to start the pipeline the task manager orchestrates the cloning testing and building of the updated code as it progresses through the pipeline the actual work of Performing all tasks is done by the task executors together the manager and the executors form the heart of the pipeline the final task performed is deployment to the production environment this diagram reflects the core architecture of seamless's deployment process from the moment a user pushes a code change when it is released into production next Jason will present the challenges and infrastructure in more detail thanks Ryan I'll begin this section by talking about the data model with seamless we can use a single pipeline for multiple Services the data model reflects this by using several one-to-many relationships each service can be run many times and each run has many stages for the database we chose postgres on Amazon RDS and also use the Prisma orm for schema updates with that in place the next step was to automate the pipeline one of the key features of seamless is that the pipeline automatically starts whenever there's a Code update to make this happen we use web hooks let me walk you through how that works first we create a web hook in the repository using the octokit library once that's set up GitHub will let us know of any code updates then when the user makes an update to the code GitHub sends us a web hook which is a post request with details about the update such as the commit message and the hash finally the backend processes the webhook and starts the pipeline so in the Pipelines needs to coordinate and execute a series of tasks to handle this workflow we needed to find an infrastructure component to serve as a task manager at first we try to job queue and it worked well for linear workflows however as the workflows became more complex the job queue just wasn't flexible enough for what we wanted it to do we needed something that can handle conditional logic like whether to use a staging environment or not we also needed a way to track the state of the entire pipeline so after looking to a few options we decided that the task manager should be implemented as a state machine which is a model of how a system behaves you can think of it as a flowchart from how to get from start to end and if we want to use this to model a system we have to map out all the possible States as well as the conditions for moving between the states and sometimes we need to make decisions which the direction which determine the direction that we go into next now that we understand what state machines are let's see how we can use them to model a cicd pipeline so this is our pipeline implemented as a state machine like the previous diagram it transitions through multiple States between start and end The Source test build and deploy stages are the same ones we talked about earlier and in the middle of this diagram it shows two decisions we have to make which are whether to use a staging environment and whether to approve staging after we've reviewed it so in order to turn this state machine into infrastructure we used an AWS service called Step functions and just as a reminder this state machine is a task manager it doesn't actually perform the tasks themselves its job is just to trigger the tasks and wait until they're done so that it can move on so where are these tasks actually performed well that's the job of the task executors and to better understand what they do let's first go over what a task is seamless a task is a JavaScript program we use execca which is a package for running shell commands we also use the AWS SDK to interact with infrastructure for example during the build task we use execut to build a Docker image and in the deploy task we use the SDK to launch that image into production now that you know that tasks or JavaScript programs we can now talk about the infrastructure for running them we looked at two options containers and virtual machines our initial idea was just to run everything on a virtual machine but after more research we decided to go with containers because they're more lightweight and faster to spin up this is because unlike virtual machines they don't include a guess to list it's going to take up a lot of space and the reason why fast stand up time was so important to us is because different teams have different commit frequencies so we wanted to support a variety of usage patterns after settling on using containers our next step was to figure out how to manage containers on AWS so on the top here is our task manager which needs to execute some tasks in AWS there are two services that can do this for us ECS on ec2 and ECS fargate both of them can run containers but the main differences who manages the virtual machines where the containers are placed with ECS on ec2 the user has more control but with fargate AWS does all the management or are you okay the deciding factor was that we actually needed that additional control some of our tasks run Docker commands inside of Docker containers and we achieved this by having access to the host machines therefore we chose ECS on ec2 for executing tasks now if you take a look at the bottom there you can see that all the tasks are running by themselves however a pipeline is a series of connected steps and it's important for the task to be able to share data with each other and that was the next challenge we had to tackle as the pipeline runs some tasks depend on the output from previous ones for example in the source stage we call in the code and in the test stage we run tests on that same code to avoid cloning twice we store the data in a central location so that all attacks can access it we decided to create a shared Docker volume on the AWS elastic file system or ESS it can connect to any number of virtual machines or containers it also scales automatically so we don't need to set capacity in advance now one problem with a shared file system is naming conflicts if the pipeline multiple times data could be overwritten when cloning the source code and to prevent this we organize the data by commit hat which means that each version of the code gets its own folder and another benefit of doing this is that it enables parallel execution meaning multiple pipelines can run simultaneously and write to the same file system without conflicts this is a critical feature for scaling which we will talk about later as an alternative to EFS we also considered AWS elastic block storage although it would have offered higher performance it's only designed to be mounted to just a few ec2 instances in comparison EFS can mount to many instances and containers which made it the better choice for our use case let's take a moment to step back and see how this core architecture solved the four challenges that we talked about earlier first who created the data model instead of a postgres database next GitHub sends a web hook which Auto triggers the pipeline then we use step functions to manage the task and they're executed using ECS and also use EFS to share data finally the new code is deployed to production now that we've covered the core pipeline infrastructure we'd like to explain how some of the tasks were implemented and here's the full list of all the pipeline tasks that seamless performs or click on the code then we run quality checks and unit tests after building the docker image we have the option of running integration tests and finally we deploy to staging and production and if something goes wrong we can perform a rollback next we'll take a closer look at two of these integration tests and rollbacks so the goal of integration testing is to make sure that multiple microservices are working correctly together we'll review two ways of doing that and which of those seamless uses let's say we just updated service a which is shown in green here and we need to test it with Services B and C with this first approach to integration testing we test service a against live production instances in BNC this is convenient and simulate the real world scenario but it's also risky any destructive calls we make could unintentionally impact the production system so the second approach is just to avoid production altogether we spin up a b and c in an isolated test environment and do all of our testing there while this does add complexity and requires additional resources it eliminates the risk of impacting production and because of the safety benefit seamless uses this second approach so to make all this work we use Docker compose which is a service for container management and networking users provide a configuration file that specifies the dependencies for service a in this example that's D and C which Docker composed polls from a container registry and when all the services are running we can execute the integration test after integration tests passed and we deploy the updated service if we still find issues in production we need a way to revert to a previous version and that's where rollbacks come in if you recall earlier we talked about the safety velocity trade-off using cicd can help us move fast but when we do it might be easier for bugs to creep into production so we need a rollback feature for restoring service if something breaks as shown on the left here seamless can retrieve a ball a list of all the document images the user has deployed and if we find a bug in production we can hit the rollback button and revert to a previous stable version which is then redeployed as part of the process the buggy version is removed in order for rollbacks to work we tag each Docker image with the git commit hash either in the UI you can see a list of available rollback images for this service each service can be rolled back individually without impacting other services in the system so we just covered how rollbacks help when there are bugs after deployment we also want to catch issues and alert users before deployment that's why if any tests fail themeless won't deploy that code to production and to keep track of those test failures and other problems we built some monitoring features we set up a dashboard for status updates a logging service and a notification service here's the dashboard which we saw earlier as you can see under the pipeline and Stage headings statuses start at in progress and are automatically updated to either success or failure status updates are also sent as user notifications via email lack and pagerduty both the logs and the statuses are received from the backend using web sockets so next let's take a closer look at that logging service when it comes to troubleshooting deployments having a real-time log feed is really useful that's why we're using the websocket API to connect the dashboard to the back end this lets the user see updates in near real time as for backend log processing we wrote a JavaScript package which is used by all the tasks this allows us to fully customize the log payloads and also have more control over where the logs were stored and so we needed to quickly store and retrieve large volumes of logs we used a redis cache the logs are only kept for 48 hours because they lose relevance after a while now that we've covered pipeline monitoring with a dashboard and a logging service let's look at some of the scaling optimizations we designed our infrastructure to accommodate a high volume of pipeline if the user is making frequent commits or adding microservices the infrastructure must be able to support multiple runs in parallel we handle this by launching new instances of AWS step function for each pipeline run this diagram shows the pipeline being triggered by different services at different times and multiple runs occurring simultaneously the next Improvement we made was to scale the backend server the back end is a containerized Express server with node is though a single container might work just fine but when the application grows and we have more and more pipelines running it could get overwhelmed though we decided to use fargate and a load balancer this means that when load increases fargate will spin up as many containers as needed automatically scaling to meet demand and distributing traffic with a load balancer this enables the backend to handle High load and that is the last feature we're going to cover today now to bring everything together Ethan is going to present the final seamless architecture all right let's start with the flow from source to production and we'll begin from the top left so first of all one of our code is updated GitHub sends a web hook to the back end which is an Express server running on fargate the back end then retrieves the pipeline data that it needs from a postgres database it sends this data to the task manager which is a state machine simple formatted using step functions now the state machine actually executes each of the pipeline tasks in its own container using ECS on ec2 and all these containers share data with EFS finally the updated service is deployed to staging and production fargate clusters now let's highlight some of our monitoring features during the pipeline run task manager sends notifications through SNS the logs from the task containers are stored in Alaska cash redis and finally all status updates and logs are sent to the dashboard through the websockets API Gateway currently seamless is a cicd pipeline for a niche use case specifically node-based containerized microservices that deploy to ECS fargate we've identified several areas that could maybe approve of the future in order to support more use cases and functionality in particular we'd like to add some more testing options support more deployment options support additional run times and also cash dependencies between pipeline executions help speed up the pipeline and this concludes our presentation thanks for attending and I would be happy to answer any questions so the first question and this is probably one we can all take um what were the highlights of working on seamless and uh maybe Json or Ethan can start yeah I can give that a go um I like to work out seamless yeah I'd say if there was anything that um at least personally I enjoyed the most it was like writing infrastructure as code for the first time um I was I had an interest in the past but uh actually you know writing stuff with What's called the cloud development kit for AWS and writing code that you could then see deployed into you know production environment in this case uh you know one of the things was the state machine that we built it was like sort of a serverless thing with step functions and essentially Define all the logic and code uh deploy and then you know you have it hosted on an AWS account and I think seeing that process was pretty cool um but yeah yeah I could go um of course I had a lot of interesting experiences with the cdk and all the technical side but I really liked working on a team of people who are really dedicated and enthusiastic about building something like in the past I haven't had that experience a lot so this is a very um refreshing and um also very satisfying experience for me just to deal with these two guys in Capstone and then uh I'll guess uh I'll give uh an answer and then we have a couple more questions in coming in after so uh yeah for me I really liked working on the web hook part um of setting up like the web hook and interacting with the GitHub API and you know one of the bigger reasons was is it seemed so intimidating and it kind of felt um you know kind of impossible at first and then uh fortunately like we have Capstone mentors and like our Mentor uh really kind of gave me the confidence to to take a look at it and and to just kind of read about the API and see what I can do and it gave me like a lot of confidence to do that so I really like kind of appreciated that uh uh that extra like uh motivation that I needed so we got a couple more questions this next one uh can you talk about the scaling that fargate allows yeah uh we use the AWS Cloud development kit or cdk sped up and provision all the infrastructure and specifically for the backend service we're using the higher level construct called application load balance Target service from the AWS EC patterns Library and this creates the cluster as well as a load balancer however you need to specifically specify the metrics that it observes and uses to perform the scaling and so on that construct you can define a property called the auto scale task count set the minimum capacity and maximum capacity which basically means how many containers you want at least and how many containers you want at most and as far as the metrics you can use different ones like CPU memory for us we use request per Target asymmetric we set it to a thousand requests and then if it exceeds that it's going to add additional clusters to handle the load we've also set scaling in and scaling out cooldown durations for one minute so it can scale in a gradual way and not leave users hanging okay thanks Jason we have uh another uh technical question that uh I think Ethan's gonna take here and then after a couple more general ones so uh first if a pipeline builds multiple microservices how do you specify the control of whether to continue if one microservice fails yeah that's a good question um so the way seamless actually works in terms of building microservices is the microservice deployments are like entirely decoupled from one another so um essentially we have you know this one-to-one attachment between CI CD pipelines and the microservices so if you run a CI CD pipeline for one of those microservices it doesn't um say redeploy the entire infrastructure it just redeploys that specific microservice there's a way to do that in fargate with like a force new deployment option the cdk that we uh played around with um so say if like that one microservice fails when it's building it will just never make it to deployment but you know the entire production environment will just stay in the same state um so yeah I hope that answers your question Jenny okay we have uh another question probably uh I think Jason could probably take this one uh could you explain a bit more about running uh a Docker container inside of a container uh how would you configure the environment uh for the container that runs a task yeah I'm really glad you asked this question because it's something that we struggled with a lot and so is it correctly observed um everything runs in Docker container and sometimes we need to do Docker build inside of those containers and so there are two options we explored the first one is what we call I guess what I would call true Docker in docker and there is an image on Docker Hub called DND which has Docker Damon installed inside of a Docker container and so what you can do with that is have a parent-child relationship or you have a child container running in a parent container now remember the downside to that is security to have that sort of scenario you need to run that parent container in privilege mode meaning that it has full access to the host and so it's not a good idea so after some more research and reading the blogs from some of the docker developers um there is a second approach meaning if you don't actually need this real parent-child relationship and all you need is to just run Docker build or you know some other sort of doctor Docker push other documents then you don't have to expose the system to those sorts of security vulnerabilities it can do something which is a Buy Mount so I'll go into this a little bit so when you have the docker engine running on your virtual machine it runs a Docker Daemon or service and that binds to the Unix socket on the host machine going one level of inception deeper when you have the container it also has Docker David installed and also has its Unix socket and so what you can do is bind the internal containers Unix socket to the parent container stalker Unix socket and you're basically the child is using the parents Docker statement in order to do Docker stuff and so it goes from a parent-child relationship to kind of a sibling sibling relationship so our task container is performing work aside itself not inside itself I hope that answers the question thanks Jason so we have uh one more question I guess uh I'll take and then we'll close it after so this is from an anonymous attendee and it says team surgeon or team Rodney asking for a friend and so this of course refers to the uh famous rivalry and launch school and like having been in Capstone we've uh like had a lot of great uh help from you know both Sir John and Rodney Sir John and data structures and algorithms and Rodney's been our team mentor and you know for having been our team mentor and worked with us so much I guess uh seamless has to be uh within the team Rodney camp and uh yep so thanks to uh everyone and I guess we'll uh close it out there foreign 