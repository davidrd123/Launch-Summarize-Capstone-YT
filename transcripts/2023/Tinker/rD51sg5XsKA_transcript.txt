foreign hello everyone thank you for joining us today Welcome to our presentation of Tinker it's a project we are all incredibly proud of we've spent the last few months working on it and we could be more excited to share it with you today so thank you for coming let's meet the team first off I'm Jared Naylor and I'm here with my teammateshara and Peter hewa we're a team across two time zones uh showing Peter both from the Pacific Northwest Peter from Vancouver and she'll from Seattle well I am from the flat lands of West Texas a little pre-presentation hype for the team here uh show is an absolute surgeon when it comes to recognizing a problem and then solving it and if there was one word to describe Peter it would be precise as for myself I'm an excellent height man let's talk tinker so first what is tinker tinker is an open source self-hosted back-end as a service or Bass that accelerates development speed through automated configuration deployment and an intuitive table editor we'll get into how Tinker is able to do all this a bit later on but the question for the moment is who is Tinker for Tinker is for the developers that want to avoid all of this these are just a few of the installation and configuration challenges required to set up a back-end server and its database for developers with a fresh idea for a proof of concept that initial enthusiasm could be worn down one configuration at a time ticker is for a developer looking to move quickly from idea to proof of concept when building from idea to proof of concept the development of a web application may be slowed by technical challenges unrelated to the core concept all right now that we've covered what Tinker is and who was for we'll next examine the deployment and configuration challenges that a software developer faces and some services to offload some of those concerns then we'll examine how Tinker tackles some of those challenges and how it fits into the broader landscape of existing Solutions we'll also take a look at the components that make up tinker's architecture the decisions that drove us there and the challenges we faced during its development first we'll start at the very beginning the basic three-tier architecture the traditional architecture for web applications consists of a front end back end and a database the front end facilitates interactions between the client and the application back end the back end receives and processes request from the front end then issues appropriate responses and depending on the request the back end May interact with the database to manage data storage or retrieval as most of you are aware development often begins from the local machine once the developer believes the application is production ready the application must be deployed even for a simple application this process involves setting up a web server and possibly database which involves various technical configurations dependencies and settings developers may find this process time consuming or challenging if they are unfamiliar with the Technologies used when the application is ready for deployment developers have options on how to deploy the backend to a server one approach is to use on-premise where server equipment and space is managed by the company itself another approach is to choose an infrastructure as a service which abstracts away the physical management of the server however what the user gains in convenience through the abstraction they lose in control over their infrastructure and that brings us to tinker's domain back in as a service or a bass similar to infrastructure as a service bass abstracts away the configuration of the backend server it abstracts away configuration complexity such as managing databases web servers application servers virtual machines and containers necessary to keep an application running once again the abstraction does come with a trade-off reduce control and flexibility over the back end architecture so why choose a bass well configuring a back-end server remains challenging and time consuming as we hinted earlier a Litany of configuration steps need to be completed for developers who lack the expertise or desire to handle server-side infrastructure bass allows developers to shift their focus toward front-end development another benefit is in restful web applications API endpoints responsible for handling common crud operations can be automatically generated the developer no longer has to create boilerplate data data endpoints for their front end to issue requests to for these reasons Bas services are popular to get front-end heavy applications deployed quickly if a developer goes in the direction of using a bass an important consideration in deciding which service to use is what type of database the application will use and whether the Bas supports it databases roughly fall within two categories relational and non-relational relational databases share a common core restrictive schemas and utilizing SQL for querying the developer defines the structure of the data through the schema and by applying constraints and by applying constraints to the table's columns this structure enables complex queries such as joins on the other hand nosql is a broad term that covers databases that are schema lists which makes it easier to evolve the data model over time and offers more flexibility to store data in different ways for these reasons nosql may be an attracted may be attractive to an early stage application where the data's final structure is yet undetermined however using a nosql database may present challenges as the application evolves One Challenge is that nosql databases have diverse approaches particularly regarding schema handling which ranges from schema lists to even mimicking a relational database SQL on the other hand adheres to International standards developers already versed in SQL would need to learn a new query language or database specific syntax when prototyping SQL backend is a service users have one less concern to address a prototype can be developed quickly using a nosql Bas but what are the eventual consequences of this design decision one consequence of choosing a nosql database is the difficulty in modeling relationships between entities data often relates to other data and if an application's data is highly relational this could push more business logic to the front end in a SQL database foreign Keys establish relationships between tables combining data through joins can be left to the database engine lastly SQL databases are universally asset compliant but the same cannot be said for nosql data inconsistency and data duplication can become an issue for a prototype using nosql as it matures When selecting a nosql database for short-term gains once you consider the long-term implications that may arise once the application matures in many cases a relational database is considered as the default choice for these reasons we decided Tinker should use a relational database foreign however most bass Solutions use a nosql database due to the flexibility and data storage that nosql allows companies like AWS and Google offer best Solutions such as AWS amplify and Google Firebase with proprietary nosql databases for those looking for an open source solution that uses a relational database their superbase tinker's use case of rapid prototyping with structured data most aligns with the capabilities of Superbass a unique aspect to Superbass is that it simplifies using a relational database through a user interface through the UI users can perform common database operations such as creating tables and adding rows this allows a user to make schema modifications as their application evolves in addition to the abstraction of writing SQL queries superbase also automates back-end configuration Super Bass does however host user projects by default which may be an issue for some users looking for more control Superbass offers the option to self-host but the user would need to deploy the individual open source components that make up Super Bass on dedicated servers in the cloud in this scenario the user avoids the installation and configuration of backends but now I'm still with the installation and configuration of the cell phone C Bass here is a diagram of a few bass services and Tinker in comparison although super bass was the inspiration for Tinker we sought to develop a framework that could mimic their user-friendly dashboard while also streamlining self-hosting on Virtual private servers in the cloud to give our users more control I'll now pass it off to show to talk about tinker's architecture thank you Jared as introduced my name is show and I'll be walking y'all through tinker's architecture and the puzzle pieces that make Tinker possible first off as a high level overview of what Tinker is architecturally Tinker is comprised of two user-facing components the command line interface the CLI and the dashboard UI the CLI is responsible for automated AWS resource provisioning for users to self-host tinker's admin backend and any needed backend servers for their front-end applications all within their AWS environment on the other hand the dashboard UI is a browser-based interface that empowers users to manage each server's database using an intuitive table editor all right let's look at the dashboard UI and the table editor the dashboard allows users to manage all of their backend databases after selecting a backend users can manage their database using table editor shown here this table editor is the heart of tinker's dashboard UI it allows a user to create edit delete tables insert edit and delete rows or data it also allows users to set constraints such as primary Keys foreign keys not null and unique this table editor allows working with a postgresql database extremely easy because it allows a user to create the entire schema of their database without having to write a single SQL statement and for someone not familiar with SQL statements and SQL syntax this table editor can be very helpful for speeding up the development of the front-end applications tinker's command line interface was published as an npm package for easy download and use it is responsible for handling all AWS resource provisioning that make up tinker's self-hosted infrastructure through the deploying create commands users can deploy the team cures admin backend and use your backend servers it handles deletion of those resources when no longer needed through the delete and Destroy commands Tinker was made possible to the Myriad of options and components that AWS offered AWS offered the automation tools and Cloud infrastructure we needed for Tinker to offer a seamless self-hosted backend as a service some of those core components we used were ec2 instances for both Tinkers backend server any new servers the user may need for their front-end applications cloud formation for all of our automation an application load balancer for handling traffic to the ec2 instances cloud formation is the heart of tinker's automated self-hosted infrastructure Tinker CLI uses cloudformation to set up and deploy tinker's infrastructure within a user's AWS environment cloud formation handles the entire deployment a provisioning process of tinker's dashboard DUI ec2 instances load balancer configuration and domain name management all this is made possible through customized cloud formation templates which are sent to cloudformation whenever a CLI command is executed such as deploy create or delete now that we have a high level understanding of tinker's architecture next we'd like to talk about the journey and the challenges we face that ultimately led to the architecture we just explored so first off during tinker's development we needed an API layer to handle communication to the postgresql data layer we needed this API to handle requests from both the dashboard UI and users front-end applications a requirement for this API layer was that it needed to dynamically generate API endpoints unique to its respective database for example if our admin database had a table for projects then the API would have to have an endpoint that allowed us to execute crud operations on our projects table if a user's database had a table for movies then the same thing they would need an API endpoint that allowed them to execute quad operations on their movies table both of these API endpoints needed to be generated dynamically by the API and not hard-coded to fulfill these requirements we chose to use post-dressed an open source web server that automatically generates a restful API from a postgresql database schema by inspecting the database it creates HTTP endpoints that allow applications to interact with database resources using standard HTTP methods and restful conventions postgresst also handles tasks like request routing filtering sorting pagination and access control we considered building the API layer from scratch using a web server framework such as Express and writing logic to dynamically create routes and endpoints however it made more sense to adopt an existing open source solution why reinvent a really good Freewheel for a user application trying to use postgres these endpoints might look something like this an application might try to connect to the people endpoint which connects to the people table within the database and perform a queries on that table using query parameters these query parameters are what gives an application the ability to sort filter paginate or join data Brooklyn table within the table at each endpoint postgresst also supports HTTP myths these HTTP methods allow a user greater versatility when managing their data these HTTP methods give a user crud operations on their data when connecting to their database through the postgres API although postgres solve many of our API needs and became an integral part of Tinker it lacked the same level of flexibility that a custom solution would have offered in subsequent sections we'll explore the challenges that arose from choosing postgresqst so you can imagine that when we went to build our dashboard UI and table editor postgres automatically generated endpoints would prove extremely effective sending requests such as selecting inserting editing and deleting rows or data through the table editor became very straightforward and simple however postgres had a few limitations first it did not have endpoints for data definition language such as creating editing and deleting tables which was absolutely crucial for making a functional table editor second there was no way for us to add custom endpoints to postgres which could have circumvented the first problem so to address these issues we turn to postgresql's stored procedures and postgres remote procedure call or RPC API endpoint so what are postgresql's stored procedures stored procedures are functions within a relational database that can store and execute SQL statements they accept arguments and can return a value postgresql well-stored procedures were absolutely amazing as they allowed us to keep all SQL statements and logic within the SQL database and outside of the front-end code these stored procedures allowed us to create all the necessary SQL statements for creating a functioning table editor some of those stored procedures included creating tables editing table names editing individual columns within those tables and deleting tables the example shown here is US storing a create table function which takes four arguments creates a new table then returns true when the new table is successfully created so now that we know what stored procedures are what are remote procedure calls or rpcs rpcs are a mechanism that allow a computer program to execute code or procedures on a remote server as if they were local functions or procedures in other words rpcs enables communication between different systems allowing them to invoke functions or methods in a distributed manner our PCS are great because they facilitate code reusability resource sharing and scalability in this example the client is sending a requests to the server to invoke the function Foo the server receives this request and executes the function Foo and passes in the given argument the server then Returns the result of the function through back to the client so what would this look like for tinker in tinker's case the dashboard sends a request to postgres RPC endpoint specifying the stored procedure we want to invoke in this case create table postgres dynamically generates RPC endpoints for all stored procedures within its respective postgresql database in this diagram we want to create a new table and send the required arguments foreign postgres takes the required arguments and sends the request to postgresql to invoke the stored procedure create table postgrescue will take the given arguments and executes the stored procedure it then sends the results back to postgres team then postgres returns those results back to the dashboard the dashboard takes those results and updates the UI from this you can imagine that when combined SQL stored procedures and postgressts are PC endpoint open the door to endless versatility and that versatility gave us the needed solution for executing SQL statements not possible with postgres other endpoints as the title might suggest postgres lack of custom endpoints presented us with another problem we weren't able to use postgresst as a means of communicating to AWS cloud formation this is relevant because initially we wanted the dashboard UI to handle the provisioning and deletions of user back-end servers so we now have to figure out how we were going to provision and delete backend servers with cloudformation these are the options we considered to circumvent postgres limitations we could have coded logic into the front end that would allow it to communicate its information directly but that would require tinker's dashboard to handle a user's AWS credentials and we wanted to minimize the risks associated with sending a user's credentials over the network so that option was out we already had a working API so creating a custom API server just for this problem seemed counter-intuitive so that was out we could have provisioned a serverless function using AWS lambdas but this also required sending users credentials over the network so that was out as well which left us with option 4 expanding team pures CLI if we look at this diagram that shows how Tinker is initially deployed we can see that in order to communicate with cloudformation we need a user's AWS credentials and those credentials are handled by aws's CLI when installing and configuring the AWS CLI a user enters their credentials and those credentials are stored in a config file it is from this config file that credentials are pulled from when we deploy Tinker through cloud formation Tinker never handles the user credentials directly and we wanted to keep it that way for security purposes so expanding the CLI to handle other AWS provisioning seemed a logical choice so we can see here that after expanding the CLI it now handles all AWS provisioning and deletions the user credentials are still only handled by AWS components this worked out great for two reasons first by clearly separating the responsibilities of the CLI and the dashboard UI we were able to consolidate all AWS provisioning related code and cloud formation templates into the CLI and keep the front end code free from any resource provisioning logic second we avoided potential security concerns when embedding a user's AWS credentials within Tinker we took the security of a user's AWS credentials very seriously and wanted to avoid handling them within any part of the group however despite the added benefits of expanding the cli's functionality there was a trade-off the dashboard would now no longer be able to manage user back-end provisioning and would now be strictly limited to data manipulation and definition this made the dashboard UI less intuitive as users would now be required to create and delete backend servers through their terminal Peter will now take it from here again to you with our next puzzle piece thank you Cheryl a challenge we faced while developing Tinker was database security since users postgresql database and postgres API are public facing that raised concerns for us due to their lack of default security measures to overcome the security concern we leverage Json web tokens for authentication and postgresql roles for authorization authentication establishes the user's Identity or who they are while authorization checks whether that user has permission for their request postgresql uses roles to manage privileges they allow administrators to Define permissions assigned to roles by doing so permissions control what a role can perform in this example a read-only role has permission to read tables but does not have permission to create tables insert data or delete data Json web tokens are used to transmit secure information as a Json object for Authentication they consist of three parts separated by periods a header a payload and a signature the header contains the tokens type and cryptographic algorithm used for its signature the payload contains information about the user the signature is created by encoding the header payload and a secret key with the cryptographic algorithm postgres verifies the signature to determine the token's authenticity when a request is made to postgresst it decodes the Json web token using its configured Secret if the Json web token is valid postgres assumes the claim role in postgresql if the claim Rule and the Json web token does not exist in postgresql or if the secret used to encrypt the Json web token does not match postgres secret the authentication process fails and the request is rejected when connecting to postgresst without a Json web token postgres will connect the user to postgresql's Anonymous role which has predefined limited privileges in tinker's case the anonymous role is only privileged to handle user logins with logging in the user provides their credentials with that a Json web token in the request postgres assumes the anonymous role the anonymous role has permission to check the provided credentials with the sort credentials if they match the login is successful another challenge that we faced was implementing https for tinker as part of the initial ticker deployment the user inputs the region information and the domain they want to use during deployment a randomly generated Json web token secret is created postgres servers are configured to use this secret to decode Json web tokens from incoming requests Additionally the secret is displayed as an output for the user users include the secret in the sign up form when creating a new account the user only needs the secret for sign up but won't need it later for login when submitting the sign up form the front end generates a Json web token using the provided secret and includes init the signup request to the backend upon receiving the signup request postgres decodes the Json web token and stores the new user's credentials and the admin database now users can sign in from different clients without needing the secret whenever a user signs in postgres verifies the provided credentials with the store credentials and the admin database upon successful login postgresql mints a Json web token containing the admin role future requests to postgresst include this Json web token which authenticates them as admin users and authorizes them to interact with postgresql databases an issue identified with the sign up Flow Design is that postgres does not Implement https that meant that Json web tokens and user credentials and requests are exposed to potentially malicious actors Json web tokens are encoded not encrypted this means that the Json data can be seen by anyone intercepting the requests we needed something Upstream of postgres to handle https traffic we chose to use an application load balancer or ALB responsible for handling all https requests to Tinker components and routing HTTP Downstream to establish an https connection a TLS handshake first takes place during this process the server sends its TLS certificate to the clients the client then verifies the authenticity of the certificates another approach was to create a certificate for each user back-end ultimately this is more complex than centralizing the TLs certificate in the application load balancer another challenge we faced was how to handle migration when a user is looking to migrate off of Tinker although there are many benefits to using a back-end as a service there is a living possibility that the bass chosen will end their services or the user may need to customize their backend such as setting data retention policies or choosing geographically where data is stored for example needing to comply with data sovereignty laws which requires storing data in a specific location but that location may not be supported by the managed backend as a service which requires user to migrate their application and these are reasons why we chose ticker's infrastructure to be self-hosted in a user's AWS environment however even for a self-hosted solution like Tinker we recognize there are still reasons why a user may want to eventually migrate off tinker for example if a user wants to migrate from AWS to different cloud service or switch to an on-premise servers we wanted to minimize that pain in cases like these postgresql's PG dump command can backup data and schema these files can then be used to recreate the user's database during migration in a new environment the final challenge we faced was automating deployment during the initial deployment of the admin backend and for user back-end deployments nginx postgresst and postgresql must be installed and configured this is done using cloud formation templates which Define the AWS resources that get deployed and also the configuration of them this means that templates include Linux commands to install configure and run nginx postgresped and postgresql the problem that we ran into through this approach was that the cloud formation templates included a long list of Linux commands that were difficult to understand as a template grew its maintenance became increasingly cumbersome to streamline the deployment process we utilize docker Docker can package applications and their dependencies into isolated containers these containers can be deployed and run consistently across any underlying Linux based host in short it simplifies the process of installing and configuring applications we leverage Docker to run nginx postgresst and postgresql in containers since Docker containers are isolated and postgres needs to communicate with postgresql we use Docker compose to create a network that connects them within a Docker compose template services are defined for nginx postgresst and postgresql by adopting Docker we simplify our cloud formation templates rather than cluttering the templates with numerous installation and configuration commands Docker containers encapsulate all these steps this extracted the logic from the cloudformation template and placed in an organized location the docker compose template Docker added an additional layer of complexity which impacted both our development of tinker and its users these are seeking to fine-tune their postgresql configuration now have to understand Docker and Docker compose for example changing settings such as setting how much memory postgresql uses the cache size the maximum concurrent database connections what database activities are locked and where stored these tasks are now less user friendly since users are now required to go into a containers command line which increases the complexity of the process although we achieved our objectives for Tinker we identified opportunities for future improvements there were two features we were interested in developing one we'd like to incorporate an SQL editor directly into the dashboard UI this feature would give the user full SQL functionality second we lack the ability to import data from a CSV file and also export data to a CSV file additionally we like to provide a user-friendly database dump through the dashboard UI and that concludes our presentation thank you for taking the time to hear about tinker's Journey we'll now answer any questions you have and Jared will moderate that discussion yep long time listener first time caller just to push your questions to the chat or the Q a and we'll do our best to get them answered for you what was your favorite part of working on Tinker uh I guess I could take that one hour maybe we can we can kind of go around the horn here I I really enjoyed building out the dashboard I think that was really interesting um I think like tackling like infrastructure concerns you know kind of for the first time for a project this size was was really interesting too so between those two things but I'm definitely leaning towards the the dashboard just because you get the uh the visual payoff once it's complete uh what do you think Joe I think one of the most satisfying moments was uh when the dashboard connected to the back end and all the automation worked and the https worked and everything was working the way it was supposed to and just that moment was like a Hallelujah moment so I think one of the the greatest I think my one of my favorite parts of working on tinker and as a group was the camaraderie that was involved and then the satisfaction that came once everything was was finished I really enjoyed working on the the front end part um majority of my work was on the front end and I really enjoyed that as well but I think yeah I think for the most part the most enjoyable part was working as a team and then just having that finished product that satisfaction of we made this it works it's awesome so yeah that's a great answer I'd like to add camaraderie to my answer all right so I'm adding that Peter what do you think yeah I mean I don't want to repeat what show just said but I really enjoyed uh working with uh like a really excellent team I really enjoyed uh you know working on a project that was difficult but with team members that were uh really highly motivated and all share a very high level of technical competence so I would say like the coding part wasn't so much the challenge um and wasn't really more of like the interesting discussions we had I think I think a lot of the interesting discussions we had were like you know evaluating different choices and considering the trade-offs um so so yeah I I really had fun uh working working as a team awesome another excellent answer okay so we do have one in the Q a chat here so uh this is sort of posed to all of us because it's not too specific to the project I think it's just more about like the development of it um did you find any particular AI tools useful in researching slash building a project to this complexity and great job you guys looks awesome thanks Josh uh I guess I think oh yeah sorry guys no Peter go ahead oh yeah I was gonna say I I think that um yeah there's been definitely uh you know a lot of of Buzz around AI in the past year um and we're definitely not strangers to using AI tools um so so like chat gbt I think is a is an excellent resource when it comes to to doing research um or making some like boilerplate code to understand something but I think it does have to be like a nuanced take on that um because a lot of times you know you might ask questions to an AI but the answers you get may be not entirely correct or um sometimes it's just flat out wrong so so I think it can be definitely leveraged to to do research um as a tool for your for your coding but um it definitely needs to be taken with a grain of salt sometimes so so I would use it as like one tool in my tool belt but not exactly something I would rely on to build out you know the entire application so to speak for sure for sure yeah you got to see it through the critical lens because sometimes you can just take one look at the response it's not even close to what you were looking for um but you know sometimes it's incredibly valuable so that is that's a good answer there okay we've got another one here um this is about working with the AWS components uh so Peter I think this this might be for you uh what was it like to learn and Implement AWS components for your project sure um I would say that you know AWS was definitely like a very daunting thing for us at the start of this project because we had high level understandings of what some of the components were um but but really to to build something of the scale using AWS we we felt a little bit um nervous for sure because there are so many AWS products and and the documentation is is quite difficult to parse so our learning process for that was um to read as much documentation as we could you know leveraging things like stack Overflow um watching tutorials and videos and I think having the Hands-On practical aspect of this project um having to you know play around with AWS try things that really helped to solidify the theoretical knowledge you get from from Reading documentation because ultimately you know you can read as much documentation as you want but I think the real learning theater was just kind of playing around tinkering seeing what works seeing what doesn't um and so so that was kind of our our learning process for AWS anchoring nice plug nice plug there um yeah any other questions push them to the the chat or the Q a okay here we go so how did you come across this project idea I think we can pose that one to the group here yeah show hitter um so the beginning of our research phase for a project um idea we were in like all over the place we were all over the place um and then Peter found Super Bass which was the the idea behind tinker and I I fell in love with Super Bass um I I really enjoyed working with um postgresql uh back in uh core with Ruby but one one caveat ahead with that was like I really hated working with SQL through the terminal like it was a really big pain and the look was horrible and I when I found or when Peter uh told me about Super Bass I absolutely fell in love with the table editor and and for me that was the the biggest pulls of of being able to you know look at data a relational database with a table editor and then being able to alter and manage that data through the table that it was was absolutely mind-blowing to me um and then and then from that we expanded into looking into back end as a service um but I think Super Bass and their their product and their take on how to manage um data and relational data I think was was a really big police in my mind it was and so we just we just rolled with that for sure yeah I'd agree with you there sure um there's like would Peter first introduced us to Super Bass it was just sort of undeniable for all of us you know because we had spent so much time in like the SQL course or um you know like early on in like the uh building out a web application so we were very familiar with like working from the command line and to see everything just streamlined in like a really nice looking dashboard UI and then the fact that you could manage your own projects it just it was such like a natural direction for us to take um yeah looks like we got looks like we got another one here in the chat um so this one could be posed to the group again so what were some of the most diff difficult designed design decisions I'm sorry okay so what were some of the most difficult design decisions I think uh I think one that comes to mind for sure is um is is deciding how to provision the uh the user back ends um uh so our initial vision for that was to to do that from the the dashboard um and that was something that I think Super Bass also does so there's like uh there's a button you can click to to spin up a back end um and we felt like yeah that was you know very user-friendly very user intuitive um and well definitely possible uh We've ultimately found that you know there was a lot of caveats to that particular design Choice um so so we looked at a lot of different options um ultimately we went with uh creating user back-ends from the CLI uh which is a little bit less user intuitive but also I think had a very nice separation between the responsibilities of the dashboard which uh manipulates the postgresql database and data and the CLI which manages all of the AWS resource handling so so in this case there wasn't really like a like a clear-cut answer of which direction to go um which made it challenging and so ultimately we kind of had to weigh the pros and cons of each and decided to go with um with with all of the AWS management through the CLI hmm definitely yeah because the original Vision you know we wanted to have it all go through the dashboard um but at a certain point you know mess around with credentials and passing them around that's just not something that we really wanted to do um yes it looks like we got another question here um so how did you handle the division of labor and what did you learn about working as a team that you feel helped things go more smoothly it's a good question uh yeah I mean we can kind of go around the horn with this one if that's right with you guys I feel like we did a lot of things in sort of like two prongs because we were a team of three so like whatever whatever was the most necessary necessary at the moment then we kind of put two on it and then the other person would be building out like sort of like working in parallel so um just to give an example like Peter's very organized you know so you had some good ideas for the infrastructure from the very beginning and shows very good with design so you had some really good ideas for um building out the front end or that dashboard UI so we're kind of I was kind of the deckhand yeah I'm going back and forth a little bit between those two um but then you know if we needed more Hands In One Direction or or less hands in the other end we would just kind of like shifted responsibilities depending on what the uh I guess like what what the most urgent need was um you know when there was a time we were having issues I guess not issues but getting everything up and running like the the infrastructure then all the focus was on that um when there was things that was less urgent or less challenging then we could kind of divide the responsibilities between those two sides of our project yeah and I can contribute to that as well um so so like Jared said I think in terms of like uh division of labor um I think uh it kind of fit nicely in terms of like to what tasks kind of played into you know certain people's strengths so um you know a show is very good a design so in terms of like doing the the dashboard the front end doing a lot of the the graphics uh definitely played to his strengths um so so ultimately a lot of times it's like hey I'm really interested in doing this um so then that person would kind of tackle that uh that particular area um we did this sometimes like collaborate you know work um three people on one problem sometimes if if it was a really challenging thing like uh working with AWS um and in terms of like helping or things that helped us to work as a team smoothly I think that communication is definitely a big part in in terms of how we were able to pull this off um just being able to you know talk to each other on slack every day and having just uh we had two team meetings so one in the morning kind of like discussing what our plan is for the day kind of map out uh our activities for that day and then a second meeting like in the middle of the day to kind of recap what we did uh what we're gonna do this afternoon and potentially kind of plan for next week so I think being in constant communication um really helped and and I I prefer like over communicating versus Under communicating so so if there's any issues or things um you know that um were were trolling us you know to just kind of you know bring it up in an open environment so so I really enjoyed that uh having like a very open Communication channel with with the team for sure for sure yeah and I think we all kind of made it a point to develop a relationship early you know we're going through prep we started meeting up a few times a week and then just throughout Capstone we were doing those meetups meetups twice a day and just everybody's always in the loop always connected so I think that was that's a great Point Peter and it was really helpful all right are there any more questions in the Q a or either to the chat we'll give it another minute and then uh Fold It Up okay all right yeah it looks like that is all the questions for us today so we just want to thank everybody again for for attending our presentation um again we're really proud of it so we're happy to share it with y'all 