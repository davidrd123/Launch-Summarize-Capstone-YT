Welcome to our presentation of Tinker, an open-source self-hosted backend-as-a-service (BaaS) project that we have been working on for the past few months. We are incredibly proud of this project, and we are excited to share it with you today. Thank you for joining us.

Let's introduce the team. I am Jared Naylor, and I am here with my teammates, Shara and Peter Hewa. We are a geographically distributed team, with Peter from Vancouver, Shara from Seattle, and myself from West Texas. Shara is excellent at problem recognition and solving. Peter is precise in his work. And I am here to provide enthusiastic support.

So, what is Tinker? Tinker is an open-source BaaS that accelerates development speed through automated configuration, deployment, and an intuitive table editor. It is specifically designed for developers who want to avoid the installation and configuration challenges of setting up a backend server and a database. These challenges can slow down the development process and wear down the initial enthusiasm of developers with fresh ideas.

To overcome these challenges, Tinker provides a self-hosted backend-as-a-service solution. This allows developers to quickly move from idea to proof of concept by abstracting away the complexity of backend server configuration. With Tinker, developers can focus on front-end development and let the BaaS handle the backend infrastructure.

Now, let's dive into the deployment and configuration challenges that developers face. When a developer believes their application is ready for production, they need to deploy it on a server. This process involves setting up a web server and possibly a database, which requires technical configurations, dependencies, and settings. This process can be time-consuming and challenging for developers who are unfamiliar with the technologies involved.

One solution for deployment is to use an on-premise server, where the company manages the server equipment and space. Another option is to use an infrastructure-as-a-service (IaaS) provider, which abstracts away the physical management of the server. However, this abstraction comes with a trade-off in control over the infrastructure.

This is where Tinker comes in. Tinker is a backend-as-a-service (BaaS) that abstracts away the configuration of the backend server. It simplifies the process of deploying a backend by automating the configuration of databases, web servers, application servers, virtual machines, and containers. With Tinker, developers can shift their focus from server-side infrastructure to front-end development.

Why should developers choose a BaaS like Tinker? Setting up a backend server remains challenging and time-consuming. With a BaaS, developers can quickly deploy their front-end heavy applications without getting bogged down by backend configurations. BaaS services are especially popular for rapid prototyping and getting applications deployed quickly.

When choosing a BaaS, developers need to consider the type of database their application will use. There are two main categories: relational databases and non-relational (NoSQL) databases. Relational databases have strict schemas and use SQL for querying. On the other hand, NoSQL databases are schema-less and provide more flexibility in storing data.

Most BaaS solutions use NoSQL databases for their flexibility and ease of use. However, Tinker is unique as it uses a relational database. The team decided to use Superbase, an open-source solution that simplifies the use of a relational database through a user interface (UI). Superbase allows users to perform common database operations such as creating tables and modifying the schema through the UI. It also automates the backend configuration, providing a user-friendly way to work with a relational database.

Tinker's architecture consists of two user-facing components: the command-line interface (CLI) and the dashboard UI. The CLI is responsible for automated provisioning of AWS resources for self-hosting Tinker's admin backend and any necessary backend servers. The dashboard UI is a browser-based interface that allows users to manage their backend databases through an intuitive table editor.

The table editor is the heart of Tinker's dashboard UI. It enables users to create, edit, and delete tables, as well as insert, edit, and delete rows of data. Users can also set constraints such as primary keys, foreign keys, not null columns, and uniqueness. This table editor simplifies working with a PostgreSQL database by allowing users to create the entire schema without writing SQL statements.

Tinker's CLI is published as an npm package, making it easy to download and use. It handles all the AWS resource provisioning required for self-hosting Tinker's infrastructure. It supports commands for deploying, creating, deleting, and destroying resources. AWS provides the necessary automation tools and cloud infrastructure for Tinker to offer a seamless, self-hosted BaaS solution.

CloudFormation is a key component of Tinker's automated infrastructure. It handles the entire deployment and provisioning process, including setting up EC2 instances, load balancer configuration, and domain name management. Tinker's CLI uses custom CloudFormation templates to deploy the dashboard UI, EC2 instances, and load balancer.

During the development of Tinker, the team needed an API layer to handle communication with the PostgreSQL data layer. They chose to use PostgREST, an open-source web server that generates a RESTful API from a PostgreSQL database schema. PostgREST automatically creates HTTP endpoints based on the database schema, allowing applications to interact with database resources using standard HTTP methods.

PostgREST handles request routing, filtering, sorting, pagination, and access control. It simplifies the process of building API endpoints and reduces the need to write custom code for CRUD operations. By using PostgREST, Tinker could dynamically generate API endpoints unique to each database, allowing users to interact with their data effectively.

While PostgREST solved many of Tinker's API needs, it had limitations when it came to data definition language (DDL) operations, such as creating, editing, and deleting tables. To address this, Tinker leveraged PostgreSQL stored procedures and remote procedure call (RPC) API endpoints.

PostgreSQL stored procedures are functions within a relational database that can execute SQL statements. Tinker used stored procedures to handle the DDL operations required for a functioning table editor. By keeping all SQL statements and logic within the database, Tinker was able to simplify the front-end code and provide a seamless experience for users.

In conclusion, Tinker is an open-source self-hosted BaaS that accelerates development speed by automating configuration, deployment, and providing an intuitive table editor. It is designed for developers who want to avoid the challenges of setting up a backend server and database. Tinker uses Superbase, an open-source solution that simplifies working with a relational database. It provides a user-friendly dashboard UI and a command-line interface for provisioning AWS resources. Tinker's architecture utilizes CloudFormation for automation and relies on PostgREST and PostgreSQL stored procedures to handle API and DDL operations. With Tinker, developers can quickly move from idea to proof of concept, focusing on front-end development while the BaaS handles the backend infrastructure. When working with a user's database, it is important to have a table for movies, as well as an API endpoint to execute CRUD operations on that table. To meet this requirement, we decided to use PostgREST, an open-source web server that automatically generates a RESTful API from a PostgreSQL database schema. PostgREST inspects the database and creates HTTP endpoints that allow applications to interact with the database resources using standard HTTP methods and RESTful conventions. It also handles tasks like request routing, filtering, sorting, pagination, and access control.

While we considered building the API layer from scratch using a web server framework like Express, it made more sense to adopt an existing open-source solution. Why reinvent the wheel when PostgREST fulfills our API needs efficiently? Using PostgREST as the API allowed applications to connect to the movies endpoint, which corresponds to the movies table in the database. Applications can then perform queries on this table using query parameters, which provide the ability to sort, filter, paginate, or join data within the table.

In addition to supporting standard HTTP methods, PostgREST also provides CRUD operations on data when connecting to the database through the PostgreSQL API. This made it convenient for users trying to use Postgres for their applications, as they could send requests such as selecting, inserting, editing, and deleting rows or data through the table editor. PostgREST's dynamically generated endpoints made these operations straightforward and simple.

However, PostgREST had a few limitations that we needed to address. Firstly, it did not have endpoints for data definition language operations like creating, editing, or deleting tables. This was crucial for making a functional table editor. Secondly, there was no way to add custom endpoints to PostgREST, which could have helped circumvent the first problem. To overcome these issues, we turned to PostgreSQL's stored procedures and Remote Procedure Call (RPC) API endpoint.

Stored procedures in PostgreSQL are functions within a relational database that can store and execute SQL statements. They accept arguments and can return a value. These stored procedures allowed us to keep all SQL statements and logic within the SQL database, separating it from the front-end code. We created stored procedures for various tasks, such as creating tables, editing table names, editing individual columns, and deleting tables. For example, we created a stored procedure for creating a table, which took four arguments, created a new table, and returned true when successful.

Remote Procedure Calls (RPCs) are a mechanism that allows a computer program to execute code or procedures on a remote server as if they were local functions or procedures. RPCs enable communication between different systems, allowing them to invoke functions or methods in a distributed manner. In Tinker's case, the dashboard sends a request to the PostgREST RPC endpoint, specifying the stored procedure to invoke. PostgREST dynamically generates RPC endpoints for all stored procedures within the PostgreSQL database. To create a new table, the required arguments are sent to PostgREST, which then sends the request to PostgreSQL to invoke the stored procedure. PostgreSQL executes the stored procedure, sends the results back to PostgREST, and PostgREST returns those results back to the dashboard.

Combining SQL stored procedures and PostgREST's RPC endpoint provided us with endless versatility. This allowed us to execute SQL statements that were not possible with PostgREST's predefined endpoints. However, the lack of custom endpoints in PostgREST presented another problem—we couldn't use PostgREST to communicate with AWS CloudFormation. Initially, we wanted the dashboard UI to handle the provisioning and deletion of user backend servers. To address this limitation, we expanded Tinker's CLI (Command Line Interface).

During the initial deployment of Tinker, the user would input the region information and the domain they wanted to use. A randomly generated JSON Web Token (JWT) secret was created, which was used by PostgREST to decode JWTs from incoming requests. The secret was displayed as an output and included in the signup form when creating a new Tinker account. This allowed users to sign in from different clients without needing the secret. Postgres would verify the provided credentials with the stored credentials, and upon successful login, a JWT containing the admin role would be minted. Future requests to PostgREST would include this JWT, authenticating the user as an admin and authorizing them to interact with the PostgreSQL databases.

However, one issue identified with the signup flow design was that PostgREST did not implement HTTPS, which meant that JWTs and user credentials in requests were exposed to potentially malicious actors. To overcome this security concern, we leveraged an application load balancer (ALB) responsible for handling all HTTPS requests to Tinker components and routing HTTP downstream. The ALB facilitated the establishment of an HTTPS connection, providing an extra layer of security. The TLS handshake between the server and clients ensured the authenticity of the TLS certificate.

Another challenge we faced was database security. As the users' PostgreSQL database and PostgREST API were public-facing, we had concerns about lack of default security measures. To address this, we used JSON Web Tokens (JWT) for authentication and PostgreSQL roles for authorization. Authentication establishes the user's identity, while authorization checks whether the user has permission for their request. PostgreSQL uses roles to manage privileges, allowing administrators to define permissions assigned to roles. By doing so, permissions control what a role can perform.

When it comes to migration, there are situations where a user may want to migrate off of Tinker. To facilitate this, PostgreSQL's pg_dump command can backup data and schema, which can then be used to recreate the user's database during migration in a new environment. This allows for a smooth transition off of Tinker if the user needs to switch cloud services or move to an on-premise server. By having the ability to migrate seamlessly, we lessen any potential pain points for users.

Lastly, we encountered challenges in automating deployment. During the initial deployment of the admin backend and user backends, we needed to install and configure Nginx, PostgREST, and PostgreSQL. To streamline the deployment process, we utilized Docker. Docker allowed us to package applications and their dependencies into isolated containers, simplifying the installation and configuration of the components. We used Docker Compose to create a network connecting the containers for Nginx, PostgREST, and PostgreSQL. This made it easier to manage and deploy these components consistently across different environments.

By leveraging Docker, we simplified our CloudFormation templates as we no longer needed to include numerous installation and configuration commands. Instead, the logic was extracted into Docker containers, reducing the complexity of the templates. However, this also added an additional layer of complexity for Tinker's development and its users who needed to fine-tune their PostgreSQL configuration. They now had to understand Docker and Docker Compose to make any customizations. But overall, Docker helped streamline the deployment process and improve consistency.

In conclusion, Tinker's design and implementation involved several challenges, including managing the API layer with PostgREST, overcoming limitations with custom endpoints, integrating authentication and authorization, ensuring HTTPS communication, handling migration, and automating deployment. Throughout these challenges, we made decisions that prioritized security, versatility, and simplicity. Tinker's self-hosted infrastructure in a user's AWS environment, combined with PostgreSQL, PostgREST, Docker, and other components, provided users with a powerful tool for managing their databases and ensuring the efficiency and security of their applications. The TLs certificate in the Application Load Balancer (ALB) presented us with a challenge. We needed to handle migration for users looking to move away from Tinker. While there are many benefits to using a backend as a service (BaaS), there is also a possibility that the chosen BaaS could end their services, or the user may need to customize their backend. This customization could involve setting data retention policies or choosing the geographical location for data storage. For example, users may need to comply with data sovereignty laws that require data to be stored in a specific location. However, the chosen location may not be supported by the managed backend service, which would require the user to migrate their application. This is why we decided to host Tinker's infrastructure in the user's own AWS environment. 

Even for a self-hosted solution like Tinker, we recognized that there may still be reasons why a user would want to eventually migrate off Tinker. For example, if a user wants to switch from AWS to a different cloud service or move to on-premise servers. To minimize the pain of migration, we decided to use PostgreSQL's `pg_dump` command to backup data and schema. These backup files can then be used to recreate the user's database during migration in a new environment.

Another challenge we faced was automating the deployment process. During the initial deployment of the admin backend and user backends, we needed to install and configure Nginx, PostgREST, and PostgreSQL. We achieved this using CloudFormation templates, which define the AWS resources to be deployed and their configuration. However, as the templates grew in size and complexity, maintaining them became increasingly cumbersome. 

To streamline the deployment process, we decided to leverage Docker. Docker allows us to package applications and their dependencies into isolated containers, which can be deployed consistently across any Linux-based host. This simplifies the process of installing and configuring applications. We used Docker to run Nginx, PostgREST, and PostgreSQL in containers. Since PostgreSQL needs to communicate with PostgREST, we used Docker Compose to create a network that connects them. In the Docker Compose template, services are defined for Nginx, PostgREST, and PostgreSQL. 

While Docker simplified our CloudFormation templates, it also introduced an additional layer of complexity. This impacted both our development of Tinker and its users who wanted to fine-tune their PostgreSQL configuration. Now, users need to have an understanding of Docker and Docker Compose to make changes such as adjusting the memory usage or database connection settings. This additional complexity made the process less user-friendly as users had to work with the containers' command line.

Although we achieved our objectives for Tinker, we identified areas for future improvement. There were two features we were interested in developing. First, we wanted to incorporate an SQL editor directly into the dashboard UI. This would give users full SQL functionality and enhance their experience. Second, we wanted to enable data import from and export to CSV files. Additionally, we aimed to provide a user-friendly database dump and restore feature through the dashboard UI.

Thank you for taking the time to learn about Tinker's journey. Now, we are open to answering any questions you may have. Jared will moderate the discussion and you can post your questions in the chat or Q&A section.

Question 1: What was your favorite part of working on Tinker?

Response: Each team member shares their favorite part. One member enjoyed building out the dashboard due to the visual payoff it provided. Another member found satisfaction in seeing the automated processes working seamlessly. Additionally, the camaraderie and satisfaction of working as a team were mentioned as a highlight.

Question 2: Did you find any AI tools useful in researching/building a project of this complexity?

Response: The team mentions that they made use of AI tools like ChatGPT for research purposes. While it provided valuable information and boilerplate code, they emphasized the need to critically evaluate the answers and not rely solely on AI tools for building the entire application.

Question 3: How did you handle learning and implementing AWS components for the project?

Response: Initially, AWS seemed daunting due to the broad range of products and complex documentation. To overcome this, the team extensively read documentation, leveraged resources like Stack Overflow, and watched tutorials and videos. However, hands-on experience and experimentation played a crucial role in solidifying their theoretical knowledge.

Question 4: How did you come up with the idea for this project?

Response: In the early stages of researching project ideas, the team explored various options. Eventually, they came across Super Bass, a product that inspired the creation of Tinker. The team fell in love with the table editor feature offered by Super Bass, as it simplified working with relational databases. This led them to consider implementing a backend-as-a-service concept like Super Bass but with additional enhancements.

Question 5: What were some of the most difficult design decisions you had to make?

Response: One challenging design decision involved how to provision user backends. Initially, the team planned to do this through the dashboard, following Super Bass' approach. However, they encountered complications related to security and managing credentials. Ultimately, they decided to handle backend provisioning through the CLI, which offered a cleaner separation of responsibilities between the dashboard and AWS resource management.

Question 6: How did you divide the work among team members, and what did you learn about working as a team?

Response: The team describes their approach of dividing tasks into two prongs, with each team member focusing on specific areas. Depending on urgency and workload, responsibilities were shifted between team members. Effective communication and collaboration were key, and they highlight the camaraderie and satisfaction that came from working together. One design decision we had to make was how to provision user back-ends. Initially, we planned to do it through the dashboard, inspired by Super Bass. We thought it would be user-friendly and intuitive. However, we soon realized there were many caveats to this design choice. We explored different options and ultimately decided to create user back-ends from the CLI. While this approach is slightly less intuitive for users, it created a clear separation of responsibilities between the dashboard, which handles the PostgreSQL database and data, and the CLI, which manages all the AWS resource handling. It was a challenging decision, but weighing the pros and cons, we chose to handle all AWS management through the CLI.

The original vision was to have everything go through the dashboard, but we quickly realized that dealing with credentials and passing them around was not ideal. We didn't want to compromise security, so moving away from the dashboard for certain tasks was necessary.

Regarding the division of labor, working as a team of three, we divided our tasks based on necessity and individual strengths. For example, Peter's organization skills and design expertise made him well-suited for working on the infrastructure and the front end of the dashboard UI. Jared took on different roles, often being the deckhand, shifting between tasks. We would adjust responsibilities based on urgency and the most pressing needs of the project. If we needed more hands in one area, we would allocate resources accordingly. Communication played a vital role in our teamwork. We held daily team meetings on Slack to discuss our plans for the day, recap our progress, and outline future tasks. We made a conscious effort to communicate openly and address any issues in the team environment. Overcommunication was preferred over undercommunication, and it helped us work smoothly as a team.

Developing a strong team relationship was important to us, and we established it early on during the preparation phase. We scheduled regular meetups and continued to stay connected throughout the Capstone project. We held team meetings twice a day, ensuring that everyone was always in the loop. This level of collaboration and connection significantly contributed to the success of our project.

Thank you for attending our presentation and for your questions. We are proud of our work and delighted to share it with all of you.