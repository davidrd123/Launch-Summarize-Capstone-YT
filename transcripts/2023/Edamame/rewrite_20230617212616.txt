Welcome, everyone! Today, we'll be discussing the development of Edamame, a load testing tool for real-time collaboration applications. Our team consists of Albert, Ginny, Luke, and Rachel, representing different parts of Canada and the US. Let's dive into the details and structure of our presentation.

In Part A, we'll begin by defining load testing and its purpose. We'll explore why companies conduct load tests and the significance of real-time collaboration applications, such as Slack, Coda, and Miro. Additionally, we'll delve into the unique considerations that load testing for collaboration apps entails, including the implementation of additional protocols like websockets. We'll also explore existing solutions in the industry to address these challenges.

Moving on to Part B, we'll outline our solution. We'll start by presenting the high-level architecture of Edamame, providing an overview of how it operates. We'll then discuss the specific challenges we encountered during the development process and how we addressed them through our architecture. Now, let's begin with Rachel, who will explain the concept of load testing.

Load testing involves simulating user behavior at high volumes to evaluate how a system responds. It helps developers identify performance issues, such as delayed or failed requests. One common approach is to use an HTTP benchmarking tool, which sends a large number of HTTP requests to a server endpoint to mimic high traffic. However, load testing entails further considerations, such as scale, protocols, metrics, and visualization.

Scale refers to the number of virtual users simulated during a load test. Developers define the behavior of virtual users within a test script and specify the duration and total number of virtual users. The precise number of virtual users that can be simulated depends on available resources, including CPU and memory. For extremely high-scale tests, a distributed architecture with multiple machines may be necessary, albeit with added complexity.

When load testing collaboration applications, the protocols involved become crucial. While HTTP is the main protocol for web applications, collaboration apps often require additional protocols like websockets. Developers must modify their test scripts to simulate these protocols accurately and ensure comprehensive testing of the entire application.

To obtain meaningful insights, developers must carefully consider the metrics collected during load testing. Server-side data, such as CPU and memory usage, provides backend performance metrics. However, load testing tools primarily focus on collecting client-side data, such as HTTP response times, which directly impact user experience. Instead of relying solely on average response times, developers can use percentiles to gain a clearer understanding of the distribution of response times.

Visualizing data collected during load testing is crucial. Developers can present data as a summary at the end of the test or in a graph that plots the data against time. The latter approach offers granular insight into the performance of the system over time and enables identifying critical points where performance begins to degrade.

To summarize, when conducting load tests, developers should consider scale, protocols, metrics, and visualization. The specific choices within each category depend on the developer's use case. In the case of Edamame, we focus on collaboration apps. Now, let's pass it over to Ginny, who will delve deeper into collaboration apps and the challenges they pose for load testing.

Real-time collaboration apps facilitate communication between users in real-time. These apps incorporate features like shared whiteboarding, where multiple users interact simultaneously on a shared canvas. The rise of remote work has led to an increase in the usage of such collaboration apps. Examples include messaging apps like Slack, whiteboarding tools like Whimsical and Miro, and productivity tools like Coda.

Collaboration apps heavily rely on websockets, a protocol that enables persistent connections between client and server. Unlike traditional HTTP requests, websockets allow bidirectional communication and quick data transfer, making real-time updates possible. The ability to send unsolicited messages and receive immediate responses enhances the user experience.

A specific challenge faced by collaboration apps is fan-out. When a user sends a message in apps like Slack, an HTTP request is sent to the server. However, due to the underlying websockets implementation, the server needs to distribute this message to multiple users, triggering a fan-out effect. Efficiently handling this fan-out ensures that real-time updates are accurately propagated to all relevant users.

Load testing collaboration apps requires considering the unique characteristics associated with websockets and the fan-out challenge. Edamame, our load testing tool, is designed to address these challenges effectively. Now, I'll hand it back to Albert, who will proceed with outlining our solution architecture.

We've discussed the problem space and considerations related to load testing collaboration apps. Now, let's focus on the high-level architecture of Edamame and its specific challenges. Our goal was to develop a load testing tool that accurately emulates real-time collaboration scenarios.

To achieve this, we designed Edamame with a modular architecture. Its core components include a load generator, a workload orchestrator, and a metrics collector. The load generator simulates virtual users, while the workload orchestrator manages their behavior and actions during the test. The metrics collector gathers relevant data for analysis and visualization.

One of the challenges we faced was managing the scalability and synchronization of the load generators to emulate large-scale tests effectively. We incorporated a distributed architecture that allowed load generators to run on multiple machines and orchestrated their behavior centrally.

Another challenge was accurately simulating the behavior of virtual users in collaboration apps, which required handling the fan-out effect efficiently. We developed custom logic within the workload orchestrator to ensure proper message distribution, mimicking real-world collaboration scenarios.

Additionally, integrating the collection and visualization of metrics posed a challenge. We implemented a metrics collector that efficiently captured data, including response times and other relevant metrics. Visualizing this data accurately allowed developers to gain actionable insights into the performance of their applications during load tests.

Despite these challenges, Edamame enabled developers to conduct comprehensive load tests for real-time collaboration apps. Its modular architecture and custom logic ensured accurate emulation of user behavior, efficient fan-out handling, and meaningful metric collection.

In conclusion, we've covered the problem domain, the architecture of Edamame, and the challenges we encountered during its development. By focusing on collaboration apps and considering factors like scale, protocols, metrics, and visualization, Edamame provides an effective load testing solution. Thank you for listening, and we're now ready to take questions. In this video, we will discuss collaboration apps and the importance of building an effective load testing tool for them. Collaboration apps are applications that incorporate real-time communication, allowing users to collaborate on shared tasks. Examples of collaboration apps include messaging apps like Slack, whiteboarding tools like Whimsical and Miro, and productivity tools like Codep. These apps rely on the WebSocket protocol for real-time communication.

WebSockets are a protocol that operates over HTTP and creates a persistent connection between the client and server. Unlike the traditional HTTP request-response cycle, WebSockets allow for bi-directional communication, where the server can send unsolicited messages and the client can send messages without waiting for a response. This persistent connection enables real-time updates in collaboration apps and makes data transfer quick and efficient compared to HTTP.

One specific challenge that collaboration apps using WebSockets face is called fan-out. Fan-out occurs when a single message in a channel results in multiple WebSocket messages being sent out to all connected users. For example, when a user sends a message on Slack, that message is sent as an HTTP post request to the Slack servers. The message is then sent back out to all other users connected to that channel using WebSockets. If a popular message receives reactions, each reaction creates a new message that also needs to be sent out to all connected users. This fan-out pattern can potentially result in a large number of WebSocket messages being sent out by the server in a short amount of time.

To effectively load test collaboration apps, it is important to simulate this fan-out pattern accurately. Load testers need to generate a large number of virtual users, typically in the six-figure range, to simulate realistic usage. This often requires a distributed load testing approach, where the load is distributed across multiple machines. Load testers should also simulate both HTTP and WebSocket traffic to ensure comprehensive testing of the application architecture. Metrics collected during load testing should be relevant to both protocols and provide insights into end-user experience. Near real-time data visualization is also crucial, as developers need to monitor test results and react quickly to any performance issues.

There are various options available for load testing collaboration apps. Managed cloud-based services, like k6 Cloud, BlazeMeter, and Gatling Enterprise, handle the complexities of distributed load testing in the cloud. These services are powered by open source load testing tools and are suitable for developers who want to avoid the complexities of managing infrastructure themselves. However, they may have limitations in terms of data ownership and custom configurations, and they come with a price.

On the other end of the spectrum are self-hosted DIY tools. Companies can build their own load testing solutions tailored to their specific needs. While this approach offers more flexibility in terms of data ownership and custom configurations, it requires dedicated teams to manage the complexities of building and maintaining the load testing infrastructure.

An alternative solution that combines the best of both worlds is using licensed or open source solutions hosted by the company itself. This approach allows organizations to have more control over the load testing process while still benefiting from the convenience and features offered by managed services.

Edamame is an open source, self-hosted, distributed load testing framework specifically designed for real-time collaboration apps. It supports load tests of up to 200,000 concurrent virtual users and provides support for both HTTP and WebSocket traffic. Edamame offers custom metrics that give deeper insight into performance and a near real-time dashboard to monitor test results.

While Edamame has its limitations, such as not supporting tests requiring more than 200,000 users and lack of advanced features like CI/CD integration and scheduled tests, it is a competitive solution for self-hosted distributed load testing.

To set up Edamame, users need to install it on their local machine and then initialize its cloud infrastructure on AWS. Edamame deploys various components, including load generators, a coordinator, data handling components, and a visualizer. These components work together to generate load, handle data, and provide near real-time visualization of test results.

In conclusion, load testing is crucial for ensuring the scalability and performance of collaboration apps that rely on WebSockets for real-time communication. Edamame offers a competitive solution for self-hosted distributed load testing, providing support for both HTTP and WebSocket traffic, custom metrics, and a near real-time dashboard. By accurately simulating the fan-out pattern and monitoring test results, developers can ensure their collaboration apps can handle the demands of real-time communication at scale. Edamame is an open source self-hosted distributed load testing framework designed specifically for real-time collaboration apps. It aims to be a competitive plug-and-play solution for self-hosted distributed load testing in messaging, whiteboarding, and productivity tools. Edamame addresses several key considerations in this space. Firstly, it supports load tests of up to 200,000 concurrent virtual users. Secondly, it supports HTTP and websockets for protocols. Additionally, it provides custom metrics that offer deeper insight into performance. Furthermore, it includes a near real-time dashboard for developers to react to live updates.

However, Edamame does have some limitations. It currently does not support tests requiring more than 200k users. It also only supports HTTP and websocket traffic, so if additional features are needed, users will have to extend Edamame themselves. It lacks some advanced features, such as CI/CD integration and scheduled tests. These are areas where Edamame can still improve.

Now, let's delve into the specifics of how Edamame works and how it interfaces with the cloud for distributed load testing. Edamame runs on Amazon Web Services (AWS). To get started, users need to create an AWS account and install Edamame on their local machine. They can then set up Edamame's cloud infrastructure using the "edamame init" command, which serves as the setup before running any tests.

The Edamame framework deploys five main components in the cloud. The first two components are responsible for load generation, which involves generating HTTP and websockets traffic. For this task, Edamame uses the open-source tool "k6" as its load generating program. The k6 runners act as the muscle, generating load, while the coordinator serves as the brain, ensuring synchronization among the k6 runners.

The next two components of Edamame handle data. This includes the pipeline that receives all the data for metrics and the database that manages storage. Finally, there is the visualizer, which allows users to view the results of their tests in near real-time.

It's important to note that the k6 runners and the data pipeline in the diagram are not colored because they are only necessary when running tests. The "edamame init" command sets up the basic structure, and these resources are provisioned when tests are actually executed.

To run a test, users can utilize the "edamame run" command, providing a test script. Edamame then scales up the necessary k6 runners and data pipeline for the test. The number of runners provisioned will depend on the test script. Let's say, for example, that three k6 runners are needed. Once everything is ready, a signal is sent to synchronize the runners, and they start sending HTTP requests and establishing websocket connections with the specified servers. Client-side data for metrics is then sent to the data pipeline, where it is processed and sent to the database for storage. The visualizer retrieves the data from the database, enabling users to access a dashboard with live results. Once the test concludes, the components for the load test are scaled down, returning to the initial resting state.

Now, let's discuss the challenges encountered while building Edamame. Three main challenges arose during the development process. The first challenge involved coordinating distributed load tests with over 100k virtual users. The second challenge was processing over one million data points per second in real-time for visualization purposes. Lastly, there was a need to extract useful insights across multiple protocols, specifically HTTP and websockets.

Addressing the first challenge, running distributed load tests with over 100k virtual users proved difficult with just one machine. To overcome this limitation, the decision was made to distribute the tests among multiple machines. These tests were run in the cloud using Amazon EC2 instances. Docker containers were also utilized on these instances to provide a standardized environment and bundle dependencies. To ensure synchronization between the load tests running on separate machines, the team turned to Kubernetes and the k6 operator, allowing for coordinated distributed load testing. The k6 operator simplifies the process by automating tasks such as monitoring pods and ensuring tests start simultaneously.

The second challenge required processing over one million data points per second in real-time. Initially, options were explored such as writing raw data directly to a database or aggregating on the k6 runners. However, these approaches presented limitations, leading to the adoption of central aggregation using Statsite. Statsite is a performance stats D server written in C that leverages the statsd protocol. It uses a probabilistic data structure to estimate percentiles accurately without sorting the vast amount of data points. Using this setup, the team was able to reduce the data to a manageable number of data points, which could then be written to the database.

Lastly, the team encountered the challenge of extracting useful insights from both HTTP and websockets during load tests. Although k6 provided default metrics for HTTP, it lacked comprehensive metrics for websockets. To address this, the team extended k6 by writing a custom extension in Go. This extension emitted the custom metrics, along with the default k6 metrics, into the data pipeline, where it could be stored and visualized.

By overcoming these challenges, the Edamame framework provides an open source, self-hosted distributed load testing solution for real-time collaboration apps. It supports high concurrent user loads, integrates with popular protocols, and offers custom metrics for deep performance analysis. However, there is still room for improvement in terms of scalability, additional protocol support, and advanced features like CI/CD integration and scheduled tests. In load testing, calculating the 99th percentile response time can be challenging. If we calculate the 99th percentile of response times from each runner and then average them, it may not give us the correct value. This is because if a specific runner is receiving much faster or slower response times, it can skew the results. For example, if machine one has a 99th percentile response time of 500 milliseconds and machine two has a 99th percentile response time of 2000 milliseconds, the straight average would be 1250 milliseconds. However, if we combine all the data into one central machine, the correct calculation for the 99th percentile is 2000 milliseconds. Therefore, aggregating the data on the runners is not a viable approach.

To address this issue, we implemented central aggregation using stat site. Stat site is a performance stats D server written in C that uses the statsd protocol for implementing and aggregating metrics. It uses UDP to reduce latency and is fast because it aggregates trend data points using a probabilistic data structure. This means it doesn't need to sort the millions of data points to find the 99th percentile, but rather estimates the distribution based on a smaller sample size and accurately calculates the percentiles. By using a stat site server, we were able to handle over a million data points per second and write only 20 data points or less to the database every five seconds.

Another challenge we faced was extracting useful insights from both HTTP and websockets during load tests. While visualizing the data, we realized that k6, the load testing tool we were using, did not provide all the necessary metrics for websockets by default. To overcome this limitation, we extended k6 by adding five additional metrics. We achieved this by writing a custom extension in Go that emitted these metrics along with the default k6 metrics into our data pipeline. This allowed us to visualize all the desired metrics for both HTTP and websockets. Some of the metrics we added included failed handshakes and abnormal closures, which were crucial for evaluating websocket load tests.

With all the required metrics available, we were able to visualize both HTTP and websocket metrics graphically in Grafana. This allowed developers to monitor the graphs in real-time and react accordingly during load tests. By solving these three challenges, we successfully built Edamame. We coordinated distributed tests with over 100,000 virtual users, processed more than a million data points per second in real-time, and obtained valuable insights for both HTTP and websockets using our custom Go extension.

Thank you all for listening to our walkthrough of the project. We hope you found it informative. Now, we will open the floor for questions.

Question 1: Why is there a hard limit of 200,000 users? Couldn't the k6 runner be scaled to accommodate any number of users?

Our current hard limit of 200,000 users is due to a bottleneck in our central aggregation data pipeline, specifically our stat site server. With the observed number of virtual users, which translates to approximately 2 million metrics, we feel optimistic about increasing this limit through performance tuning or exploring alternative data pipeline architectures. However, with our current implementation, 200,000 users is the maximum we can handle.

Question 2: Why the name "Edamame"?

The name Edamame was chosen as it references "edamame pods" and also sounded friendly to us. We went through many different names, spending days searching for the right one. Jenny even provided a list of Greek characters from Greek myths, but none of them stood out to us. Ultimately, Edamame resonated with the team.

Question 3: How did you collaborate when developing and deploying your cloud infrastructure?

We utilized GitHub to create products and issues, enabling us to outline the steps and progress of building Edamame. Through GitHub, everyone could pull the latest main commit and test the changes locally while contributing new features. This allowed effective collaboration and tracking of development tasks.

Question 4: Did you explore other messaging protocols besides websockets?

Yes, we did explore other messaging protocols. However, our focus was on load testing collaboration apps, which heavily utilize websockets. We wanted to ensure that our load testing tool catered specifically to these apps. While there are numerous other protocols to consider, such as for video streaming or WebRTC, we chose to specialize in the collaboration app space.

Question 5: What other advances do you anticipate in this space that could improve Edamame?

In terms of improving Edamame, there are a few areas we would focus on. Firstly, we would explore ways to increase the number of virtual users beyond the current bottleneck of our centralized server. This could involve implementing specific data structures on the runners to aggregate data before sending it to the server. Additionally, advancements in data pipeline architectures could provide more scalability and performance. Ultimately, the future of load testing will depend on the evolving needs of companies and the technologies they adopt.

Thank you once again for attending our presentation. If you have any further questions, please feel free to reach out to us individually. Have a great day!