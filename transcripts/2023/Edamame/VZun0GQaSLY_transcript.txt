foreign hey everybody welcome and thank you very much for coming so today we are going to talk about how we built edamame which is a load tester for real-time collaboration apps okay so my name is Albert I'm here with Ginny Luke and Rachel and we come from different parts of both Canada and the US and here's what we're going to talk about today so in part A what we're going to do is lay out the problem so first we'll Define load testing what is it how does it work and how and why do companies even conduct load tests in the first place next we'll talk about real-time collaboration apps apps like slack Coda and Miro and we'll explain why we decided to focus on these sorts of applications then we'll move on to talking about how load testing has a few different considerations when applied to collaboration apps as you can imagine real-time applications work a little bit differently and they sometimes require additional protocols like websockets to make them more efficient which is going to change the nature of what we're actually load testing and then the fourth point we'll cover is we'll walk through some existing Solutions and we'll answer the question of how are companies actually solving these problems today then in Part B we'll outline our solution so we'll first walk through the high level architecture of how edamame is set up so that you can get a bird's eye view of how edamame works and then finally we'll talk about actually building edamame and the specific challenges that we faced and how we solve these challenges with our architecture so I'm not going to hand it off to Rachel who will kick off part A by explaining what load testing is awesome so as Albert mentioned we are now going to discuss what load testing is and the problem it solves and we'll start by looking at a tweet so recently Ticketmaster crashed due to 14 million swifties overwhelming the site when they tried to simultaneously Buy Taylor Swift concert tickets and this elicited some pretty unhappy responses from customers as I'm sure you all can see for developers providing great user experience is crucial and having a site crash isn't good for business so how can developers prepare for such cases of overwhelming traffic well this is where load testing comes into play load testing is the process of simulating real application user Behavior at large volumes and measuring how the system responds specifically developers May measure if the system is taking longer to handle client requests or if it's failing to complete requests one simple way to conduct a load test is to use an HTTP benchmarking tool and what this means is is that developers will use a tool to consecutively send a lot of HTTP requests to a specific back-end server endpoint to mimic large volumes of traffic and this seems pretty straightforward but there actually can be a lot more considerations involved when conducting a load test specifically developers may want to consider the scale of the test the protocol or protocols involved in the test what metrics they're collecting and how they're visualizing those metrics and there are other considerations too that developers may want to think about but these are the main ones in scope that we are going to look at more closely now so starting with scale what is thought until exactly scale is concerned with the total number of concurrent virtual users that are simulated during the load test as context a virtual user is programmatic behavior that's defined within a script also referred to as a test script so here we have a simple example and within this function there's an HTTP get request that's being sent to an endpoint and the exact nature of this test script can vary depending on the load testing tool that's being used but generally developers will specify the actions they want their virtual users to simulate during the test and they will also specify the duration of the test and the total number of vu's so now that we know what a virtual user is we can revisit scale which again is concerned with the total number of concurrent virtual users that are being simulated and approximately 40 000 virtual users can be simulated on one local machine but this is just an approximation the actual number that can be simulated will depend on the local resources that are available such as how much CPU the machine has and how much memory is available and it also can depend on the behavior that the virtual users need to enact to all the test is running so really a developer may only be able to simulate a fraction of this 40 000 estimate and if that's the case then if a developer is working for a site like Ticketmaster and how to simulate millions of users then they'll need to switch to a distributed architecture and this means they have to go out and secure more machines that they can run the load test on and again the benefit is that they can now more mimic or mimic more closely through the real life situation by simulating more virtual users to match the expected amount of traffic for their application but it comes at the cost of more complexity as they now have to go out and manage all these extra servers so now that we've considered scale we will talk about the protocol or protocols involved in the test and developers will want to consider the types of traffic that they will want to stimulate during the test and this depends on the actual types of traffic that their application simulates so when a developer is conducting a load test they want to mimic real world situations closely to ensure that the application can handle large volumes of real-life traffic so when they want a developer is thinking about the protocol or protocols that they need to simulate they'll want to think about the types of traffic that real users are simulating or using and for web application applications that generally means that developers will want to simulate HTTP traffic because HTTP is the main protocol of web applications um and up to this point all the visuals of our local machines interacting with back-end server endpoints have been um using the HTTP protocol as our local machines have been sending HTTP requests to a specific endpoint but there can actually be more protocols involved in applications besides just http for example applications may have components that include other protocols such as the websocket protocol or amqp and if that's the case then developers will want to alter their telescopes that to make sure that they are simulating these other protocols too so that they are comprehensively testing their entire application so besides the scale and protocol of the test developers will also want to consider the metrics involved specifically developers will want to think about the data they're going to collect and how they're going to summarize that and developers can collect a lot of data during their tests but more is not necessarily better ideally developers would want to collect a few metrics that will provide them meaningful insight into how their system is performing while the load test is running so what does meaningful metrics look like well there are two main types of data that provide insight into how a system is performing under load and that's client-side data and server-side data and we'll start by looking at server-side data so server-side data tracks things like CPU and memory up the backend servers and these things aren't explicit to the user that's interacting with an application but they can be helpful to track because they impact how a user's how I uh how a user's experience is interacting with an application however load tests load testing tools often can't collect server-side data as it's out of scope and so oftentimes load testing tools will collect client-side data an example of a meaningful metric of client-side data is HTTP response times and this is Meaningful because it gives insight about a user's experience because again the end goal of a load test is to understand how the user's experience is when they're interacting with an application so if the HTTP response time is 100 milliseconds then a user can happily navigate through a site but if an HTTP response time is two and a half seconds then the site will probably feel pretty slow and noticeably glitchy a user might not want to use the application ever again and so once a developer decides that they want to track an HEB response time they'll also want to consider how they're going to summarize it because here we have only four data points so maybe we can see pretty clearly that there are three uh acceptable HTTP response times but as soon as the magnitude of the virtual users being simulated increases there are a lot more data points that are going to come in and trying to create any meaningful conclusions looking at all these granular data points can be pretty difficult so one way to summarize HTTP response times is to calculate an average however an average isn't always helpful because for HTTP response times this metric can follow oftentimes will follow right skewed distribution so the average doesn't necessarily indicate whether that's the majority of users experience or whether it's an outlying experience because because of the right SKU outliers can skew the average towards the right end the distribution in terms of magnitude so to look at this more concretely we have two scenarios scenario a and scenario B and both have an average hddb response time of two seconds however under scenario a only a majority of users actually have an acceptable HTTP response time of under half a second whereas under scenario b a majority of users have an HTTP response time over a second and so the average is misleading because if a developer doesn't have these granular data points just by looking at the average they don't know whether the response time for most users is poor or if it's got a so instead of calculating an average developers can instead calculate percentiles and percentiles are more helpful because they give a developer a clearer picture of their distribution and therefore can tell them what's going on for a majority of users for example if a developer calculates the 98 percentile as 1.8 seconds then they know concretely thought 10 of their user base has a response time over 1.8 seconds so after deciding all the types of data a developer wants to collect and how they're going to summarize it they will also want to consider when they're going to display that data and there are too many times when the data can be displayed it can be displayed as an end of test summary as we see on the left here or it can be displayed as or plotted in a graph against the time at which it's collected and this latter approach on the right can be more helpful to developers as it provides more granular Insight specifically we can see here at about the two minute Mark the response time shoots up and we can track that against the number of virtual users so by plotting against time developers are able to see more clearly at what point the performance of their system starts to shift so we've covered quite a bit of ground here but to recap when a developer is conducting a load test they'll want to think about the scale of the test the protocols involved in the test the types of data they're collecting and how they're visualizing that data and the exact answers to the questions posed with posed against these categories here can vary depending on a developer's use case and for us at edamame we are interested in collaboration apps so I will now pass it off to Jimmy who will discuss in more detail collaboration apps and what it means to build an effective load testing tool for collaboration apps thanks Rachel so let's think about what real-time collaboration apps are and we're going to talk about how they work and then after that we are going to consider if they have any specific characteristics that we need to keep in mind when we're thinking about how to load test them collaboration apps are apps that incorporate some kind of real-time communication the example you see here is a shared whiteboarding application in this case users are creating visuals together on the same canvas they can make edits see each other's cursors comment on things and mainly watch changes to their whiteboard appear on their own screen in real time thanks to the rise of remote work we've seen an influx of these kinds of applications in our daily lives so some examples of collaboration in the apps in the real world include messaging apps like slack whiteboarding tools like Whimsical and Miro and productivity tools like codep these apps in particular have something in common that allows for real-time communication they all use that websocket protocol that Rachel mentioned just a few slides ago to so what are websockets well websocket is a protocol it operates over HTTP and uses the underlying TCP layer to create a persistent connection between client and server let's break that down a bit first a quick refresher on HTTP so in the HTTP request response cycle the client will first send a request to the server and then when that server receives the request it can then send a response back to the client when the client receives the response this interaction is complete websockets on the other hand are facilitate a persistent connection so in this case once a client is connected to a websocket server they can communicate with each other whenever they want a server can send unsolicited messages and a client can send messages without having to wait for a response this bi-directional communication is what makes the real-time updates of collaboration apps possible in fact this persistent connection makes data transfer very quick and efficient this is because the connection doesn't have to be re-established each time they communicate with each other as it happens with http messaging speeds of about 100 milliseconds make it all seem like it's happening in real time so real-time collaboration apps that use websockets have to deal with a specific challenge called fan out to see how this works let's take a look under the hood at what happens when we send messages on Slack so when a user sends a message to a slack Channel an HTTP post request is sent to slack servers this message then gets sent back up to all other users connected to that Channel with websockets so if a channel has something like a thousand users a single incoming post request will result in a thousand websocket messages going out that's not where it ends though let's say that this sent message is really popular like the CEO of a company is congratulating all the employees if a bunch of people start liking or reacting to that message it's going to create even more fan out each time someone adds a heart a smiley or another Emoji it's going to create a new message this message also has to be sent back out to all 1000 users connected to the channel this means that a single message in the channel has the potential to result in a thousand times a thousand or one million websocket messages being omitted and since all this is happening in real time all these messages need to be sent out really quickly the websocket server has to do a ton of work in a really short time window fan out is an issue for all of our example real-time collaboration apps not just Slack so to see if their infrastructure can withstand this it's important to conduct load tests that accurately simulate this kind of fan out pattern with that in mind let's take a look at what a load tester Built For This use case might look like using the considerations ritual outline for us earlier so that first consideration is scale how many virtual users do we need to generate this will vary from app to app but we did find a general range of about six figures for the collaboration apps that we looked at Miro for example experienced a year of Rapid scaling where they expanded from about twelve thousand to one hundred thousand concurrent users when slack was conducting their load tests they scaled from about 5000 virtual users to 500 000 virtual users per test numbers in this six figure range will typically necessitate a distributed load test the second consideration is protocol what's the best way to simulate collaboration app users in this case the load tester should simulate both HTTP and websockets because this is how collaboration apps communicate with clients collaboration apps are typically built with separate Services when to deal with HTTP traffic and when to deal with websocket traffic if a load tester relies on sending out HTTP requests alone as in our earlier example of the HTTP benchmarking tool this won't test the entire system in this case incoming requests will hit the HTTP server the HTTP server will then send messages along to the websocket server and it's the websocket server's job to send those messages out to any connected websocket clients however if the virtual users in the load test do not maintain any persistent websocket connections there won't be anyone there to talk to that fan out pattern that can be so resource intensive will never get recreated instead the websocket server will just accept those messages and then it won't have anything to do so to make sure that the architecture is being tested comprehensively the load testers virtual users should simulate websocket connections this will give the websocket server something to talk to and ensure that that fan out a potential performance bottleneck is being simulated accurately in the load test third we need to consider what kind of data is Meaningful for collaboration apps since these kinds of applications are utilizing both HTTP and websocket protocols the load tester should collect metrics that are relevant to the concerns of both those protocols and informative of how end users are experiencing both those protocols relevant and informative metrics for HTTP will focus on the request response cycle in this case we're looking at things like HTTP response time which as Rachel mentioned we can do by looking at percentiles relevant and informative metrics for http for websockets are more concerned with the persistent connection for example instead of looking at the number of websocket errors in general we might want to take a more granular look at what's happening when errors occur developers might check the count of websocket abnormal closures in order to be informed of when the server is dropping connections but looking at the count of websocket failed handshake can tell developers when the server is having trouble making new connections finally we need to consider how data is visualized in the case of collaboration apps near real-time graphs are typically preferred and this is because developers sometimes need to run load tests against production production is where live servers interact with real end users since load tests are meant to find the limits of these resources developers will need to monitor their test results in near real time this will allow them to react to any performance degradation as it happens by either scaling up production to handle the additional load or by stopping the test so to summarize an effective load tester for collaboration apps should have the following characteristics we're looking at a scale of virtual users in about the six figure range which means the distributed architecture those virtual users should be able to simulate both HTTP and websocket traffic to ensure that the application architecture is being tested and metrics relevant to the concerns of both those protocols should be collected finally providing near real-time data visualizations will give users access to how that test is performing sooner in case performance starts to degrade next Luke will talk about the current Solutions landscape and give us an understanding of the choices real-time collaboration apps have when it comes to load testing thanks Jenny so when it comes to distributed load testing for these collaboration apps there can be very different levels of abstraction so on the left we have the most abstracted solution which is managed cloud-based services and on the right we have self-hosted DIY tools which we categorize as we subtracted let's start with the former managed cloud-based services are solutions that run the distributed load test for you and handles all the complexities in the cloud now there's a few well-known services like this for example k6 cloud is a big one so it's Blaze meter as well as Gatling Enterprise now all of these services are provided by companies that offer open source load testing tools so for instance k6 cloud is powered by k6 now since it's open source developers can install k6 on their single local machine and run a load test but once they reached a scale of just about 40 000 users they need to distribute like Rachel mentioned and these open source tools well they don't really have a simple way to do this essentially this means that developers will maybe have to write some code or maybe manage some infrastructure themselves just for load testing now the complexity really depends on the implementation but it can be difficult enough where developers could choose to use a paid service like k6 Cloud to do the heavy lifting the main Pro of using a managed cloud-based service is in the name right it manages all the complexities for you which is really great for Developers that said they do have some disadvantages for one um data ownership which might be an issue if the company has any privacy concerns there's also the fact that there's limited custom configurations you can make since you're using a managed service and finally there's a price which can be quite high as you can imagine just because it's doing all the work let's now go to the other end of the spectrum self-hosted DIY tools DIY tools are Homegrown Solutions where essentially the company builds their own load testing solution now we did find several examples of companies that did this and one example is Slack's Koi pond which is their proprietary load testing framework for simulating slack clients specifically it's designed only for slack with their scale their client behavior and their considerations in mind with DIY tools the main advantage is that you can have all the features you want like the scale data ownership custom configurations it's all possible as long as you can build and manage it and that's the drawback with this approach too you have to build it and you have to manage it this often means there's a dedicated team just for handling all these complexities like with klaypon which is managed by Slack's performance infrastructure team so what happens if you want the Best of Both Worlds you want a solution that manages many of the complexities of building and running a load tester and at the same time you want the advantages of self-hosted DIY tools like data ownership or custom configurations this is where you would want to use something in between now there are a few variations of this but one of them is when companies use licensed or open source solutions that they host themselves and ultimately this is the space edamame belongs in so introducing edamame edamame is an open source self-hosted distributed load testing framework that is specifically configured for real-time collaboration apps We Believe edamame is a competitive plug-and-play solution for self-hosted distributed load testing when it comes to these messaging whiteboarding and productivity tools when we go back to our list of considerations these are the answers edamame provides first we support load tests of up to 200 000 concurrent virtual users we support HTTP and websockets for protocols and we provide a set of custom metrics that give deeper insight into performance it also comes with a near real-time dashboard that allows developers to react as they want to live updates like Jenny mentioned that said edamame has its limitations the first is that we do not currently support tests that require more than 200k users and we also only provide HTTP and websocket traffic meaning you'll have to extend edamame yourself if you need these features we also don't provide some Advanced features like Ci CD integration and scheduled tests these are the areas edamame can still improve in all right so now that we've established the problem domain let's jump into Part B when we take a look at edamame specifically starting with how edamame works and how it interfaces with the cloud to perform distributed load testing edamame run on runs on Amazon web services or AWS once the user creates an AWS account they then need to install anamame on their local machine next we want to set up edamame's Cloud infrastructure using using the command edamame init which is essentially the setup before we run any tests here edamame deploys five main components to the cloud the first two components have to do with load generation which is all about generating the HTTP and websockets traffic and this is where we make the choice to use the open source k6 as our load generating program the k6 runners on the right can be thought of as the muscle that are generating load while the coordinator is the brain making sure that the k6 runners are in order and synchronized the next two components deal with data handling this includes the pipeline that first receives all the data for metrics as well as the database that handles it finally we have the visualizer which allows us to see the results of our tests in near real time now one thing to note is you'll see that the k6 runners and the data pipeline in this diagram are not colored in and that's because currently we're not running any tests so we don't actually need these components just yet the command edamame init just sets up the skeleton and these resources are only provisioned once we run a test so let's do that we can run a test with the command edamame run where the user provides a test script edamame will then scale up the k6 runners and data pipeline needed for the test now this differs depending on the test script but in this case let's say we need three k6 runners and that's what edamame will provision when this is ready we send a signal to the coordinator to synchronize and start the runners the runners then sends HTTP requests and establishes websocket connections to the servers specified in the test script any client-side data for metrics now gets sent down to the data pipeline where it gets processed and then sent to the database for storage finally the visualizer grabs the data from the database which gives the user access to a dashboard that they can use to see live results once the test is done any components for the load test are scaled down and we're back to our initial resting state so that was a very quick high level overview of how edamame works now I'll hand the mic back to Albert we'll explain the choices we made for these components in more detail all right thanks Luke so as we were building edamame we ran across three major challenges that we had to solve the first challenge was coordinating distributed load tests of greater than 100K virtual users the Second Challenge was then processing a million plus data points per second in real time so that we could visualize the data while it was coming in and the third challenge was extracting useful insights across multiple protocols which in our case was HTTP and web sockets now as you might notice these challenges actually fall in line with the architecture that Luke went over previously so the first challenge of generating 100K plus virtual users falls into the low generation category the second challenge of processing a million plus data points falls into the data handling category and the third challenge of extracting useful insights falls into the data visualization category so let's start with challenge number one which is how did we actually run a distributed load test of greater than 100K virtual users so as we mentioned we ended up picking k6 which is a very performant load testing tool but even in using k6 we were still unable to provide the load that we wanted when just running on one machine so instead what we ended up having to do is distribute the test um so that will look something like this so in this example as you can see we have multiple machines that are all issuing requests to one back-end server so since we don't want to run our own servers we also made the decision to run these tests in the cloud and what that means in Practical terms is that our test would run on Amazon's machines which are called ec2 instances we also decided to use Docker containers on these ec2 instances that would run k6 so this provided us a standardized environment where we could run k6 and bundle all the dependencies that it needed as well now after we made this decision the question became how do we actually synchronize these load tests that are now running on Docker containers and exist on completely separate machines so here's what a synchronized load test looks like as you can see here we have two machines that are running a five minute load test each of these machines are scaling up to 10K virtual users and together they're going to hit a peak load of 20K virtual users however if for some reason the load test on different machines are not synchronized we're going to get something that looks like this instead so in this example as you can see both machines are still trying to run 10K virtual user tests but because the tests are not synchronized they're never actually going to hit that Peak load of 20K virtual users as you can see machine number one ramps up to 10K virtual users and machine number two at that point is just starting the test so it will only hit a peak load of 11 kbus so in order to run an effective test that reaches the peak load that you want you need to make sure as a developer that all of your tests are running in a way that's synchronized with each other so there are a lot of ways to actually synchronize a test but k6 have very good support for kubernetes so we decided to use kubernetes along with the k6 operator in essence what kubernetes is is a container orchestration tool and it's a way to allow containers running on multiple hosts to communicate amongst each other and the operator pattern in kubernetes allows developers to write custom code that will automate tasks that a human operator would typically do so in our case that meant monitoring pods and making sure that pests were starting at the same time so the k6 operator allows us to more easily run this type of distributed load test so as you can see here the k6 operator is inside of eks which is a managed kubernetes service on AWS and that's where a lot of the setup is done for us by AWS so it simplifies some of the work that we would have had to do and as we've mentioned the k6 operator is the brain that coordinates the test and the runners are the actual muscle that execute the code so here's what the flow is going to look like using the k6 operator first the operator is going to create an initial an initializer which performs things like error handling so for example it's going to check whether the load testing script that a developer uploaded is a valid script next it's going to create the runners that are going to execute the test and as you can see here we currently have one Runner ready so the k6 operator will monitor these Runners and make sure that they're ready to run before executing the entire test and then once the runners are ready the Kasich starter is created and that's going to send a start signal to actually begin the test so this approach using the kubernetes operator is the k6 operator in kubernetes solves our problem of synchronization and it allows us to test across many different machines and scale to loads of over 100k virtual users so once we solve this problem of generating more than 100K virtual users our bottleneck moved down into the data pipeline which is that we now had to process 1 million plus data points per second and we wanted to do all of this in real time so for all its benefits k6 actually ends up outputting all of the raw data from a load test so it ends up generating a lot of data the amount of gender data generated is going to depend based on the size of the test and based on the types of metrics that you want to collect with 100K virtual users you can get up to or even greater than a million data points per second so there are three ways that we explored in order to actually process these 1 million plus data points in real time and I'm going to go through all three of them so the first approach was just writing the raw data straight to a database the second approach was aggregating on the k6 runners in order to reduce the data a little bit before sending it off and the third approach was aggregating centrally which meant sending all the data to a centralized server aggregating on that server and then writing to a database so I'll walk through all three of these approaches so the first method that I mentioned is writing the raw data straight to a database as you can see postgres can handle about 1K rights per second and even influx DB which is a very performant time series database handles around 100K rights per second so it is actually possible to improve the number of Rights from a database perspective through techniques like sharding but that would have added a lot of complexity and so it wasn't a path that we ultimately decided to go down the second option was to actually aggregate on each machine that was running k6 so in this case the way that it would work is we would write a k6 extension so that instead of outputting the raw data it would aggregate on the machine to cut down from say a million data points to something that was more manageable and then it would send off that data however the problem here is a mathematical problem so as we mentioned we we wanted to use percentiles because they're more accurate and descriptive for load tests but if we take the response times from each Runner find the 99th percentile and then average them across Runners we're not guaranteed to get the correct value especially if a specific Runner is for some reason receiving much faster or slower response times so you can see that here where we have machine one and machine two machine one is seeing a 99 percentile of 500 milliseconds machine two is seeing two thousand milliseconds and if you just take a straight average you end up getting a 99 percentile of 1250 milliseconds but if you combine all the data into one server you see that the actual correct calculation for a 99 percentile is 2 000 milliseconds so the Insight here is that if you want to be guaranteed to get the correct percentile value you need to make sure that all of the raw data is available on one Central machine so aggregating on the runners was not a path that we decided to go down so here's the solution that we ended up going with and it was Central aggregation with stat site so what is stat site statsite is a performance stats D server written in C and statsd is just a very simple protocol that was written by Etsy for implementing and aggregating metrics statsite uses UDP when receiving data in order to reduce latency and Stat site is especially fast because it Aggregates Trend data points like the percentiles we talked about using a probabilistic data structure so what this means is that it doesn't need to sort the 1 million plus data points in order to find say the 99th percentile instead the algorithm is going to estimate the distribution based on a smaller sample size and then come to a accurate estimate of the percentiles based on that and so using a stat site server allowed us from to go from an input of a million plus data points per second to in the end writing 20 data points or less to the database every five seconds okay so after we solve these two problems the final problem that we needed to address was to make sure that we could actually extract useful insights from both HTTP and websockets during our load tests when we started trying to visualize our data we noticed that by default k6 actually didn't have the most actual metrics for websockets and this is because k6 is not necessarily A specialized load testing tool for websockets so what we did is we ended up extending k6 to add five additional metrics the way that this worked practically is that we wrote an extension in go and this extension would emit our custom metrics into our data pipeline along with all the default metrics that k6 emitted as well and that would all get written to our database where we would have access to visualize the data and this let us visualize everything that we wanted for both HTTP and websockets so here are a few of the metrics that we added I'm not going to list all of them but I'll reference to that Jenny mentioned earlier which are failed handshakes and abnormal closures failed handshakes is the number of websocket connections that can now be established and abnormal closures is the number of connections that are dropped and so these are two metrics that were not included in the k6 default library but through our research we we found that these were very important metrics to evaluate a websocket load test until we wrote a custom extension to include these among three other ones that we included as well and finally once we had all those metrics available we were then able to visualize both HTTP and websocket metrics in grafana so all of this information will roll in live into grafana as the load test is running and developers will be able to see their graphs update in real time and react accordingly for both HTTP and websocket load tests and that's it so at the end of the day we were able to solve these three challenges in order to build edamame we were able to coordinate distributed tests of greater than 100K virtual users we were then able to process the 1 million plus data points per second in real time and finally we were able to extract useful insights not just for HTTP but also for websockets with our custom go extension so thank you everybody for listening I hope that walkthrough was informative and I will now open up the floor for questions Okay cool so I'm going to read out these questions if you guys have questions just send them through and I'll read them out and then we can answer them um so the first question here is why is there a hard limit of 200k users couldn't the k6 runner be scaled to meet any number of users um how did you arrive at 200k as a sensible number yeah so I can give a shot at answering this so 200k is our current hard limit because we currently have a bottleneck in our Central aggregation data pipeline which is mainly our stat site server as Albert mentioned um 200k virtual users approximately um that's about 2 million metrics which is all that we've tested up to now we we feel pretty optimistic about being able to increase this limit with more Performance Tuning or perhaps by exploring another data pipeline architecture but with our current implementation 200k is the bottleneck yeah so hopefully that gives you some insight to your question yeah okay so next question why the name edamame okay I guess I can answer this one uh let me think so I think it had something to do with pods because we're using kubernetes and we cycle through many many many names I think we spent a couple days on it and Jenny in fact listed gave us a list of about a thousand Greek characters from uh from Greek myths and nothing stood out to us so edamame is a reference to edamame pods and also it was a friendly name so we really liked it all right so question number three how did you guys collaborate When developing and deploying your Cloud infrastructure um Ethan can you can you expand on what you mean unless somebody knows Rachel if you want to answer that question I can take a shot short uh I mean we used GitHub to create products and issues for um how we were building edamame and like the steps we took while building it out um and through GitHub like everyone was able to pull the latest main commit locally and test out like whether things were working or not and then also add features onto it um so hopefully that answers that question but if that doesn't feel free to ask another question yeah feel free to clarify Ethan um okay next question did you explore other messaging protocols Beyond websockets so I can answer this question um so yes we did explore other protocols uh the reason as Jenny mentioned the reason we went with web sockets is because we were really focused on we noticed that there were a lot of uh of collaboration apps recently because of the whole remote work phenomenon and so we started wondering we researched load testing and we started wondering how would you actually load test one of these collaboration apps and would it be different and so we looked at the technologies that they used and so websockets was a key technology that uh was used in these collaboration apps but definitely there are a bunch of other protocols that you might want to load test and so depending on the type of app that you're using um that might change right if you're doing some sort of video app maybe you want to you want to load test your webrtc server it just depends but yeah we definitely looked into them but we were specialized in collaboration apps okay so next question what other advances do you anticipate in this space that could improve edamame um what other advances so I'm not sure if I fully understand the question um I think in general in terms of maybe I'll speak to how we would improve edamame in general and some of the things that that we would want to do so we scaled as Luke mentioned to 200k Virtual users um however we know that we kind of have a bottleneck at the centralized server and so there's a couple of options that we can use to expand that because as I mentioned with the percentile issue all the data needs to hit currently a centralized server but there's a lot of specific special data structures that we could use on the runners to aggregate the data before sending to the server so that is our current bottleneck and there are some ways that we could increase the amount of virtual users um and then other than that it's just going to depend probably on the company and what their needs are for load testing cool okay it doesn't look like there are any more questions so I guess we'll end it here thank you everybody very much for coming and listening to our presentation and again if any other questions come up uh we're open so feel free to message us individually and uh have a good night everyone 