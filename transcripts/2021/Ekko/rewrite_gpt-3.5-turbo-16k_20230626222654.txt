Today, we will discuss the development of Echo, a real-time message processing framework. Before diving into the details, let me introduce our team. Although we are geographically dispersed, this project provided us with a valuable opportunity to learn how to collaborate effectively remotely. Throughout this presentation, you will hear from each team member as we cover different aspects of Echo.

Let's start by discussing real-time web applications. These applications are prevalent in various forms, such as live dashboards, location tracking apps, and chat applications. The defining characteristic of a real-time web application is its ability to provide users with a perception of instant interaction. A user's perception is crucial in any real-time data exchange, and it guides our understanding of what constitutes a real-time web application.

To achieve real-time interaction, we need to shift from the traditional pull-based approach of data transfer, where data is stored and retrieved upon explicit user requests. Instead, real-time web applications follow a data push pattern, where new data is immediately sent to clients as soon as it is generated. This instantaneous delivery creates a seamless and dynamic user experience.

Another important aspect of real-time web applications is the shift from the server-client model to a publisher-subscriber model. In this model, we focus on the roles played by devices rather than the devices themselves. Data is exchanged in the form of messages, which can include various types of information, such as text messages, location data, and more. This abstraction allows us to concentrate on the functional aspects of the system rather than the implementation details.

Within the publisher-subscriber model, we often encounter the one-to-many pub-sub pattern, where a single publisher sends messages to multiple subscribers. There is also the possibility of a many-to-many pub-sub pattern, although it is more complex to implement due to the number of connections involved. In many-to-many scenarios, a central hub is often employed for efficient message distribution among publishers and subscribers.

Additionally, it is common for devices in real-time web applications to serve as both publishers and subscribers. For instance, in a chat application, a device acts as a publisher when sending a message and as a subscriber when receiving a message. This bidirectional communication further enhances the real-time nature of the application.

Now, let's explore how we can implement a real-time feature, using a customer service chat box as an example. When adding real-time features to an application, we face certain challenges. Researching real-time protocols, we come across options such as Server-Sent Events, HTTP Long Polling, HTTP Streaming, WebRTC, Web Transport, and WebSockets. However, since our chat feature requires bidirectional communication, most of these protocols are not suitable. WebSockets emerge as the most prevalent choice for implementing real-time features in web applications.

Creating a proof of concept with WebSockets is relatively simple by integrating a WebSocket library such as Socket.IO into your application code. However, when attempting to scale the real-time functionality or add more real-time features, you encounter various difficulties. As you invest more time into managing the real-time infrastructure, the focus on your application code diminishes. Scaling can also become challenging when an application's popularity surpasses the capacity of a single server, requiring a horizontally scaled architecture.

WebSockets pose unique challenges when working across multiple server instances. Moreover, managing the pub-sub model and synchronizing publisher, subscriber, and message data between servers can become complex. Additionally, you may find that your application's scalability requirements differ from its real-time needs. These factors collectively emphasize the difficulty of managing real-time connections and suggest the necessity of dedicated real-time infrastructure, which is provided as a service by leading companies in the field.

As you continue to add real-time features to your applications, you will encounter another concept: real-time middleware. Real-time middleware acts as an intermediary in the message processing pipeline. For example, in our customer service chat scenario, translation could be an example of real-time middleware that intercepts messages, translates them, and sends them back to the customer. Other examples include message transformation, message enrichment, message filtering, and chat bots.

Real-time middleware serves various purposes, such as altering the content of messages, enriching messages with additional information, filtering messages based on specific criteria, and analyzing message data for decision-making. This pattern has been recognized by industry leaders in real-time infrastructure services, as they observed clients resorting to sending messages from the real-time infrastructure to their own application servers for additional processing. This practice defeats the purpose of decoupling application servers from real-time infrastructure and undermines scalability.

To summarize, real-time web applications require a shift from pull-based data transfer to a data push pattern, facilitated through the publisher-subscriber model. WebSockets are the most common protocol used for implementing real-time features. However, managing real-time connections and scaling real-time functionality can be challenging, leading to the need for dedicated real-time infrastructure. Real-time middleware acts as an intermediary for modifying, enriching, filtering, and analyzing messages. By understanding these concepts and challenges, you can effectively implement real-time features in your applications while ensuring scalability and performance. Real-time middleware is a concept that involves intercepting messages and translating them into different languages based on the user's location. This is particularly useful when dealing with customers from different countries. Middleware acts as a bridge between the sender and recipient, allowing the messages to be transformed and enriched in some way. Examples of real-time middleware include Uber's app, which filters drivers based on proximity, profanity filters in chat applications, and chat bots that analyze messages and respond accordingly. This concept has been recognized by companies like PubNub and Ably, who have developed solutions to address the need for real-time middleware.

Both PubNub and Ably have noticed that their customers were recoupling their application servers with their own infrastructure in order to implement the necessary business logic for real-time messaging. This led to scalability issues, as the application servers needed to handle the demands of real-time messaging. To address this problem, PubNub developed PubNub functions, which are proprietary serverless functions that can be deployed within their platform. Ably took a different approach by allowing users to create serverless functions using various providers and integrating them into their Ably pipeline. Both solutions aim to remove the business logic from the application code and provide a scalable infrastructure for real-time messaging.

Currently, there are three main options for building real-time features: using a third-party service, using an open-source solution, or building it from scratch. Each option has its own trade-offs and the choice depends on specific use cases. Third-party services are easy to set up, but they may limit control over data and infrastructure. Open-source solutions offer more control, but the available options vary. Building from scratch provides complete control but requires significant effort.

To fill the gap in the market, we developed Echo, an open-source framework that allows users to deploy their own real-time infrastructure with real-time middleware. Echo consists of four main components. The Echo server manages real-time messages for applications. Echo functions provide the middleware for processing messages in transit. The Echo CLI tool simplifies the management of Echo functions and infrastructure. Lastly, the Echo client enables the development of real-time applications on top of the Echo server. With Echo being open source and self-deployed, users have full control over the infrastructure and can customize it to their needs.

To demonstrate how Echo works, we'll walk through the process of setting up the Echo infrastructure and deploying a real-time application. The Echo CLI tool provides commands for managing the Echo infrastructure. By running the Echo init command and providing AWS credentials, we can spin up a new Echo infrastructure using the AWS Cloud Development Kit. This process typically takes 10 to 15 minutes. The Echo infrastructure consists of an Echo server deployed on AWS Elastic Container Service, an application load balancer, an S3 bucket, and an Elastic Cache instance. The Echo server is responsible for managing real-time messages.

Once the infrastructure is set up, we can deploy a real-time application that utilizes the Echo server. The Echo apps directory in the Echo directory contains demo applications that help developers become familiar with building real-time applications using Echo. For our example, we'll use the chat demo application. This application consists of an HTML file that includes the Echo client script and a JavaScript file that creates a new Echo client instance. The Echo client instance requires an Echo server endpoint, a JSON Web Token, and a UUID. These values can be generated using the Echo JWT command provided by the CLI tool.

With the chat demo application connected to the Echo server, we can test its functionality by opening the HTML file in two different browsers, simulating two different Echo clients. Both clients are subscribed to different channels, which act as chat rooms. When one client publishes a message, the Echo server emits that message back out to all subscribed clients. The Echo server is responsible for managing the real-time messages for the application.

To add real-time middleware to the application, we need to deploy Echo functions. The Echo functions directory within the Echo directory contains demo functions that can be deployed to AWS Lambda. These functions are designed to process messages in transit and enhance them according to specific requirements. For example, the demo angry function capitalizes the text and adds exclamation points to messages on the angry channel.

Using the Echo CLI tool, we can navigate to the Echo functions directory and deploy the functions to AWS Lambda. The Echo deploy command deploys the functions, and the Echo status command confirms their deployment. An associations.json file is used by the server to determine which functions are associated with which channels.

After deploying the Echo functions, we can test their functionality within our infrastructure. When a client sends a message on a specific channel, such as the angry channel, the Echo server sends the message to the associated Echo function for processing. The function modifies the message and sends it back to the Echo server, which then broadcasts it to all subscribed clients. This allows for real-time processing of messages without the need to refresh or update client-side or server-side code.

The architecture of the Echo infrastructure includes the Echo server, AWS Lambda functions, an application load balancer, an S3 bucket, and an Elastic Cache instance. Each channel is associated with a specific Echo function, enabling message processing in real time. As clients send messages, the Echo server directs the messages to the appropriate Echo function for processing before broadcasting them to subscribed clients.

Building Echo presented several engineering challenges. The first challenge was implementing Echo functions, the real-time middleware. We had to determine the best technology to use for implementing these functions. The second challenge was managing the data that links specific Echo functions with real-time messages. We solved this by using an associations.json file. The final challenge was ensuring that Echo could scale up as load and demand increased. This challenge was addressed by making Echo compatible with AWS Lambda, which provides the ability to scale automatically based on demand.

In conclusion, Echo is an open-source framework that allows users to deploy their own real-time infrastructure with real-time middleware. It addresses the need for real-time messaging with customizable solutions. By using Echo, users have control over their data and infrastructure, and can easily deploy, update, and destroy Echo functions. Echo functions enable real-time message processing without the need to make changes to client or server code. The engineering challenges of implementing Echo were addressed by selecting the appropriate technology, using an associations.json file for managing data, and ensuring scalability by integrating with AWS Lambda. In this coding Capstone project video, we will explore a real-time middleware called "Echo" and its "Angry" demo function. The demo function has a specific file structure and format, allowing it to be deployed to AWS Lambda. The purpose of the demo function is to take the payload message, capitalize the text, and add a few exclamation points. Now that we have seen how these functions work, let's return to our CLI tool to deploy them.

Using the CLI tool, we can navigate to the "Echo Functions" directory and check the status of our functions. We can see that none of them are deployed. To deploy them, we can simply run the "Echo deploy" command. This command deploys the functions to AWS Lambda, where they can run alongside our server. After running the deploy command, we can check the status again to confirm that the functions have been deployed. 

There is one more step to complete. We need to run the "updateassociations.json" file. This file is used by the server to determine which functions are associated with which channels. We will provide more details on the "associations.json" file later. Returning to our chat demo, let's see if these deployed Echo functions are working. And indeed, they are! Our real-time messages are being processed seamlessly, without the need for browser refresh or updates to client or server-side code. Instead, we use Echo functions on our local machine and deploy them to AWS Lambda for processing.

There are numerous benefits to implementing real-time middleware like Echo in this manner, which we will discuss further. For now, it is evident that Echo functions make it easy to create, deploy, update, and destroy these Lambdas. Let's now examine how these deployed Echo functions operate within our infrastructure. On the right side, we can see the Echo functions that have been deployed to AWS Lambda.

When a client sends a message on the "Angry" channel, the server receives it and sends the message to the Angry Lambda for processing. The processed message is then sent back to subscribed clients. The same process occurs for the "Backwards" and "Robot" channels. The server determines which Lambdas are associated with each channel by referring to the "associations.json" file.

Now that we understand what Echo is and how it works, I will hand it over to Dory, who will discuss the engineering challenges faced during the development of Echo. Thank you, Drew. The engineering challenges we encountered can be categorized into three main areas. The first challenge was implementing Echo functions as a real-time middleware. We needed to determine the best technology to use for this purpose.

We decided against placing the real-time middleware on the Echo client due to security and performance concerns. Placing it on the Echo server would require redeploying the server every time the middleware is updated, and it would mix unrelated business logic that needs to scale independently. Another option was to use a server dedicated to message processing within the real-time service, but this would introduce similar problems to putting it on the Echo server.

We ultimately chose to use serverless functions for our real-time middleware. Serverless functions allow us to break down our business logic into independent pieces that can scale separately. For example, if we have a chat app using the Echo server and filtering out profanity from messages, as well as a geolocation-based app that needs to incorporate directions from the Google Maps API, using serverless functions allows us to scale the profanity filtering independently from the directions processing.

Serverless functions also offer flexibility and reusability across multiple apps. When functions are updated or added, the rest of the real-time service remains unaffected. However, it is worth noting that serverless functions have limitations, such as maximum compute length and payload size. They also add complexity to the infrastructure and can have a startup delay for the first execution. Despite these challenges, serverless functions align with our goal of providing a flexible framework to meet changing business needs. Moreover, this approach is consistent with how services like Ably and PubNub handle message processing.

Let us now move on to the second engineering challenge: managing Echo functions. Once we determined the location of our real-time middleware, we needed to establish how to manage it, specifically linking the middleware with real-time messages. Since clients publish and subscribe to channels, we decided to link specific channels with specific Echo functions. For instance, in a chat app with a profanity filter, we can create a channel where all messages are processed through the filter before being sent to subscribers.

We also considered the possibility of using multiple Echo functions to process a message. For instance, imagine needing to translate a chat message to another language. In this case, we can chain Echo functions so that the processed message from the profanity filter passes through the language translation function before being returned to subscribers. To manage the middleware, we created an "associations.json" file.

This file organizes the associations by application, with each application having an array of channels. Within each channel, there is an array of Echo functions to be used. When processing a message, Echo uses all the specified functions in order. The "associations.json" file resides in an S3 bucket, an AWS cloud storage service. The developer is responsible for manually updating this file and uploading it using the Echo CLI tool.

In terms of updating the server nodes, we explored two approaches. The first involved using the AWS service called CloudWatch to send a message through SNS (Simple Notification Service), which would notify the Echo server of the new data. The second approach was to directly update the Echo server when uploading the JSON file to S3. We decided to implement the second option because it didn't add complexity to our infrastructure. In the CLI tool, developers can use the "Echo update" command, which simultaneously uploads the JSON file to S3 and sends a PUT request to the Echo server with the file as the payload.

Allow me to demonstrate how to chain Echo functions using a little demo. First, we'll create a new Echo function called "demo emojify" by creating a new folder in the Echo Functions directory. In the index.js file, we will define the message processing logic for this function. In this case, we'll add an angry emoji before and after the message text. We then deploy the new Echo function to AWS Lambda using the CLI tool.

Next, we need to edit the "associations.json" file to add "demo emojify" to the "angry" channel, right after the "demo angry" function. Finally, we update Echo with the new associations file using the update command. This process uploads the associations file to S3 and sends a PUT request to the Echo server.

Let's see the Echo function chaining in action in our demo chat app. Publishing a message to the "angry" channel shows that it is successful. The message is capitalized using the first Echo function and gets angry emojis added using the second before being returned to subscribers. And now, I will pass it over to Alex to discuss our final engineering challenge.

Thank you, Dory. The last engineering challenge we encountered was ensuring that Echo could scale horizontally. We wanted Echo to be able to scale up and down as needed, especially when dealing with high numbers of users and message volumes. To achieve this, we leveraged the infrastructure and policies provided by AWS. The primary component of Echo that needed to scale was the Echo servers.

To support flexible user connections and increased message volumes, we considered using AWS Fargate to package our server application as a Docker container. This allowed us to scale the server tasks up and down dynamically based on workload. Fargate considers factors like CPU and memory usage to scale the tasks accordingly.

However, when scaling up to multiple server instances, we encountered challenges with establishing WebSocket connections for our clients. By default, the application load balancer routed the two-part WebSocket handshake to different server nodes, resulting in connection failures. To address this, we added configuration to our AWS CDK deployment code. Sticky sessions were enabled to ensure that each client was routed to the same initial server instance, allowing successful WebSocket connections.

Once the WebSocket connections were established, we faced another challenge. Messages published to one server were not being broadcasted to all subscribers across multiple servers. To solve this, we utilized the Socket.IO Redis adapter library. This library leverages an AWS Elasticache Redis instance to broadcast events to all Socket.IO server nodes. With this setup, when a message is published to a channel, it is emitted to all subscribers, regardless of the server instance they are connected to.

To validate the performance of our infrastructure, we conducted load testing using artillery.io. Our first test scenario aimed to determine the sustainability of high message volumes through the Echo server. We successfully sustained 50,000 messages per minute, which included the invocation of Lambda functions for message transformations. In total, half a million messages and Lambda function invocations were sent over a 10-minute test period. AWS Elastic Container Service scaled up by spinning up two additional Echo server containers to handle the increased load.

Our second test scenario focused on supporting a large number of subscribers receiving messages. Echo successfully sustained 600,000 messages received per minute for a single channel. These numbers exceeded our expectations and did not max out our server capacities or cause failures. We could have pushed the load even further, but it was unnecessary for our use cases.

The final engineering challenge we encountered was synchronizing state between all Echo server instances. We needed to ensure that all server instances had the latest version of the "associations.json" data. We achieved this by manually updating the "associations.json" file and then using the Echo CLI tool to upload it to S3 and send a PUT request to the Echo server. This process synchronized the data across all server nodes without introducing extra complexity.

Overall, we have successfully navigated the engineering challenges while building Echo. Through the use of serverless functions, effective management of Echo functions, and horizontal scaling, we have created a real-time middleware that is flexible, scalable, and efficient. The combination of these engineering solutions is why Echo is a powerful tool in managing real-time services. We made some configuration changes to our CDK deployment code. One change was enabling sticky sessions as a policy for our managed server instances. This allowed each echo client to be routed to the same server instance it was initially assigned. With this change, we were able to establish successful WebSocket connections.

However, we encountered a new challenge when we established these WebSocket connections. We noticed that messages published to separate servers weren't being broadcasted to all of our subscribers. To illustrate this issue, let's consider a scenario with two instances of our echo server.

When we have two instances, the load balancer will connect one user to server instance A and another user to server instance B. In this scenario, both users are subscribed to the same channel, so they can chat with each other. Alice has a WebSocket connection to server A, and when she publishes a message, server A receives it and publishes it to the channel. However, only the WebSockets connected to server A will receive the message. Bob, who is connected to server B, won't receive it.

To address this problem, we implemented the Socket.IO Redis Adapter library. This library uses a Redis instance hosted on AWS's Elasticache service to broadcast events to all of the socket.io server nodes. So now, when Alice's message is published to server node A, it is automatically emitted to all subscribers, even if they are connected to other servers. This infrastructure setup, combined with sticky sessions, ensures that all subscribers receive the published messages.

To validate some of our main use cases, we performed load testing using artillery.io. Our first test scenario aimed to determine if Echo could sustain a high volume of messages. We found that Echo was able to handle 50,000 messages sent per minute. Each of these messages is also transformed by Lambda functions, which act as our serverless middleware. During the test, we sent a total of half a million messages, and the Lambda function invocations were successfully processed over a period of 10 minutes. To handle the additional load, AWS Elastic Container Service spun up two additional Echo server containers.

For our second test scenario, we wanted to see if Echo could support a large number of subscribers receiving messages. We discovered that Echo was able to sustain 600,000 messages received per minute on a single channel. We were pleased with these numbers, as they didn't max out our server capacities or cause any failures. We could have potentially increased the load even further, but it didn't seem necessary based on our use cases.

An engineering challenge we faced was figuring out how to synchronize the state between all of our Echo server instances. We needed to ensure that all server instances had the latest version of the associations.json data, which pairs channels with the Echo functions executing on the associated messages. Initially, when updates were made to the associations.json file, we would send a put request with the updated data as the payload. However, this request would be routed to just one of our Echo server instances. To notify all instances with the updated data, we used the standard Redis package. The server that receives the message publishes the file to the Redis Elasticache cluster, and the other server instances subscribe to the Redis cluster and receive a copy of the new associations' data. This ensures that all our server instances are in sync and share the associations' data among them.

Echo is an open-source framework that allows developers to easily add real-time infrastructure and in-transit message processing to web applications. Throughout this project, we have demonstrated the flexibility of Echo and the numerous possibilities it offers. By combining a real-time server with serverless functions as middleware for in-transit processing, Echo opens a world of options for developers. We are excited to see what you build with Echo. You can test drive our code and demo applications by visiting our GitHub repository, and a case study will be available on our webpage soon.

Now, let's open the floor for questions.

Question: What was the biggest challenge in building Echo?

Answer: The biggest challenge we faced was during the deployment phase when working with AWS infrastructure. Debugging issues that arose during deployment was a team effort, as problems could arise from different components such as the CDK code, the Echo client, or the Echo server.

Question: What was the most rewarding part of building Echo?

Answer: Each team member had different rewarding aspects of the project. For one team member, the most rewarding part was working on the CLI tool, which initially seemed like a daunting task but turned out to be achievable step by step. Another team member found the ideation phase and exploring different ideas to be highly rewarding. And for another team member, the most rewarding moment was when they were able to download the CLI code from npm, install it on their machine, and use it to deploy Lambdas and build a real application. It was a turning point that sparked excitement about the possibilities of Echo.

Question: What did the MVP of Echo look like, and how did you approach building it?

Answer: The initial MVP of Echo was a simple WebSocket server and client. We started with a proof-of-concept by setting up a basic Express app with the Socket.IO library, which allowed us to quickly establish a real-time connection. From there, we gradually built a production-ready version of Echo with additional features.

Question: Was working with AWS smooth sailing?

Answer: Working with AWS had its challenges. While some parts were relatively easy, such as using the SDK, there were also difficulties in dealing with permissions and coordinating different services. The documentation for the AWS Cloud Development Kit (CDK) was sometimes opaque, requiring thorough reading to find specific information.

Question: Was seeing the load testing results as gratifying as imagined?

Answer: Yes, seeing the load testing results was truly gratifying. It was one of the highlights of the project. Testing Echo in action and witnessing its ability to handle a high volume of messages and a large number of subscribers was impressive. It validated the effectiveness of our implementation.

Question: Why did you choose the domain of real-time and, specifically, real-time middleware?

Answer: Initially, we explored a webhooks project, which demonstrated the data push model. This led us to the real-time space, where we discovered PubNub, a company that offered real-time services. Real-time seemed like an interesting domain to explore. Additionally, we found an existing Capstone project called River, which provided a drop-in service for real-time communication. We saw an opportunity to expand on River's concept and address the challenges of the multi-pub/sub model, which led us to develop real-time middleware within Echo. During the development of our coding Capstone project, there was a particular configuration that gave me a lot of trouble. I spent about a week banging my head against the wall, trying to apply it to our infrastructure. We could have done it manually, but we wanted to incorporate it into our deployment code. It definitely wasn't a smooth process. It's frustrating when you know what needs to be done, but you're struggling to find the specific code snippet or configuration that turns something on or off.

In addition to this, some parts of the project were easy, especially when working with the software development kit (SDK). However, we encountered difficulties with permissions and integrating different services that needed to work together. We also had questions about the results of load testing, and I can confidently say that it was as gratifying as I had imagined. Load testing was one of my favorite parts of the project. It was amazing to see the tool we had spent weeks building actually in action. We set up 50,000 connections and fired them at the echo server, which in turn hit the lambdas and middleware. Over the course of 100 seconds, we hit 500,000 lambdas and 500,000 messages. It was truly impressive to witness the effectiveness of our work.

As for why we chose the domain of real-time, particularly real-time middleware, there were a few reasons. Initially, we were looking into web hook projects, which involve a data push model where one server automatically pushes data to another server. This led us to explore the real-time space. We discovered a company called PubNub that offered real-time services, and we found it to be an interesting area to explore. However, we also came across an existing capstone project called River, which built a real-time service that could be seamlessly integrated into existing applications. We wanted to take that concept a step further by creating a service that could handle the many-to-many publisher-subscriber model. As we delved deeper into this space, we encountered several intriguing problems and ultimately decided to focus on real-time middleware for our project.

Looking ahead to the future of Echo, there are several features we would like to add. One of them is message persistence, where messages can be stored for users who may have unstable internet connections. This way, if someone goes offline, Echo can send them any messages they missed. Additionally, we want to implement end-to-end encryption for the messages. This would ensure that the Echo server does not have access to user data, as the messages would be encrypted during transit. Another important feature we want to incorporate is handling in-order delivery of messages. Currently, we prioritize delivering messages quickly, but there is a chance that messages could be delivered out of order due to differences in processing time among the lambda functions. This is a challenge that all major players in the space, such as PubNub and Ably, leave to the developers to address. Implementing a solution to ensure in-order delivery would be a valuable addition to Echo.

In terms of usage, Echo is relatively new and open source. So far, we are the only ones who have used it extensively. As a team, we have been the first to explore and work with Echo, having recently completed the project. However, we are excited for others to try it out and provide feedback. If any of you watching this video are interested in using Echo, please feel free to do so and let us know about your experience.

Reflecting on the experience of researching, planning, and building Echo, I can confidently say that it would have been a significantly different task if we had not attended the Capstone program. The knowledge and skills we acquired during Capstone were vital in understanding the problem space and figuring out how to approach the project. It would have been a much more challenging undertaking without the guidance and insights we gained from the program. Capstone equipped us with the tools to avoid common pitfalls and provided resources and support to help us think critically about our ideas and tackle engineering challenges. The program helped us stay focused and validated our decisions, leading to a more directed and smoother development process.

In conclusion, we appreciate everyone's attendance and participation in this Capstone presentation. We understand that Capstone presentations can sometimes feel distant when you're still working through core programming concepts, but we hope that through this demonstration, we have given you a taste of what you can achieve using Echo. We would love to hear from anyone who ends up building something with Echo. Feedback and insights from the community are invaluable to us. Thank you all once again for your support and interest in our project.