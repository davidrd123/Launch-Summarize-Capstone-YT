Good day everyone! I would like to welcome you to our presentation and express our gratitude for attending today. Over the past few months, our remote team of four software engineers, located in different cities, has been dedicated to developing a project called Monsoon. In this presentation, we will introduce you to the concept of load testing, provide an overview of Monsoon, discuss its design decisions and architecture, demonstrate its functionality, and share our plans for future work. We will then open the floor for questions from the audience.

Monsoon is an open-source serverless framework designed to run browser-based load tests in the cloud. This tool is essential for companies seeking to create and maintain robust, scalable websites and web applications. Load tests simulate traffic hitting your servers, enabling you to measure how your app performs under different load profiles. The results are valuable for both business stakeholders and engineering teams. From a business perspective, load testing helps identify performance issues when apps struggle to handle existing traffic, leading to revenue loss. For engineers, load testing provides insights into the production system's capacity, response time, and failure rates under heavy loads.

To illustrate load testing in action, let's consider a case study involving Boost Health, a rapidly growing startup in the health and wellness industry. Boost Health is planning a major product launch for their new brain boost supplement, which is expected to generate high levels of traffic. The company's engineers are tasked with ensuring their website can handle the anticipated 4,000 concurrent users.

Boost Health's website is a single-page application (SPA), a type of web app that consists of a single HTML page. Unlike traditional websites, SPAs dynamically update content using browser-side JavaScript, resulting in faster development and improved performance. However, this characteristic presents challenges for load testing using certain tools.

One tool the Boost engineers initially tried was Apache JMeter, a well-established, protocol-based load testing tool. Protocol-based load tests operate at the HTTP protocol layer, requiring developers to manually write code to simulate each HTTP request triggered by a user action. However, JMeter cannot effectively test SPAs with extensive JavaScript code, rendering it unfit for Boost's purposes.

Realizing the limitations of JMeter, the team explored a different option: Selenium. Selenium is a browser-based load testing tool, which simulates web traffic using real web browsers, capable of handling SPAs. Unlike protocol-based tools, Selenium focuses on end-user actions rather than individual network requests, simplifying the testing process.

The Boost engineers initiated a browser-based load test using Selenium, first simulating five concurrent users without issues. However, when they attempted to test 100 concurrent users, resource limitations became apparent. Selenium runs locally on a developer's laptop, and testing with multiple browser instances proved too resource-intensive.

To overcome resource limitations, the Boost team turned to cloud-based browser-based load testing platforms and discovered Flood. Although capable of generating the required 4,000 concurrent users, Flood's high costs made it unfeasible for Boost's needs.

In light of these challenges, the Boost engineers embarked on a search for a cost-effective cloud-based solution and came across Monsoon, an open-source tool designed explicitly for browser-based load testing in the cloud. Monsoon offers a streamlined and scalable approach to load testing SPAs.

Monsoon leverages a powerful browser-based testing library called Puppeteer, which simulates user interactions on the Boost Health website. Using Puppeteer, engineers can write scripts that describe end-user actions in a simplified manner. Monsoon then uses these scripts to create and control headless Chrome instances, executing the desired actions defined by the Boost engineers.

Monsoon's architecture consists of four key components: load generation, transformation, storage, and visualization. With Monsoon, the load generation occurs in the cloud. When a test is initiated, Monsoon provisions multiple compute instances to generate the required load, scaling linearly based on the test's needs. Once the test concludes, the compute instances are freed up.

Monsoon manages the challenge of handling large volumes of data generated during load tests by aggregating performance metrics for each end-user action in 15-second intervals. These metrics are stored in a time-series database optimized for handling temporal data. Monsoon also provides a locally hosted weather channel dashboard for real-time visualization of performance metrics, including response time, concurrent users, transaction rate, and pass ratio.

In summary, Monsoon facilitates simple and scalable browser-based load testing for small to medium-sized businesses. By addressing the challenges posed by SPAs and utilizing cloud resources, Monsoon provides insights into app performance in near real-time. Our tool carves out a niche in the browser-based load testing space, allowing companies like Boost Health to effectively test and optimize their websites. Hosting a load testing tool in the cloud presents challenges due to the large amount of data generated by recording the actions of virtual users executing the boost test script. To address this, Monsoon calculates aggregated metrics for each end user action at 15-second intervals. These metrics are stored in a database, and the results are visualized using a locally hosted dashboard called Weather Channel. The dashboard allows boost engineers to monitor response time, concurrent users, transaction rate, and pass ratio in near real time.

Monsoon is designed to simplify and scale browser-based load testing by utilizing the cloud. It provides small to medium-sized companies with a simple and scalable solution to assess the performance of their SPAs. Unlike other options in the market, Monsoon is optimized for this use case, allowing companies to gain insights into their app's performance in near real time. The architecture of Monsoon consists of four key components: load generation, transformation, storage, and visualization.

Using Amazon Web Services (AWS) as its cloud provider, Monsoon is deployed directly into Boost's AWS account. To initiate a test, a Boost engineer utilizes a simple command from the Monsoon CLI, passing the test script and configuration parameters. Monsoon then spins up the required number of compute instances to generate the desired load, and the testing begins. The results of the initial test runs are pre-processed within the compute instances, and the data enters Monsoon's data pipeline for transformation.

The transformed, aggregated test results are then stored in a time series database optimized for ordered, timestamped data points. Monsoon's locally hosted dashboard, Weather Channel, provides engineers with a visual representation of the performance of their site under load. Key metrics such as response time, concurrent users, transaction rate, and pass ratio are available in near real time, allowing engineers to assess performance as the test progresses.

Comparing Monsoon to other browser-based load testing options, two typical paths companies like Boost might take are using a cloud-based software-as-a-service (SaaS) solution or building their own load testing tool. Cloud-based SaaS platforms, like Flood, offer scalability and ease of use but can be costly. On the other hand, a do-it-yourself approach can save costs in terms of ongoing platform fees but requires significant engineering hours and may not be feasible for smaller teams.

Monsoon bridges the gap between these options by providing a serverless, open source load testing solution designed for small to medium-sized companies. By deploying directly into Boost's AWS account, Monsoon gives Boost ownership of their data while keeping costs significantly lower than traditional SaaS platforms or DIY solutions. Users only pay for the AWS infrastructure costs incurred during testing.

The design decisions that shaped Monsoon's framework include a focus on high scalability and providing near real-time results. In terms of scalability, Monsoon optimized its load generation engine by utilizing Google's Puppeteer, which allows for more simulated virtual users per compute instance compared to industry-standard Selenium. This reduces the required compute resources and costs.

For data processing and transformation, Monsoon utilizes an ETL (Extract, Transform, Load) data pipeline. This approach reduces transmission times and costs by storing and analyzing transformed data instead of the entire raw dataset. Monsoon's ETL pipeline significantly reduces database writes and storage requirements, resulting in cost savings.

To achieve near real-time results, Monsoon extracts, transforms, and loads data into a time series database at regular intervals. This allows engineers to monitor tests and spot issues as they arise. Implementing a near real-time data pipeline introduces complexity but also enables engineers to leverage live test results and turn testing into an event at their organization.

Monsoon's architecture consists of four key components: load generation, transformation, storage, and visualization. The load generation component utilizes a Node.js app that runs on AWS Fargate and utilizes ECS (Elastic Container Service) for container orchestration. The app leverages the Weather Station library, user scripts, and a coordination script called runner.js to create virtual users, simulate their actions, and record metric data.

The transformation component focuses on processing and aggregating the captured data from virtual users. Monsoon's data pipeline follows the ETL pattern, transforming and storing the data in a time series database optimized for ordered, timestamped data.

The storage component holds the transformed, aggregated test results in the time series database. By utilizing an ETL approach, Monsoon reduces data transmission, storage costs, and ensures data integrity for analysis.

Lastly, the visualization component is provided by Weather Channel, a locally hosted dashboard that displays metrics in near real time. Weather Channel allows boost engineers to monitor key performance metrics such as response time, concurrent users, transaction rate, and pass ratio.

In conclusion, Monsoon is a cloud-based, open source load testing tool designed for small to medium-sized companies. Its architecture consists of four components: load generation, transformation, storage, and visualization. Monsoon leverages AWS for scalability and utilizes techniques like ETL and near real-time data processing to provide developers with insights into their app's performance. By being both cost-effective and easy to use, Monsoon offers businesses a simple and scalable solution for browser-based load testing. The real-time framework for our coding Capstone project has both advantages and disadvantages. On the positive side, engineers can monitor their tests in near real-time, allowing them to identify and address issues as they arise, rather than waiting for the entire test to finish. In some cases, the load test itself can become an event at the company, with teams reserving conference rooms, ordering food, and watching the load test results live.

However, implementing a near real-time data pipeline also comes with its complexities. There are multiple components involved, making it technically challenging to coordinate them all effectively to avoid data loss or double counting. We will discuss this complexity in more detail later in the presentation.

Now, let's dive deeper into the architecture of our project, specifically Monsoon. Monsoon's architecture is comprised of four key components: load generation, transformation, storage, and visualization. 

The load generation component is responsible for creating virtual users who perform scripted actions on the target website or web app and record metrics about those actions. At the core of this component is a node app, which includes the Weather Station library, a user script, and runner.js. The Weather Station library allows developers to write test scripts using methods from the web performance API. The user script contains instructions for the headless browser instances on what actions to perform. Runner.js coordinates the execution of these actions by creating and controlling multiple headless Chrome instances through Puppeteer.

To run our node app, we utilize AWS Fargate, a serverless compute engine that eliminates the need for managing servers. We containerize our node app and deploy it to AWS Elastic Container Service (ECS), which is used for container orchestration. For each block of 20 virtual users, ECS spins up additional Fargate instances to execute our containerized node app.

Once the load generation process generates session data for virtual users, we move on to the transformation architecture. This component takes the session metric data stored on each Fargate instance, transforms it into a time series data format, and loads it into a time series database. The runner.js file stores session metrics on the local file system of the Fargate instances, alongside the load generation scripts. The normalizer.js file retrieves this data every 15 seconds, applies transformations, and moves the transformed data to an S3 bucket for more permanent storage.

During the transformation process, normalization is a crucial step. It involves combining multiple data points with different timestamps into a single data point with a normalized timestamp. This step is necessary to facilitate data aggregation and ensure uniform timestamps for the subsequent aggregation process.

The transformed data is stored in an S3 bucket organized by timestamp, end user action, and Fargate instance name. However, the data in its current format is not suitable for a time series database. To address this, we utilize AWS Lambda, a serverless compute service, to further transform the data. The Lambda function aggregates the metrics for each timestamp from the S3 bucket and creates a single time series data point, which is then posted to the time series database.

The storage component of Monsoon's architecture is designed to store the data generated during the transformation step. Given the nature of the data, a time series data format is a good fit. We utilize Amazon Timestream, a fast and scalable serverless time series database service, which makes it easy to store and analyze trillions of events per day. Users connect to the Timestream service and create tables within it to store their time series data.

Finally, the visualization component of Monsoon's architecture queries the storage layer and displays the retrieved data in a graphical user interface (GUI) called Weather Channel. Weather Channel is a locally hosted React app that directly queries the Timestream database service. It uses the Victory library for modular charting and data visualization, as Victory has native support for time series data. Users can view the data in near real-time, customize the displayed metrics, and even receive email notifications for specific events.

In summary, Monsoon's architecture consists of load generation, transformation, storage, and visualization components. Load generation creates virtual users, while transformation normalizes and transforms data for storage in an S3 bucket. The storage component uses Amazon Timestream as a time series database, and the visualization component displays the data using the Weather Channel GUI.

Now, I will hand things over to my colleague Min to walk you through a demo of how to install and use Monsoon. To begin with, let's run the "monsoon destroy" command after the "tidal" command. Now that we have seen a demonstration of Monsoon, let's proceed to examine some of the implementation challenges we faced while building it. Two particular challenges stand out: metrics and timing.

Firstly, let's explore the issue of metrics. During the initial design phase of Monsoon, we dedicated a significant amount of time to determining which metrics to measure for a load test. There were numerous options available, but we needed to identify the metrics that would be most valuable to our end users. Upon examining existing load testing tools, we discovered that all of them primarily focus on response time as their core metric. Initially, this seemed reasonable, but upon deeper investigation, we found that response time is a highly subjective term. It could refer to the time between when a client sends a request and when that client receives the first byte of response, or it could consider the duration until the DOM content is fully loaded. We even contacted customer support at Flood for clarification on how they calculate response time, but we were unable to obtain a definitive answer.

To address this ambiguity, we decided to define response time as the difference between the starting and ending times for a given end-user action. This definition makes it easier for developers to calculate response time in their load tests. As we mentioned previously in our architecture discussion, within Monsoon, we determined that response time is the most important metric to collect for a load test, given the absence of a standardized definition.

Moving on, our second implementation challenge is related to timing, which was a larger and more complex issue. When a developer initiates a load test using Monsoon, the core code sends an event to a starting Lambda function, which in turn spins up the appropriate number of containers. However, it can take anywhere from 15 to 60 seconds for a container to be ready to run. This poses a problem because for effective load generation, all the containers need to start simultaneously. To overcome this challenge, we introduced a waiting time mechanism to ensure that all containers are in a ready state before Monsoon begins the actual test. This was the first timing issue we faced.

Once all the containers are up and running, the next challenge is how to coordinate the timing of data generation and processing. We need to carefully orchestrate our data pipeline. As mentioned in the architecture session, within each container, a runner script is responsible for generating headless Chrome instances using Puppeteer and running headless desktops to generate data. This raw data then needs to be normalized and sent to our S3 bucket. To provide near real-time results to developers, we cannot wait until all the raw data is generated before starting to process it. Therefore, our normalization script periodically pauses to check for new raw data every 15 seconds.

Once the normalization script has successfully moved the processed data into the S3 bucket, we encounter the third timing issue—how to transfer the data from the S3 bucket to our TimeStream database. To solve this, we created an event trigger to invoke our Metronome Lambda function once every minute. The Metronome Lambda function then digests a collection of pre-generated TimeStream records to identify any new batches of data. If any are found, the aggregated Lambda function is invoked, gathering the new batch of data and sending it to TimeStream. This resolves the third timing challenge.

With these timing challenges addressed, we now have a functioning data pipeline capable of moving raw test result data from containers to an S3 bucket and then to the database, performing necessary transformations along the way. However, there is still future work planned for Monsoon.

Firstly, we would like to rewrite certain parts of the project. Currently, Monsoon is entirely written in JavaScript and TypeScript. While this works well, certain components, such as the normalization script and the ingestion Lambda function, handle significant data processing tasks. These components could benefit from a faster runtime performance and better CPU utilization, which could be achieved through reimplementation in a compiled language.

Secondly, we aim to provide additional libraries to support load tests written for multiple testing tools. For example, we could develop a version of the Weather Station library compatible with playwright desktops.

Additionally, we plan to implement a new feature that checks load test scripts for correctness before running them. Currently, developers can initiate a load test even if the test script itself does not adhere to our instructions. This can lead to wasted developer time and unnecessary AWS costs. By incorporating a pre-check behind the scenes as part of the Monsoon start command, we can prevent these issues.

Lastly, we plan to enable developers to export load test results to a CSV file. This feature would allow developers to save and analyze the results in a format that is convenient for their specific needs.

In conclusion, that covers our presentation of Monsoon. We would like to express our gratitude for taking the time to join us today, and now we are open to any questions you may have.

Richard asks how much money the Boost company could save by using Monsoon. The cost savings would depend on the duration of the test they wish to run. Comparatively, using Flood to run a one-hour test can cost around $100 for 4000 users. With Monsoon, the cost would be significantly reduced to approximately $20. This represents a significant cost saving, especially considering that Monsoon does not have any platform fees, which can be significant with existing browser-based SaaS solutions.

Richard Cole asks about the challenges faced in learning the full stack. Learning the full stack can be quite challenging, particularly in the Capstone project. You are required to quickly learn and apply multiple new technologies within a short timeframe. The project pushes you to develop a skill called "just-in-time learning," where you acquire and utilize new knowledge as needed. This skill is crucial for success in the Capstone project.

With no further questions at the moment, we sincerely appreciate everyone's participation and thank you for joining us. It has been a great experience, and we value your time and engagement.