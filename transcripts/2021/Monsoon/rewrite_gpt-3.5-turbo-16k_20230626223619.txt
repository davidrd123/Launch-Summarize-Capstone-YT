Welcome to our presentation. Thanks for joining us today. Over the past few months, our remote team of four software engineers from different cities has been working on a project called Monsoon. We will provide an overview of Monsoon, including its architecture, design decisions, and future plans. Additionally, we will demonstrate Monsoon and discuss the implementation challenges we faced.

Monsoon is an open-source serverless framework designed for conducting browser-based load testing in the cloud. Load testing is a crucial tool for companies seeking to build and maintain scalable websites and web applications. It involves simulating traffic on the servers to understand how the app performs under different workloads. Both business stakeholders and engineering teams benefit from load testing. The results provide insights into an app's ability to handle concurrent users, response time under increasing loads, and potential issues such as failed requests.

To illustrate load testing in action, let's consider Boost Health, a rapidly growing startup in the health and wellness space. They are launching a new product called Brain Boost and are expecting a significant increase in traffic on launch day. The Boost Health engineering team needs to ensure that their single-page application (SPA) can handle the anticipated load. SPAs differ from traditional websites as they update the content on the same page using JavaScript, resulting in a more dynamic and responsive experience for users.

The Boost Health engineers initially attempted load testing using Apache JMeter, a protocol-based load testing tool. However, JMeter lacks a JavaScript interpreter and is incapable of executing JavaScript code, making it unfit for testing SPAs. They then explored browser-based load testing and discovered Selenium, a tool that utilizes real web browsers to simulate web traffic. Unlike protocol-based testing tools, Selenium operates at a higher level of abstraction, making it less complex and more suitable for load testing SPAs.

The engineers conducted browser-based load testing using Selenium but faced resource limitations when testing a higher number of users. Testing 100 concurrent users proved to be too resource-intensive for a single machine. They explored cloud-based load testing platforms and came across Flood, an industry leader. However, the cost of using Flood was prohibitive, leading the engineers to seek a more cost-effective solution.

Their search led them to Monsoon, an open-source tool geared towards browser-based load testing in the cloud. Monsoon leverages Puppeteer, a powerful browser-based testing library, to simulate user actions on the Boost Health website. With Puppeteer, the engineers can script user actions easily, simplifying the load testing process. Monsoon provisions multiple compute instances in the cloud to meet the scaling needs of the load test. After the test, Monsoon aggregates the performance metrics for each user action and stores them in a time series database.

Monsoon's architecture consists of four components: load generation, data transformation, storage, and visualization. Amazon Web Services (AWS) serves as the cloud provider for Monsoon. The load generation component starts a test by providing the test script and configuration parameters to provision the necessary compute instances. The test commences, and the results undergo preprocessing before entering Monsoon's data pipeline. In the transformation step, the aggregated results for all virtual users are processed. The transformed data is then stored in a time series database optimized for time-ordered data. Finally, Monsoon provides insights into app performance through its locally hosted Weather Channel dashboard, which displays real-time metrics such as response time, concurrent users, transaction rate, and pass ratio.

Monsoon offers a straightforward and scalable solution for small to medium-sized companies looking to load test their SPAs. By addressing the specific challenges of browser-based load testing, Monsoon has carved out a niche in the load testing space.

In conclusion, Monsoon is an open-source serverless framework that enables browser-based load testing in the cloud. It simplifies the load testing process for SPAs and provides near real-time insights into app performance. With its scalable design and cost-effective implementation, Monsoon offers a viable solution for load testing in the cloud. Hosting a load testing tool in the cloud presents challenges, particularly when dealing with the significant amount of data generated by each virtual user executing the actions described in the boost test script. Monsoon was developed to address these challenges by managing and extracting meaningful insights from the data. Monsoon calculates and aggregates metrics for each user action in 15-second intervals, storing the final metrics in a database. The results are then visualized using a locally hosted dashboard called Weather Channel, which allows boost engineers to monitor site performance under load. All metrics are available in near real-time, eliminating the need to wait for the test to complete before assessing performance.

Monsoon's high-level architecture consists of four key components: load generation, transformation, storage, and visualization. Monsoon utilizes Amazon Web Services (AWS) as its cloud provider, deploying directly into Boost's AWS account. In the load generation step, a boost engineer initiates a test using a simple command from Monsoon's command-line interface (CLI), providing the test script and configuration parameters. Monsoon spins up the necessary compute instances to generate the required load. The testing begins once the initial results are ready.

After the load generation, Monsoon performs a pre-processing step within the compute instances to aggregate the test results for all virtual users. The aggregated results are then stored in a time series database optimized for ordered data points. This storage step ensures efficient handling of the large amount of data generated during testing. Finally, Monsoon provides near real-time insights into app performance through its locally hosted Weather Channel dashboard.

In summary, Monsoon simplifies and scales browser-based load testing using cloud infrastructure. Its capabilities are mapped onto a high-level architecture diagram, with emphasis on load generation, transformation, storage, and visualization. Monsoon is designed for small to medium-sized companies, offering an open-source solution optimized for load testing up to 20,000 concurrent users. It is cost-effective compared to cloud-based software-as-a-service (SaaS) solutions or building a DIY load testing tool. By leveraging AWS services in a serverless manner, Monsoon provides an easily usable and affordable solution for load testing.

To ensure scalability, Monsoon focused on two key design decisions: optimizing the load generation engine and implementing a scalable data pipeline. By choosing Google's Puppeteer over Selenium, Monsoon achieved four times the number of simulated users per app instance, reducing the compute resources required and resulting in cost savings. The data pipeline was optimized using an extract, transform, load (ETL) pattern, which reduced transmission times and costs. Although ETL introduced transformation complexity and locked the data into a specific form, these trade-offs resulted in significant data storage and transmission savings.

Monsoon's design also prioritized near real-time results for developers. By extracting, transforming, and loading data into the database at regular intervals, Monsoon enabled engineers to monitor test results as they happened, allowing for immediate issue detection. While this near real-time approach introduced complexity in coordinating components and avoiding data loss or duplication, it provided developers with timely insights and turned load testing into an event for the company.

Looking into Monsoon's architecture in more detail, the load generation component simulates virtual users by utilizing a node app. This app includes critical components such as Weather Station, a user script, and Runner.js. Weather Station leverages the web performance API to measure the duration of user actions. The user script contains instructions for the headless browser instances, guiding their actions on the target website or web app. Runner.js serves as a coordination script, controlling multiple virtual users and storing session metrics for each action.

AWS Fargate, a serverless compute engine, was used to run the containerized Monsoon node app. AWS Elastic Container Service (ECS) provided container orchestration, allowing for scalability beyond 20 users. By containerizing the app and utilizing AWS services, Monsoon ensured efficient load testing while keeping infrastructure costs in check.

In conclusion, the rewritten transcript provided a concise overview of Monsoon, an open-source load testing tool designed for small to medium-sized companies. It explained Monsoon's key components and architecture, emphasizing its scalability and near real-time results. The rewritten transcript captured all the essential technical details while reducing the length by half. In the world of real-time frameworks, there are both pros and cons. On the positive side, engineers can monitor their tests in near real-time, identifying and addressing issues as they arise instead of waiting for the entire test to complete. Additionally, if the load test is of significant importance, it can become a noteworthy event within the company, with teams reserving conference rooms, ordering food, and watching the load test results live. However, implementing a near real-time data pipeline comes with its challenges due to its complexity. Coordinating multiple components to avoid data loss or double-counting data points can be technically challenging. We will discuss this complexity further later in the presentation.

Now, let's delve deeper into Monsoon's architecture. The architecture can be divided into four key components: load generation, transformation, storage, and visualization. The load generation component focuses on creating virtual users who perform scripted actions on the target website or web application and record metrics about those actions. At the core of this component is a Node.js application consisting of three critical components: the Weather Station library, a user script, and Runner.js.

The Weather Station library, built on the Web Performance API, allows developers to write code that measures the duration of individual user actions. The user script contains instructions for the headless browser instances on what actions to perform and in what order. Combined with Puppeteer, this allows Monsoon to program virtual users. Runner.js serves as a coordination script, creating and controlling multiple headless Chrome instances to simulate virtual users. It executes the commands listed in the user script and stores the resulting session metrics for each virtual user's actions.

The load generation process relies on the AWS services ECS and Fargate. AWS Fargate is a serverless compute engine that eliminates the need to manage servers. To run the Node.js application and scale it to simulate more than 20 users, the application is containerized and passed to ECS (Elastic Container Service). For each block of 20 virtual users, ECS spins up additional Fargate instances to execute the containerized application.

Once the load generation process yields session data for the virtual users, it enters the transformation phase. Here, the session metric data stored on each Fargate instance is transformed into a time series data format and loaded into a time series database. The Node.js application, specifically the normalizer.js file, pulls data from the Fargate instance's local file system every 15 seconds. This data is then transformed, grouped by time window, and moved to an S3 bucket for permanent storage.

Normalization in this context refers to a preprocessing step that allows data aggregation. It involves combining multiple data points with different timestamps into a single data point with a normalized timestamp. The normalizer.js script groups session metric data by time window, preserving statistical significance while minimizing data size.

The transformed session metric data in the S3 bucket is not yet in an appropriate format for a time series database. This leads to another transformation step involving an AWS Lambda function. AWS Lambda is a serverless compute service that runs individual serverless functions. Every three minutes, the aggregating Lambda function pulls data from the S3 bucket and aggregates metrics with the same timestamp, creating a single time series data point. This data point is then posted to the time series database.

The storage component of Monsoon's architecture is crucial for appropriately storing the data generated during the transformation phase. Time series data, which is a series of ordered data points that track changes over time, is considered the best fit for Monsoon's purposes. Amazon Timestream is a fast and scalable serverless time series database service that offers efficient storage and analysis capabilities for trillions of events per day. Users connect to the Timestream service and create tables to store their time series data.

Moving on to the visualization component, it queries the storage layer and displays the retrieved data in a graphical user interface (GUI). Monsoon's visualization architecture includes a locally hosted React app called "Weather Channel." The Weather Channel app queries the Timestream database service and utilizes the Victory library for modular charting and data visualization. As the Victory library has native support for time series data, it is an ideal choice for displaying the data. The Weather Channel dashboard allows users to view near real-time data, select specific end-user actions, toggle metrics on or off, and even receive notifications about their results.

Now, let's transition to a demo of how to install and use Monsoon. Monsoon is available as an NPM package, so it can be installed on a local machine using the command "npm install -g monsoon." After installation, users should verify their AWS configuration file and ensure it does not contain a default profile. The "monsoon edit" command allows users to set up their AWS credentials. This creates a new Monsoon directory and sets up the necessary files and configurations.

To deploy the infrastructure required for the load test, users execute the "monsoon deploy" command. This sets up a custom VPC, Lambda functions, an S3 bucket, and a Timestream database. Users can then configure their load test by modifying the testConfig.json file and writing their test script in the testScript.js file within the Monsoon directory. Multiple tests with different configurations can be organized in different subdirectories, created using the "monsoon new test" command.

After configuring the load test, users can run the test by executing the "monsoon start" command. This initiates the load test and provides information to read while waiting for the results. After approximately seven minutes, the load test results start streaming into the Weather Channel dashboard. Users can view the results locally by running the "monsoon channel" command and accessing the dashboard through their browser. The dashboard allows users to refresh to see the most up-to-date results, choose specific end-user actions to display, toggle metrics, and hover over data points to see metrics at specific points in time.

Once the test is completed and the results are viewed, users can tear down the infrastructure by executing the "monsoon stop" command. Additionally, if users want to remove all directories and associated files related to Monsoon, they can run the "monsoon destroy" command.

We have now covered the demo of Monsoon and will move on to discuss some implementation challenges we encountered during its development. Firstly, we faced challenges in deciding which metrics to include in the load test. While response time seemed to be the most essential metric based on existing load testing tools, we found it to be a loosely defined term. After extensive consideration, we defined response time as the difference between the starting and ending times for a given end-user action. To facilitate calculation, we provided a library called MonsoonWeatherStation, which developers can use to measure response time in their own load tests.

The second implementation challenge pertained to timing. When starting a load test, there was a delay between initiating the event and the containers being ready to run, resulting in containers not starting simultaneously. To counter this, we introduced a waiting time to ensure all containers were in a ready state before running the test script. We encountered other timing issues related to coordinating data generation and processing among multiple containers.

In conclusion, we have discussed the architecture of Monsoon, including the load generation, transformation, storage, and visualization components. Monsoon leverages AWS services such as ECS, Fargate, S3, Lambda, and Timestream to create a scalable and efficient load testing framework. We also provided a demonstration of how to install and use Monsoon, along with insights into the implementation challenges faced during development. To begin, let's run the command "monsoon destroy" after executing the "tidal" command. Now that we have seen a demonstration of Monsoon, let's discuss some of the challenges encountered during its implementation. Two specific challenges stand out: metric selection and timing issues.

Regarding metric selection, we extensively considered which metrics to choose for load testing when designing Monsoon. After examining existing load testing tools, we discovered that response time is commonly used as the core metric. However, we found that response time is a vague term and lacks a standardized definition. To address this, we defined response time in the context of Monsoon as the difference between the starting and ending times for a given end-user action. We also developed a library called Monsoon Grader Station to facilitate the calculation of response time in load tests.

The second challenge we encountered was related to timing. When using Monsoon, developers initiate a load test by sending an event to a starting lambda, which then spins up the necessary number of containers. However, it takes approximately 15 to 60 seconds for each container to be ready. This delays the start of the test and affects the accuracy of testing the specified number of concurrent users. To mitigate this, we introduced a waiting time period to ensure that all containers are in a ready state before the test is run.

Once all the containers are up and running, we faced the challenge of coordinating the timing of data generation and processing. Inside each container, a runner strip is responsible for generating headless Chrome instances using Puppeteer and collecting the necessary data. However, in order to provide near real-time results to developers, we cannot wait until all the raw data is generated before processing it. To address this, we implemented a normalization strip that pauses every 15 seconds to process any new raw data generated and then sends it to our S3 bucket for storage.

After processing the data in the S3 bucket, we encountered a third timing issue: how to transfer the data to our TimeStream database. To solve this problem, we created an event bridge to invoke our Metronome Lambda function every minute. The Metronome function checks for any new batches of data in the S3 bucket and then sends it to TimeStream using the Aggregated Lambda function.

With these timing issues addressed, we successfully developed a working data pipeline in Monsoon capable of moving raw data from the containers to the S3 bucket, performing necessary transformations along the way, and finally storing the processed data in the database. Now, let's discuss some future work planned for Monsoon.

One aspect of future work involves rewriting certain parts of the project in a compiled language like Go. Currently, Monsoon is written entirely in JavaScript and TypeScript, but certain components, such as the normalization strip and the irrigating lambda, require heavy data processing and could benefit from faster runtime performance and better CPU utilization.

Another planned improvement is to provide additional libraries that support load testing tools other than Monsoon. For example, we could create a version of the Weather Station library compatible with Playwright desktops, thus expanding the tool's compatibility.

Furthermore, we aim to implement a new feature that checks the correctness of the test scripts before running them. Currently, developers can initiate a load test even if the test script does not adhere to our instructions. This can lead to wasted developer time and unnecessary AWS costs. To address this, we plan to incorporate a pre-check as part of the Monsoon start command, ensuring that tests are correctly written before execution.

Lastly, we plan to implement the ability for developers to export load test results to a CSV file. This feature will enable users to conveniently analyze and share test data in a commonly used format.

In conclusion, we have presented an overview of the Monsoon project, highlighting its implementation challenges and future improvements. We would like to express our gratitude for your time and attention. Now, we would like to open the floor for any questions you may have.

Richard asked: "How much money would Boost Company have saved by using Monsoon?" In response, we would like to clarify that the money saved depends on the duration of the test. However, if they were to run a one-hour test, using Monsoon would reduce costs to approximately one-third of what they would have incurred with other solutions. For example, running a test with 4,000 users on Flood would cost around $100 per hour, whereas using Monsoon would cost around $20. Monsoon also eliminates platform fees associated with existing browser-based SAS solutions, further reducing costs.

We have received another question from Richard Cole: "What challenges did you face in terms of learning the full stack?" Learning the full stack was challenging due to the sheer quantity of information to absorb within a limited timeframe. The Capstone project requires learning various technologies and employing just-in-time learning techniques. As a result, learning the full stack within the Capstone project timeframe necessitates a fast-paced assimilation of knowledge.

Thank you all for joining us today. We appreciate your participation and input.