hi everyone i'd like to welcome you to our presentation and thank you for attending today so for the past several months we've been hard at work on a project called monsoon we built this project as a fully remote team four software engineers spread out across four different cities my teammate alex is located in vancouver raul is in the san francisco bay area min is in chicago and i'm stephanie located in los angeles so the plan for today i'll start out with an introduction to the overall domain of load testing then raul will give us an overview of what monsoon is and what types of teams should be using it next alex will talk about the overall design decisions we made when building our framework and walk us through monsoon's architecture then min will show us a demo of monsoon talk about some of the implementation challenges we faced and also discuss our plans for future work on monsoon and finally we'll open it up for some questions from the audience monsoon is an open source serverless framework for running browser-based load tests in the cloud this sounds like a lot of words but we'll break it down piece by piece as we go through our presentation today load tests are a critical tool for companies that want to build and maintain robust scalable websites and web apps they simulate traffic hitting your servers allowing you to measure exactly how your app performs under different load profiles these results are valuable to both business stakeholders and engineering teams on the business side when an app can't cope with its current amount of traffic the app's performance degrades or the app may stop working entirely this results in an immediate drop in the company's revenue from the engineering team's perspective loan testing provides valuable feedback about your production system how many concurrent users can it handle how does response time change as load increases and is there an uptick in failed requests under heavier load to see load testing in action let's spend some time with the engineering team at boost health a rapidly growing startup in a health and wellness space the stages are high boosts marketing team has spent months planning the product launch for its new brain boost supplement it's boost biggest product launch ever and business executives want to close new rounds of venture capital funding based on the success of brain boost boost expects more traffic than their site has ever seen on launch day they're predicting peaks of 4 000 concurrent users meaning 4 000 potential customers trying to access their website simultaneously the engineers at boost are tasked with making sure the site can handle all these expected users the boost website is a single page application a fact that has important implications for load testing as we'll see in a bit let's start out by examining what an sba is a single page application is a web app comprised of just a single html page unlike a traditional website after the initial page load there are no page reloads the content of the single page is just repeatedly updated on the fly using browser side javascript sbas have some significant advantages over more traditional websites they reduce strain on server resources they also speed up development by decoupling the front-end code from the back-end code finally sbas provide a much more dynamic responsive experience for the end user since there are no full page reloads all three of these benefits are very important to boost thus their site is an spa to make sure this spa can withstand the customer traffic on launch day the engineering team's first instinct is to load test the site let's formally define load testing now in the world of software it's the process of directing simulated traffic at any application that handles requests be that a website web app api endpoints even load balancers and caches then measuring that software's performance under the load so load testing is exactly the tool the boost engineers need one of the boost engineers used a well established open source load testing tool called apache jmeter at a previous job so this is the first option the team tries meter is categorized as a protocol-based load testing tool protocol-based load tests are the original type of load test they involve traffic simulation at the http protocol layer for example if loading a web page triggers http requests for 75 sub-resources with protocol-based testing the developers will need to write code to request the original page and all 75 of those sub-resources the boost health engineers want to test the customer adding brain boost to her cart a process that breaks down into three different actions first go to the boost health main page second view the brain boost product details and third add brain boost to the cart to simulate this workflow the boost engineers need to program jmeter to send 125 different http requests in the correct order so this is clearly a great deal of work for the team on top of this jmeter is a complex tool and the learning curve is steep so the team settles in for many painful days of work ahead after the team gets their first jmeter load test working the results seem strangely incomplete recall the boost website is an spa and it turns out jmeter is a poor choice for load testing spas sbas are javascript intensive but jmeter has no javascript interpreter and therefore can't execute any javascript code and the same holds true for other protocol based load testing tools therefore the bulk of an spa is untestable with jmeter this is a deal breaker another developer on the team knows of a different tool called selenium and knows that selenium can be used for browser-based load testing so what is that browser-based load testing simulates web traffic using real web browsers rather than naked network requests since we're using browser instances to direct traffic to the site being tested those browsers clearly have built-in javascript interpreters and are fully capable of handling spas unlike protocol-based load testing tools an even more fundamental difference between protocol-based load testing and browser-based load testing tools like selenium is this is the core unit we care about is that the individual network request or is it an action an end user takes which could actually result in a hundred or more network requests in boost's case it's an action a website user takes like loading the home page viewing product details or clicking the add to cart button furthermore thinking in terms of end user actions rather than lower level network requests means browser-based load testing is significantly less complex than its protocol-based counterpart when planning how to test the boost site the developers can think at a higher level of abstraction which reduces bugs makes for a better developer experience and save significant amounts of developer time returning to selenium it's a suite of browser-based test automation tools it was never actually designed for load testing but that's how selenium came to be used by many developers with selenium the boost engineers don't need to worry about http requests anymore all they need to do is script their three end user actions so the team runs their first browser-based load test using selenium it's a fairly small scale test simulating five users concurrently visiting the boost health website this test goes off without a hitch next they test 100 concurrent users this test doesn't go so well selenium is a tool that runs locally on one of the developer's laptops and testing 100 users means spinning up 100 browser instances this is too resource intensive for a single laptop and the actual number of users the boost team needs to test is 4000 not 100 so the team has run out of local computing resources before they're able to apply sufficient load to the boost site clearly this is unmarkable because browser-based load testing is so resource intensive the boost developers need a solution that's hosted in the cloud researching cloud-hosted browser-based load testing the team quickly discovers a platform called flood flood is an industry leader in the cloud-hosted browser-based load testing space their platform is definitely capable of generating the 4 000 concurrent users boost needs to load test their site however flood is very expensive we'll return to blood later but for now the cost is a major drawback enough to rule flood out so let's summarize where the boost engineers are right now they initially tried protocol-based load testing this was unsuccessful because protocol-based tools can't test an sph javascript code next they tried local browser-based load testing this too was unsuccessful because browser-based load testing is too because local browser-based load load testing is too resource intensive for a single machine third they tried browser-based load testing in the cloud with the flood platform however this proved prohibitedly expensive so the team searches for a more economical option for browser-based load testing in the cloud in short order they come across an open source tool called monsoon now i'll pass things over to my colleague raul he'll give us an overview of what monsoon can do and whether it's a good fit for boost health great thanks stephanie as a reminder monsoon is an open source serverless framework for running browser-based load load tests in the cloud reading this the boost engineers think they found something worse for the investigation so we as the creators of monsoon will now lay out the capabilities of our framework to help the boost engineers make their decision monsoon makes it easy to load test your single page application we use a tool called puppeteer to simulate users visiting boost health website puppeteer is a powerful browser-based testing library for node distributed as an npm package puppeteer provide us with a high level api that give us full control over headless chrome instances puppeteer is very easy to use for example to simulate a user filling out a form you just pass a css selector and the data as arguments to the type method and to simulate a user clicking a button just pass the button css selector to the click method thanks to puppeteer unsoon is able to extract away a lot of the complexity of the load testing process if the boost engineers go with monsoon as their load testing framework they'll no longer need to worry about writing to use code to hit specific backend api endpoints instead then write a simple puppeteer script that describes the end user actions they want to simulate then they'll configure the load testing by specifying the number of simulating users and the duration of the test once we have the test scripting and its configuration monsoon uses puppeteer to create and control headless chrome instances if each of those chrome instances executes the actions defined by the boost engineer in the in their test script hitting thousands of ha or hundreds of back-end api endpoints in the process then monsoon records performance metrics for each end user action and saves the data in json format this process repeats for the duration of the load test so we see that monsoon streamlines the load test process process for the boost engineers now how do we scale things up so that boost can easily test 4 000 concurrent users visiting their site on launch day we met this requirement by designing monsoon to work in the cloud at the start of the load test monsoon provisions multiple compute instances the number of instances scales linearly with the need of the test then once once the test is over monsoon frees up all of those compute instances currently monsoon is able to handle up to 20 000 virtual users more than enough to meet boosts test testing needs however hosting a load testing tool in the cloud comes with certain challenges as we mentioned earlier data is recorded for each virtual user executing the actions described in boost test script this can generate a serious amount of data a one hour test can easily generate hundreds of millions of data records boost may one may well want to run a one hour test but generally that amount of data is overkill so hoka monsoon helped manage all of that data and also extract meaningful insights from it monsoon handles this challenge by calculating aggregating metrics for every end user action in increments of 15 seconds the finalized metrics are then stored in a database the final step is visualizing the results monsoon does this with a locally hosted dashboard called weather channel this this lets the boost engineers see how their site actually perform under the load specifically they can track response time concurrent users transaction rate and pass ratio additionally all of these metrics are available in near real time so you don't need to wait until the test completes to see how your site or app performs to summarize monsoon uses the cloud to make browser-based load testing simple and scalable now let's see monsoon's capabilities mapped onto a high level architecture diagram as you can see we broken monsoon down into its four key components low generation transformation storage and visualization monsoon uses amazon web services as its cloud provider and all the monsoon infrastructure is deployed directly into boosts aws account in our load generation step a boost engineer kicks off a test by using a simple command from monsoon cli this passes the test script and its configuration parameters to the infrastructure which then spins up as many compute instances as are needed to generate the required load then the testing begins once we once we have our first router's results ready monsoon preform performs a pre-processing step inside the compute instances this red is the results to enter monsoon's data pipeline beginning the transformation step we see here in this step the test results for all virtual users are aggregated then we have storage in this step the aggregated result test results are stored in a time series database this type of database is optimized for time series data which is which is simply data points that are ordered by timestamp we'll talk more about time cs data and where we opt for attempts database later in the presentation and finally monsoon give us gives you near real-time insights into your app performance via its locally hosted weather channel dashboard in summary we built monsoon to allow small to medium-sized companies to load test their spas in a simple and scalable way allowing them to get insights into its performance in near real time because we design monsoons specifically for this use case we've been able to carve out our own niche in the browser-based load testing space to demonstrate this let's take a look at boosts other browser-based load testing options there are two typical paths boost might choose the first is a cloud-based software-as-a-service solution like the flood platform mentioned earlier in the presentation the second is a do-it-yourself approach tasking the boost engineers with building their own browser-based load testing tool let's examine the traders for a cloud-based solution first a platform like flood is highly scalable able to simulate tens or even hundreds of thousands of concurrent users it's also generally very easy to use it's usually hosts a hosted platform meaning the end user doesn't need to configure or deploy any infrastructure this makes the overall process quite straightforward however the downside is cost a single test can cost thousands of dollars and a small to medium-sized company like boost may have a difficult time justifying spending that much on yet another sas product turning to a diy approach a major benefit here is that once the platform is built operation is much cheaper you you will only pay for the underlying cloud infrastructure no ongoing platform fees however building a distributed load testing platform requires a lot of engineering hours those engineering hours at expense are expensive and will be hard to justify for a company of boost's size furthermore which doesn't have a huge engineering team to build a distributed load testing tool from scratch it had to pull engineers engineers of projects that are central to the core their core business neither of these options seems like a good fit for boost so what should they do looking around for a third wave brings us back brings us back to the royal friend monsoon monsoon glues together all the resources needed to create a real-time load testing platform then deploys it directly to boost's aws account thanks to its serverless nature monsoon is just as easy to use as existing cloud-based star solutions like flood additionally since monsoon would be deployed directly to the boost aws account boost retains ownership of its data however monsoon is neither as a scalable nor as feature-rich as all their class-based sas products or highly highly customized diy solutions monsoon is an open source solution intentionally optimized for small to medium-sized companies that want to load test their spis for up to 20 000 concurrent users the good news is that monsoon team has already invested the engineering hours so you don't have to we also charge no platform fees you only pay the aws charges steaming from the infrastructure spoon up to run your test this makes monsoons enough significantly more affordable than either cloud-based sat solutions or diy approaches now i'll hand it over to alex who will dive a bit deeper into the design decisions we made while building monsoon great thanks rahul early on we made two design decisions that strongly shaped the monsoon framework the first was choosing to focus on high scalability the second was striving to provide near real-time results to developers using monsoon we'll discuss scalability first we set out to build a framework able to simulate as many users as possible for as long as possible as cheaply as possible to accomplish this we focused on the scalability of the load generation engine and of the data pipeline first let's look at how we optimized monsoon's load generation engine we had several options for simulating virtual users via headless browser instances selenium is the industry default one selenium app instance is able to simulate five virtual users we compared this to google's puppeteer discussed earlier where one puppeteer app instance is able to simulate 20 virtual users this difference in simulated users per app instance is hugely important since each app instance requires its own execution environment increasing the number of simulated users per app instance reduces the compute resources you need and since aws's billing model is only pay for what you use using compute resources more efficiently directly leads to cost savings by choosing puppeteer over selenium to generate load we were able to test four times as many users for the same price in addition to optimizing our load generation engine for scalability we optimized the data pipeline for scalability as well because simulating virtual users and extracting session metrics generated or generates so much data monsoon naturally uses a pipeline to move data from one point to another in our system two common data pipeline patterns are elt and etl the e stands for extract the l stands for load and the t stands for transform the choice of one pattern over the other has significant impact on scalability elt is the current industry standard with the lt we load all the raw data into storage before making any changes to that data benefits to this approach includes simplicity and flexibility because all the data is stored together all transformations can be done in one place and since you hang on to all the raw data you're free to run further analyses and change the data's final form at any point in the future but elt has drawbacks as well you need to transmit all data from the extraction point into storage this can result in substantial bandwidth costs and transmission times furthermore once the data is loaded into storage you'll need to pay ongoing storage fees etl is also an industry standard with etl we transform the data between the extraction and storage points so that we are only storing data that's already been transformed benefits to this approach include reduced transmission times and costs you reduce the volume of data right after the extraction step thus reducing the time and bandwidth costs associated with moving that data into storage and since you're not storing your entire raw data set storage costs are naturally reduced but there are cons to etl the first is transformation complexity your data may require transformations at multiple steps along the data pipeline this can become technically complex the etl pattern also lacks flexibility since we're only storing transform data we effectively lock ourselves into a final form of the data before starting the pipeline another concern with etl is data integrity if something goes wrong in the pipeline causing you to lose all or part of the transform data you don't have raw data to fall back on the data would need to be regenerated which may or may not be possible ultimately we found the etl pattern a better fit for our use case we were willing to deal with the transformation complexity and the final data shape lock-in actually wasn't the problem for us every load test monsoon runs shows the same metrics so the final shape of the data never changes we realized significant data storage and transmission savings by making this trade-off let's quantify this difference by looking at database rights say we run two different load tests with a test script virtual user count and duration held constant if monsoon's data pipeline were to follow the elt pattern the number of writes required increases by a factor of 800 and to generate the same final graph in the local weather channel dashboard two megabytes of information in an etl pipeline would need to be 24 gigabytes of information in an elt pipeline as we mentioned earlier aws uses a billing model of only pay for what you use therefore if we use less storage and transmit less data it costs less so holding the infrastructure budget constant a user is able to run load tests that are many orders of magnitude larger since we designed monsoon with an etl data pipeline so these were the design decisions we made to make monsoon highly scalable now let's turn to the design decisions we made so that monsoon can display results in near real time first let's clarify some terminology real-time data is data that's collected processed and analyzed on a continual basis the resulting information should be available to the end user immediately after being generated and near real time is just real time but with a delay introduced beforehand teams view a situation as it existed in the recent past rather than as it is right now note that there isn't a precise cut off for what is or isn't considered near real time so why did we opt for near real time load tests can be really long it's not uncommon to run a test for hours days or even weeks because of this monsoon can handle tests of arbitrary length the limiting factor is only the user's aws budget but we don't want our users to have to wait until a test completes before they see results we'd like them to see results as soon as possible this means that monsoon needs to extract transform and load data into the database on a regular basis so it can be queried and displayed on the locally hosted dashboard building monsoon is a near real-time framework has pros and cons on the pro side engineers can monitor their tests in near real time spotting issues as they come up rather than waiting until the entire test completes additionally if the load test is important enough it can become something of an event at the company running it while researching our project we actually came across stories of large teams reserving conference rooms ordering food and watching the load test results rolling live but a major drawback to implementing a near real-time data pipeline as the complexity there are a lot of components and it's technically challenging to coordinate all of them so as to avoid losing or double counting data points we'll return to this complexity later in the presentation now let's move on to examine monsoon's architecture in greater detail as we're all covered in his overview earlier monsoon's architecture can be broken down into four key components load generation transformation storage and visualization the purpose of our load generation architecture is to create a given number of virtual users have them perform scripted actions on the target website or web app and record metrics about those actions at the core of our load generation architecture is a node app the app's critical components are weather station a user script and runner.js developers can use the weather station library to write their test script weather station makes use of methods from the web performance api built in the browsers and gives developers a way to write code to measure how long individual user actions take our next load generation component is the user's test script it's a set of instructions describing the exact actions the headless browser instances should take and in what order combined with puppeteer this test script is what allows monsoon to program a virtual user for example returning to the engineers at boost health their user test script might instruct the headless browser instances to load the main boost health web page wait two seconds load a product details page wait 10 seconds and then click the add the card button runner.js is a coordination script it creates 20 headless chrome instances controlled by puppeteer to simulate 20 virtual users runner.js then executes then instructs each virtual user to execute the commands listed in the user's test script and stores the resulting session metrics for each of those virtual user actions to summarize our node app creates virtual users instructs them to perform actions and saves metric data about the actions taken but how are we running this node app we use aws vcs and fargate aws fargate is a serverless pay-as-you-go compute engine that allows you to focus on building applications without managing servers in effect it's an aws service that runs containerized applications to run our app and scale to more than 20 users we containerized our node app and passed it to ecs the elastic container service ecs is used for container orchestration for every block of 20 virtual users we wish to simulate ecs spins up in an additional fargate instance to execute our containerized node app for example to simulate 400 users ecs spins up 200 fargate instances each of which executes one instance of our containerized app so now that our load generation process has yielded session data for our virtual users what do we do with it to answer this let's take a closer look at our transformation architecture high level the transformation architecture takes the session metric data stored on each fargate instance transforms it into a time series data format then loads it into a time series database earlier we mentioned that the runner.js file in our node app stores session metrics from every virtual user action we're storing these metrics by temporarily saving them to the local file system of the firegate instances alongside our load generation scripts the node app also runs a file called normalizer.js normalizer.js pulls the fargate instance's local file system for new session metric data every 15 seconds the normalizer code applies transformations to any data it finds and moves the transform data to an s3 bucket for more permanent storage now let's clarify our definition of normalization since it can be a slippery term in the data science world we consider it a pre-processing step that facilitates data aggregation when we normalize raw data we're taking multiple data points with different raw timestamps and combining them into a single data point with one normalized timestamp if you think of calculating a moving average that's a good mental model this normalization step is necessary because the aggregation step that follows requires uniform time stamps so the core logic of normalizer.js groups the session metric data by time window and a happy side effect of our normalization process is that we preserve statistical significance while minimizing data size now let's examine our destination s3 bucket more closely once metrics sent by normalizer.js start hitting the s3 bucket the s3 bucket will contain files organized based on timestamp end user action and fargate instance name for a given end user action a single file represents the metrics for all virtual users on one fargate instance within a single 15 second time window here for example the json file contains metrics from all 20 virtual users on the mi underscore w8 ni fargate instance for the click first link action at the unix timestamp listed but even in this transform state the data currently sitting in the s3 bucket isn't appropriately formatted for a time series database so we'll need another transformation step this time involving an aws lambda function aws lambda is a serverless event-driven compute service it's similar to fargate but while fargate runs entire apps in a serverless environment lambda runs individual serverless functions every three minutes our aggregating lambda function pulls the s3 bucket to determine if any new data has arrived for each new timestamp the lambda aggregates all the metrics in the json files with that time stamp creating a single time series data point then we post that data point to the time series database now let's examine the storage component of monsoon's architecture its purpose is to appropriately store the data generated during the transformation step as with any system this data should be stored in a format that lends itself to the data shape and access patterns in monsoon's case a good fit is the time series data format first let's define what time series data is a time series is a series of data points that are ordered chronologically their excellent tracking changes over time examples include stock charts ocean title charts and line charts in excel with monsoon we generate synthetic load on a website and track session metrics as the test progresses end users are looking to track how key metrics like page load response times change over time this type of data naturally lends itself to the time series data format amazon timestream is a fast and scalable serverless time series database service that makes it easy to store and analyze trillions of events per day time stream can be up to one thousand times faster than a relational database and as little as ten percent of the cost timestream is a database service therefore users don't host time stream themselves instead they connect to the time stream service then create tables within the service each table contains one or more related time series as we mentioned in the transformation section data in our s3 bucket is aggregated by stamp then converted into a time series data format and written the time stream because time series data is a first class citizen in time stream the database can quickly and efficiently read and write time series data an added benefit is that the data returned by a time stream query is already in the shape we need to eventually display it visualization is monsoon's final architectural component the visualization architecture queries the storage layer then displays the data it retrieves in a graphical user interface we call our dashboard weather channel it's a locally hosted react app that directly queries the time stream database service this dashboard app uses the victory library for modular charting and data visualization as victory has native support for time series data you'll get a chance to see weather channel towards the end of our presentation that wraps things up for monsoon's architecture now i'll hand things over to my colleague min great thank you alice now we'd like to walk you through a demo of how to install and use much soon we publish monsoon as an npm package so you can just run npm install global to install monsoon on your local machine after you install the package you want to verify that your hidden iws config file is correct and does not contain a default profile next you can issue the monsoon edit command the init command also prompts you to enter your iws gradient source so that monsoon can deploy infrastructure within your aws account let's create this creates a new monsoon desktop directory inside your current working directory you need to cd into monsoon test and be sure to secure all of the other monsoon cli subcommands from winding monsoon test the next step is to issue the monsoon deploy command this spins up the iws infrastructure required to run your load test this includes a custom vpc multiple lambda function an s3 bucket and a timestream database from your motion test directory you set your test strip in the teststrip.js file and configure your upcoming load test by modifying the testconfig.json file if you anticipate running multiple different load tests with different test rate and configurations you can set up additional sub directories to organize other different tests just issue them also new test command let's look at an example test strip and its configuration here we have a test strip that loads the boost home page goes to a product details page then adds that product to the cart the test is configured to last one hour we start out with 20 concurrent users then ran then ramp up to 40 hundred concurrent users over a 50 minute window with the test script and configuration ready to go you next run the monsoon slide command this officially gives up the load text and provides a bit of light reading while you wait after about 7 minutes yellow text results will start streaming into the weather channel dashboard you can view these results locally by issuing the mercenary channel command then opening up a browser and visiting localhost port 5000 within the dashboard you will see your data in near real time so you can refresh your browser to cmos to see the most current results clicking on the button at the top of the screen you can choose which end user action you like to see results form and you can also toggle particular metrics on or off to see only the results that interest you weather channel also allows you to human and email to get different best tips on your results and if you hover over a line you can see your metrics at that exact point in time here we see what happened as we ramped up low on the boost hair website to 400 concurrent users the results are not good response times has increased all the way to 60 hundred milliseconds analyzing these results the bus engineers determined that their current infrastructure went credit for their upcoming products launch without running this low test the bus engineers wouldn't have known about this weakness in their production system return into our demo once your test is over and you view your results you can tear down your infrastructure by issuing the monsoonal command if you also like to delete all directories and environment files related to my soon run the monsoon destroy command after the tidal command command now that we've seen a demo of monsoon let's move on and examine some of the implementation challenges we face by building monsoon two challenges stand out in particular the first relates to metrics specifically to respond time and the second relates to timing let's explore the magic issue first early in the process of designing monsoon we spend a lot of time considering exactly what matrix to take for a load test there were all sorts of things we could measure but which metrics would our end users find most useful examining existing load testing tools we found that all of them focus on response time as their core matrix at first glance this seems reasonable but digging deeper we found response time to be a very fixy term maybe it's the last time between when a client is your request and when that client receive the first buy up to respond or maybe we keep the timer going until the dom content is fully loaded we even contacted customer support at flood to get details about how they calculate respond time and we still weren't able to get complete clarity because the call unit of the test strip is one end user action we chose to define response time as the difference between the starting and ending times for the given end user action to make it easy for developers to calculate respond time in their load test we draw a library called monsoon grader station as we mentioned earlier in the architecture session in summary we determine response time to be the most important metric to collect for a low test since there is no standardized definition of response time we thought hard about how we should define it in the context of our use case so in a monsoon low test respond time is the difference between the starting and ending times for the given end-user action our other implementation challenge is related to timing this was by far the larger and more timeless issue when a developer using monsoon this is the monsoon's recommend to kick off a low test the coi code sends an event to a starting lambda which in turn spin up spins up the appropriate number of containers but it can start anywhere from 15 to 60 seconds for a container on friday to be ready to run since load generation happens in cellular containers if the containers are not all starting together then we no longer truly testing the specified number of concurrent users to solve this problem we introduce a waiting time of treatment to make sure that all containers are in a ready state before monsoon actually runs the test trip this was the first time in issue we encountered but there were more to come once other ones all the containers are up and running how do we coordinate the timing of data generation and processing we have to carefully orchestrate our data highlight to start this second time initial as mentioned in the architecture session inside each container a runner strip is responsible for generating headlight chrome instances with puppeteer and running the desktop to generate the data that raw data needs to be normalized and then send up to our sg bucket but in order to present near real-time results to developers we cannot wait until all the raw data is generated before starting to process it therefore our normalization strip pause for new raw data every 15 seconds now that the normalizing strip has moved the process data into the s3 bucket we run into a third timing issue how do we get the data sitting in the s3 bucket into our time stream database we handle this by creating an event breakthrough to invoke our metronome lambda once every minute the metronome digests a collection of pre-generated time stem to see if any help is fine if any have the metronome lambda involves the aggregated lambda which gathers the new batch of data and sends it to time stream after serving this third time in ecu we had a working data pilot capable of moving raw data draw test result data from containers to an s3 bucket and then to the database performing transformations along the way now let's turn to some future work we have planned for monsoon first we like to rewrite certain parts of the project and goals in its current state monsoon is written entirely in javascript and thai strip this certainly works but there are parts of the project particularly the normalizing strip and the irrigating lambda that do a lot of heavy lifting in terms of data processes these pieces could benefit from the faster runtime performance and better cpu utilization but compile language lego second we like to provide additional libraries so that we can support desktrip written for multiple load testing tools for example we could provide a version of the weather station library thus compatible with playwright desktops third we like to implement a new feature that checks desktrip for correctness before actually running them as it stands now a developer can start a load test even if the test strip itself is not written according to our instructions this can raise valuable develop per time while also incurring iws views this pre-check would be incorporated behind the scenes as part of the monsoon start command and the final feature we like to implement is allowing developer to export the low test result to a csv file so that's our presentation of monsoon we'd like to thank you for taking the time to join us today and now we open intensive for any questions you may have okay so i see a question from richard so how much money would the boost company have saved because they used your monsoon program i think i can take this one uh the i mean it really depends on the duration of the test that you want to run uh if they wanted to run a one hour test um i i think the like the difference is like uh like like you reduce the cost to a third um yeah so i i think it costs like around 100 an hour for 4 000 users in flood so for us it's like 20 dollars something like that yeah yeah so it's a significance uh it's a significant amount of money um and also we don't have platform fees and the the platform fees are um [Music] that can be a lot with the um existing browser-based sas solutions okay so we have a question from richard cole so what challenges did you face in terms of learning the full stack um i i can start out i think maybe we can all kind of uh give give answers here but um yeah it is it is very challenging like in in capstone you're going to be learning um a lot of technologies in a short period of time and you need to rely on sort of a just-in-time learning kind of skill and the capstone project will definitely force you to learn and apply that skill um [Music] yeah in terms of challenges i i'd say it's just kind of quantitative in my case i think just quantity of stuff coming at you and and the pace um okay so i don't see any other questions right now so um i'd really like to thank all of you for joining um and yeah it's been great i really appreciate it you 