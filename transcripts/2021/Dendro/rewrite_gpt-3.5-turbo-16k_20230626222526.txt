Thank you all for joining us today. My name is Nick and I am here with my teammates Andrew, Peter, and Angel. Over the past few months, we have been working on a project called Dendro. This project was developed remotely as a fully distributed team spread across the United States. Today, we will be discussing what Dendro is, why we believe it is a valuable framework, the team it is designed for, the design decisions we made, the components of the framework, and the features of Dendro through a demo. We will also talk about the implementation process and the technical challenges we encountered.

To start, let me introduce Dendro. Dendro is an open source serverless monitoring framework specifically designed for small distributed applications. In recent years, there has been a shift towards more ephemeral, abstracted, and distributed computing infrastructure. Dendro was created to help small teams navigate the challenges of operating in a distributed system. While infrastructure and architecture innovations have led to simpler components at higher levels of abstraction, they have also created more complex systems. With the ability to deploy and integrate various software components, the individual nodes in a system's topology have become more decentralized and harder to track and monitor. Dendro allows you to collect, centralize, and store log and metric data emitted by the different pieces of your system. This means that when an issue arises in production, there is no need to scramble and SSH into multiple nodes to find the source of the problem. The log data needed for debugging is already collected, processed, and tagged with the relevant information before being stored in a single database. This empowers users to minimize costly downtime and proactively enhance system performance. Catching problems before users experience them is crucial because relying on ad hoc fixes can make the system more fragile over time.

To demonstrate what Dendro does, let's consider an example. Imagine there is a small company called Naptime that has built a sleep monitoring app. They have a simple three-tier system in place, with a web server and application running on one server, and a database on another. While their application has been relatively stable, they do not have a monitoring solution in place. When an outage occurs or a critical bug is reported, they have to manually check each server and service to identify the issue. This involves verifying if each server is up, if the services running on those servers are functioning properly, and if the services are receiving and responding to requests correctly. They can use tools like the ping command to check server status and utilities like top to monitor system health. But this manual process is time-consuming and not scalable. Additionally, checking logs becomes increasingly complex as the system scales.

In the case of Naptime, they can look into the access log file of their web server to identify errors. This log file contains information about each request made to the system, such as the requester's IP address, request time, requested file path, server response code, and request duration. By analyzing the access log file, Naptime can find evidence of 500 level errors and gather insights about request trends and performance. However, as the system scales, monitoring distributed applications becomes exponentially more difficult. Managing and comprehending real-time log streams from multiple servers is practically impossible. Dendro addresses this challenge by capturing logs as they are written and storing them centrally, eliminating the need to SSH into each server during an outage. By collecting, centralizing, and retaining log and metric data, Dendro provides a single, comprehensive view of system state and performance. This simplifies debugging, reduces context switching, and improves overall efficiency.

Now, let's look at Dendro's architecture and how it can alleviate the challenges of operating a distributed system. Dendro includes a collection agent that is installed on each node in the system. This agent gathers log and metric data from both the services running on the server and information about the server itself. The collected data is then transported to a central database through an infrastructure pipeline. Once in the database, Dendro provides an interface for querying and visualizing the data, allowing for a holistic understanding of system health and state.

Dendro offers several benefits that make it stand out in the monitoring landscape. While there are many monitoring solutions available, we have optimized Dendro for a few key factors. It allows for automated collection of logs and metrics without requiring modifications to the application code. The collection agent seamlessly taps into existing log files and scrapes server metrics. Dendro's pipeline ensures data is streamed to the central database in real time, enabling monitoring and preventing data loss. The centralized log management solution provided by Dendro simplifies the routing and aggregation of logs from various sources. It eliminates the need for custom scripts or separate destination configurations. By storing logs and metrics in a single database, Dendro improves debugging efficiency, reducing the time spent searching through multiple databases and machines for relevant information.

In summary, Dendro is a serverless monitoring framework developed specifically for small distributed apps. It helps teams tackle the challenges of operating in a distributed system by providing a centralized solution for collecting, storing, and querying log and metric data. Using Dendro allows users to minimize downtime, proactively enhance system performance, and significantly reduce the toil associated with manual monitoring and debugging. With automated log and metric collection, a streamlined data pipeline, and a comprehensive interface, Dendro empowers teams to gain insights into their system's health, state, and performance.

Thank you for your attention. We will now proceed with a demo showcasing some of Dendro's features, followed by a discussion on the implementation process and technical challenges we faced along the way. The distributed nature of operating a system presents challenges in terms of debugging, data accessibility, and time consumption. Dendro aims to address these issues by providing a centralized logging solution that simplifies the debugging process and allows for easy querying and exploration of data. By implementing Dendro, engineers can save time and reduce the manual toil associated with troubleshooting distributed systems.

Dendro is not designed to compete with all existing monitoring solutions in the market. Instead, it focuses on optimizing certain key aspects that other solutions currently lack. Its primary goal is to reduce the toil experienced by engineers by providing a centralized logging solution. This enables quicker and more efficient debugging of system issues in production.

To provide an overview of the functionality offered by Dendro, let's consider the example of a system called Naptime. Dendro includes a collection agent that is installed on each node of the system. This agent is responsible for gathering log and metric data from both the services running on the server and the server itself. The collected data is then sent to a central database through an infrastructure pipeline.

The architecture of Dendro involves various components, such as the collection agent, the pipeline, and the database. These components work together to ensure seamless data transfer and processing. The collection agent collects data from the services and the server, while the pipeline transfers the data to the central database. Once the data is successfully populated in the database, Dendro provides a user-friendly interface for dashboarding and querying the data, allowing for a comprehensive view of the system's health and state.

Dendro's architecture aims to simplify the setup and maintenance process for users. The collection agent integrates with existing log files and scrapes metrics from the server without requiring any changes to the application code. The agent streams data to the pipeline in real time, ensuring data is not lost even if a virtual machine is spun down unexpectedly. The pipeline processes the raw data and stores it in a time series database optimized for writing, processing, and querying time-based data.

Dendro organizes the data in the central database by creating tables that represent different capture sources. Each row of data retains the identity of the host machine or process, allowing for easy context switching while debugging. This unified approach to data storage minimizes the need for context switching and maintains high resolution in the data, enabling granular analysis of server performance and state changes over time.

Dendro's value proposition lies in its ability to save time, money, and frustration in the event of a system outage. It is designed to be painless to install and requires minimal maintenance, making it an ideal solution for small or early-stage distributed applications without a pre-established monitoring plan. The ease of deployment and low maintenance nature of Dendro make it a strong contender in the market.

Comparing Dendro to other monitoring solutions, it becomes apparent that each option presents its own challenges. Buying a monitoring solution from a vendor like Datadog offers convenience but comes with a high price tag. Operating an open-source platform like Elastic requires technical expertise and infrastructure management. Building a custom solution is not feasible for small companies with limited resources and competing priorities. This is where Dendro fits in, offering a low toil solution that combines ease of deployment, centralized data ownership, and time series data optimization.

To achieve a low toil framework, Dendro focuses on integration ease, infrastructure provisioning, and maintenance reduction. Integration is made simple by providing a single collection tool that can monitor multiple services without custom integration coding. The deployment process is streamlined through the Dendro CLI, which automates various API calls to manage the logging pipeline. By leveraging serverless technology, Dendro minimizes maintenance and ensures stability without requiring users to provision and manage infrastructure.

Dendro also prioritizes the treatment of time series data as a first-class citizen. By storing logs in a time series database, Dendro enables powerful time-specific querying functionality. This allows for deeper insights and analysis of system performance over time. Time series data is particularly valuable for metrics such as request duration, status codes, and requests per second, as it helps identify trends and anomalies.

Another key benefit of Dendro is its ability to provide insights into distributed systems quickly. Naptime's architecture, for example, includes multiple nodes and network connections, which can make debugging cumbersome. Dendro addresses this issue by aggregating logs into a single database and offering a dashboard for querying and viewing health metrics. This proactive approach eliminates the need for engineers to manually check each node, reducing the time to resolve outages and improving overall efficiency.

In summary, Dendro is a centralized logging solution that simplifies the debugging process and enhances data accessibility for distributed systems. Its architecture includes a collection agent, an infrastructure pipeline, and a time series database to ensure seamless data transfer, processing, and querying. By prioritizing ease of deployment, centralized data ownership, and time series data optimization, Dendro offers a low toil framework that saves time, money, and frustration. It suits small companies without an established monitoring system while providing insights into system issues efficiently. Balancer four app servers and three databases, totaling eight nodes, with 16 network connections that could fail in the event of an error. The challenge lies in identifying which node failed and which logs to check when an error occurs. Log aggregation and centralization are crucial to streamline this process. By decreasing the time between an outage occurring and resolving it, we aim to minimize the time users experience problems. Our goal is to be proactive rather than reactive, catching and fixing issues early on, before users notice any slow page loads or outages. Debugging during an outage is always more stressful, so Dendro helps detect and correct system bugs before they bring everything down. It does this in a more explicit manner, pointing out exactly what is broken, far more precisely than a user would report. This allows for quicker resolution of outages.

Reducing the mean time to resolve an outage involves decreasing the time it takes for a team to become aware of it. With Dendro, engineers no longer need to check each of the eight nodes individually. By aggregating logs into a single database and providing a dashboard to query data and view health metrics, Dendro helps efficiently identify and solve outages without the need for engineers to play detective.

Now that we understand how Dendro fits into the ecosystem of monitoring solutions, let's take a look at what we have built. I will now pass it over to my teammate, Angel, to demonstrate the functionality of Dendro.

Dendro comprises four major components, each consisting of smaller elements. The first step is setting up collection for each service in your distributed app that you wish to monitor. Using a collection agent called Vector, Dendro gathers, transforms, and sends the distributed logs to the pipeline. Vector converts the raw, unqueryable text-based logs into JSON format before sending them for further processing. By analyzing the logs, important information such as the requested path, request completion time, and event occurrence time can be deduced. Proper configuration of Vector is essential, listing all the log and metric sources and sinks, capturing and transforming logs using regular expressions, and configuring AWS credentials. Vector's remap function adds a type property to the gathered data, allowing for identification of its origin. With a configuration file of over 400 lines manually created, streamlining the process is essential. Dendro provides a configure command that prompts users to select the services they wish to monitor and dynamically generates the configuration file.

Once Vector is properly configured and the logs and metrics are gathered and transformed into JSON, they are sent to the pipeline. The first stop is Amazon's Kinesis Firehose, an event streaming platform that captures, transforms, and delivers streaming data in near real-time. Firehose aggregates the data into 60-second or one-megabyte chunks to reduce processing overhead. The aggregated data is temporarily staged in an S3 bucket for error checking before being stored in TimeStream, our time-series optimized database. A Lambda function, written in Go for speed, transforms and loads the data into TimeStream. With TimeStream, the transformed data can be analyzed, queried, and processed using a SQL-like query language designed to work effectively with time-series data sets. This ability to offload work to the database level speeds up the analysis and reduces the load on compute resources.

Alerting is a critical aspect of monitoring. Dendro monitors incoming logs and notifies users in real-time of system failures. The integrated CloudWatch centrally views all the logs from the cloud infrastructure. Metric filters can be set up on these logs to trigger alarms, which then publish events to Simple Notification Service topics, sending email alerts to subscribed users. This immediate notification allows users to react promptly to errors as they occur.

To enhance data exploration and visualization, Dendro provides a custom-built dashboarding server. The dashboard allows users to visualize and query data in near real-time as it is being produced. It includes charts for anomaly detection, trend analysis, and data forecasting. This dynamic dashboard queries TimeStream directly and allows users to view, export, and monitor the health of their distributed system. CloudWatch logs are also accessible from the dashboard, providing a comprehensive overview of the system's overall health.

Now that we have covered each component of Dendro individually, let's see how they come together as a whole. The installation process involves installing Dendro on each server to be monitored. Once installed, Dendro needs to be configured for each server, specifying the services to monitor. The pipeline is then deployed, creating the necessary AWS infrastructure. Vector is started on each monitored server, which streams logs and metrics to the pipeline. The Dendro frontend provides a user interface to view the monitored services, real-time data charts, and query the database. Email alerts are sent if any errors are detected. The entire process from installation to data visualization and alerting is demonstrated end-to-end.

Setting up Dendro on a server requires downloading and installing the NPM package, followed by installing the data collecting agent, Vector. Dendro's configure command allows users to select the services they wish to monitor and generates the necessary configuration files. Deploying the pipeline involves creating the AWS infrastructure, which takes a few seconds for each step. Once the pipeline is deployed, Vector is started on the server, streaming logs and metrics to the pipeline. The Dendro frontend provides a dashboard to visualize the monitored services. Dendro also sends email alerts based on configured alarms. The tear-down command allows for easy removal of deployed AWS services.

With Dendro set up and data flowing through the pipeline, the frontend dashboard can be accessed on any machine with Dendro installed and configured. The dashboard provides real-time charts of the monitored services, allowing users to track their performance. In case of an error, users receive prompt email alerts. The query page allows users to directly query the TimeStream database and retrieve specific data, helping in troubleshooting and analysis. Dendro provides a comprehensive monitoring and alerting system, enabling effective management of distributed systems.

In summary, the Dendro project aims to streamline the monitoring and management of distributed systems. By aggregating logs, centralizing them, and providing a user-friendly dashboard, Dendro enables proactive detection and resolution of issues. With real-time alerting, users can quickly respond to errors, minimizing downtime. The installation and deployment of Dendro are straightforward, allowing for easy set-up on multiple servers. The dashboard provides valuable insights into the health and performance of the distributed system. Dendro significantly reduces the mean time to resolve outages, ensuring efficient management and monitoring of distributed systems. In the current pipeline, there are no components set up yet. To deploy the pipeline, we can run dendrodeploy. Upon running this command, it will prompt us to verify that our credentials have the necessary permissions by providing a link. Once verified, it will then ask if we want to set up alerting, which will send us an email in case of specific failure conditions. Each step of this process takes only a few seconds. The configuration options are used to create database tables that match the monitored services. This deployment process typically takes around 15 to 60 seconds, as dendro sends 37 API calls and spins up services in the background. After successful deployment, we can run dendro list again to confirm that our pipeline has been deployed. In the latest list, we see that our database tables are now named "host metrics" and "postpress metrics," matching the configured options. Finally, we have the option to run the tear down command to remove all deployed AWS services. This command would ask for confirmation before deleting the resources. The tear down command simplifies the process and ensures no extra AWS services remain, costing unnecessary money. Upon successfully tearing down the services, we can run the list command one more time to confirm their deletion. Now that the pipeline is set up, we can proceed to send data to it using Vector. But before that, let's run dendro list once again to show that we have a pipeline set up. Now, we can start Vector by passing in the configuration file that was generated from the configure command. This configuration file ensures that we are monitoring the appropriate services on this machine. Once Vector is running, it will start streaming logs and metrics to our AWS pipeline. With data passing into our database, we can now visit the front end by running the command "dendro start server" from any computer where Dendro is installed and configured. After running the command, we can visit "localhost:3000" in a browser to access the front end. On the home page of the front end, we can see the services being monitored and verify that the pipeline is successfully writing records to the database. Moving on to the charts page, we can view real-time data for the currently monitored services. If there are any issues with their servers, they will receive email alerts immediately, thanks to Dendro. For example, if an Nginx server returns 500 response codes, they will be notified. On the charts page, there is a chart that shows a dip in 200 responses and an increase in 500 responses. Armed with this information, Naptime's engineers can head over to the query page to investigate further. They can query the database to gather more details about what happened. In the given example, they query the Nginx access logs for all 500 errors that occurred in the last two hours. This is made easy by using a time series database. Furthermore, they have the option to export the data and perform a detailed analysis to identify the root cause of the problem. As depicted, Dendro is relatively easy to set up and use. It only took a couple of minutes to get up and running. However, building Dendro itself was not a quick and easy task. Now, let's delve into some of the technical challenges we encountered during the development of Dendro. The first challenge was Amazon's optimistic response object. Typically, when one request is dependent on another, you expect to receive a response before making the subsequent request. However, with Amazon, they may return a response before the service is actually deployed. To overcome this challenge, we implemented long polling with exponential backoff. We repeatedly queried Amazon to check if our operation had been completed, increasing the time between failed polls at an exponential rate to avoid overwhelming the server. Once Amazon returns a state that is not loading, we take appropriate action based on that state change. The next challenge we faced was transforming data in real time, specifically writing raw text logs to a time series database. To achieve this, we had two data transformation steps, one in the collector and another in the lambda function. We had to parse the plain text log using a regex pattern specific to the log format. This parsing process resulted in a structured JSON object. All of this transformation occurred during the log collection process before the log left the host server. The second data transformation took place in the lambda function. Here, we converted the data from an octet stream to JSON format. Firehose, which accepts data from any source, transforms the JSON into an octet stream. To ensure the octet stream is JSON-parsable, we injected missing tokens using regex and parsed the bitstream into JSON. With the JSON data, we can now extract the necessary information based on the key we added during the first transformation. This extracted data is then written to a database table. Looking at the diagram, we can see that the lambda function handles various tasks, from converting the octet stream to JSON, to sorting and extracting data, and finally writing it to a table. This was a crucial step in building Dendro. The final implementation challenge we faced was determining the best visualizations for our dashboard. We had to decide what metrics to measure and visualize to help users resolve errors efficiently. Storing too little information would limit the insights provided by Dendro, while storing too much would increase costs and slow down queries. To strike a balance, we adopted Tom Wilkie's RED method, which focuses on rate, errors, and duration. Time series data works well with this method, as it involves monitoring request rates, error rates, and latency distributions. By understanding these metrics, engineers gain insights into the behavior of their architecture over time. Although Dendro is feature complete and ready for use, there are future work items that could enhance the framework further. One such item is providing users with the ability to inject custom transforms into the pipeline. Currently, Dendro only supports basic data transformations with Vector's remap function. Another area of improvement is built-in support for MySQL, which Vector does not currently support. Finally, on the dashboard, we aim to provide syntax highlighting for SQL queries and additional graphs and visualizations to aid in resolving outages. We believe these future enhancements will strengthen the overall Dendro framework. Thank you all for joining us and listening to our presentation about Dendro. If you have any questions, we will now take a few minutes to answer them. One of the questions is how did you find working together as a team and do you have any tips and tricks for successful collaboration? Communication was key throughout our journey. In the initial phase, we met multiple times a day to research and implement the project. As we progressed, we found a balance between meetings and actual progress. It is important to strike the right balance to ensure efficient collaboration. Another question is what was your favorite part of working on this project? Each team member had their own favorite part. One enjoyed learning about the AWS infrastructure and services used, while another found pair programming particularly exciting. Researching different aspects of the project was also a favorite among team members. Overall, we found joy in various stages of the project. Another question asks if we are using InfluxDB for the time series database. No, we are not using InfluxDB. We opted for AWS's Timestream, which is a time series database launched in January. It offers features such as automatic data transition from hot to cold storage based on predefined thresholds. Timestream allows us to retain data for a specified duration, while keeping costs low by utilizing services like S3 Glacier for cold storage. The next question relates to log shipping and asks what we are using. For log shipping, we utilize Vector, a relatively new tool in the market. This ties in with the following question, which asks why we didn't use Filebeat or Logstash as an alternative to Elastic. We did not use Logstash because it requires the JVM, which we wanted to avoid having users install. While Filebeat, like Vector, is a single binary installation, we chose Vector for its superior performance. On Vector's homepage, it demonstrates that its log shipping from file to TCP is 20 times faster than Logstash and 10 times faster than Filebeat. Additionally, Vector's data transformation steps were easy to work with, allowing for seamless integration with our pipeline. Another question inquires if the tear down command checks for active Vector agents. Currently, the tear down command does not check for running instances of Vector. Since Vector runs on a distributed system, uninstalling Vector from each instance would require additional implementation. However, we have discussed the idea of a page that shows where Vector is installed, allowing users to see running instances in a distributed system. The next question asks if it is possible to set alert thresholds. Unfortunately, currently, it is not possible to set alert thresholds within Dendro. Our current approach is to send an email alert whenever an error occurs. The content of the email includes information about the error and its corresponding log entry, enabling users to take immediate action. We understand the importance of customizable alert thresholds and will consider it as a potential future enhancement. AWS may not always provide complete transparency regarding the inner workings of their backend systems. However, it is believed that they likely utilize technologies such as S3 Glacier for storing metrics. This makes it cost-effective to keep metrics, and users can easily log in and make changes if they wish. To simplify the experience for working engineers on small teams who may not have expertise in this area, we have implemented opinionated default settings throughout the pipeline. These settings are designed to minimize the amount of administrative work required. We have pre-built a pipeline for users, focusing on minimizing toil.

For log shipping, we are using a tool called Vector. While Vector is a relatively new addition to the market, it is proving to be a reliable solution for our needs. Some may wonder why we chose Vector as an alternative to Elastic. The reason for not using tools like Filebeat or Logstash is that Logstash requires the JVM, which we wanted to avoid users having to install. On the other hand, Filebeat is built with Go, making it easy to install just by using a single binary. Vector, like Filebeat, provides fast log streaming from the host server to the desired destination. In fact, Vector claims to be 20 times faster than Logstash and 10 times faster than Filebeat. Additionally, Vector's documentation is extensive and user-friendly, making it an ideal fit for our project.

Moving on to some questions, one viewer asks if our deployment script checks for active Vector agents when tearing down. This is an excellent question. However, the current implementation of the tear-down script does not check for running instances of Vector. This is because Vector runs on a distributed system, and uninstalling Vector from each instance would require additional implementation. A potential solution we have considered is implementing a page where users can view the locations and number of instances where Vector is installed, particularly in distributed systems. This way, users can monitor the running instances themselves.

Another question comes in regarding the possibility of setting alert thresholds. Unfortunately, our current implementation does not allow for setting alert thresholds based on percentages. The system currently sends an email when an error occurs, including details such as the server and error message. So, for now, the system can only notify users when errors occur, without incorporating any percentage thresholds.

Finally, someone asks about the evolution of the architectural design. Was the final architecture readily apparent, or did it involve a lot of experimentation? It turns out that there was a significant amount of experimentation involved. Originally, the plan was to send logs directly from the host server to Firehose and then on to TimeStream. However, due to the relative newness of TimeStream and its limited integration capabilities, we had to redirect the logs to S3 first and then use Lambda to transform them before storing in TimeStream. Although this required additional steps, utilizing Lambda for sorting and transforming the data ended up being beneficial in the long run.

To add onto this, the research phase of this project was quite extensive. Initially, we had minimal knowledge in this area and were considering building our own version of log stash or vector. However, through continuous reading and learning, our understanding grew, and we arrived at the final architecture we have now. This evolution in our thinking and approach was quite an exciting journey. It's amazing how research can lead to unexpected paths and expand our knowledge in unforeseen ways. In fact, our initial interests in event-driven architectures and streaming led us to explore technologies like Kafka, as well as dive into the realm of logs. It's remarkable how one research topic can snowball into a multitude of related areas.

As no more questions are presented, we would like to extend our gratitude to everyone for attending this session. It has been an enjoyable experience, and we appreciate your support and engagement. Thank you all for being here.