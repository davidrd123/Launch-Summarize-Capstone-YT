so hi everyone thank you for joining us today my name is nick my teammates are andrew peter and angel and we've spent the past few months building dendro as a fully remote team spread out over the united states today we'll start off by talking a bit about what dendro is why we think it's a useful framework and discussing what kind of team we designed dendro to be optimal for then we'll discuss what kinds of design decisions we made in service of owning that niche explore what services make up each conceptual piece of our framework show off some of dendro's features with a demo and finally discuss what the process of implementation was like and some of the technical challenges that we faced along the way so first let me introduce dendro to you dendro is an open source serverless monitoring framework for small distributed apps the last decade has seen a paradigm shift in computing infrastructure systems are becoming more and more ephemeral abstracted and distributed we built dendro in order to help small teams handle the challenges of operating a distributed system innovations in infrastructure and architectures have yielded simpler components at higher levels of abstraction but behind those simpler pieces lurk more complex systems we are able to deploy and stitch together many more different kinds of software components today resulting in vastly increased complexity as individual nodes in your system's topology become more decentralized they also become more complicated to track and monitor dendro enables you to collect centralize and store log and metric data emitted by the various disparate pieces of your system that way when something goes down in production there's no race to ssh into a dozen different nodes to figure out what's gone wrong all of the log data you would use for debugging has already been collected processed and tagged with the servers and services that originated the individual records before being stored in a single database for querying all of this empowers the user to both greatly reduce costly downtime or outages and proactively improve the overall performance of their systems proaction is key you want to catch problems before your users do because if you're constantly applying ad hoc band-aid fixes in recovery mode you're making your system more fragile over time instead of less to illustrate just exactly what dendro does let's use the example of a small company named naptime naptime has two engineers and they recently completed the beta build of their sleep monitoring app they invited some friends and family to test the app and are preparing for their full launch their first few users have been very helpful in testing and discovering some key bugs in their application code but their system itself has proven to be resilient so far thanks to its small size and simplicity naptime built a simple three-tier system in order to avoid premature optimization with their web server and application running on one server and their database on another the two engineers have been busy developing features and shipping their first few builds when an outage or critical bug pops up it derails their development flow naptime doesn't currently have a monitoring solution in place so they're totally reliant on either hearing from users when something breaks or coming across an issue themselves by poking around the system what happens when an outage is reported say for example a couple of users reach out to say that they're trying to access their home pages but they're just receiving an error the engineers decide on three priority debugging steps first to ensure that each server in their system is up then ensure that each of the services running on those servers are up and finally to check whether those services are sending and receiving requests and responses successfully first if they have a list of their static ip addresses they can use the unix command line utility ping to check whether each of the servers is up at all if the servers are up it might also be useful to see some metrics on their health napty might run a utility like top on each of the servers to see a summary of the system's health like overall cpu load or available memory as well as what's actually running on the server if everything checks out with the system's health it's time to start looking into the individual services running on each server if they're interested in a running process like an application they might check the status of their process manager each service generally has a different method of checking its health after the most obvious checks like is something even running come the next steps what if none of their checks turn up an issue performing each of the aforementioned steps manually will tell them about how something is performing at the moment of observation but what about just before they checked or how about at the time the complaints started coming in the log files that each of naptime's servers and services write to can provide the answers to these questions let's say that the naptime team is curious about what's going on with their nginx web server one of their next steps would be to take a look at the access log file in the nginx subdirectory of var log web servers like nginx and apache produce access logs where each line contains information about every request sent to the system like the ip address of the requester the time of the request the path of the requested file the server response code the time it took the server to fulfill the request and more if there are 500 level errors occurring exploring the access log of their web server is where naptime would find evidence of them the overall file that naptime finds would look something like the example on the left with each line resembling the example on the right an interested engineer can take logs like these and analyze them either one by one or aggregated in order to achieve a number of insights like finding individual specific events in the past or graphing trends over time like 500 response codes or aggregated requests per minute and then it allows them to alert specific users when predefined heuristic thresholds are met or surpassed let's consider naptime's architecture and processes after they scale a bit more they still have a three-tier architecture with presentation business logic and storage abstracted away from each other but now they have more overall servers and services collecting and processing logs is straightforward when everything is on a single machine you just use some software to handle aggregation but when you have a distributed system with multiple nodes instead of just software you need both software and some infrastructure that can handle both transportation to and storage in a new central location suddenly naptime is in charge of eight nodes instead of two scaled out app servers more unique services a load balancer and multiple databases what is debugging like for them now what if mongodb suddenly stops responding or one of the app servers can no longer write to the main postgres instance how will naptime isolate what went wrong let alone actually fix it their number of servers and services keeps growing and the number of connections between all of them continues to grow exponentially how is the team supposed to get a holistic understanding of what's happening across the entire topology of their system monitoring is toil that is if you don't have an automated framework taking care of it for you as we saw one key component in monitoring is examining the log output of your systems that can mean reading through files thousands of lines long like engine like naptime's nginx access log one line at a time no one wants to read log files line by line if it can be avoided after all one of the main motivations of the field of software engineering is automating the toil out of processes on unix-like systems a common strategy to win to gain a quick insight into what's happening with the service is to run the command tail f on the log file to which that service writes tail prints the last n lines of a given file to standard out and the f flag indicates that the program should continue to follow streaming new lines to standard out as they are written to the file this process worked fine for nap time when they wanted to watch the real-time stream of log messages on one machine or two but what about now that their system is distributed across nearly a dozen servers are there engineers going to open a dozen terminal windows and ssh into each individual server monitoring distributed applications is exponentially harder they simply would not be able to watch and comprehend the stream of logs coming in for every service on each server in real time naptime would want to be able to capture those logs as they're written or emitted and save them for later ideally on a central server of some kind so they don't have to go ssh-ing around to find what they need during the stress of an outage the visibility a team like naptime seeks into the inner workings of their deployed system can be summed up by the concept of observability we define observability as a potential property or feature of a system that has been designed by a team fully aware of some key limitations of distributed systems specifically the limitations that no complex system is ever fully healthy because of the nature of software and the exponential growth of complexity and linkages between systems and that distributed systems in particular are pathologically unpredictable when you have multiple distributed systems communicating with each other over the wire with each hosted on a potentially ephemeral node the possible failure conditions are nearly endless what if a server is down what if a service is down what if two pieces are both up but are failing to communicate for some reason the list goes on and on in light of those challenges a team must also keep in mind that the solution to the problems presented by the previous bullet points is that systems should be designed to facilitate debugging because things will break and because we know that hope is not a strategy but preparedness is and so things can get messy very quickly without a single centralized platform for handling the collection transportation and storage of your data those valuable records can get lost or never be collected in the first place developing a strategy for the collection centralization and retention of your log-in metric data means that you don't prematurely forfeit the benefits that logs and metrics provide dendro acts as that strategy a team can ssh into each of the nodes in their system run the simple command npm i dendro go through some configuration and start shipping all of their login metric data off of their servers and onto a central store the logs and metrics produced can come either from services running on the server like nginx or a database or from the server itself in the form of metrics like cpu load or available memory dendro provides a single centralized log management solution untangling the web of data producers and consumers no more routing logs from individual sources to multiple destinations over time no more custom aggregation scripts cron jobs or any of that dendro collects the logs and metrics emitted from distributed servers and services over time building a single source of historical state that single source of truth about system and service state and performance empowers the user to ensure that all output or generated data is reliably and automatically captured without having to do it manually to avoid context switching or jumping between databases and machines trying to dig up whatever went wrong and finally to reduce debugging time because everything exists in a single database and can be queried and explored like normal relational data each of these benefits brings the user closer to the holy grail for engineers the ability to free up their time from manual toil and so now that we've discussed a bit about the difficulties of operating a distributed system let's zoom in on dendro and see how exactly it can help alleviate them hopefully by now we've piqued your interest in dendro you might have realized that using dendro helps you reduce the toil that your engineers experience and that setting up a centralized logging solution is something you should do for your distributed system sooner rather than later so you have your data ready and waiting to help you debug system issues in production when you need it the most dendro isn't trying to compete with every monitoring solution out there it's a crowded field but we do think there are a few key things that we optimize for that other solutions currently do not but before we get into that let's look at what we actually do offer from end to end so what does dendro help you do well let's think back to the example of nap time dendro includes a collection agent that the team installs on each node in their system that collection agent gathers log and metric data from both the services running on the server as well as information about the server itself but how does that data make the jump from those machines to the central database that we see on the right hand side while that requires an infrastructure pipeline in addition to the deployment of software on each of the systems once the data is successfully populating the database dendro then provides a convenient interface for dashboarding and querying the data this allows for a comprehensive unified window into the health and state of naptime's system this is an overview of dendro's architecture and the infrastructure behind this framework we'll dive into it in more detail later but for now let's stick to the main conceptual pieces we've already discussed how dendro helps you set up automated collection of logs and metrics from your services you don't need to change a single line of your application code dendro runs as a fully decoupled agent process on your nodes tapping into the log files to which your service is already right and scraping metrics from the server but collecting your logs and metrics is only the tip of the iceberg the collection agent streams data at the time of generation to our pipeline getting the data off of the machine as quickly as possible is important for both your ability to monitor in real time as well as ensuring that data isn't lost if an ephemeral virtual machine gets spun down outside of your control the pipeline also features a processing step where we take the raw data coming off of your servers and rework it into predictable structured output before storing it in a time series database optimized for writing processing and querying time series data dendro builds a single source of truth for you with each table representing a different capture source while preserving the identity of the host machine or process for each row of data all of this exists in the same place allowing you to keep context switching at an absolute minimum while debugging while also maintaining high resolution in your data so that you can visualize and explore granular changes in the performance and state of your servers and services over time and so the reason we think dendro is so useful is that if you do not currently have a system for monitoring your system we can guarantee that dendro will save you time money and anguish the next time you have a system outage we know that engineers are busy and generally prefer building new features to process improvements so it was crucial that we made dendro as painless to install as possible so that it makes sense to do it today instead of adding a ticket to your icebox and so we built dendro with that goal in mind for the specific niche use case of a small or early stage distributed application without an existing plan for monitoring their system and we think that having that overarching goal and some related sub goals that we'll discuss later in the presentation has allowed us to slot in nicely to a crowded field let's look back at the case of nap time since they scaled up their architecture their debugging process has become harder and harder if they were to start researching a monitoring solution that worked for them they would find that companies generally consider three options buy operate or build so if they were to buy a solution they might look at working with a sas vendor like datadog and so outsourcing your monitoring to a vendor makes the whole process very easy and painless but that ease comes with a pretty steep price and not every small company has enough runway to justify spending money on yet another sas product the next option for them then is to operate their own system open source platforms like elastic were developed to make this relatively simple however if you want to use the open source elastic stack you're still responsible for hosting all of it on your own infrastructure and managing an elastic search server requires knowledge of java and prolonged attention but what if naptime's engineers don't have that expertise or time the third option build simply is not feasible for a small company if you're a tech giant like facebook or twitter with a huge budget and plenty of infrastructure you can roll your own observability platform but for a small company their engineer's time is much better spent on their own core business product that keeps the lights on but what if there was another option that's where we think dendro fits in dendro combines the ease of deployment and low maintenance of a sas solution thanks to our collection agents and fully serverless pre-existing pipeline dendro mirrors the ownership of data promised by open source or diy solutions log data can be extremely sensitive and you don't necessarily want someone else having access to your records in their god view or admin platform however dendro isn't nearly as feature feature-rich as any of the other solutions we're built for a specific use case and can't come close to offering the richness and depth of a data dog elastic stack or a highly custom diy platform one of dendro's core differentiators is that we treat time series data as a first class citizen the time series database in our pipeline is optimized for inserting and indexing time-based data and comes with a whole slew of time series query and aggregation functionality that elasticsearch does not support but what we trade for the ease and speed of setup and time series optimization is cost aws pricing can be opaque and tricky to calculate at times but dendro certainly costs more to keep running than the elastic stack or a diy platform because you're not operating the platform on your own infrastructure now i'll hand things over to andrew who will tell you a bit more about the design decisions we made that allowed dendro to exist in that niche that we just discussed thank you nick since a logging pipeline isn't something your users will immediately benefit from it's easy to push it off to the next sprint in favor of shiny new features that's why our primary goal is to help teams start collecting centralizing storing and getting value out of their logs and metrics today so they have them when they need them we broke this main goal into several sub goals to direct our implementation process these goals include creating a framework that is manageable for small teams filling the gap in the market that is the lack of services that create time series data that treat time series data as a first class citizen and finally decreasing the time it takes to resolve outages the problem with integrating services is it takes time this is time engineers could be spending on creating new features or fixing existing bugs this is time that could be spent on areas that will immediately benefit your users naptime's team is small they have mission critical work and new features piled up and don't have time to go figure out how to string together a logging pipeline that's why we wanted to create a low toil framework a lot oil framework is one that is easy to set up and maintain helping your core development flow without adding to your busy workload to overcome this problem we felt we had to keep integration easy provide the infrastructure for the pipeline and keep it manageable by small teams the first step of a load toil framework is ease of integration for us to aggregate logs we need users to install a log collection tool onto their servers we wanted something that could collect transform and route all observability data with one tool moreover we wanted a tool that can monitor multiple services without requiring users to write custom integration code we then needed to provision and deploy the logging pipeline for our users to automate as much as possible operators can use the dendro cli to deploy manage view and tear down the logging pipeline the deploy command alone makes over 30 different api calls the entire cli handles over 75 different api invocations making the pipeline as easy as possible for users to manage and finally a low toil framework means a framework that requires little maintenance and is manageable by small teams naptime's engineers know all too well provisioning servers is difficult and they can crash at seemingly random times if dendro added extra stress and overhead people would be less likely to use it we wanted to avoid that at all costs that's why dendro is completely serverless every service of dendro's logging pipeline is managed for you by aws this means you have less maintenance naptime doesn't need to worry about dendro crashing or becoming bogged down when their peak hours occur now that we went over the three things that go into making a low toil framework let's look at the next problem elastic and other services are optimized for text-based querying elasticsearch is great at searching through text-based indexes but what about time how does naptime calculate and view their number of requests per second this is why we treat time series data as a first class citizen time series data is a collection of observations obtained through repeated measurements over time for example time series data can measure the average response time of a web server over the course of a week now we treat logs as time series data because they already are time based storing them in a time series database unlocks a large range of time-specific querying functionality that helps dendro improve your ability to extract insights from your logs over time if we look at web servers such as apache or nginx we can think of a few important metrics that use time as a measurement number one request time duration what happened yesterday that caused our average request time to double number two request status codes what happened at 4 pm that started causing 500 status codes and number three requests per second when are our slowest hours so we can do some maintenance the final problem we will look at is getting insights into distributed systems is cumbersome if we take a look at naptime's architecture we can see they have a load balancer four app servers and three databases this is eight different nodes and 16 different network connections that could fail if some error occurs where did it happen what node failed what logs did they check without log aggregation and centralization naptime would have to check each node individually that's why our goal is to decrease the time between an outage occurring and the time it takes for you to resolve it we don't want your outages to be a murder mystery and we don't want you or your team being reactive by the time a naptime engineer is assigned and reads a ticket a user may have been experiencing a problem for hours or even days instead we want you to be proactive dendro helps you catch issues early fixing them before complaints about slow page loads or outages start coming in debugging during an outage is always more stressful so dendro helps you catch and correct bugs in your system before everything comes down and it does it in a more explicit way letting you know exactly what's broken more precisely than a user ever would helping you pinpoint exactly what went wrong decreasing the mean time to resolve an outage includes decreasing the time it takes for your team to become aware of it that's why dendro monitors incoming logs and notifies you of system failures this notification informs you in real time what service failed and what the error was with dendro naptime's engineers no longer need to check eight different nodes by aggregating logs into a single database and by providing a dashboard to query data and view health metrics we help solve outages efficiently without your engineers having to play detective now that we've talked a little bit about how the decisions we made help to situate dendro in the ecosystem of monitoring solutions it's a good time to show you what we have actually built for that i will hand it over to my teammate angel all right thank you andrew so as we've seen dendro is made up of four major parts each of which is made up of smaller components the first step is collection for each service in your distributed app that you want to monitor you're going to set up collection if you want to look at the diagram on the left you can see each service has dendro deployed on it we're using a collection agent called vector to gather transform and send our users distributed logs to the pipeline now raw logs are text based unqueryable and generally hard to work with with vector we can take this raw plain text nginx access log and convert it to json prior to sending the log off to our pipeline for further processing looking at this log we can see several important pieces of information first we can see the path that was requested next we can see the amount of time it took to complete the request and finally the time the event occurred for all of this to work we need to configure vector properly this is done by listing all your log and metric sources and seeing syncs writing esoteric regexes to capture and transform those logs and configuring your aws credentials here we're also using vector's remap function to add a type property to this gathered data this type property is important as it allows us to identify where this part a particular piece of data originated from so a fully fleshed out configuration file is over 400 lines long this caused our team plenty of headaches when we first were manually typing it out in order to streamline this we've provided a configure command that provides a prompt select which services our users want monitor with these services selected we're now able to dynamically generate this huge config file for them now with vector properly configured and our logs and metrics gathered and transformed to json vector then sends it off to the pipeline the first stop being amazon's kinesis firehose firehose is an event streaming platform that captures transforms and delivers streaming data it works in near real time by ingesting new data in one minute buffers and finally it is completely serverless adding no additional infrastructure to manage in today's distributed world data from one service flows to multiple other services connecting all of these is difficult time consuming and hyper specific kinesis fire hose is a platform that helps decouple services from one another instead of connecting every service together in a hyper-specific manner we can now have each service published to or consumed from fire hose with many vector instances shipping data firehose serves to aggregate these streams of data into one megabyte or 60 second chunks whichever comes first this helps to reduce the overhead when processing the data firehose then stages this data into an s3 bucket s3 is a simple scalable object storage service this data is staged temporarily in this bucket to allow our users to view any records that have errored out before being stored in time stream our alerting infrastructure which we will examine later allows us to immediately notify our users of such errors these records are deleted automatically after two days and as soon as they're received the bucket invokes our lambda function now lambda is a serverless compute service meaning you can execute code without provisioning servers functions can be invoked in response to certain triggers making it event driven and it could scale with the workload by concurrently invoking more instances of your function when needed this ability to continuously scale is perfect for dendro because a log data tends to be very bursty and varies widely we use the lambda function to transform and load data into our time series database initially we wrote the lambda function in javascript and later on we rewrote it in golang this is because the lambda function is pivotal to the throughput of data ingestion and we decided using a compiled language for speed would be best this rewrite to go netted us a twofold increase in throughput when processing 1000 records the gain and speed could be even more remarkable when processing significantly more documents once invoked our lambda function then retrieves records from the bucket and stores them into separate tables in time stream based on the type property which we added using vector this takes us to timestream our time series optimized database with our data extracted transformed and finally loaded into timestream we're able to now analyze and query this data across arbitrary periods of time we've taken raw logs and metrics and transformed it into a form that's easily analyzable and processable this is possible because time stream provides a sequel like language to query this data this query language is extremely powerful because not only is it already familiar to most developers but it also adds some very powerful functions that work with time series data sets this allows you to perform interpolation as well as calculate the derivative or integral find correlations and perform filters and reductions on entire time series data sets all of this is performed at the database level completely offloading the work from your more costly general compute resources with this infrastructure in place we're able to build a very robust monitoring and alerting system let's now take a closer look at our alert infrastructure we're able to email our users as soon as errors in their distributed app arise and a big portion of this is possible because of cloudwatch cloudwatch provides a centralized view of all the logs coming from our cloud infrastructure we're able to set up metric filters on those logs that when activated will set off an alarm and this alarm publishes an event to a simple notification service topic which in turn will email any users that subscribe to the topic during setup this allows us to watch for any errors as they occur and immediately notify our users finally we arrive at our custom built data exploration and dashboarding server we've provided a dashboard to help visualize and query your data in near real time as it's being produced this dashboard is available to for view on any machine in your distributed system that has dendro installed we've created charts that allow our user to detect anomalies and see trends create forecasts and create forecasts visually these charts are dynamically generated and provide an up-to-date view of their gathered data using this dashboard we're able to query timestream directly and view any returned rows for convenience as well as export this data with a simple click of a button cloudwatch logs are also available for view in the dashboard this centralized hub provides a quick visual overview of the distributed system's overall health now that we've individually examined each component of dendro we're now better equipped to view dendro as a whole we've seen how we collect records transform and store them and finally how this allows us to monitor an entire distributed system i will now pass it off to my teammate peter who will demo dendro end to end thanks angel so how do we set up dendro well if we think back to the example we saw of naptime whose two engineers are currently running an 8 server setup they would need to install dendra once on each server remember that each box here represents a machine that naptime is maintaining so eight boxes in total next we'll run through what it takes to install dendro on a single server we'll take a server running a postgres database as our example on the bottom here i'll walk us through the installation process for this particular machine so dendro is an npm package which means the first step is to download and install it just like any other mpm package the second step is to install the data collecting agent vector on the local machine dendro provides a command which helps the user to install vector given their particular package manager the third step is to configure dendro for that server using the dendroconfigure command this step results in two configuration files one a file that vector uses to monitor the selected services on that machine and two a file that dendro uses to deploy the pipeline including aws credentials and database tables that match the selected services for that server so next we'll see the configure command in action here we are on our postgres server about to configure dendro on this machine again we'd have to do this for each node that we want to monitor so we run dendroconfigure and we have to choose which services we would like to monitor here we'll monitor postgres and the host machine health for postgres we'll collect health metrics we'll go with the default values for postgres config and for the host machine we'll collect all metrics if we hadn't already input our aws credentials we would be prompted to do that as well we can then run dendro review to confirm that our configuration is correct okay okay so now dendro knows which services you want to monitor on your machine the next step is to actually build the pipeline again the pipeline provides a centralized location where all of the monitoring data from our various servers will live for a quick refresher here is our pipeline's architecture on the left are the various servers on which we configure dendro and install vector everything to the right of collect is the aws pipeline which we will set up next let's build the pipeline by running the dendro deploy command this will set up the aws infrastructure and this is the fourth out of the five steps we need to set up dendro here we are on our server about to deploy the pipeline first we'll run dendro list to see what our current pipeline looks like here we see that we don't have any components of the pipeline set up yet then we can run dendrodeploy to deploy the pipeline first it asks us if our credentials have the appropriate permissions providing a link where you can confirm that they do then it asks if we want to set up alerting which will send us an email under specific failure conditions it only takes a few seconds for each step again it's using the configuration options to create database tables which match the monitored services this process can take anywhere from 15 to 60 seconds as dendro sends 37 api calls and spins up services in the background and after it's deployed we can run dendro list again to see that our pipeline has been deployed and as we see here our database tables are host metrics and postpress metrics matching the configured options finally we can run the tear down command to remove all deployed aws services it will ask us to confirm that we want to delete the resources the tear down command is for ease of use and to ensure that there are no extra aws services floating around in your account costing you money finally we can run the list command one more time to confirm that the services have been deleted okay now that the pipeline is set up we're ready to send data to it so we'll do this by running vector which i'll show next now we're going to run vector okay but first we'll run dendro list again to show that we have a pipeline set up okay then we'll start vector passing in the configuration file generated from the configure command this file ensures that we're monitoring the appropriate services on this machine vector is now streaming longs and metrics to our aws pipeline now that we have data passing through to our database we can visit the front end which is accessed by running the command dendro start server from any computer that has dendro installed and configured and then visiting localhost port 3000 on a browser here's what the frontend looks like there are three pages listed on the sidebar the home page a charts page and a query page here on the home page we see what services are being monitored and we can check that the pipeline is successfully writing records to the database on the charts page we see real-time data for the services we are currently monitoring going back to our company naptime say there was a problem with one of their servers because they were using dendro they immediately got an email alert letting them know that their nginx server is returning 500 response codes here we also have a chart that shows us a dip in 200 responses and an increase in 500 responses once nap times engineers know about this they can go to the query page and query the database to find out more about what happened in this example we query the nginx access logs for all 500 errors that occurred in the last two hours this is easy to do because we're using a time series database we can also export the data and begin to drill down to find the source of the problem as we can see dendro is very easy to set up and use we just did it in a couple of minutes but making dendro was not quick and easy andrew will now talk more about some of the challenges that we encountered when building dendro thank you peter let's now look into some of the technical challenges we faced while building dendro we had to overcome three major challenges the first challenge we ran into was amazon's optimistic response object when one request is dependent on another you expect to be able to make a request get a response and then make the second request however with amazon they may return a response before the service has actually been deployed so when you send the subsequent request it fails to overcome this we utilized long polling with exponential back off we ask amazon over and over again if our operation has been completed increasing the time between failed polls at an exponential rate in order to avoid overwhelming the server or hitting a rate limit once amazon returns a state that is not loading we can react to that state change accordingly the next implementation challenge was transforming data in real time or more specifically how do we write roth text logs to a time series database to achieve this we have two different data transformation steps the first is in the collector and the second is in the lambda we have already discussed how both work but to reiterate logs are typically emitted as plain text here we have an example of an nginx log in order to prepare it for insertion into our database we first have to parse it using a regex pattern written specifically to match this particular log format parsing that plain text log with this regex pattern gives us this a structured json object all of this happens during the log collection process before the log has left the host server the second data transformation happens in the lambda where we take an octet stream and write it to the database firehose can accept data from any source in json xml and numerous other formats firehose just accepts that data without checking the validity of it unfortunately for us the easiest way to accept any data is to convert it to binary and keep appending it to a binary blob this is where fire hose converts our json into an octet stream an octet stream is a binary file which in dendro's case is a fancy way of saying it's not json parsible if we convert that binary data to a string though it kind of looks like an array of json records however it's missing a couple tokens utilizing regex we can insert those missing tokens and parse the bitstream into json once in json we can now check the type key that we injected during the first data transformation step with this key we know what source this particular data came from what data to pluck out and store and what table to write it to looking back at this diagram we can see this lambda function is doing quite a bit of work taking an octet stream parsing it to json sorting and plucking out data to finally writing that data to a table was one of the one of the single most important steps to building dendro the final implementation challenge was visualizations the challenge here wasn't in the sql queries but deciding what do we measure and what do we visualize when naptime servers are down and they view the dendro dashboard what metrics will help them resolve their errors if we store too little information then dendro cannot provide meaningful insights however if we store too much information we will dramatically increase costs and slow down queries so a balance has to be made tom wilkie who previously was a site reliability engineer at google created a micro service oriented monitoring philosophy called red this works well for our use case considering micro service architectures are typically distributed systems the red method focuses on rate errors and duration all of which work well with time series data rate is the number of requests per second errors is the number of those requests that result in an error response and duration is the duration of time those requests take to complete tom wilkie once said understanding the error rate the request rate and then some distribute distribution of latency gives you a nice consistent view of how your architecture is behaving and that's what we wanted dendro to provide a consistent view of how your architecture is behaving over time this brings us to our final section of course an open source project is never fully finished while we think dendro is feature complete ready for people to use and immediately benefit from there are certain future work items that would strengthen the overall framework let's briefly discuss a few of them we would like to provide our users the ability to inject custom transforms into the pipeline currently we only have vectors remap function to provide very basic data transformations we would also like to provide built-in support for mysql which vector does not currently support and finally on the dashboard we would like to provide syntax highlighting for sql queries as well as more graphs and visualizations to help resolve outages thank you all for joining and listening to our presentation about dendro if you have any questions we would be happy to take a couple minutes and answer them okay the first question how did you find working together as a team do you have any cool tips and tricks for how you did this successfully well uh i would just like to start off by saying that um communication definitely is key um when we were first researching this uh topic and implementing it we were meeting multiple times a day and that was definitely very useful as we got more settled we would only meet maybe once a day or once every couple of days um but yeah communication definitely is key there is a balance to meetings though uh because they do take a lot of time so you just want to find the balance between meetings and actually you know progressing the project okay what was your favorite part of working on this project i guess that's the question for everyone well i really liked uh learning about the aws uh infrastructure i guess just the services that we used in interacting with aws i thought it was a lot of fun i guess that combined with all the pair programming i did particularly with angel when we worked on that i mean i didn't i had never really done pair programming before and that was kind of a new thing for me and i really enjoyed that i don't know what other anyone else has to say about that uh i think my favorite part was it's going to sound kind of lame but it was in the research phase um monitoring isn't you know you know that fun of a topic however all the research that went into you know logging pipelines data transformations um all of the different processes such as stream processing or batch processing i mean all of that was just really like cool to learn about and i think it'll you know definitely be beneficial moving into industry with it anyone else um i feel like people have talked about my favorite parts already i really enjoyed um research learning about the different pieces of you know infrastructure that aws provides sort of the infinite possibilities and stringing them together in different you know different orders um yeah pair programming is fun too all of the above yep okay okay next question are you using influx db for a time series database yeah so maybe i can answer some of michael's questions uh in a row but no so we're not using influx db we're actually using timestream which is an aws time series database it's relatively new um it launched in i think january or so and so um we were excited to get into that and that actually gets us into another one of your questions which was about data retention and so one of the sort of key selling points of time stream for us was that it can automatically move data from hot to cold storage for you based on some predefined threshold and so i think as of right now our default is that it moves data from hot to cold after two months um and i believe you know i'm not totally sure because aws isn't always totally uh transparent about how things actually work in the back end but i believe they probably use you know s3 glacier or something like that so it's pretty trivial cost wise to keep your metrics there but you know i think people can for the interested user people can always log in and change stuff around i think we tried to have sort of these like opinionated default settings for a lot of the pipeline that ties into keeping things low toil the idea that you know if you're if you're a working engineer on a small team and this is not your particular area of expertise we wanted you to be able to not have to worry about too much stuff so we have some sensible defaults we have this pre-built pipeline for you um everything sort of in service of that low toilet goal and what are you using for log shipping so we're we're using vector for that one um which is a relative like new entry to the market and i think this you know this question goes hand in hand with the next question which is you say this is an alternative to elastic but why not use file bead or logstash and so logstash requires the jvm which we didn't want users to have to install filebeat um is made with go so you know it's um it's just a single binary to install which is nice so is vector however on vector's you know home page uh it shows that uh file to tcp so like streaming logs from from the host server to you know wherever it needs to go is 20 times faster than long stash and 10 times faster than filebeat along with that we found their data transformation steps like the documentation was phenomenal and it was really easy to work with so it just melded really well with what we were trying to do okay awesome uh does stare down check first for active vector agents um that's a really good question um no tear down does not check for the uh running instances of vector um because we're vectors running on a distributed system we would have to then go into each of those systems and uninstall vector and we just haven't implemented that functionality yet one thing we did talk about implementing though is a page to view where you actually have vector installed so you can see exactly where how many instances are running for a distributed system it might not be a problem if five percent of certain layers servers crash it will probably be a problem if 50 of them crash though is it possible to set alert thresholds uh so currently unfortunately not what we do is let's think of an nginx web server when when an error occurs it will log you know a fatal error and then the error message that went with it so when that log passes through the pipeline we will send an email that um you know this service on this server failed and here was the message so we can't do anything about percentages right now we can only just say hey this error occurred at this time okay and final question what did the evolution of action architecture look like was the final arctic architecture fairly obvious or did it take a lot of experimentation it took quite a bit of experimentation actually because we originally wanted it to go from from you know your host server to fire hose directly to time stream but unfortunately time stream is so new and it's still there the team's still working on integrating with other services that you know from fire hose we had to go to s3 and then we had to go to lambda to eventually get it into into timestream however there's the benefit with actually utilizing the lambda as we could then you know sort the data transform it so even though we had to add a couple pieces it ended up actually working in our favor in the long run andrew is it okay if i add on to your answer feel free well i just wanted to say like the research phase of this whole thing was kind of crazy i thought like i mean we started out we didn't really know much about this at all and originally we were thinking of like building basically like a log stash or a vector and that was our original idea and then we kind of read more read more read more learn learn learned and this is what we eventually came up with but i mean that evolution when you talk about the evolution of that it was really kind of a a wild process i would say i don't know if anyone else has thoughts about that yeah it is funny how you can start off researching a particular thing and then as you educate yourself more about you know a realm um things balloon pretty quickly because i think part of how we got to this project in the first place uh was that we were interested in sort of event driven architectures and streaming and so we had started reading about kafka a little bit which is uh so so kafka is like an event streaming platforms very similar to kinesis uh which is aws version kafka is now sort of maintained by a company called confluent but it was invented by uh this guy jake kreps while he was working at linkedin i think andrew had a slide with a quote from him and it's it's all in this sort of like pub sub world of decoupling uh producers from consumers and having everything go from like you know m times n to m plus n connections um and from there we got into logs and it all sort of spiraled so i definitely agree with peter that like it's pretty incredible how you start with research and then everything sort of zooms out from there okay as we have no more questions any final words from dendro team thanks for all being here it was really fun yeah yeah thank you 