Hello everyone, thank you for joining us. We are Sheila, Chris, Andrew, and I am Melinda, and we are the creators of Gander. Gander is an open-source solution for deploying isolated ephemeral apps based on your pull requests. But what does that mean?

Simply put, Gander allows you to view a live deployment of your code changes within minutes of creating a pull request, without any additional steps required from you as a developer. Let's walk through how it works.

When you create a pull request on GitHub, GitHub automatically runs a check called "Create Review App." By clicking into that check, you can see the build workflow running and building the review app. Once the build is done, you can return to the pull request and find a link. Clicking on that link will take you to a live deployment of your app, where you can test the features you're developing.

In a typical development team, there are usually multiple features being worked on simultaneously, meaning there are multiple pull requests open. Gander will spin up separate review apps for each open pull request, and each pull request will have its own semantic link.

For example, let's say we have PR9 and PR10 open on our to-do app. PR9 will have a link starting with "to-do app 9," and PR10 will have a link starting with "to-do app 10." Clicking on each link will lead you to two separate live deployments, each showcasing a different feature. And if you look at the to-do lists within each review app, you'll see that they are different, as each review app has its own isolated database.

We believe Gander is a useful tool, and today, we'd like to share our journey in creating it. We'll start by explaining the basics and making a case for why review apps are valuable. Then, we'll discuss the existing solutions for adding review apps to your development flow. After that, we'll introduce Gander and what it offers. We'll also dive into how we built Gander and the interesting design decisions we made. Finally, we'll touch on some of the challenging problems we encountered.

Let's begin by discussing the concept of review apps and their importance in the development flow. Review apps are temporary deployments of your applications, typically in an isolated environment separate from your development machine. These deployments are easily shareable via live links, making it convenient for others to give feedback on the features you're developing.

In the traditional development cycle, new features are usually visually reviewed only after they have been merged into the main branch. This means that developers have to wait for the end of the cycle before receiving feedback from non-technical stakeholders. This delay can lengthen the development process and lead to merging unsatisfactory features into the main branch.

Review apps offer a solution to this problem. They shorten the feedback loop by allowing developers to receive feedback on their code changes before merging them into the main branch. Review apps are created upon creating a pull request, allowing developers to share their work with other team members, clients, and stakeholders. By reviewing the features earlier in the development process, problems can be identified and fixed more efficiently.

Now that we understand the benefits of review apps, let's explore the existing solutions for implementing them into our development workflow. There are two main categories of review app solutions: pipeline services and review apps as a service.

Pipeline services, such as Heroku Pipelines, provide review apps as part of a fully integrated development pipeline. While these services offer comprehensive solutions, they limit the flexibility of development cycles and require hosting the application with them. If you're looking for a simple review app solution without a complete development pipeline, these services may not be suitable.

Review apps as a service, like Tugboat and Reply, offer a platform for deploying and managing review apps. However, they often come with more features that require additional configuration. They also host applications on their own infrastructure, limiting your control over the code. Additionally, these services typically require a subscription fee, regardless of usage frequency.

These existing solutions may not meet the needs of teams that desire a simple, open-source solution for deploying review apps without excessive configuration. This is where Gander comes in.

Gander is designed for organizations or teams that want complete control over their code and data while maximizing cost efficiency. It offers the benefits of a DIY solution while automating the build, deploy, and tear-down process of review apps. By embracing the pay-as-you-go model of cloud computing, Gander ensures that you only pay for the compute resources you use.

When you incorporate Gander into your development workflow, you gain the ability to preview live deployments of your feature changes within minutes of creating a pull request. Sharing these previews with stakeholders is as simple as sending a link, with no need for login or user management. Gander supports various languages and frameworks and automates the provisioning and teardown of the necessary infrastructure.

However, it's essential to note that Gander currently supports applications using Postgres as a database and hosted on GitHub. It requires a monolithic architecture, where all components of the app are contained within a single app server. If your application splits responsibilities into separate services, Gander may not be suitable.

Before we dive into a demo of how to set up Gander, let's briefly look at its architecture. At a high level, Gander consists of three main components: a CLI tool, workflow automation files in your GitHub repository, and AWS infrastructure.

The CLI tool is installed globally on your machine and serves as your primary interaction with Gander. It enables setup and tear-down processes. The workflow automation files are created by Gander and reside in your GitHub repository. Finally, the AWS infrastructure is the backbone of the review app deployments.

To get started with Gander, you need to run the "gander setup" command after installing the CLI tool. The only prerequisite is having an authenticated AWS CLI on your machine. During the setup, you specify the AWS region for your review apps, and Gander provisions all the necessary infrastructure. It's important to note that this setup is a one-time process, regardless of how many repositories you integrate with Gander.

Once you've completed the setup, creating a pull request will trigger a build based on your code changes within GitHub's infrastructure. AWS resources are then provisioned, and the application build is deployed. You can view and interact with the review app by visiting the generated URL on your pull request.

Gander offers a convenient and customizable solution for deploying review apps. It combines the benefits of a DIY solution with the automation and cost efficiency of a pay-as-you-go model. By incorporating Gander into your workflow, you can streamline the feedback loop, involve more stakeholders in the review process, and preview features in an isolated environment.

Now that we have a clear understanding of Gander and its benefits, let's dive into the technical details of how we built it and some of the design decisions we had to make. We'll also discuss the challenging problems we encountered along the way.

Thank you for joining us on this journey. We're excited to share Gander with you, and we hope it becomes a valuable tool in your development workflow. Now, let's proceed with a demo of how to set up and use Gander. Gander is an automated infrastructure provisioning and teardown framework. It fully automates the creation and removal of all infrastructure components. Currently, it only supports applications using PostgreSQL as a database and hosted on GitHub. Your app must employ a monolithic architecture, meaning that all components, including the user interface, business logic, and database access layer, must be contained within a single app server. If your system splits any of these responsibilities into separate services, Gander is not suitable for you.

Before diving into a demo of how to set up Gander, let's take a high-level look at its architecture. Gander consists of three main components: a CLI tool for interacting with Gander, workflow automation files in your GitHub repository, and the underlying AWS infrastructure for deploying review apps. The diagram represents the flow of events, where a pull request triggers a build based on code changes within GitHub's infrastructure. Required resources are provisioned in AWS, and the deployed application can be tested and accessed via a Gander-generated URL.

To get started with Gander, you need to install it and then run the setup command. A prerequisite is an authenticated AWS CLI on your machine. During setup, you'll provide the desired AWS region for your review apps, and Gander will handle the provisioning of all necessary infrastructure. It's important to note that the infrastructure created is specific to your review apps, and you only need to run the setup command once, regardless of the number of repositories you integrate with Gander. Gander also creates a restricted AWS user for interacting with its resources, and you'll need to securely store the generated access keys and create a DNS record for your review apps' domain.

After setup, the next step is to initialize Gander within your application. Using the Gander init command in your repository's root directory, you'll provide information about your application to customize the generated files and infrastructure. The init command will inject these files into your repository and prepare your AWS infrastructure for the review app. Once this is done, you commit and push the new files to your remote repository, and add the AWS access keys to your repository's secret store on GitHub. That's it! Your project is now set up to deploy live previews whenever a pull request is made, without the hassle of manual configuration.

Now let's delve into the design decisions behind Gander. The first decision was how to trigger a new review app build and where to execute that build. We considered two options: setting up a virtual private server (VPS) or using GitHub Actions. While a VPS would give more control over the build environment, it would require additional authentication to interact with the repository and incur idle resource costs. We ultimately chose GitHub Actions, a workflow automation tool provided by GitHub. Actions execute on GitHub servers, eliminating the need for separate build servers. This choice aligns with the pay-as-you-go model and provides convenience for users, allowing for around 250-300 pull requests per month under GitHub's free tier for Actions.

The next decision was how to generate application builds from source code for different frameworks. Instead of building containers directly, we build container images containing executable code. Images are stored in a registry, and when running a container, the server pulls the image from that registry. For databases, we can use an image from the public Docker Hub registry since it's not application-specific. For application images, we considered options like user-provided Dockerfiles or Gander generating them. However, to prioritize user convenience, we chose to use Cloud Native Buildpacks. Buildpacks automatically detect the language and structure of a project, allowing Gander to generate review apps for a wide range of frameworks without requiring expertise in Docker and containers.

With review apps built, the next step is deployment. We host review apps on Amazon Web Services (AWS) and manage containers within that environment. Managing containers involves aspects like placing them on servers, distributing them efficiently, restarting them on failure, and ensuring communication between containers. Containers are isolated from each other, allowing for multi-tenancy without dependency conflicts. By deploying containers, we can scale up servers when needed and automatically replace failed containers. This approach ensures the availability of review apps even in the face of container or server failures.

In conclusion, Gander aims to simplify the process of creating automated review app deployments. By automating infrastructure provisioning and utilizing Cloud Native Buildpacks, Gander enables the creation of review apps for monolithic applications without the need for extensive configuration. With the integration of containerization and AWS, Gander efficiently manages and scales review app deployments while ensuring high availability and isolation between applications. Transcript:

The goal of the Gander project is to support any framework that the user may need, as long as they can write a Dockerfile to containerize their application. However, one of Gander's design objectives is to prioritize convenience for the end user. In order to achieve this goal, we would prefer not to require the user to have expertise with Docker and containers. One alternative option is to have Gander generate the Dockerfile for the user. This would improve the user experience and align with our goal of convenience. However, this choice would limit the types of frameworks we could support, as we would need to write Dockerfile-generating code for each supported framework individually.

A third option, which we have chosen to use, is to leverage cloud native build packs. Build packs allow us to have the best of both worlds. They are open-source tools that transform source code into a runnable container image. Build packs have the ability to automatically detect the language a project is written in and understand how the project is structured within its repository. This functionality enables build packs to containerize applications written in a wide variety of frameworks. By using build packs, Gander can generate a review app for the project as long as it can be containerized with cloud-native build packs.

Once we have built review apps, we need to serve them. We achieve this by deploying the apps in containers on Amazon Web Services (AWS). To manage multiple review apps, we need a container orchestration solution. This involves placing containers on servers, efficiently distributing the containers, and scaling up additional servers as needed. Additionally, we want to ensure that the application remains available even if a container or server fails. All these container management operations fall under the concept of container orchestration.

There are two options for container orchestration: managing it ourselves or using an externally managed solution. Managing container orchestration ourselves would provide granular control over all aspects, such as scaling and routing setup. Kubernetes and Docker Swarm are two popular solutions for self-managed container orchestration. On the other hand, using an externally managed container orchestration service would delegate the implementation details to a service provider. Scaling and fault detection would happen automatically, and there would be fewer configuration options. We decided to let Amazon ECS (Elastic Container Service) handle container orchestration for Gander's review apps. Using ECS allows us to focus on simplicity and convenience, as granular control over orchestration is unnecessary for a project like Gander, which only needs to manage two containers per review app.

Next, let's discuss server management. ECS offers two ways to launch containers: EC2 (Elastic Compute Cloud) and Fargate. EC2 provides more control over server size, processing power, and access to file systems but also requires us to ensure that server resources are used efficiently. Fargate, on the other hand, fully handles server management, allowing us to focus on the application without worrying about server resources. Considering that Gander will only run a relatively small number of containers at any given time, the limited efficiency control of Fargate is not a significant concern. Therefore, we chose to use Fargate for its simplicity and cost-effectiveness.

One limitation of using Fargate is that we cannot directly access the host machines on which our review apps live. However, we found a solution for data persistence. While containers have access to an ephemeral file system, data may be lost if a container restarts. To overcome this, we utilize Amazon's Elastic File System (EFS) which allows us to map a volume to the containers. EFS is an Amazon product accessible over the network file system protocol and charges based on the amount of storage used. Any data written to EFS is mirrored, ensuring it is not lost during container restarts caused by network faults, application errors, or server failures.

Finally, we address the routing of traffic to the isolated review apps. By default, Amazon generates unique DNS names for each task using Elastic Network Interfaces (ENIs). However, these DNS names change if a task restarts, making it challenging to maintain accurate links. To solve this problem, we utilize an Application Load Balancer (ALB), an Amazon service that acts as a layer 7 load balancer. The ALB routes traffic based on the host name of the URL, ensuring that even if a task restarts, the correct app is still accessible through a consistent URL.

In summary, for each pull request, we trigger a GitHub workflow on their servers, which builds a container image of the application server. The image is then pushed to Amazon's infrastructure, and a review app with the database and application server is started, with data persisted to EFS. This review app is registered with the load balancer, enabling easy routing.

For developers using Gander, after installing the Gander CLI and running the setup and initialize commands, every pull request triggers a GitHub workflow that builds and deploys an isolated, full-stack review app automatically on Amazon's infrastructure. This review app allows non-technical stakeholders to participate in the code review process, thereby enhancing development velocity by reducing the feedback loop.

In terms of future work, we plan to add support for different types of databases, such as MySQL. We also aim to add support for arbitrary startup scripts and include support for Rails-style migrations that require runtime for database actions.

Thank you for watching our presentation. If you have any questions, please feel free to ask in the Q&A section. In this coding Capstone project, our main objective was to increase development velocity by shortening the feedback loop. As we discuss future work, we plan to add support for different types of databases, including arbitrary startup scripts and rail-style migrations that require runtime exceeding the database.

Thank you all for attending our presentation. We will remain available for a few minutes to answer any questions you may have. Please feel free to drop your questions in the Q&A section. Let's dive into the questions now.

Cody asked about expanding the project to run on Google Cloud Platform (GCP) and supporting other databases like MySQL. To answer, it would be quite challenging to transition the project to GCP because many of our solutions are closely tied to Amazon's infrastructure. While it is possible to achieve something similar on GCP, our current implementation utilizes Amazon-specific tools such as CloudFormation and specific implementations. Adding support for other databases would require additional configuration, as our current implementation focuses on PostgreSQL. The difficulty level for implementing this change is uncertain.

Expanding on Cody's question, our ECS configuration for Gander relies on specific mechanisms utilized by the Docker PostgreSQL image. Consequently, supporting other databases would involve researching the expectations and behaviors of those database images in terms of file structure and script execution.

Rodney asked each of us about the most interesting part of the project. Personally, I found the most interesting aspect to be seeding the databases. The challenge lay in figuring out how to run databases alongside the application. But once we solved that, we faced another problem: how to seed a user's schema and data onto the database, ensuring it was ready for testing. We came up with a roundabout solution, which we will elaborate on in our upcoming write-up.

For me, the most interesting part was working with GitHub Actions. I found it fascinating to see everything launch in an event-driven architecture. However, the testing process became a bit tedious, involving waiting for pull requests to be processed and checking for results. Despite the lengthy debugging sessions, it was a rewarding experience.

Seeding the database was an intriguing challenge for all of us. The complexity of the problem made it the most interesting part of the project. Additionally, working with the AWS CLI was exciting for me. Provisioning various AWS architectures and learning about different script-writing methods and options was a completely new and enjoyable experience.

On a different note, I found our group's evolution throughout the capstone project and this specific project to be particularly interesting. We learned how to work together effectively, leveraging our individual strengths and weaknesses while relying on each other's expertise. It was a fulfilling experience.

Moving on to Cody's next question about the time required to build a review app for a large-scale Rails application with extensive code and data. Building a Rails app generally takes about 10 minutes, but the build time can be longer due to the execution of the buildpack and image pulling. It is worth noting that Stagehand is not a joke; it addresses a different problem effectively.

Julius asked about the scalability of Gander. In terms of an organization with hundreds of pull requests and review apps per day, Gander's limitations depend on the complexity of the apps being previewed. Gander can handle a high volume of pull requests and review apps since we use an elastic approach, particularly with Fargate. The organization's scalability will primarily depend on their budget and resource allocation.

Joe's question focused on how well Core prepared us for this project. Core provided us with the necessary background and knowledge to tackle this capstone project effectively. Its curriculum indirectly prepared us for the specific challenges we encountered during the project, making it an essential component.

Milto Stella enquired about the essence of our project. Just to clarify, we did not build a container orchestration system. Instead, we utilized AWS ECS, which is Amazon's container orchestration system. It functions similarly to EKS, but without Kubernetes.

Managing and securing the AWS details utilized for deploying ephemeral review apps is quite straightforward. Users enter their details into GitHub secrets, where they are stored securely. Besides the AWS CLI on the user's machine, the data is stored exclusively in GitHub secrets, which are as secure as the user's machine and their trust in GitHub's security measures.

Daniel wondered if we ran the cloud-native build pack only during initialization and how we handle changes to the app image during development. To clarify, we run the cloud-native build pack on every pull request or push to an open pull request. If changes occur midway, such as a push to an open pull request, the app is disposed of, and a new image is built using the cloud-native build pack.

Daniel further asked if we considered making it a user option to build the review app for each pull request. While we did consider it, we ultimately decided to have every pull request trigger a review app build. However, implementing this feature would be relatively straightforward. We could add a flag that users could set in the comments, such as "do not build," and modify the app to listen for this flag.

Julius inquired about the programming language used for the project, as Gold was added to Capstone. The project itself is written in Node.js, but we tested some Go applications for building purposes. Personally, I found the Go language intriguing, despite its non-object-oriented nature. It was a fun experience, especially when considering statically typed languages.

Regarding dividing responsibilities during the development process, we underwent a trial-and-error process. We began with individual spikes, where each team member worked on specific proof-of-concept elements, and then regrouped each morning to share our progress. This allowed us to teach and learn from each other, integrating our work cohesively. We also utilized pair programming at times. Group meetings played a vital role in successfully dividing responsibilities and fostering effective collaboration.

We were fortunate to have a mentor throughout the project. Our mentor guided us primarily on the project narrative, helping us shape our presentation and write-up. Additionally, they provided valuable insights and scrutinized our thinking, serving as a valuable sanity check.

Testing the framework was indeed challenging due to the heavy infrastructure dependency. Consequently, manual testing became the primary testing method. However, we did begin writing unit tests for certain parts of the CLI that were unrelated to cloud infrastructure. Manual testing was a significant part of our process.

Interaction with our mentor was primarily focused on guidance, shaping our narrative, and reviewing our implementation ideas. They served as an excellent sounding board, providing an external perspective to ensure our project was on the right track.

We appreciate the positive feedback and curious question from Justin. We initially explored various topics and settled on this project when we came across Stagehand. The idea of incorporating a database into Stagehand appealed to us, and despite the challenges we faced, we persevered and successfully implemented it.

The security of AWS details used for deployment is managed by the users themselves. The details are entered into GitHub secrets, which are as secure as the user's machine and the trust placed in GitHub's security measures.

We did not personally implement automated testing for this project due to the heavy reliance on infrastructure. However, we did conduct thorough manual testing to ensure the functionality and integrity of the framework.

Our experience with Core greatly prepared us for this project. The background and knowledge gained during Core provided us with the foundation necessary to tackle this capstone project effectively.

As Julius correctly pointed out, we used AWS ECS, which is Amazon's container orchestration system. However, we did not build a container orchestration system from scratch.

The question about managing AWS details securely has already been addressed earlier, but I will reiterate that the user enters their details into GitHub secrets, which ensures their security.

The idea for this project originated from the exploration of different topics. When we came across Stagehand, we saw the potential of adding a database component, which led us to pursue this project.

Expanding on Cody's question about GCP and other databases, we acknowledge that transitioning the project to Google Cloud Platform (GCP) would be challenging due to our reliance on Amazon's infrastructure. However, a similar implementation may be possible on GCP with the right tools and resources. Adding support for other databases would only require additional configuration since our current implementation is focused on PostgreSQL.

Regarding how we divided responsibilities during the development process, we relied on a combination of individual work, pair programming, and regular group meetings. Each day, we would decide on the division of tasks based on the progress we had made and our collective understanding of the project. This approach allowed us to effectively utilize our strengths and collaborate as a team.

Our mentor played a crucial role in guiding us through the project. They helped shape our presentation and write-up, providing invaluable insights and feedback on our implementation ideas. Their expertise and guidance were instrumental in ensuring the success of our project.

Interacting with our mentor was mainly centered around seeking guidance, refining our project narrative, ironing out implementation details, and receiving feedback. Their expertise and input greatly influenced our decision-making and overall project direction.

In terms of testing, due to the heavy reliance on infrastructure, manual testing became the primary approach. However, we did explore writing unit tests for certain components of the CLI that were independent of cloud infrastructure. We strived to ensure the reliability and functionality of our framework through thorough testing.

Core prepared us effectively for this project by providing the necessary background knowledge and skills required to tackle the capstone project. The curriculum equipped us with the ability to navigate documentation, which was crucial in our efforts to solve the challenges we encountered.

To sum up, we successfully completed our coding Capstone project, focused on improving development velocity by reducing the feedback loop. We plan to expand the project's capabilities by adding support for different databases, incorporating arbitrary startup scripts, and enabling rail-style migrations. We appreciate all the support and feedback from our audience, and we'll be available to address any further questions. Thank you for your attention. The project was originally written in Go, but since Gold was added to Capstone, the team is curious to hear feedback on it. Currently, the project is written in Node, but they are testing some Go apps to be built with it. Despite Go not being object-oriented, the team finds it very interesting and a fun statically typed language to work with.

During the development process, the team initially struggled with dividing the work. They started by doing individual spikes, which consisted of small pieces of work and proof of concept tasks. They would meet every morning to teach each other about their individual work and discuss how it would fit together to achieve the desired functionality. Group meetings involving all team members were also held frequently.

Decisions on how to split up the work were made on a daily basis, with consideration given to who would work on what each day. The team excelled in communication, meeting for about an hour and a half every morning and sometimes once a day. This allowed them to spend a significant amount of time working together as a team, which proved to be effective. As a result, they were all aware of what everyone was working on and were able to collaborate well.

The team admits that there was a trial and error process involved in their approach, but they acknowledge it as a valuable learning experience. They had to figure out the best way to divide the work and adjust their strategies as they went along.

Regarding their interactions with their mentor, the team primarily sought guidance on the project narrative and shaping their presentation and write-up. The mentor provided insights and challenged their thinking by attempting to identify any flaws in their plans. The mentoring sessions served as a sanity check for the team, as they could consult someone with industry experience to ensure their ideas made sense and were practical.

In response to a question about the technologies used to build their project, the team mentioned two main technologies: GitHub Actions and AWS. They also referenced the importance of Cloud Native Buildpacks and mentioned that the command-line interface (CLI) was written in Node. They clarified that the GitHub repository for the project would be made available once they publish their write-up in a week or two. Interested individuals were encouraged to keep an eye out for the write-up, which would contain a link to the repository.

When asked about their confidence in their job search after completing the project, the team expressed optimism, hoping that in a few weeks they would feel more confident and secure in their job prospects.

In their final words, the team thanked everyone for attending the first capstone presentation of their cohort. They expressed their gratitude for the experience and excitement for the upcoming capstone series. They also invited any further questions, suggesting that people can reach out to them via Slack or other means of communication. The video ended with praise for the team's work, followed by appreciation from the entire team.