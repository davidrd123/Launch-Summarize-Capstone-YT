Hi, I'm Holden, a member of Team Textrix, and our Capstone project is called Retrospect. Today, I will introduce Retrospect, explain why we built it, how it can be used, and its ideal use case. Then, we will discuss existing solutions in this space, followed by a detailed explanation of how Retrospect works. Next, Nicole will discuss the design challenges associated with Retrospect and how we handled them. After that, I will walk you through two demos showcasing Retrospect in slightly different use cases. Finally, Svetlana will explain the installation and spin-up process for Retrospect.

So, what is Retrospect? It is a full-stack tracing tool designed for debugging small distributed or monolithic applications. Retrospect records browser events and back-end request data and presents them in a single user interface. This allows developers to easily debug their applications with service traces and session replay.

Let me explain how debugging with Retrospect looks like. Imagine you have an e-commerce website called Bob's Shop, which has a microservices architecture with an authentication service, inventory service, payment service, and shipping service. Retrospect is connected to both the client and the back end. Sometimes, the application breaks, and in this case, a user was unable to check out and filed a complaint with Bob.

Before we start debugging, let's take a look at Bob's Shop. We have a recording of the user interface where we can see the user adding items to the cart, including the email ivan@gmail.com. This will be important later during the debugging process. The payment method information is not recorded by Retrospect, as Bob added some special class names to prevent it from being recorded. Also, the user clicked the submit button multiple times without success.

Now, let's dive into the debugging process. Bob has two pieces of information: the user's email (ivan@gmail.com) and the fact that the checkout failed. To start, Bob searches the back-end data for the user's email using Retrospect's user interface. He sorts the data by date to find the most recent information related to the failed request. By examining the details, Bob notices that the waterfall chart at the top looks unusual, with only three bars instead of the usual five. This indicates that the checkout process was interrupted in the back end. He also sees that the status code returned to the user is 400, confirming that an error occurred. However, this information is not enough to identify the exact problem.

Bob clicks on the chapter ID to access more detailed information. He recalls that the waterfall chart looked incorrect, so he examines the last bar in the chart and discovers that the last request was sent to the inventory service's "inventory not available" endpoint, returning a 200 status code. This tells Bob that the checkout process ended after reaching the inventory service, but he still doesn't know why. He realizes that something must have happened prior to the inventory service being hit that caused the checkout process to end prematurely.

At this point, Bob is stuck and needs more information. Unfortunately, the user is not responsive, so he can't get their input. However, with Retrospect, Bob can click on the session ID and see a replay of what happened in the browser before the error occurred. He can see that the user added apples and cherries to the cart and can review the entire checkout process. This is valuable information because Bob can confirm that the user entered the correct credentials and identify any possible front-end actions that triggered the error.

Based on his analysis, Bob concludes that the checkout process stopped at the inventory service, and he knows that the user was trying to buy cherries and apples. He realizes that these two pieces of information are likely connected. When he closely examines the session replay, he notices that an impossible action occurred â€“ the user successfully added cherries to the cart, even though they were out of stock. This explains why the checkout process broke. Bob realizes that something went wrong in his front-end and possibly back-end code since these products shouldn't have been visible or added to the cart. Although he doesn't know the exact line of code to fix, he now has a starting point for debugging the issue, thanks to Retrospect.

Now, let's imagine what the debugging process would be like without Retrospect. Bob encounters the same error in his application, but this time, he doesn't have Retrospect to assist him. He would need to individually check each of his services, examining logs to find clues about the error. However, logs can be challenging to read and may not provide a clear answer. Bob may not even know if there is an answer within the logs. He checks each service one by one, but he can't find any indication of the error. Without Retrospect, he has to rely on the user's responsiveness and memory, which is unreliable. He reaches out to the user, but they don't respond.

Retrospect is different because it is an observability tool for recording both front-end and back-end activities. It captures not only back-end tracing data but also browser activity. This reduces the time Bob spends searching through logs and makes him less dependent on the customer's cooperation. With Retrospect's session replay feature, Bob can review exactly what the user did before the error occurred without needing their input.

So, what is observability, and why did we focus on tracing? You may already be familiar with two key pillars of observability: metrics and logs. Metrics provide numeric values over time, such as status code frequencies or average response times, to identify irregular patterns that could indicate a problem. Logs, on the other hand, contain contextual information with timestamps and can help pinpoint the source of an issue by reviewing the logged data. However, modern distributed applications require more than just metrics and logs. Tracing is essential to understand where to look for potential problems before resorting to logging.

To illustrate this, let's consider the game of telephone. Debugging in a distributed backend architecture is similar to the game of telephone because both involve passing information through a series of entities. In the game, you ask each person in line what they said to trace any distortions introduced. In a distributed architecture, it is not as straightforward. Users may not remember what they said, microservices may not remember their responses, and it is challenging to identify where distortions occurred. This is where Retrospect comes in.

Retrospect can be compared to the first person in the game of telephone who writes down their name and what they said, passing the paper on to the next person. By the time the paper reaches the end, if there are any distortions, you can trace them back using the recorded information. In contrast, logging would be like giving each person their own separate paper and asking them to write down everything they said. While logging provides detailed information, it does not help identify where distortions occurred because those individual papers are not passed on. Tracing, as provided by Retrospect, is more efficient for identifying issues in distributed architectures.

Now, let's explore some commonly asked questions that Retrospect can help answer. It can tell you which service was down during the checkout process or at what point the process stopped. For example, Retrospect highlighted that the process stopped at the inventory service for Ivan. Retrospect can also provide information about which account the customer was logged into when a particular event occurred. In Ivan's case, Bob could verify that Ivan was using the correct credentials during checkout. Additionally, Retrospect can identify which service took the longest to respond, which we will demonstrate in a later demo.

However, there are some questions that Retrospect cannot answer. It cannot pinpoint the exact line of code that generated an exception in a service, as that would require logging. Additionally, it cannot provide average response times for services, as that is a question for metrics. Retrospect can only tell you which services were slow for specific requests.

To visualize the tracing process, here is a service diagram for Bob's application. The client connects to the API service, which then connects to the authentication, inventory, payment, and shipping services. The service trace, displayed as a waterfall chart in Retrospect's UI, reflects the order of service calls: first authenticate, then inventory, payment, and shipping. This trace shows where the request went, what data it contained, and how long it took. The root span is the first span, serving as the parent of all child spans. The API span can't finish until all child spans, especially the shipping span, have finished. The trace represents the complete path of a single request, combining the root span with its child spans.

To summarize, Retrospect is a valuable tool for debugging distributed or monolithic applications. It provides full-stack tracing capabilities, recording both browser events and back-end request data for easy debugging in a single user interface. With Retrospect, developers can quickly identify where the process failed, confirm the user's actions, and find starting points for debugging. Unlike other solutions, Retrospect reduces the reliance on user cooperation and makes debugging more efficient. By capturing both front-end and back-end activities, Retrospect enhances observability in modern distributed applications. It complements metrics and logs and provides essential tracing information to pinpoint potential issues.

Now, I will pass it over to Earl to discuss the observability tools Bob could use for this use case using existing solutions. The inventory service for Ivan includes information about which account the customer was logged into when a specific event occurred. This allows for verification, such as Bob confirming that Ivan Jones used the correct credentials during the checkout process. The system also provides details on which service took the longest to respond. A demo will be presented later to provide a clearer illustration of this functionality. However, there are some questions that the system cannot answer, such as identifying the specific line of code that generated an exception within a service or determining which services, on average, are the slowest. Those types of inquiries would fall under the realm of metrics. The system can only identify which services were slow for a specific series of requests.

To better understand what tracing looks like with machines, let's examine a service diagram for Bob's application. The diagram shows the client connecting to the API service, which connects to the authenticate, inventory, payment, and shipping services. The service trace, which is a waterfall chart, reflects the order in which these services need to be called: first authenticate, followed by inventory, payment, and shipping. It's important to note that, in Ivan's case, the checkout process was stopped at the inventory stage, so the payment and shipping services are not visible in the trace.

By combining the service trace and the diagram, we can see the relationship between tracing and the overall architecture. The service trace is essentially the diagram spread across time. Each span in the trace represents a single operation within the system. In this example, we have spans for authenticate, inventory, payment, and shipping. The arrows indicate where the request went, what data it contained, and how long it took, as indicated by the length of the span.

The root span is the first span in the trace and acts as the parent of all subsequent child spans. The API span cannot finish until all of its child spans, especially the shipping span, have finished. This represents the initial request hitting the architecture and triggering the entire trace. The trace is essentially the combination of the root span and all of its child spans, providing a complete path for a single request. This is the waterfall chart previously mentioned in the span details card.

Now let's discuss the observability tools that Bob should consider using. One option is to go with an enterprise solution offered by SAS vendors such as Sentry, Datadog, or New Relic. These solutions provide not only trace observability but also include logs and metrics, making them feature rich. However, outsourcing observability to a SAS vendor comes with the cost of losing data ownership and incurring recurring fees, which may not be ideal for a small company like Bob's.

Another option for Bob is to use open-source solutions dedicated to recording front-end events and back-end traces. Two prominent open-source solutions are OpenTelemetry for back-end traces and rrweb for front-end events. However, using these solutions would require time to learn and customize them to connect the front-end events with the back-end traces. The effort spent on setting up a DIY solution may be better directed towards the core functionality of Bob's application.

A third option that offers the best of both worlds is Retrospect. This solution provides the ease of deployment offered by enterprise solutions while still offering the benefits of a DIY approach. Retrospect connects related events and traces in the instrumented application out of the box, thanks to its ready-made pipeline. It allows users to retain data ownership as the database instance is deployed in their chosen environment. With a simple yet powerful UI, Retrospect is well-suited for debugging small microservice-based applications.

Now let's dive into how Retrospect works. It is a full-stack tracing tool specifically designed for small microservice-based applications. It consists of six components that work together to collect, connect, and select back-end traces and front-end events. The first group is the collect group, which includes the client and server agents responsible for gathering data from the instrumented application.

The client agent utilizes the open-source web session recording library rrweb with modifications to attach metadata to requests, connecting recorded events to relevant back-end traces. On the server side, the open-source observability framework OpenTelemetry is used for the server agent. Similarly, modifications are made to attach metadata and connect back-end traces to relevant front-end events.

The collect group operates by capturing full DOM snapshots, listening for database mutation events and mouse clicks. These dehydrated events are sent to the Retrospect API after appropriate metadata is attached. The server agent creates spans for HTTP requests, passing them through the microservices involved in the request. The Retrospect middleware attaches the necessary metadata to connect each span to the user session that triggered the request before sending them to the Retrospect API.

Moving on to the connect group, which consists of three components: the API server, a Cassandra database instance, and a server for scheduled data maintenance tasks. The API server receives data from the agents, transforms it for easier querying, and sends it to the Cassandra database. It also serves data to the UI by accessing the database using CQL queries.

Cassandra was chosen as the data store due to its ability to handle the high volume of events generated by the instrumented application. The scheduled task server manages the database by purging data older than a specified number of days, configurable by Bob. Additionally, it addresses a technical challenge with the current implementation of OpenTelemetry by properly propagating metadata to database spans. The unidentified spans are stored in a buffer within Cassandra until the appropriate metadata is attached by the scheduled task server.

The final component of Retrospect is the UI. It provides a user-friendly interface for selecting specific events and traces. The UI was designed with a balance between simplicity and power, taking into consideration feedback from working developers to ensure it meets the requirements of small microservice-based applications. Although it may not have all the advanced features of larger enterprise solutions, the UI's primary focus is to provide the necessary context for investigating errors and debugging microservices.

Taking a step back, let's dive into the design challenges faced while building Retrospect. The first challenge was connecting events with traces. OpenTelemetry and rrweb, while powerful in their respective domains, lacked a native way to integrate and connect front-end events with back-end traces. To overcome this, a custom span processor was developed to attach a session ID to events and traces. This required modifications to both the client and server agents to ensure the necessary metadata was properly propagated.

The second challenge was storing the collected data. Given the high volume of data, especially from user events, a database with emphasis on write speed was required. While read speed was also important, it was not the primary focus. Cassandra was chosen as the data store due to its proven ability to handle large volumes of events. The database stores traces and events in separate tables with the session ID and chapter ID used for organization and traceability.

Lastly, UI design choices were made to strike a balance between simplicity and functionality. The goal was to provide a user-friendly interface that allowed users to select specific events and traces for debugging purposes. The UI was designed through iterations using feedback from developers familiar with microservice-based applications, ensuring it catered to the needs of the target users.

In summary, Retrospect is a versatile full-stack tracing tool for small microservice-based applications. It offers a unique combination of ease of deployment, data ownership, and a user-friendly interface. By utilizing existing open-source solutions and integrating them seamlessly, Retrospect provides developers with the necessary context to identify and debug errors in their applications. Although it may not have all the bells and whistles of larger enterprise solutions, Retrospect focuses on providing detailed tracing capabilities to empower microservices in their communication. In our investigation, we found that sessions, although useful for getting a big picture of the data, were not helpful for debugging purposes. A session may contain multiple traces, but these traces are not immediately informative. If we were to overlay all the user events onto a waterfall chart, it would be difficult to deduce which events led to specific traces. Therefore, we needed a way to organize the data.

To solve this problem, we came up with the concept of chapters. A chapter is a specific set of events that lead up to a single trace. It connects one trace with all the spans within that trace. While a session can be large and have multiple chapters, each chapter focuses on the events leading up to a single trace. Whenever the client sends a new request to the server, it starts a new trace and creates a new chapter ID. This allows for better organization and identification of the data.

With this new organization system in place, we were able to neatly arrange the data. Each event is now connected to its corresponding trace or chapter. This makes it easier to understand how the events relate to each other. When we focus on a single chapter, we can see a clear picture of all the spans within that chapter. The waterfall chart displays the durations and provides a visual representation of the events. The spans related to the chapter are listed on the side, along with all the events that led up to that chapter.

Now that we had a clear plan for organizing and collecting the data, we needed to consider how we would store it. Our data requirements were specific. We had a large volume of data being created by users, so the database needed to prioritize write speed over read speed. However, read speed was still important. Additionally, our data had a structured schema, making it unsuitable for key-value stores like MongoDB. Lastly, scalability was crucial. As Bob's app grew and more web servers were added, the database needed to scale accordingly.

We explored different options and decided to use Cassandra. Cassandra offered several benefits that aligned with our requirements. It provided the fast write speeds commonly associated with NoSQL databases, while also storing data in a tabulated format similar to SQL environments like PostgreSQL. This made it easier to handle the types of queries we needed to run. Cassandra also excelled in scalability. Bob could easily add new nodes to the cluster, doubling the performance with each addition. This flexibility was not as easily achievable with traditional relational databases.

In addition to organizing and storing the data effectively, we also wanted to optimize the user interface to be highly useful to Bob. We wanted to avoid cluttering the application with unnecessary features and have a simple, efficient search capability. Every aspect of the UI needed to serve a purpose. Bob needed to be able to search through the data effortlessly, without needing to learn a new query language. This was crucial considering the complex and nested nature of the data structure. To achieve this, we provided filtering options and exporting capabilities, as well as the ability to view span details and navigate quickly to the specific chapter or session Bob was interested in.

In conclusion, by implementing chapters to organize and connect the data, using Cassandra as our database solution, and optimizing the UI for usability, we were able to create a powerful tool for Bob to effectively debug and analyze the data. Retrospect minimized the data sent over the wire by capturing incremental changes instead of full DOM snapshots and allowing configurable recording settings. Our benchmarking showed minimal impact on performance, making the tool efficient for use. In the future, we plan to add the ability to display the specific line of code causing errors and enhancements to further improve functionality. To use Docker for this task, there are two simple steps. First, you need to download the Docker Compose file and then run "docker-compose up". After completing these steps, you will be able to access our user interface on port 32100. That's all there is to it. This concludes our section, and we are now open to any questions you may have. Great! We already have one question. Let me read it out to you. "Capturing every DOM change for each user seems like a high-volume data transmission exercise. You explained the storage benefits of Cassandra in dealing with high-volume data quite well. How does Retrospect minimize the amount of data sent over the wire while keeping request-response cycles short?"

I can answer that question. We approach this issue from multiple angles. First, it's important to clarify that we don't capture a full DOM snapshot every time there's a change. Instead, we take snapshots at regular intervals and only record individual changes between those snapshots. This significantly reduces the amount of data we need to send. Second, we offer configurable recording settings that allow users to exclude certain types of events. For example, we don't record all mouse movements by default, as they generate a large number of events. Third, the actual size of the data being sent is very small. We conducted tests and found that the longest session recording operation only took about four milliseconds of CPU time, which is negligible. Additionally, using Retrospect had only a minimal impact on the Lighthouse score for Bob's application, reducing it by just one point.

Thank you for the question. Now, let's move on to some comments from the audience. Elizabeth says, "Excellent job, team Retrospect!" Lena adds, "Amazing presentation! If you had more time, what other features would you like to implement?" Lana, go ahead.

I'd like to mention one feature we would like to add in the future. We want to give Bob the option to see the specific line of code that is causing an error. So, we plan to add log functionality to achieve that.

Great! Nicholas says, "Great answer! I'm glad to see some benchmarking." Mark asks, "Why Cassandra over Shure?" I can address that question. While Shure is great for document writing, we chose Cassandra because our data has a structured relationship-like schema. Every event has a chat ID and a session ID, and every span has a parent span ID, chat ID, and session ID. With Cassandra's support for querying based on these key attributes, we can retrieve data more efficiently. Using a document store like Shure would require us to perform more complex and less desirable queries. So, having a relational-like data structure helps us in this regard.

Moving on, Laura asks, "What was the most challenging part of the project?" It's difficult to pinpoint just one, but a major challenge was knowing when to stop. There were always more interesting ways to leverage session replay and advanced techniques to apply backend tracing and instrumentation. However, we had to make the tough decision to limit the scope of the project and not add more features. It was definitely a challenge.

It seems we don't have any more questions at the moment. Is there anyone else who wants to ask something? Ah, it looks like we do have another question. "How did you do the screen recordings?" We used a session recorder that utilizes the Mutation Observer API in the browser. This recorder captures every change to the DOM, creating a buffer of events that record the user's interactions. These events are then periodically sent to our database. When providing the session replay, we gather these events and the corresponding snapshots of the DOM. We pass them to a special replayer object that recreates the DOM by dehydrating and rehydrating it as text, reapplying the HTML or CSS processing. This allows the user interface to be faithfully reproduced.

Another question that has come up is, "What process did you use as you built the project?" I can speak to that. We followed an agile approach and utilized a tool called Pivotal Tracker to stay organized. It helped us divide tasks and assign work to each team member. Sometimes, our work overlapped, but having clear stories in the tracker made it easier to coordinate and prioritize our efforts. Pivotal Tracker helped us keep a list of concrete goals for the project and ensured that we worked efficiently.

Now, I have a question for each of you. What was the most rewarding part of working on this project? I'll start. For me, it was incredibly rewarding to share our work with different people and receive their feedback. It was fascinating to see how others perceived our tool and the insights they shared with us.

I'll go next. The most rewarding part for me was working with the API server and handling all the data. Writing a high-performance API server in Go was personally fulfilling. Additionally, building the Cassandra cluster was a lot of fun.

For me, the highlight was collaborating with other developers to implement the right features and working on the user interface. Seeing immediate results and optimizing the UI's features was very rewarding.

I agree with everyone's answers. For me, the most rewarding aspect was working in a small team to tackle a complex problem. It felt great to have a group of talented individuals coming together to solve what seemed like an impossible challenge. There was a sense of satisfaction in overcoming obstacles together.

We've received some excellent feedback and comments. Thank you all for attending and participating in our presentation. We appreciate your support. Thank you!