Welcome everyone! Thank you for joining us. My name is Jordan Whistler, and I'm here with Janae Janssen, Eugene Matras, and Michael Rego. Today, we will be discussing the development process and purpose of our team's project called ABLE. We will cover why and how we built ABLE and the various features it offers. This presentation will last approximately 50 minutes, and we will have a question and answer session at the end. Please feel free to ask your questions in the chat box or the question and answer dialog box in Zoom.

ABLE is a framework that enables users to deploy A/B tests (also known as split tests) quickly and easily for Jam Stack applications. It allows users to set up and deploy A/B tests to Cloudflare and provides a dashboard powered by an open-source analytics software called Amplitude using AWS infrastructure.

To begin, let's discuss what A/B tests are, why they are important, and how they are traditionally conducted. A/B tests are experiments conducted to compare two or more versions of a website or application. These versions are randomly assigned to different groups of visitors, and their interactions are tracked and analyzed to determine which version performs better. The benefits of A/B testing include optimizing user experience, improving conversion rates, reducing bounce rates, and identifying areas for improvement.

Next, let's talk about Jam Stack architecture, which is an alternative to the traditional client-server model. Jam Stack architecture utilizes content delivery networks (CDNs) to serve static content and APIs for dynamic behavior. This approach offers advantages such as faster load times, easier development workflows, and reduced server management.

However, conducting A/B tests in Jam Stack applications presents unique challenges. In traditional client-server architecture, the logic for splitting users into groups and serving different website variants can reside either on the client or server side. But in Jam Stack applications, there is no dedicated application server, so server-side solutions are not viable. This means we need to explore options that rely on client-side logic.

Client-side A/B testing tools are quick and easy to set up, but they have some drawbacks. When a client makes a request, all the variants are served in the response, increasing the amount of data transferred and potentially impacting load times. Additionally, switching between variants may cause a flicker effect, where the wrong variant briefly appears before the correct one is displayed. These issues can be mitigated but may introduce additional complexity and waiting time for users.

Fortunately, CDNs, which are used in Jam Stack architecture, offer a solution. CDNs allow the hosting of assets in various locations worldwide. This enables a category of solutions called edge computing, where the logic for A/B testing resides in CDN nodes. Edge-side solutions leverage the benefits of both client-side and server-side approaches.

Now let's explore the options for conducting A/B tests in Jam Stack applications. Traditional client-side tools are easy to implement but can have drawbacks like increased data transfer and the flicker effect. Server-side tools offer more control but require an application server, which may not align with the serverless nature of Jam Stack. Edge-side solutions, using CDN nodes, provide a promising alternative.

To summarize, Jam Stack applications offer significant advantages like faster load times and easier development. However, conducting A/B tests in this architecture presents challenges due to the lack of a dedicated application server. Client-side tools have limitations, and server-side solutions may not align with the serverless nature of Jam Stack. Edge-side solutions leverage CDNs to overcome these challenges and provide a promising approach for A/B testing in Jam Stack applications.

In our project, ABLE, we developed a framework that utilizes the edge-side approach. This enables users to deploy A/B tests for Jam Stack applications while maintaining the benefits of CDNs and the serverless nature of Jam Stack. ABLE includes features like deploying tests to Cloudflare, setting up a dashboard powered by Amplitude, and utilizing AWS infrastructure.

During the development of ABLE, we encountered various challenges, such as integrating with Cloudflare and ensuring compatibility with different Jam Stack frameworks. We addressed these challenges by leveraging our team's expertise and collaborating effectively.

While ABLE offers a range of features, there are additional ones that could be added in the future. For example, options for custom metrics tracking, integration with other analytics tools, and further optimization for specific Jam Stack frameworks.

In conclusion, A/B testing is essential for optimizing websites and applications. Jam Stack architecture offers many advantages, but conducting A/B tests in this context requires special considerations. ABLE, our framework utilizing edge-side solutions, addresses these challenges, providing users with a powerful tool for A/B testing in Jam Stack applications. We hope that ABLE will prove useful for optimizing user experiences, improving conversion rates, and driving business success.

Thank you for joining us today. We are now open for questions, so please feel free to ask anything you'd like to know about ABLE or A/B testing in Jam Stack applications. The use case under discussion is the context of JamStack applications and the unique challenges they pose. In a traditional A/B test, the logic that splits users into groups and determines which website variant to serve is typically handled on either the client side or the server side. However, in a JamStack application, there is no dedicated application server, ruling out solutions that rely on server-side logic.

This suggests that we should explore options that rely on client-side logic. While it is quick and easy to set up A/B tests when the logic is on the client side, there is a major downside. When a client makes a request, all the variants are served in the response, leading to an increase in the volume of data being sent and potentially adding to the page's load time. Additionally, a JavaScript routine must be called to hide every variant except the one the user is supposed to see. This can result in the incorrect variant briefly flashing on the screen before being replaced by the correct one, which is known as the flicker effect. This can be corrected with a script that blocks all the variants from being shown until it knows which one to render, but this may result in the user looking at a blank screen for a few seconds before the chosen variant appears.

All of these factors, such as the increased data transfer and the expensive logic involved in switching between variants, could negate the load time reduction benefits offered by using the JamStack architecture. It's important to note that even a few seconds of waiting for a page to load can significantly impact user experience, as studies have shown that the chance of a bounce increases by 32% when page load time goes from one to three seconds, and by 90% when it goes from one to five seconds.

Given the disadvantages of client-side A/B testing for JamStack applications, we need to explore alternative methods. In the earlier discussion on JamStack architecture, we mentioned that all a website's assets are hosted on a CDN (Content Delivery Network), which contributes to the speed of JamStack applications. Most CDNs also offer a feature known as edge computing, which enables the execution of a program every time a user's HTTP request hits the CDN's local server. This opens up a whole new category of solutions for A/B testing.

Using this capability of edge computing, we can serve different versions of our content on a per-request basis, which is exactly what we need for A/B testing. So, let's explore some ways to leverage edge computing for building an A/B testing solution. Currently, a few companies offer edge-side solutions for A/B testing, although there are not as many options compared to client-side or server-side solutions. We have identified two broad categories of edge-side solutions: HTML rewrite solutions and branch-based solutions.

HTML rewrite solutions involve intercepting the payload of the client's response and altering the body directly before forwarding it to the client. In theory, this could be used for A/B testing by keeping one version of the website in the CDN and selectively rewriting variations into it as it's being served to individual users. Vendors like Outsmartly offer solutions like this, where you don't have to maintain multiple versions of your website. However, this approach presents challenges, as you have to build your website from React components and register them with the service to render differently for different variants. This may be viable for testing small changes but not for testing entirely different versions of the same website. It's also complex to add this technology to a website, especially if it wasn't originally developed with this process in mind.

Due to the difficulties associated with HTML rewrite solutions, let's explore the second category: branch-based solutions. Many CDN providers offer branch-based deployment specifically for JamStack applications, which can also be used for A/B testing. Here's how it works: you keep the code for your app in a Git repository hosted on platforms like GitHub or Bitbucket. When you're ready to deploy, the CDN's JamStack hosting platform retrieves your code from the repository and runs a build process, generating the static assets of your app. If your repository has multiple branches, it will generate a bundle of static assets for each branch. The CDN then replicates these assets across its global network and exposes a URL for each bundle. Finally, every time someone navigates to your domain, the CDN's edge compute platform can be used to choose a variant and serve it back to the client. This is known as branch-based A/B testing and appears to be a promising approach for JamStack testing solutions.

Let's discuss some of the available options for branch-based A/B testing in the marketplace. Netlify, a CDN, offers an out-of-the-box split testing feature at the edge. If you host your JamStack application on Netlify, you can take advantage of this feature to split traffic among apps built on multiple branches of your Git repository. While it's quick and easy to set up, it doesn't offer granular control over traffic segmentation. You can only direct a percentage of your traffic to the test version of your site, without the ability to target specific user groups based on criteria like device or browser. Additionally, Netlify lacks a built-in analytics solution.

Layer 0 is another CDN that offers branch-based A/B testing at the edge. Similar to Netlify, it allows you to split traffic based on criteria like cookie header, browser, or device. However, it also lacks an built-in analytics solution. 

As an alternative to these out-of-the-box options, a DIY (do-it-yourself) solution might be worth considering. Hosting your JamStack application on a CDN that offers both branch-based deployment and a robust edge compute platform with complete control over the HTTP request-response cycle, such as Cloudflare or AWS, would provide the level of control you need for A/B testing. However, you would still have to set up your own analytics solution.

Setting up a DIY A/B testing solution involves several steps. First, you manually create variants on different Git branches and deploy them to your chosen cloud provider, resulting in distinct URLs for each version. When a client sends an HTTP request to the URL, your serverless function intercepts and inspects the request to determine if the user should be included in the test. If they should, the serverless function randomly assigns the user to a group.

At this point, a routine is called the first time a person visits a web page undergoing a test, which selects the appropriate variant and serves it to the client. To ensure the test remains invisible to the user, the serverless function must fetch and deliver the correct response using only the information from the HTTP request, avoiding a redirect response that displays a different URL in the browser's address bar. Additionally, a cookie must be set to track which group the user belongs to for subsequent visits.

Monitoring user behavior and tracking metrics for each variant is essential. Most analytics tools require a tracking script to be inserted into each page that needs to be tracked, potentially requiring editing every HTML page to be served. It's crucial to track metrics correctly for each variant, which may necessitate setting up separate tracking scripts to send data to the same location. Choosing the best analytics tool and setting up the tracking process can be challenging. Hosting your own analytics using an open-source tool would require provisioning and managing additional infrastructure.

Finally, once you have identified the variant that meets your performance goals, you will need to roll back the test, deploy the winning variant, and invalidate all the caches containing the test variants. This entire process is doable but time-consuming, and updating tests in progress becomes difficult. For iterative testing to find the optimal version of your application, you would need to repeat this process multiple times.

Now, let's introduce Able, which aims to strike a balance between the powerful but challenging DIY solution and the easier but limited out-of-the-box solutions offered by CDN companies like Netlify and Layer 0. Able automates the setup of branch-based A/B tests on Cloudflare. It handles many of the tasks you would otherwise need to figure out if you were building a DIY solution. Able provides fine-grained control over the A/B testing process and automates the provisioning of infrastructure for your customized instance of Amplitude, a powerful open-source analytics tool. However, Able currently lacks a user-friendly configuration dashboard, so the setup process requires manually editing a JSON file to provide the configuration data. Nonetheless, developers might find this trade-off worthwhile due to the benefits it offers.

Having discussed the role and purpose of Able, let's delve deeper into its functionality and how our team built it. We will first demonstrate Able by walking through a potential use case and then explore its architecture and how it operates under the hood.

The use case revolves around CoffeeBean.com, a fictional online retail store specializing in selling coffee beans. The store has a small dev team and has already deployed a JamStack app on Cloudflare Pages. They decide to run A/B tests to optimize their site. To begin, they create git branches to mock up potential new front ends for their webpage. As they are using Cloudflare Pages, the new branches are automatically built and deployed as previews, each with a unique URL.

To implement the A/B tests, the team installs Able and uses its CLI to configure their Cloudflare account. Able generates a JSON file containing all the necessary details for setting up the A/B test. Each variant in the test requires a name and a URL. The file can be updated to include additional variants.

Metrics tracking is essential for monitoring user behavior. Able provides the capability to automatically inject the tracking script into the head section of each page's HTML. The script is customized for each variant. By doing this at the edge, Able ensures the tracking script is added seamlessly without requiring manual edits to the HTML.

When a variant is requested, Able intercepts the HTTP request and retrieves the appropriate variant based on the request data. This ensures users are served the correct variant without any visible changes in the URL. Able also supports sticky sessions, meaning once a user visits the site and is assigned to a variant, they will consistently see that variant during subsequent visits.

To track user behavior and collect metrics, Able integrates with Amplitude, an open-source analytics tool. It automates the provisioning of infrastructure for running an instance of Amplitude, providing developers with complete control over their analytics data.

Although Able does not currently offer a configuration dashboard, it simplifies the process of setting up branch-based A/B tests by automating many tasks. It allows for fine-grained control, facilitates metrics tracking, and integrates with Amplitude for comprehensive analytics. Despite the requirement to manually edit a JSON file for configuration, developers may find the benefits outweigh this trade-off.

In conclusion, the discussed transcript addresses the challenges of A/B testing in the context of JamStack applications and explores various solutions, such as client-side logic, edge computing, HTML rewrite solutions, and branch-based solutions. It highlights the advantages and disadvantages of each approach and introduces Able as a tool that provides a middle ground between DIY solutions and out-of-the-box options. Able automates the setup of branch-based A/B tests on Cloudflare and integrates with Amplitude for analytics, offering fine-grained control over the testing process. This thorough overview provides insights into the implementation and benefits of A/B testing in JamStack applications. The purpose of iteratively testing an application is to find the most optimal version. However, this process requires going through the steps repeatedly. To address this, the team has developed a tool called Able that aims to automate the process of setting up branch-based A/B tests on Cloudflare. Able aims to strike a balance between the difficulty of a DIY solution and the limitations of out-of-the-box solutions like Netlify and Layer Zero. It offers fine-grained control over the A/B testing process and automates the infrastructure provisioning for Ami, an open-source analytics tool. Although Able currently lacks a user-friendly configuration dashboard, developers may still find value in its offerings.

Now, let's take a closer look at Able and how it works. We'll use a fictional online coffee retail store called CoffeeBean.com as an example. This store already has a small development team and a JAMstack app deployed on Cloudflare Pages. The team wants to run some A/B tests to optimize their site. To begin, they create git branches to mock up potential new front ends for their web page. Since they are using Cloudflare Pages, these branches will automatically be built and deployed as previews with their own unique URLs.

To put these alternate pages into action and see how users respond, CoffeeBean.com uses Able. The team manager installs Able and sets up all the necessary information about their Cloudflare account using Able's CLI. The configuration data for the A/B test, such as variant names and URLs, is stored in a JSON file that Able auto-generates. Each variant can also include a script for tracking metrics. Able will inject this script into the HTML head of the page at the edge before serving it, allowing for user behavior tracking.

If the team doesn't have an analytics tool, they can choose to use Ami, a privacy-centric, self-hosted open-source analytics tool. Able handles the provisioning of the underlying infrastructure needed to run Ami on AWS. Once Ami is set up and the script tags are added to the Able config file, there are additional configuration options available. For example, the team can choose to gradually roll out test variants to a smaller percentage of users or selectively segment traffic based on device, browser headers, or cookies.

With the configuration details in place, the team can run the A/B test by deploying Able. Within seconds, the test will be up and running, and traffic will be split to the different variants. Able channels information about user behavior to Ami in real-time, allowing the team to view and analyze the data as the test progresses. If adjustments need to be made, such as increasing the traffic directed to a particular variant or fine-tuning a winning variant, the team can simply edit the config file and run Able update. Once the test is complete, Able destroy test will roll back the test, and Able destroy Ami will remove all AWS infrastructure.

Now, let's delve into the architecture of Able. It consists of two main parts: the Cloudflare component responsible for controlling the A/B tests and the Ami and AWS infrastructure used for tracking metrics. The Cloudflare component utilizes a serverless function called a Cloudflare Worker to run the A/B test. The configuration data is stored in a Cloudflare Key-Value store (KV). When a user visits the site, the request is intercepted by the Cloudflare Worker and the configuration data is fetched from the KV. The worker determines if the user should participate in the A/B test and serves the appropriate variant with the tracking script.

The Ami architecture is responsible for processing and storing analytics data. Ami runs on AWS, utilizing an Elastic Container Service (ECS) to manage a Docker container that processes the data and an RDS database to store it. An application load balancer sits between the client and the container service to evenly distribute traffic. To automate the creation of this infrastructure, AWS's Cloud Development Kit (CDK) is used.

To overcome challenges during the development of Able, the team implemented solutions. One challenge was adding separate analytic script tags to each variant of the site after they were already deployed to the CDN. Cloudflare's HTMLRewriter was used to insert the script tags at the edge. This allowed for the modification of the HTML before it was sent to the client, ensuring that each variant had its own separate script tag.

Another challenge was enabling HTTPS for the Ami application, which initially used the HTTP protocol. To secure the traffic and ensure compatibility with browsers, a Transport Layer Security (TLS) certificate was required. The team automated this process by using Cloudflare's HTMLRewriter and configuring the AWS infrastructure to handle HTTPS traffic. This ensured that the Ami application could receive secure data from users.

In summary, Able fills a crucial role in the world of application optimization and A/B testing. It offers a middle ground between complex DIY solutions and limited out-of-the-box options. By automating the setup of A/B tests and providing fine-grained control and analytical capabilities, Able streamlines the process for developers. The architecture of Able, incorporating Cloudflare for A/B testing and AWS for analytics, solves the challenges associated with tracking user behavior and ensuring secure interactions. The result is an efficient and effective tool for optimizing applications and delivering a superior user experience. In this video, we will talk about the challenges we overcame while building our coding Capstone project called "Able." We encountered several challenges during the development process that required creative solutions.

One challenge we faced was adding separate analytic script tags to each variant of a website. Most client-side analytic tools use a Javascript script tag to track user behavior. This script tag acts as a callback function that sends user session information back to the analytics software when a user visits a page. To differentiate the data from one variant to another during an A/B test, developers need to make sure each variant has its own separate script tag in the html. This can be a tedious task, but some A/B testing tools, like the one provided by Netlify, offer a solution by inserting the analytics script tags automatically.

With Able, we wanted to provide a similar service of adding the analytic script tags automatically. However, because our application follows the JAMstack architecture, where the HTML is pre-built and cached at the CDN, inserting separate script tags to each variant after the HTML is built and cached posed a challenge. To address this, we decided to use Cloudflare's HTML rewriter, a JavaScript API accessible by Cloudflare workers, to parse and modify the HTML. With this solution, when our worker fetches the cached HTML from the CDN, we can use Cloudflare's HTML rewriter to insert the correct analytics script tag at the edge before sending it to the client.

Another challenge we faced was automating the process of enabling HTTPS for our Able application. Initially, our application used the HTTP protocol, which meant that the traffic served over the internet was not secure. Moreover, most browsers would block the analytic script tags without HTTPS. To enable HTTPS, we needed a Transport Layer Security (TLS) certificate, which could be provided by AWS Certificate Manager. However, AWS Certificate Manager couldn't issue a certificate to the domain name provided by AWS. This required us to provide a custom domain for Able. We ended up issuing a certificate to the same domain name used for the A/B test on Cloudflare, with a subdomain of able. For example, if the domain for the A/B test was coffeebean.com, the certificate would be issued for ablei.coffeebean.com. Automating this process required several steps: issuing the certificate, validating it through a DNS record, waiting for validation, attaching the certificate to the load balancer, and creating another DNS record for the subdomain.

To automate the certificate process, we used the Amazon Software Development Kit (SDK) to issue the certificate for the ablei subdomain. We then made a request to Cloudflare's API to create a DNS record using the information provided by the certificate issuance response. After that, we waited for the certificate to be successfully validated by polling AWS. Once the certificate was validated, we attached it to the application load balancer using a CloudFormation template, which also provisioned the rest of our AWS infrastructure. Finally, we created another DNS record to point the subdomain ablei to the application load balancer.

Now that we have discussed some of the challenges we encountered during the development of Able, let's talk about some areas where Able can be further improved in the future. One area we would like to focus on is enabling users to deploy multiple A/B tests. Currently, Able only supports one A/B test at a time, but we aim to provide the functionality to run multiple tests simultaneously. This would enhance the flexibility and usability of the tool.

Additionally, we plan to provide a graphical user interface (GUI) for configuring tests instead of relying on users to fill out a JSON template. This would improve the user experience and reduce the chances of human error during test configuration.

Lastly, we aim to expand Able's compatibility to other JAMstack hosting platforms, such as AWS Amplify. By offering compatibility with different hosting platforms, we can reach a wider audience and provide more options for users.

Thank you for joining us in this presentation about the development of Able. We appreciate your presence, and we will be available to answer any questions you may have. John Rodney: This project is highly relevant to the work my company, Gatsby, does. I'm curious to know what each of you found most interesting about the project. Michael, would you like to start?

Michael: Believe it or not, the most interesting part for me wasn't anything technical. It was the experience of collaborating on a team like this. It was challenging to learn how to work together effectively. I had never done anything like this before, and it was a valuable learning experience. Seeing how a team's dynamic naturally evolves and how individuals naturally fall into different roles was psychologically intriguing, even more so than the technical aspects of the challenges we tackled.

John Rodney: That's fascinating. Learning how to collaborate and make decisions was also a significant part of the process. Initially, our team was a bit reserved, afraid to step on each other's toes. However, we eventually learned to develop opinions and take action. Additionally, exploring the world of Jamstack and playing around with Cloudflare was incredibly enjoyable. Overall, it was an interesting couple of months.

John Rodney: Would anyone else like to add their thoughts?

Another team member: I'll go first. I agree with what you all said. Working with this group of people has been a fantastic experience, and I want to express my gratitude to everyone. In terms of the material, I've been fascinated by the evolution of applications throughout this project. It's almost like a metaphor for the universe. As applications have been shattered apart and evolved from a singular entity with a server, a database, and a web server, my mind draws parallels to how the universe has expanded. Initially, static assets and pictures were pushed to the edge, and now we see compute being pushed to the edge. Storage will likely follow suit. This transition of technology applies not only to web pages and websites but also to other interconnected things like IoT, manufacturing, drones, and cars. This project has shed light on these connections, and I've learned a great deal from it.

Another team member: For me, the most interesting part was learning about edge computing, specifically with Cloudflare. I found their blogs on how workers function to be very informative. After playing around with Cloudflare, I started receiving ads about their new developments, which only deepened my interest in the technology space. It's been an exciting journey.

John Rodney: Great insights from everyone. Those were all the questions we had, and we're running out of time. I want to thank everyone for attending and extend a special thanks to our mentor, Daniel, for his guidance. We also appreciate the additional assistance provided by Chris and the teaching assistants. This has been a truly enjoyable experience.