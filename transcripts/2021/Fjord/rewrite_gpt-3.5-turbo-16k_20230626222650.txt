Welcome everyone! Thank you for joining us today. I am Lena, and I will guide you through the first part of this presentation. With the power of remote collaboration, myself, Austin, Sophie, and Vaheed have developed Fjord. Fjord is an open-source framework that offers a real-time API proxy for Kafka. It allows clients to stream data in real-time from any Kafka cluster through the Fjord API proxy. Using Fjord, developers can deploy a scalable infrastructure on Amazon Web Services to handle real-time streaming.

Let's begin by breaking down the definition of Fjord into its main components. The key components we will focus on are the real-time API proxy and Kafka. To understand what Fjord does, we need to first understand real-time data and what Kafka is. By exploring these components, we can then address the need for an API proxy and finally discuss the solution that Fjord offers.

Real-time data is a concept that surrounds us in our daily lives. For example, let's consider a food delivery app called Super Eats. This app provides constant updates to customers about the status of their meal orders, such as when the order is ready and the estimated time of arrival. Drivers also share real-time updates of their GPS location, allowing customers to track their progress. In this scenario, the delivery updates need to reach drivers in real-time, as the information becomes less relevant as time passes. According to Ripley, a prominent company in the real-time space, real-time means the ability to react to anything as it occurs, before it loses its importance.

Now, let's move on to Kafka. Kafka is an event streaming platform that saves events as records in a log and allows those records to be read in order from any starting point by any number of consumers. It efficiently sends large volumes of records quickly between machines and provides secure access to internal consumers and producers. Kafka has become the go-to standard for event streaming, with over 80% of Fortune 100 companies using it. For example, LinkedIn processes over 7 trillion events per day using Kafka, while Netflix deals with over 700 billion events per day.

The challenge arises when we want to make this real-time stream of data available to web-facing clients. While Kafka communicates using a binary transport protocol, web clients typically receive real-time data using techniques such as long polling, websockets, or server-sent events (SSE). This requires an intermediary to translate the traffic between Kafka's binary protocol and the HTTP protocol used by web clients. This is where the API proxy comes in.

An API proxy is a server that sits between an application and the backend server, intercepting network requests and either answering them or forwarding them as needed. It allows developers to build web applications using a set of API endpoints to communicate indirectly with the backend server. In the case of Fjord, the API proxy acts as a bridge between Kafka and web applications, pulling data from Kafka and pushing it to connected devices over HTTP. Additionally, the API proxy offers several other key features, such as handling the volume and velocity of Kafka messages, dynamically scaling the server to handle user traffic, providing additional security, customizing and grouping Kafka topics, and offloading resource-intensive tasks of real-time streaming.

While there are existing enterprise solutions for a real-time API proxy for Kafka, such as Ably and PubNub, which offer feature-rich and scalable solutions, they come with a high price and lock users into their ecosystem. Alternatively, you can roll your own solution by combining various architectural components, such as a Kafka sync connector, an API proxy server, a pub-sub mechanism for message distribution, and a load balancer for routing traffic. There are open-source solutions available for each of these components, but building a DIY solution requires expertise and can be resource and time-intensive.

In conclusion, Fjord is an open-source framework that provides a real-time API proxy for Kafka. It allows clients to stream data in real-time from any Kafka cluster. The API proxy acts as a bridge between Kafka and web applications, translating traffic between Kafka's binary protocol and the HTTP protocol used by web clients. With Fjord, developers can deploy a scalable infrastructure on Amazon Web Services to handle real-time streaming. While there are existing enterprise solutions, users also have the option to build their own solution, although it requires expertise and resources. An API proxy offers various beneficial features, including the ability to expose internal Kafka data to a large number of users over the internet. This necessitates a robust system that can handle the volume and velocity of Kafka messages. With an API proxy, thousands of end users can receive Kafka streams over HTTP. Using a separate proxy also enables dynamic scaling of the server to meet external user traffic. Moreover, a proxy acts as a gatekeeper, providing an additional security layer for the Kafka cluster. Additionally, an API proxy allows you to customize and group Kafka topics into different configurations of API endpoints. By offloading resource-intensive tasks and real-time streaming to an API proxy, you free up your focus for more value-creating activities central to your business.

Three primary enterprise solutions exist for implementing a real-time API proxy for Kafka. The first is ably, a leader in real-time streaming. Customers who already use ably's real-time API can integrate ably's Kafka connector to enable end users to stream from their Kafka cluster. The second solution is pubnub, a well-established company in real-time. Pubnub offers a bridge for Kafka, although it is still in beta testing. The third solution is migratory data, which provides a specialized product that natively integrates with Kafka. Their expertise lies in scaling websocket connections. While these solutions offer feature-rich and highly scalable options, the main downside is that they come at a high price and lock you into their ecosystem.

Alternatively, you can choose to build your own API proxy solutions. However, this approach requires multiple architectural components. You would need a Kafka sync connector and an API proxy server. To handle incoming traffic, you would also need a pub-sub mechanism to send messages to server instances and a load balancer to route traffic. Fortunately, there are open-source solutions available for each of these components. For example, red hat stream c is an open-source solution for implementing the sync connector, and push pin and centrifugal are open-source solutions for the proxy server. However, implementing your own solution is a complex and resource-intensive task, requiring expertise in the Java ecosystem. Additionally, there are associated costs with building and maintaining such a solution.

This is where fjord comes in. Fjord is a solution that falls between the DIY approach and paid services. It is open-source, scalable, and easy to deploy. With fjord, you have complete ownership of the data coming in and out. While fjord offers a limited feature set, it provides a simple developer experience, allowing you to easily add any additional features you may need. As far as we know, fjord is the only open-source platform that includes all the necessary infrastructural components out of the box to deploy an API proxy for Kafka.

Now, let's delve into fjord and its architectural components. Fjord was designed with five main goals in mind. First, it handles the real-time infrastructure necessary for internet-facing clients to stream records from Kafka topics. This relieves the burden on your infrastructure, as the fjord proxy handles the additional load. Second, fjord is designed to automatically scale up and down based on the current load. This means you only pay for the resources you need when you need them. Third, fjord ensures security by allowing you to limit access to your data stream using JSON Web Tokens (JWTs). You have control over who can access your data. Fourth, fjord is fully open-source and customizable, allowing you to explore its repositories and deploy fjord with your custom setup, including multiple Kafka clusters, consumer groups, and API topics. Finally, fjord is easy to deploy, offering a CLI that allows you to deploy fjord on AWS with just a few simple terminal commands. Within 15 to 20 minutes, you can be up and running on AWS.

Fjord is suitable for various use cases. Let's revisit the example of SuperEats, where customers can order from their favorite restaurants and drivers need to be notified in real-time when a customer places an order. When a customer visits supereats.com to place an order, the order is received by SuperEats, which uses Kafka as the backbone of its event-driven architecture (EDA). Thanks to the connection with fjord, new updates on the order are fanned out to the delivery drivers and the customer who made the order in real-time. From the drivers' and customers' perspectives, they are simply connected to the SuperEats domain or app. This enables you to deliver customized content that seamlessly integrates with your website, without having to handle the additional streaming load on your servers.

To provide this functionality, clients connect to fjord through a load balancer. This load balancer not only directs clients to the most available server but also facilitates automatic scaling of servers based on traffic. The server then pushes new records to connected subscribed clients using Server-Sent Events (SSE), while consumer groups pull information from the Kafka cluster. A NAT gateway enables Kafka traffic to reach the consumers. Additionally, a Redis server serves as middleware, decoupling the interaction between the consumer groups and the servers. This publish-subscribe mechanism ensures that servers can scale up and down while receiving streaming records from all consumer groups.

Let's explore some of the technical challenges we faced while building fjord. One of the most significant decisions we made was whether to use Server-Sent Events (SSE) or WebSockets for real-time streaming. Long polling, which requires a new request-response cycle for each transmitted record, is inefficient for real-time streaming. After considering the options, we narrowed it down to SSE and WebSockets. Both work on most modern browsers, with SSE having some limitations on Internet Explorer. Ultimately, we chose SSE as our streaming technology.

Another challenge we encountered was scaling the Fjord server and consumer. Initially, running a few instances of the server and consumer was manageable. However, as the application and user base grew, we needed a way to easily scale the number of instances. To address this, we Dockerized the server and consumer applications. Docker allows us to run multiple lightweight containers, making the server and consumer horizontally scalable. Additionally, Docker containers are portable and can be easily deployed on the cloud. To manage the large number of containers, we utilized AWS Elastic Container Service (ECS) with Fargate, which simplifies container management and server provisioning.

Maintaining reliable connectivity between the consumer groups and servers posed its own set of challenges. To solve this, we utilized a NAT gateway, allowing Kafka traffic to reach the consumers even when running on a private AWS subnet. Redis served as the middleware, facilitating communication between the consumer groups and servers through a publish-subscribe mechanism. This architecture ensures that all servers receive streaming records from all consumer groups, even as the servers scale up and down.

Finally, we encountered the challenge of deploying fjord on AWS. To make the deployment process seamless, we developed a CLI tool for fjord. The CLI guides users through the deployment steps, generating a JSON file that can be customized to specify details like the Kafka broker's IP address and security information. After customization, the fjord deploy command provisions and deploys all the necessary AWS resources, resulting in a fully operational fjord deployment in just 15 to 20 minutes.

In conclusion, fjord provides a scalable and customizable solution for implementing an API proxy for Kafka. It offers easy deployment, automatic scaling, security features, and full ownership of data. By choosing fjord, you can leverage its open-source nature and avoid the high costs of enterprise solutions or the complexity of building your own solution from scratch. Whether you are SuperEats or another business seeking real-time streaming capabilities, fjord simplifies and enhances your Kafka infrastructure. To deploy your application on AWS using Fjord, you need to gather all the necessary information for your AWS deployment. Then, run the "keyword setup" command, which will generate a JSON file that you can use to customize your Fjord deployment. This command also generates a private key that allows you to use JSON Web Tokens (JWT) for authentication.

Next, you customize the Fjord settings JSON file by adding your Kafka broker's IP address and the security information for your Kafka cluster. You can also specify the desired API topics that you wish to expose. Once you have made all the necessary customizations, you can run the "fjord deploy" command. This will initiate the AWS provisioning and deployment process, which usually takes around 15 to 20 minutes. Once it's completed, all the necessary resources will be deployed.

Let's take a look at the command-line interface (CLI) when running "fjord deploy". The CLI will output the public IP addresses of both your load balancer and your NAT gateways. If your Kafka cluster is protected by a firewall, you will need to whitelist the NAT gateway IP addresses to allow Fjord conser to access the cluster.

Deploying Fjord is simple and only requires a few commands. Here is a screenshot of the CLI output, which shows the NAT gateway IP addresses and the load balancer URL.

Developers will find Fjord easy to use because they only need to include concise client-side JavaScript on the interface used by end users. Establishing a connection stream is as simple as a few lines of code, thanks to the EventSource Web API. In the example shown, you can see that the load balancer endpoint is used to establish a new Server Sent Events (SSE) connection. The developer can add their desired logic for handling each incoming Kafka record in the specified location.

Now let's talk about some of the interesting technical challenges we faced while building Fjord. There were four main challenges that stood out to us.

The first challenge was deciding whether to use Server Sent Events (SSE) or WebSockets for real-time streaming. SSE and WebSockets are both server push options that can handle thousands of concurrent connections per server. While WebSockets is more popular and enables bi-directional communication, SSE's simplicity, efficiency, and unidirectional communication made it the better choice for our use case. Moreover, SSE's automatic reconnection feature proved to be useful for handling mobile devices' reconnection requirements.

The second challenge was decoupling the conser and the server. Initially, we had the conser publishing each record directly to the server via an HTTP POST request. However, this approach created issues when we wanted to scale the server instances. To solve this, we introduced middleware between the conser and the server. We explored two options: AWS Simple Notification Service (SNS) and Redis. While SNS required additional configuration steps and lacked sufficient examples for our purpose, Redis proved to be fast, lightweight, and easy to set up, making it the preferred choice.

The third challenge we encountered had to do with cross-origin resource sharing (CORS) when deploying our code on AWS. Browsers do not allow cross-origin requests by default, so we needed to enable CORS using special request headers. However, we faced an issue where the load balancer would timeout HTTP responses and forget about the CORS headers after a period of inactivity. To overcome this, we implemented a heartbeat or pulse for SSE. A blank message is sent at regular intervals to check if any clients have closed the connection and to signal to the load balancer that the connection is still ongoing.

The fourth challenge was load testing. Finding an open-source library that worked well with SSE was initially difficult. Eventually, we came across Gatling, which provided good documentation for testing SSE. Through load testing, we were able to push the server to handle a large number of concurrent connections. With proper auto-scaling rules, we confirmed that AWS could launch more servers as necessary to handle the increased load.

By overcoming these technical challenges, we were able to build Fjord as a scalable and efficient real-time streaming platform. Thank you for joining our presentation, and if you have any questions, feel free to ask them in the Zoom chat. In the AWS console, on the Fjord server task, you can see that the maximum capacity we achieved on one server was about 8,500 client connections per server. We achieved this using the lowest CPU and RAM settings possible for a server instance on AWS Far Gate. Thanks to Fjord's ability to scale up new servers as necessary to handle more load, we were able to use a server with the lowest power consumption possible, making it cost-efficient while still being able to serve any number of clients. This concludes our presentation. Thank you for your attention. Now, I will open the floor to questions. Please send them through the Zoom chat.

Question: How would this work in a mobile app?
Answer: Great question, Julio. In a mobile app, you would use Java or Kotlin for Android and Swift for iOS. There are libraries available for server-sent events and WebSocket communication on mobile platforms.

Question: What was the most challenging part of the project?
Answer: Thank you for the question. Personally, I found that moving from prototyping on the AWS console to using the AWS CDK was the most challenging part due to the unclear documentation.

Answer 2: For me, the most challenging part was handling the influx of information and navigating the vast world of AWS. Learning about Kafka and its configuration details was also challenging but interesting.

Answer 3: I agree with the previous answers. Deploying the code on AWS was the biggest challenge for me, as it was a whole different ball game compared to running things locally.

Question: What was the most interesting part of this project for you?
Answer: Personally, I found it interesting to learn and connect all the different pieces of the project. Using server-sent events and load testing with Gatling were also fun experiences.

Answer 2: I enjoyed learning about and using server-sent events and websockets. Additionally, exploring Kafka and load testing with Gatling were also fascinating.

Answer 3: I found all the different components of the project, like Docker and AWS, to be very interesting. Seeing everything come together and solving the problem was also satisfying.

Answer 4: One of the most interesting aspects for me was learning about Kafka and understanding how it fits into the project's ecosystem. 

Question: How did you divide efforts and collaborate on the project using Git?
Answer: During the project, we had daily team meetings, regular check-ins, and shared daily summaries. We also did pair programming and solo programming to ensure everyone was up to date. Collaboration was essential for staying on track.

Answer 2: We used various strategies throughout the project, but daily check-ins, open communication, and clearly defining tasks and goals were important for collaboration. 

Question: What is the setup like for Supereats with Kafka? Do they need to set up their own Kafka cluster?
Answer: Supereats assumes that users already have a Kafka cluster set up. Fjord is designed to connect to a business's existing Kafka cluster and expose Kafka topics to end-users. 

Question: What are your plans for future improvements to the project?
Answer: One plan for future improvements is to implement advanced security features, such as encrypted communication and network access control. Additionally, we aim to enable bidirectional communication, allowing clients to inject events into their Kafka streams based on user actions.

Question: Have you worked with each other before? How did you handle communication challenges?
Answer: We started working together during the preparation phase of the Capstone project. Although we hadn't worked together before, we learned about each other's workflows and openly communicated any issues that arose. Honesty and open discussions helped us overcome communication challenges.

Answer 2: It was a new experience working together, but we learned to understand each other's perspectives and needs. Open communication, asking questions, and defining the purpose of our work were key in overcoming communication challenges.

Question: How did you approach learning and working with a wide range of AWS services?
Answer: We followed the Launch School process of breaking down problems into smaller components and incrementally working through them. Starting locally and gradually scaling up with simple architecture helped us navigate the wide range of AWS services.

Question: How did your initial vision of the project compare to the final product? Did you have to pivot along the way?
Answer: Our initial vision aligned closely with the final product. We added a few additional features to make the system scalable, but overall, we stayed true to our initial vision. There were challenges and decisions along the way, but we stayed focused on our goal.

Answer 2: Our initial vision remained intact throughout the project, with some additional components required for scalability. While there were challenges and decisions to be made, we stayed true to our vision.

Answer 3: Our initial vision aligned well with the final product, and we only needed to tweak a few aspects to ensure scalability. We encountered obstacles along the way but remained focused on our goal.

In conclusion, working on this Capstone project was an amazing experience. We learned a lot about various technologies, such as Kafka, Docker, and AWS, and enjoyed seeing all the pieces come together. We are grateful for the Launch School community, instructors, mentors, and teammates who made this journey enjoyable and productive. Congratulations to all the other Capstone teams who produced incredible work. Thank you to everyone involved in our project and the Launch School community for their support and guidance throughout this process. During this Capstone project, we had the opportunity to explore Kafka, Docker, AWS, and delve into the world of infrastructure. Moving away from just coding and looking at these larger systems was an incredibly enriching experience. Congratulations to all the other Capstone teams for their fantastic work. I want to express my gratitude to the entire cohort for the dedication and effort they put into their projects. Your presentations were impressive, and I'm genuinely amazed by the work you all have done.

I would also like to extend my thanks to the Launch School community and everyone I've had the pleasure of connecting with throughout this journey. Your support and camaraderie have made this experience even more enjoyable. Special thanks go to Chris Sturgeon, Nick, our mentor, as well as Austin, Vahe, and Sophie. Working with all of you has been awesome, and I appreciate the opportunity.

I want to reiterate my sentiments from earlier. This experience has been extraordinary. It's essential to trust in the process, especially when doubts arise. For those considering doing the Capstone project, be assured that Chris and John are excellent mentors. Trust in your team, trust in yourself, and work diligently. It will undoubtedly be a remarkable opportunity for personal growth.

I want to express my gratitude to all those who played significant roles at various stages of this project. Your contributions were critical, and I'm incredibly thankful for your input and guidance.

Although I feel like I'm repeating myself, I can't stress enough how amazing this experience has been. I have genuinely enjoyed it, and I've learned so much. It has been an absolute pleasure to be part of this team, and I couldn't be prouder of our cohort. I want to extend my deepest thanks to everyone in the Launch School community: instructors, mentors, and fellow students. Congratulations to everyone who has made it this far. Amen.