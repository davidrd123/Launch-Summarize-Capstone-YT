all right well welcome everyone thank you for being here today um my name is lena and i'll be guiding you for this first part of the presentation thanks to the power of remote collaboration myself austin sophie and vaheed we built fjord fjord is an open source framework that provides a real-time api proxy for kafka fjord exposes data stream to web facing clients and through fierce command line interface developers can deploy a scalable infrastructure on amazon web services to handle real-time streaming so let's break down fear's definition into the main components we'll spend time on we've got real time api proxy and kafka there are two principal components to understand in order to appreciate what fjord does first the idea of real-time data and what real-time means next what kafka is and what benefits it provides at the intersection of these two components is the api proxy that's where fjord sits and thanks to the fjord api proxy clients can stream data in real time from any kafka cluster now that we've broken the definition into its free components let's dig deeper into what real time mean and then later we'll look at what kafka is afterwards we'll be able to tackle why you need an api proxy in the first place and finally what fjord offers as a solution so what is real time real time is everywhere in our daily lives let's take a simple food delivery app named superheats super eats is a fictional north american food delivery service restaurants constantly share updates for meal orders with different statuses customers can see if their order is ready and how far away it is from their home or office drivers constantly share updates on their gps location which is then used to determine how far away they are from the hungry customer in our example the new delivery opportunity needs to reach drivers in real time the more time progresses the less relevant this information is in the eyes of the driver ripley is a leading company in the real time space they define real time as the ability to react to anything that occurs as it occurs before it loses its importance it's important to note here what is instant and what is perceived as instant may differ if you and i are in the same room and you hear me talk yes that's nearly instantaneous if you send an http request there is some latency a few hundred milliseconds perhaps but you still consider it much faster than waiting a whole second it really all depends on the context as ably defined real time this is also the definition fjords adopts with real time defined now let's go ahead and move on to part one of this presentation you might be asking yourself just how kafka in real time are related we'll attempt to answer these questions next we will cover the evolution of event streaming then move into how kafka in real time relate and finally why you would need an intermediary to tie the two so let's start a conversation from the evolution of event streaming to do so we'll begin with a look at microservices microservices architecture are the standard way to scale an application through protocols like http or grpcs these services communicate with each other often exchanging data they need to complete their tasks as you keep adding more services and their communication becomes more complex this can quickly lead to what's known as the microservices deaf star you can imagine how complicated and intertwined all these lines of communication can become making sure the data is still relevant when they reach services in real time is another concern in addition different services need to be able to locate new services and be aware of services that have gone offline if we want to have a more decoupled resilient communication pattern one solution is the event-driven architecture event-driven architecture also known as eda is a messaging paradigm that offers a solution the complex network of interdependent services in a microservices architecture with edas services transmit messages to one another via a shared broker so business service one sends data to an intermediary here seen as broker which would forward the data to business service too in eda's services are classified as producers consumers or both producers generate events and consumers consume events edas allow producers and consumers to communicate with each other without even being aware of each other's existence this communication is facilitated by a broker a centralized service that acts as a published subscribe mechanism where messages fell through the broker is a mediator that is responsible for receiving storing and delivering messages to interested consumers producers can be internal services or even some external source that generates data and sends it into the broker but what happens when there are thousands of producers or consumers what happens when producers generate thousands of messages each second how would super eats handle this volume super eats is working with thousands of restaurants and connected with tens of thousands of customers and drivers all over north america each one of them can be generating events all at once so what happens now when you have enough events entering your system it becomes a stream of data a stream of data means a constant unbounded flow of data there is no real beginning no real end hence event streaming producers in our case are the drivers sending real-time gps updates on their location and customers placing orders as you can imagine too there are peak hours where more orders come in and what about the consumers we talked about in eda's well the hungry customers want updates on their meal like has someone even started to cook their burger and how much longer it'll be when it's delivered and restaurants can be classified as both a consumer and a producer they receive real-time updates on incoming orders as well as produce real-time updates on the statuses of those orders so with so much data flowing in how can a business make sure they're handling this volume efficiently when you're choosing an eda system it should be guided by your use case you'll have to evaluate event persistence and the volume and velocity of events that your system will be processing event streaming platforms use a log-based structure to durably store events while traditional ada technologies use a cue based structure where events are deleted after they are consumed with event streaming consumers are able to start reading at any point in the log with traditional ada technologies the broker holds the bulk of the integration logic consumers just receive whatever is sent to them by the broker this sets up the smart broker dumb consumer approach in contrast events remain platforms opt for the dumb broker smart consumer approach placing most of the integration logic on each consumer this enables high volume and velocity of of events and this leads us now to kafka kafka is a golden standard when it comes to event streaming platforms more than 80 percent of all fortune 100 companies use kafka linkedin where kafka was created processes over 7 trillion events per day netflix has also embraced kafka for its events messaging and stream processing needs they process over 700 billion events per day now i'll pass it on to sophie to talk more about kafka and is linked to real time let's now dive a little deeper and take a look at some key features of kafka as leaner has mentioned kafka is an event streaming platform kafka saves events as records to a log and allow these records to be read in order from any starting point by any number of consumers kafka can efficiently send large volume of records quickly from machine to machine and cafe does all this while providing secure access to internal consumers and producers one way that kafka achieves high volume and velocity of message transmission is its use of a compact binary transport protocol kafka is designed for connecting to a relatively few back-end systems over a secure network like the internet as such a binary protocol over tcp is very fitting so internal services can take advantage of the event streaming nature of kafka to receive real-time updates but how do we make this real-time stream of data also available to internet-facing clients let's first switch gears and take a look at how these client devices typically would receive real-time data from a server so in order to assimilate the web-based real-time experience persistent connections must first be established with these client devices web-based techniques like loan polling servers and events and web sockets are often employed to persist client connections with loan polling it is a pull-based technique the client would first send a http request to the server the server keeps the connection open and only responds back when it has a new message the browser then has to make a new request immediately to pull for another message with long polling it works great in situations when messages are rare but if messages come very often then this approach becomes very resource intensive since every message is a separate request with its own headers and so in this case websocket or server stone events is preferred websocket is a web api that enables a persistent bi-directional communication between the client and the server first the client will send a http request to the server the server then responds back upgrading the connection to web sockets a persistent connection is then established over the websocket portable allowing for bi-directional communication websocket is especially great for applications such as online games chat rooms real-time training system and so on with servers and events it is also a web app api it enables a persistent unidirectional communication from the server to the claim first the client would send a http request to the server the server then responds back with headers to identify the response as a stream that does not end until the client chooses to close the tab for connection with this persistent connection established the client can continuously receive push updates from the server so sse is ideal for applications that stream updates from the server like weather updates market prices updates etc it executes entirely over http which means it's native to web interfaces including phones and browsers so since these real-time techniques are all initiated over the http protocol but kafka communicates in its binary protocol this necessitates an intermediary to translate that traffic between the two protocols and implement real time an api proxy that was sit between kafka and the client devices can be designed to do just that to see how an api proxy can link kafka to real-time web applications let's start with the definition of an api proxy let's dissect the two terms api and proxy api stands for application programming interface this interface makes it easy for developers to use backend resources and not care about how the backend is implemented or what protocol it uses the api also provides clear endpoints to send your request to whether it be http or other protocols a proxy is a server that sits in front of your backend server and intercepts network requests on its behalf it can answer these requests or forward them elsewhere if needed so to reiterate an api proxy is a server that will sit between your application and the backend server developers can build web applications using a set of api endpoints so that the applications can communicate with a back end indirectly so by positioning an api proxy right between kafka and the web the api proxy can pull data from kafka over kafka's binary protocol and then push that data connected to two connected devices over the http protocol thus tying kafka to your real-time web applications in addition to this an api proxy also offers several other key features if you need to expose your internal kafka data to thousands or millions of users over internet you will need a reliable system that can handle the volume and velocity of kafka messages with an api proxy it would allow thousands of end users to receive kafka streams over http using a separate proxy also allows you to dynamically scale the server to respond to external user traffic a proxy is a middleware that also acts as a gatekeeper in front of the kafka cluster thus providing an additional security layer and using an mpi proxy will also allow you to customize and group the kafka topics you want to expose into different configurations of api endpoints last but not least this additional messaging layer allows you to offload all the aforementioned logic and the resource intensive tasks of real-time streaming and all the infrastructure involved to an api proxy so that you can focus on value creating activities that are central to your business so let's take a look at some existing api proxy solutions there are three primary enterprise solutions that implement a real-time api proxy for kafka first we have ably a leader in real-time streaming customers that already use able's real-time api can integrate a ably's kafka connector to allow the end users to stream from their kafka cluster pubnub is also a well-established company in real time they do offer a bridge for kafka but is still in beta testing then we have migratory data they offer a specialized product that integrates natively with kafka the expertise is in scaling websocket connections so with all these companies they offer feature-rich highly scalable solutions the main downside however is that you pay a high price for the convenience and the ease of use and you're pretty much locked into their ecosystem so what if you were to roll your own solutions well to do this you would need multiple architectural components the basic components would include a kafka sync connector and an apa proxy server in order to scale the server instances though to handle incoming traffic you will also need a pub sub mechanism to send messages to all server instances and a load balancer to route the traffic and there are a few open source solutions for each of these architectural components for example to implement the sync connector you can use an open source solution like red hat stream c or even or you could even build your own using the kafka connect api however this would be a complex undertaking it would require additional expertise including being familiar with the java ecosystem then existing solutions for a proxy server include open source solutions like push pin and centrifugal so as you can see implementing your own diy solution is not an easy task it is both resource and time intensive engineers would not only need to have the knowledge to spin up each of these components they have to scale them connect them maintain them and there's a price tag attached to all this work so this is where if your comes in fewer positions also somewhere between the diy approach and the paid services it is open source it is scalable and simple to deploy you also have full ownership of the data coming in and out of fjord we offer a limited feature set and a simple developer experience so that you can easily add any additional features you want and as far as we know vr is the only open source platform that incorporates all these infrastructural pieces that you would need to deploy an api proxy for kafka right out of the box so now i'll pass it on to austin to talk more about fjord thank you sophie so at this point we have a firm understanding of the problem that we set out to solve with fjord throughout this second part we'll look more closely at fjord itself we'll start by exploring fjord and fjord's architectural components and then i'll pass it to vahi to walk us through the technical challenges that we encountered on the way to start let's dive deeper into fjord and fjord's features we designed fjord with five main goals in mind first fjord handles the real-time infrastructure that allows internet facing clients to stream records from your kafka topics this means your infrastructure does not have to handle the additional load fjords proxy handles that for you we also designed fewer to be able to automatically scale up and down with whatever load you currently have this means you only pay for resources that you need when you need them fjord is also secure you can limit who has access to your data stream through the use of json web tokens jots you can restrict access to whomever you want fjord is also fully open source and customizable you can check out our repos to see how we built it and you can deploy fjord with your own custom setup including any number of kafka clusters consumer groups and api topics that you want to expose on the client side code fjord is also easy to deploy you can use our cli to deploy on aws with a few simple terminal commands and be up and running on aws in about 15 to 20 minutes who would use fjord well let's return to super eats where customers can order from their favorite restaurants and drivers need to be notified whenever a customer sends in a meal order in real time a customer can visit supereats.com to place an order the order is then received by super eats who uses kafka as the backbone of their eda and because they're connected to fjord fjord is able to fan out new updates on the order to the driver for the delivery and to the customer who made the order so now that we have this high level view of how super eats works with fjord let's take a closer look at how it's actually implemented first super eats drivers customers and any restaurants working with super needs would all connect to superheat servers via http or https for example to superreads.com these servers would deliver via either a mobile app or a browser a page that initiates an sse connection with fjord and receives push updates from your fjord cluster deployed on aws the fjord proxy in turn would pull records from your kafka cluster one note here is that your drivers and customers are not aware that they're also receiving a data stream from fjord from their perspective all they see is that they're connected to the supreme's domain or app this allows you to deliver customized content that has the look and the feel of your website without having to deal with any additional streaming load on your servers let's walk through fjord's full architecture before we dive a little deeper into the individual components and the workflow first clients connect to fjord via a load balancer this application load balancer not only directs clients to the server that's most available but also enables auto scaling servers up and down as necessary based on traffic the server pushes new records to connected subscribed clients via sse consumer groups pull information from your kafka cluster and these can be customized with any number of members to allow for parallel consumption of records from your kafka topics the fourth main component is a nat gateway which is what allows kafka traffic to get to the consumers the middleware in between the consumer groups and the servers is a redis server that decouples the interaction between the two as we'll see later this publish subscribe mechanism allows the servers to scale up and down while immediately picking up streaming records from all consumer groups before we walk through the flow of data as it goes through fjord let's look more closely at the for cured consumer and the fjord server both of these are nodejs applications and having a single application works fine when you're working with a few hundred or a few thousand customers and drivers but what happens when your application grows and your user base grows to ten thousand customers or more you need a way to easily scale the number of instances of your application to achieve this we dockerized both the consumer application and the server application rather than running on a bare metal server or a virtual machine a docker image that holds everything an application needs can be run as a lightweight container this allows you to easily run multiple instances making both the consumer and the server horizontally scalable containers are also extremely portable they're easy to deploy on the cloud working with a few instances of the consumer in the server is manageable but when you work with dozens hundreds or even thousands of running docker instances you need some kind of orchestration tool like kubernetes or docker swarm to make sure containers are running they can communicate and then if one does fail for some reason another is spun up immediately in its place aws offers elastic container service or ecs as an orchestration tool that abstracts away the management of containers and unit uses the unit of a task to represent a running container and a service to group a set of related tasks aws ecs offers two launch types ec2 and fargate with ec2 you get more control over your deployment but you also need to provision and manage the servers that your containers are running on with ecs and fargate aws manages the containers and the underlying servers that they run on so we decided to go with ecs and fargate that way fjord users like supereads won't need to worry about provisioning the underlying servers that fjord components are running on later down the line so now that we understand the dockerized server and consumer um let's follow an order as it goes through here the fjord consumer is actually an abstraction for one or more kafka consumer groups each running as an ecs service each consumer group can have one or more members running as ecs tasks the business user like super needs could add any number of consumer groups each one containing any number of members the fjord consumer runs on a private aws subnet but it's able to connect to the business's kafka cluster through that nat gateway the fjord consumer pulls the kafka broker listening for any new information and when a fewer consumer does receive that new information it will push it um sorry we'll publish it to a channel on the elastic cache for redis instance which is the fjord pub sub mechanism each fjord server is subscribed to the same redis channel so they all receive all new orders additionally the fjord server is scalable so if the demand on cpu or ram becomes too high due to either the volume of orders coming in or the number of sse connections being maintained on the front end another server instance will be spun up to handle the load once the sse connection that we mentioned earlier is established records flow from the fjord server to any and all subscribed connected clients whether they are super eats customers or the drivers or the restaurants that super eats is working with so here's the entire flow of an order as it's placed by superheat's customer which in turn reaches a connected super eats restaurant which finally triggers a new message to be sent to a super eats driver perhaps alerting that a customer near them just place an order giving the driver the option to pick it up so deploying fjord to aws can be done in four steps first install the fjord cli globally second navigate into a new directory where you want to hold all the information for your aws deployment from there run keyword setup to generate a json file you'll use to customize your fjord deployment this also by default generates a private key that allows you to use jots for authentication third customize the fjord settings json file you just created for example by adding your kafka broker's ip address and the security information for your kafka cluster the desired api topics that you wish to expose things like that and finally run fjord deploy and wait 15 to 20 minutes for aws to provision and deploy all the necessary resources and that's it so let's take a look at the cli once you run fjord deploy the process will output the public ip address of both your load balancer and your nat gateways if your kafka cluster is protected by a firewall you're going to need to whitelist the nat gateway ip addresses so that the fjord consumer can have access into the cluster and with just a few commands you can deploy keyword so here's another shot of just the output of the fjord cli you can see the two nat gateway ip addresses and you can also see the load balancer url right there and um so the super east developers will also enjoy using pure because all they need to do is include some concise client-side javascript on whatever interface the end users are using the connection stream is established with just a few lines of code thanks to the event source web api you can also see the endpoint of the load balancer that was just output on line one which is then used to establish a new sse connection on line 9. the comment on line 13 is then where the developer can add any logic they wish to handle each incoming kafka record so that's a look at how fjord came to be the key components that make up its architecture and how you as a developer would interact with for fjord next lehigh will talk to you about some interesting technical challenges that we faced take it away the heat thank you austin so we solved many technical challenges along the way while building fjord but we thought these four were the most interesting ones to talk about with you and share so let's take a look at each one probably the biggest technical decision we had to make was whether to use server sent events or web sockets for real time streaming as sophie mentioned long polling is inefficient for real-time streaming because it requires a new request response cycle for every single transmitted record so this is painfully slow and just impractical for our use case the decision was always just between the two server push options websockets and sse now both work on most browsers internet explorer was the only one that had difficulty with sse but there are workarounds for that and let's be honest who uses ie anymore anyway both are also comparable from a throughput capacity standpoint as they can both handle thousands of concurrent connections per server now websockets is often seen as the de facto technology for real-time communication and because websockets is more popular it also means that there's more community support and it's easier to get solutions on stack overflow there are more libraries that support and work with websockets the other main advantage of websockets is that it enables bi-directional communication this means websockets is probably the way to go if you want your clients to both receive and send real-time data however because our use case only required clients to receive real-time data from kafka the unidirectional limitation of ssc was actually not a problem in fact it meant that our business users did not have to worry about opening up their kafka cluster to incoming streams of data from potentially hundreds or thousands of client devices now ssc also provided a very useful feature by default which is continuous automatic reconnection attempts whenever the client lost connection to the server since our use case involves streaming data to mobile devices that may need to reconnect whenever switching cell phone towers this seemed like a worthwhile feature later when we discuss auto scaling will appreciate another reason why this auto reconnect feature proved to be very useful now because ssc uses the standard http protocol all infrastructure pieces like load balancers work right out of the box with ssc and this is not the case with websockets which requires using additional libraries and steps to configure your infrastructure properly and finally ssc is also easier to set up and deploy both on the client side and the server side you've already seen from austin how easy it is to use ssc on the client side so considering all these trade-offs we decided to go with sse simplicity efficiency and unidirectional communication were the main drivers here so after deciding on ssc the next challenge we faced was decoupling the consumer and the server when we built our first prototype of fjord we had the consumer publishing each record directly to the server via an http post request now this worked as long as we had just one server but we wanted to be able to scale the server instances to accommodate any number of clients and with this initial setup any newly deployed server would just not receive records from the consumer since they're so tightly coupled together so one solution could be to have the consumer somehow be notified whenever a server comes online or otherwise have access to a list of all the connected servers and so with every received message from kafka the consumer could then make a post request to every single connected server but this approach would put a lot of strain and pressure on the consumer which is already very busy just trying to keep up with the stream of records coming from kafka so the solution that we went with is actually a common approach which is to decouple the consumer from the server by adding some type of middleware in between so this middleware would serve as a fan out or publish subscribe mechanism to propagate the same message received from the consumer to all the online servers this decoupling would also allow consumers and servers to come online and go offline without impacting each other so we looked at two main options for this middleware sns and redis simple notification service or sns is the native pub sub mechanism offered by the aws so we actually looked into it first it is typically used to send data from a microservice to end users via text message or email but it can also be placed in between microservices to decouple their communication however implementing sns for that purpose requires some additional leg work to handle the initial subscription to an sns topic there were also some additional steps when it came to unsubscribing a server and we just couldn't find sufficient examples of other companies or organizations using sns for this purpose that to make us feel comfortable enough to use it too so next we looked at redis now while redis is more often used as a caching layer it also has a publish and subscribe functionality redis also works on aws right out of the box thanks to the elastic cache managed by server elastic ash managed service that austin talked about and best yet aws elastic cache for redis makes it super simple to automatically subscribe and unsubscribe servers from topics as they come on and off redis is fast lightweight and required minimal code changes to our consumer and our server it also has been a battle tested and is a common solution to address this exact auto scaling scenario that we have so for all these reasons that's why we chose redis as the middleware between the consumer and the server challenge we faced after decoupling the consumer on the server had to do with our code being deployed on aws so a cross-origin request is when a page tries to access a resource from a different origin by default browsers don't allow this to happen cross-origin resource sharing or course can be enabled by using special request headers so in our superheats example we can see that requests for the fjord stream are coming from the super eats domain which is different from the fewer domain on aws so for that reason we knew right away that we need to implement course and we use the course library for express since our server is built in node.js and everything was working fine when our code was just was just running locally but when we deployed our code our code to aws we ran into some issues we were noticing that after a brief period of inactivity clients were getting shut out of the servers and we'd get a course error displayed on the browser console now things were fine as long as we were regularly streaming records so the example on the left there but once the record stopped so the example on the right the client on the right with the red line once that stopped we'd get a course error and no longer be able to stream and this was happening despite the fact that we had explicitly enabled cores via the proper headers the issue as we discovered was that after a period of not receiving any more records the load balancer would time out the http responses thereby forgetting about the headers that had enabled cores in the first place and so that's why we get that course error so the solution was to implement what's known as a heartbeat or pulse for sse and the way that works is that at a regular interval so about every 30 seconds by default but we allow the user to customize that and change that so if at that interval we send a blank message to all the clients connected to each server now on the client side they don't see this blank message appear anywhere which is great but the heartbeat is useful for two main reasons first this checks whether any client has closed the connection since the last time we sent a record and if that's the case it also closes the connection on the server side so that the server doesn't waste any resources maintaining that connection and then finally best for our use case here is that this lets the load balancer know that the connection is still ongoing and so once we implemented this heartbeat we no longer had any course issues when it came time to do some load testing we first had a hard time finding an open source library that worked with server sent events there were many options for for load testing websockets like artillery and siege but not as many options for ssc we eventually came across gatling which actually had some really good documentation for how to test server sent defense gatling is built on top of java and scala so we had to first install those and we then used some minimal scala to build a few load testing scenarios and that's where the fun really began because we continuously pushed the server further and further to see what it could handle in this setup here you see that there are two servers handling about 4 500 concurrent connections each when we started producing about 100 records per second however you can see that the cpu utilization shot through the roof and hit the maximum 100 cpu capacity and this actually crashed the server because we hadn't properly configured the auto scaling rules yet but once we did set up the autoscaling alarms we confirmed that aws launched more servers whenever our various alarms were triggered so this is a screenshot of the aws console on the fjord server task in the highlighted red box at the bottom you can see that the max capacity we were able to go to on one server was about 8 500 client connections per server and this was using the lowest cpu and ram settings possible for a server instance on aws far gauge so thanks to fjord's ability to scale up new servers as necessary to handle more load we were able to use this server with the lowest power possible which allows us to be cost efficient and still be able to serve any amount of clients necessary and this concludes our presentation thank you so much for your attention and being here today i will now open the floor to you would love to answer some of your questions uh you can send in your questions and comments through the zoom chat okay we have a first question how would this work in a mobile app great question julio so the way this would work is on a mobile app you would use so if it's on android you would use java or kotlin there's a library for server set events that you would use and same thing for swift on ios so the event source api [Music] is what you would use on a browser but there's libraries available for mobile as well so katarina is asking what was the most challenging part of the project okay thank you for the question um i guess i'll go first um so we start prototyping different pieces of fjord on the aws console so moving this to the aws cdk wasn't the most straightforward um as the the documentation wasn't the clearest so to me that was the most challenging part all right i can go next thanks for that question katarina for me it was handling the fire holes of information we're just learning so much all the time but also the aws world is pretty vast and i found that to be quite a challenge to to learn that part yeah to sort of tag along with that uh definitely a ton of new information and one big area of our project is kafka and learning all the configuration details and setup that kafka requires so that was a really cool and at the same time really challenging part of the project yeah i have to agree with everybody here really the deploying the code on aws was to me was the biggest challenge uh you know we're we're so used to running things locally um and this whole different ball game when we talk about production level code you know like the example that i shared you know for the course issue we didn't even think that that was going to be an issue because things were running fine locally when we discovered that was a problem once we deployed stuff in production and what was the most interesting part of this project to you i think trying to just figure out all the different pieces and connecting them together i would say for me um i loved learning and using server set events we actually also had to use websockets initially we built just a prototype server using websockets as well so that was also fun learning kafka and then oh yeah load testing using gatling was super fun just you know because for up to that point things were running just kind of theoretically and you know minimally just like with 10 maybe 15 connections max so it was fun to do some load testing and see how much you know we could push this yeah i agree with the heat there i think learning about the different technologies and actually testing them out um that's a lot of fun um also solving the problem actually seeing all the pieces coming together that's also um very satisfying yeah kind of like what vahid said all those pieces were super interesting to me i think for me one of the biggest challenges which i thought was fun and interesting to solve was also just to figure out the kafka because the api proxy we created is made for kafka so just figuring out how kafka thinks that whole ecosystem was quite interesting yeah uh same here like there there were so many different components of the project that all just had to come together to make it work um a couple really interesting areas i thought one one was docker just getting to getting more and more familiar with docker and that whole world of uh infrastructure and um was was was really interesting and then aws of course is just huge so much information so much documentation really cool to to work our way through that okay daniel asks up the law after all the research is done and you decide on what to build how does starting the project look like for example how to divide efforts and project collaboration with git i guess i can take that one so um i'm not also like clarify if i'm not answering your question but basically how we went about this was daily team meetings everyone was kept up to date with you know daily check-ins we had daily summaries that we would use and we would also do pair and solo programming to just make sure everyone is up to date on what's happening is any is does anyone else want to add anything to this question yeah it was kind of like just changed along along the way you know different points were using different strategies and just kind of evolved organically but yeah like like i mentioned just the daily check-ins um and you know people talking about what they wanted to work that day and okay carl says great presentation team fjord uh those diagrams are amazing what's the setup like for super eats with kafka is that something that they have to set up themselves or does fjord provide some level of convenience if they don't have a kafka cluster already yeah great question i think we're just going on the assumption that someone whoever would use fjord already has kafka deployed so this piece is just once you have kafka deployed you can use fjorge to expose kaffir topics to end users yeah that's a good possible future thing to work on if you were your 2.0 oh sorry go awesome i was just gonna add that uh yeah there's a whole complex world of kafka that this project is not centered around this this project center really is a connector for uh kafka moving streaming data from kafka to the browser or to end users of some kind and there's four there's several other um major apis that exist within the world of kafka that we're just assuming that that user has set up um in their system and right rana asks what future improvements are planning for the project more deployment options and so on thank you for that question man this is a really good question and there's actually a few things that we know we want to work on um first thing would be security features so right now the fjord consumer connects to a business's kafka cluster just by using a plain username and password which is great but there's a lot more advanced security features that kafka offers like encrypting communication and making use of network control access so that's definitely one thing we want to implement and the second would be enabling bi-directional communication so right now kafka is just designed with one-way server push streaming event in mind and even though this works great for our use case we'd like to give the option to businesses to inject events into their kafka streams based on client actions we could potentially do this with websockets or even ssc over http 2. katarina asks have you worked worked with each other before and during work on your project have you encountered communication challenges and how have you dealt with them maybe i can take this question um during the prep prep phase of capstone we started that's when we started working together um and then all throughout capstone and then uh up to capsule projects so um but before that we obviously have not worked with each other but i mean it's been it's been quite a few months now so uh we um yeah we we learn a lot about each other and um we've gotten used to each other's workflow and in terms of communication um they were definitely hiccups but we were pretty straightforward with each other and then um yeah i think that's how we sort of uh pulled through and we saw some of the issues together i don't know if anyone wants to add to that from that team yeah i think one thing we heard from chris a lot during capstone is to not be afraid to have hard conversations right sometimes conflict is is tough to deal with and sometimes people just want to shy away from it and not bring it up but i think we found it really productive and useful to just be honest and just bring up issues as they come along and just to make sure we're all on the same page so those were very important and and necessary and you know if anything i think it just makes the team stronger makes the team know that you know we can handle anything yeah i agree with sophie and vaheed and i would just add like um we we did have like obviously we're different people with different perspectives and that's what's great um but also agreeing on the problem at hand and just understanding different perspectives and needs and then figuring out what we need to align on an approach i thought was pretty helpful so you know asking questions is a great way to deal with it defining the purpose of what we're building or what we need to get done also definitely helps and just a disclaimer allison and i have worked before so we did have for our friendship coming in but aside from that it was brand new working with sophie and vahid but also very enjoyable okay kyle asks uh or says great job everyone i'm super impressed by the project and working with aws services can incur a steep learning curve which you mentioned earlier how did you approach working with such wide range of services and uh start start with that question um yeah i would say uh great question and you know we followed the the launch school process of breaking down problems uh to the smallest possible problem and um just moving incrementally step by step uh first working locally then you know working with the most simple architecture that we could find on aws which would be an ec2 instance and then starting to look into how you scale those individual pieces and what kinds of additional components and complexity that requires um yeah that's what i would say anyone else have anything to add there perfect answer nothing to add for me awesome okay and finally janet says awesome work for your team how did your initial vision of yours compare to the final product did you have to pivot at all during the process as you learn more and encounter obstacles yeah that's a really good question janae thank you um i i would say uh that our initial vision of fjord actually aligns pretty closely to what we have as an end result with just a few additional pieces necessary for making it a scalable system um for instance we weren't always so just as vague mentioned when um he was talking about technical challenges and decisions um we we weren't always sure we were going with elastic cache and redis and we were looking at other potential messaging middlewares there and it looks like we don't have any more questions so any final words yeah um so it was a pleasure working with everyone it was a super cool process to get to know kafka and docker and aws and just the world of infrastructure like to move away from code and just start to look at these bigger systems it was a really cool process and yeah congrats to all the other capstone teams who have done super awesome really cool work as well and thank you thank you everyone yeah just like congrats to cohort first of all i'm really amazed with all like the work you put in and your presentation it was really cool to see also i just want to say thank you to the logical community and everyone i've connected with this journey has been a lot more fun because of you also thank you to chris sturgeon nick our mentor and of course austin vahe and sophie it's been awesome working with you so thank you i'm gonna say all the same thing yeah it's been it's been an amazing experience um yeah trust in the process uh you know there'll be times when you're gonna doubt yourself you know of course i'm speaking to host students who are interested in doing capstone um but yeah trust in the process you know chris or john are awesome mentors trust in your team trust in yourself and work hard and it's it's going to be a phenomenal growing experience i think it certainly was for for me and uh probably everyone in the cohort um so yeah and again thank you to all who were so critical at various junctures of this this process and experience yeah i feel like i will be repeating everything repeating myself but um it's been an amazing experience it has been so much fun i've learned so much it's been such a pleasure to be a part of this team i'm proud of this cohort so thank you to everyone in the launch school community thank you to our instructors our mentors and congratulations to everyone who has made it this far amen 