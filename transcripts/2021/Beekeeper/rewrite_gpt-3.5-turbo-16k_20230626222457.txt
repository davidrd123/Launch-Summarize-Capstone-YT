Welcome, everyone! Today, we'll be discussing Beekeeper, an open-source backend as a service for a virtual waiting room. By the end of this presentation, you'll understand what Beekeeper is and how it solves the problem it addresses.

Let's begin with the problem that Beekeeper aims to solve. One common problem is when a burst of traffic overwhelms an existing infrastructure, leading to a denial of service. For instance, let's consider the fictional Seal Brewing Company, which plans to have a special sale for their beer, expecting a large influx of website visitors. To prevent their site from crashing, Seal Brewing redirects their customers to a virtual waiting room, known as the Beekeeper waiting room, which can handle heavy traffic. Beekeeper then forwards the traffic to Seal Brewing's site at a manageable rate.

Here are the characteristics of this use case: it involves a heavy burst of traffic from a one-time event, which is known in advance. Customers will only know about the event and where to go once it is communicated to them through marketing channels like emails and social media posts. This setup allows Beekeeper to intercept the traffic and direct it to the waiting room URL. Furthermore, the event requires some dynamic action from the users, such as logging in or making a purchase on the final destination site.

Before concluding that a virtual waiting room is necessary, it's essential to consider alternative solutions. One possibility is scaling the existing infrastructure either vertically (adding more CPU and RAM) or horizontally (adding more hardware). However, there may be limitations that prevent scaling in these ways. Additionally, it may not be cost-effective to invest in additional hardware for occasional bursts of traffic. Furthermore, implementing these changes would require time and effort from the engineering team.

Another option is rebuilding the current functionality on new infrastructure, potentially using a cloud provider that offers easy scalability. While this would solve the problem and prevent waste, it would involve a complete rebuild and significant time and effort. Seal Brewing, however, might be satisfied with their existing infrastructure for most of the year and only require a solution for occasional events like a Black Friday sale.

This brings us to Beekeeper, a backend as a service (BaaS) solution. BaaS means that Seal Brewing doesn't have to make infrastructure decisions since Beekeeper handles it for them. Although Beekeeper is deployed on Seal Brewing's own AWS account, providing flexibility and control, Seal Brewing doesn't need to worry about the underlying infrastructure. Beekeeper is an npm package that can be easily installed and used through a command-line interface. It generates an AWS-based waiting room with a URL that Seal Brewing can provide to their customers for the event.

Now, let's take a closer look at the Beekeeper infrastructure. The CLI tool creates AWS services programmatically using the AWS SDK. If you're unfamiliar with AWS, don't worry—I'll explain it in a way that makes sense. When a user clicks on the generated URL, it triggers a GET request to the Beekeeper route on the API Gateway. This route then triggers a lambda function, which we call a producer lambda—a temporary function that scales up and down based on traffic load.

The producer lambda performs several tasks. First, it checks if the incoming request has a cookie specific to Beekeeper. If not, it generates a random token, sets custom response headers, and creates a cookie with the token. It responds to the request with a 302 redirect, guiding the user to the waiting room. Under the hood, the waiting room consists of static assets stored in an S3 bucket. The random token and cookie are crucial for tracking users and determining when they should be forwarded to the final destination. This is where the dynamic action specified by Seal Brewing, like logging in or completing a purchase, takes place.

Now, let's return to the producer lambda logic. Another important task it handles is pushing the generated token into a queue, representing the users waiting in line. On the other side of the queue, we have one or more lambdas known as conser lambdas. These conser lambdas dequeue tokens from the queue at a predetermined rate set by Seal Brewing. This rate serves as a throttling mechanism, regulating the traffic flow. It's worth noting that we have fewer conser lambdas than producer lambdas since the latter scales dynamically to accommodate bursts of traffic while the former only needs to dequeue tokens at a slower rate.

After dequeuing a token, the conser lambda writes it to DynamoDB, an AWS NoSQL database. Now, let's revisit the waiting room in our diagram. The frontend code includes client-side JavaScript that sends AJAX requests every 20 seconds to the polling route on the API Gateway. These requests include the cookie, allowing the API Gateway to retrieve the token. Instead of triggering a lambda like before, the polling route directly queries the database to check if the token has been written. If the token is found, the API Gateway responds to the polling request, informing the frontend to redirect to the final destination. If the token is not yet written, the polling continues, and the user waits.

In summary, Beekeeper provides an infrastructure that can handle high traffic loads. Users are placed in a queue, and tokens are dequeued at a specific rate determined by Seal Brewing. Users wait in the virtual waiting room while the client-side code repeatedly checks if it's their turn. Beekeeper is designed for one-off events with bursty traffic, where existing infrastructure scaling is not feasible or efficient. While it may not be suitable for the largest events, like a major product release, Beekeeper is a robust open-source solution, offering control and flexibility.

Next, my teammate Aaron will demonstrate how all of this works. Take it away, Aaron! Beekeeper is a version of a NoSQL database. To summarize the concept, the front-end code in the waiting room contains client-side JavaScript that sends AJAX requests every 20 seconds to a specific route on the API gateway called the "/polling" route. The user's cookie is sent along with each request, and the API gateway accesses the cookie to extract the token. In this case, instead of triggering a lambda function, the polling route directly queries the database to check if the token has been written. If the token is found, the API gateway responds to the polling request with an object instructing the front end to redirect to the final destination. If the token is not found, the polling continues and the user waits.

Beekeeper is designed to scale and handle high volumes of traffic. Users are placed in a queue and are only taken off the queue at a specific rate, determined by Sel Brewing. Users wait in a waiting room, and the code continually checks if it's their turn by querying the database, where the token is stored. The answer to whether it's their turn is determined by whether the token has been written in the database.

Beekeeper is primarily targeted towards organizations experiencing bursty traffic due to a one-off event. These organizations may have constraints that prevent their existing infrastructure from scaling or make it unfeasible in terms of return on investment. Beekeeper allows organizations to retain control over the infrastructure of their waiting room.

In terms of traffic capacity, Beekeeper can handle up to 5500 requests per second in terms of hardware. This translates to 300,000 people in a single minute. However, when the number of users in the waiting room reaches around 200,000, the current implementation starts to approach its limits due to the polling of the API gateway every 20 seconds. Therefore, Beekeeper is not suited for the largest bursty traffic events, such as a Sony PlayStation release, but it is not limited to small events either. It is a robust open-source solution that fulfills its intended purpose.

Now, let's move on to the demo. Getting started with Beekeeper is simple. First, the Beekeeper module needs to be installed globally using the npm install g beekeeper cli command. This enables the use of the Beekeeper command, which allows the creation, destruction, and manipulation of virtual waiting rooms. Alternatively, the module can be installed locally and the Beekeeper command can be run using npx. Once installed, the beekeeper init command needs to be run to set up the waiting room. This command asks a series of questions to configure the waiting room, such as the profile name, waiting room name, region, URL for redirecting users, maximum number of users allowed per minute, and whether to enable dynamic rate throttling.

After providing the required information, the beekeeper deploy command is given to create the customized waiting room. This command sets up various components such as a master role, an S3 bucket for assets, a Simple Queue Service (SQS) for cookie data, DynamoDB for the database, API gateway, and lambdas. Once the deployment is complete, the waiting room URL and client check endpoint URLs are provided. The waiting room URL is used to direct users to the waiting room, while the client check endpoint URL can be added to the backend to prevent users from bypassing the queue.

During the demo, the waiting room URL is accessed, and a lambda intercepts the request, issues a cookie, sends the cookie value to the SQS, and redirects the user to the waiting room. In the waiting room, there is an estimated time for the user to enter the site, calculated based on the number of messages in the SQS and the user throughput rate per minute. A countdown timer triggers an AJAX request every 20 seconds to the API gateway, which queries the database for the cookie value. If the cookie value is found, the user is redirected to the final destination.

In this specific demo, as the only user in the waiting room, there was no redirection after the first polling cycle. This is because there is a lambda that can only be triggered every 60 seconds, which retrieves messages from the SQS and populates the database with the cookie values. Users can only move past the waiting room once their cookie exists in the database.

Once the traffic flood ends and the site can handle the traffic, the waiting room can be turned off using the beekeeper off command. After turning off the waiting room, anyone visiting the original link will be sent directly to the endpoint. To demonstrate this, the original URL is accessed, and the redirection to the endpoint is observed.

To clean up and remove all Beekeeper AWS components, the beekeeper destroy command can be used. This command removes all roles, permissions, and components associated with Beekeeper. It's a simple process that can save time when dealing with multiple apps running on AWS.

Creating additional waiting rooms is as simple as running the beekeeper init command for each new waiting room profile. To check the existing waiting room profiles, the beekeeper config command can be used.

Now, let's dive into the major design decisions of Beekeeper and explore how each component of the infrastructure works. Initially, Beekeeper was designed with a diagram of a simple waiting room using a Lambda Edge function deployed on a CloudFront distribution. While this infrastructure was simple, fast, and globally distributed, it posed challenges when adding desired features. It also raised the question of whether a globally distributed system was necessary for the target audience. Considering that Beekeeper targets clients with limited infrastructure serving local one-off events, such as concert sales or vaccination appointments, it made more sense to deploy the infrastructure in a specific region.

Therefore, the CloudFront distribution was replaced with an API gateway, which acts as a frontend to the backend services. This gateway serves as a central hub for all traffic and provides control over requests and response formats. By abstracting route handling logic away from lambdas and using the gateway as a proxy to other services, the infrastructure was simplified and the Lambda pool became more homogeneous.

The Beekeeper gateway utilizes three routes. The main route serves as a proxy to the producer lambdas and is the URL given to users to direct them to the waiting room. This route invokes a lambda function and either redirects the user to the waiting room or to the endpoint depending on the infrastructure status. The other two routes serve as proxies to the database, returning data in different formats based on preset mappings. One route is used internally to pull the database for user queue status, while the other is provided to the client for user verification.

To track users and determine if it's their turn or if they've passed through the queue, cookies were initially used. A token was created when the user first visited the promotional link and stored in the user's browser as a cookie. Client-side JavaScript in the waiting room checked if it was the user's turn by reading the cookie. The API gateway extracted the token from the cookie and queried the DynamoDB to determine the user's turn and retrieve the endpoint URL. However, this approach required cross-origin requests between the waiting room site and the API gateway, which presented challenges due to cross-origin resource sharing policies.

An alternative approach to tracking users is to use query strings. Instead of setting the token as a cookie, it is exposed as a query string parameter in the user's URL. The waiting room site can easily retrieve and pass along the token using client-side JavaScript. This tracking method can be enhanced by recording the user's IP address and cross-referencing the token with the IP address.

These design decisions and the overall structure of Beekeeper result in an efficient system for managing bursty traffic and providing a seamless user experience in a waiting room environment. The system has two routes, each returning data in a different format based on preset mappings. One route is used internally to retrieve the database for a user's status in the queue. The other route is given to the client as a means to verify if a user has passed through the queue. To track users and determine if it is their turn to be redirected to the end point, a token is created when the user first visits the beekeeper promotional link. This token is sent to the queue for processing and stored in a user's browser as a cookie. Client-side JavaScript in the weight room checks if it's the user's turn by reading the cookie. The API gateway then queries the DynamoDB using the token stored in the cookie. If it's the user's turn, the DynamoDB returns a truthy value and the user is redirected to the end point.

This cookie approach works, but it requires a cross-origin request between the weight room site and the API gateway. By default, cross-origin requests are typically prohibited by the same origin policy, unless specific headers are present. To include cookies in the request, a special flag or object must be included. However, some browsers are starting to block cross-site tracking, which would break this tracking approach.

An alternative tracking approach is to use query strings instead of cookies. The token would be exposed as a query string parameter in the user's URL. This way, the token is visible in the URL string on the weight room site and can be easily retrieved and passed along using client-side JavaScript. However, query strings are less secure as they are exposed to the user and can be easily manipulated or transferred between users. Additionally, if a browser window is closed, the query string is lost and the user would need a new token.

Cookies are more secure as they are hidden from the user and stored by the browser until cleared. However, cookies have cross-origin resource sharing (CORS) issues that require specific headers to transfer. Some browsers like Safari are starting to block cross-site tracking, which poses a challenge. Nonetheless, cookies were chosen due to their better security, and the CORS issue was mitigated by making a DNS change and placing AWS resources on the same domain.

To store and process a large number of user tokens in a first-come-first-served manner, a message queue was used. The AWS Simple Queue Service (SQS) was used to store messages until they are retrieved by the consumer lambda at a specified rate. There are two types of queues provided by AWS: standard queues and FIFO (first-in, first-out) queues. FIFO queues provide in-order processing, deliver messages only once, and have high throughput. However, they have a maximum throughput limit.

To handle a load greater than the maximum throughput provided by FIFO queues, a standard queue was chosen. While the standard queue does not provide guaranteed ordering and may have possible duplication, it has near unlimited throughput. Messages arriving in duplicate were not an issue because the write events are idempotent. This choice of using a standard queue was made as it provided greater capacity, and any out-of-order message arrival or duplication was unlikely to affect the user experience.

The throttling mechanism in Beekeeper's infrastructure was implemented by using a cron job that runs every 60 seconds. AWS does not allow a frequency of less than a minute, so a lambda was configured to self-trigger on an automated schedule using a CloudWatch event rule. The initial approach involved the cron job triggering a consumer lambda that retrieves 10 messages from the SQS, writes them to DynamoDB, and then deletes them from the SQS. However, this approach had scalability issues as it required adding more consumer lambdas, which reduced the number of producer lambdas due to AWS account concurrency limits.

To address the scalability issue, two alternative approaches were explored. The first involved running a for loop within the consumer lambda to process multiple batches of messages. However, this approach was not accurate in terms of throttling and was abandoned. The second approach introduced a new lambda called the trigger lambda, which calculated the number of consumer lambdas and the number of loops they would run. This approach was scalable and highly accurate, so it was implemented.

Dynamic rate throttling was implemented as an optional feature that allowed the throttling rate of consumer lambdas to be adjusted based on the response time of the final destination endpoint. A special lambda called the dynamic rate throttling lambda was deployed to measure the response time and adjust the throttling rate accordingly. The dynamic rate throttling lambda compared subsequent pings to a baseline latency value stored in DynamoDB and adjusted the rate of the consumer lambdas accordingly.

The infrastructure's capacity was load tested using the artillery load testing tool. The average total duration time for the producer lambdas was 160 milliseconds, factoring in both the lambda code start time and invocation time. This information was used to estimate the system's limits. It was discovered that the number of producer lambdas needed to handle incoming traffic was a bottleneck due to the AWS account's concurrency limit. The load testing also identified the need for adjustments in the throttling mechanism and highlighted the importance of accurate and scalable infrastructure.

In conclusion, the transcript covered various aspects of the coding Capstone project, including routes, user tracking, the choice between cookies and query strings, the implementation of a message queue, different approaches to throttling, and load testing. These details were summarized and condensed in the rewritten transcript, while still maintaining the technical essence of the content. This transcript discusses the implementation of a coding capstone project. The project involves the use of for loops to run and invoke certain functions. Specifically, if the Steel Brewing Company selects a rate of 3,300 requests per minute, the trigger lambda will invoke a total of four conser lambdas. Three of these lambdas will complete 100 loops each, while the fourth one will complete the remaining 300 loops.

The conser lambdas run on a for loop, and with each iteration, they check the Simple Queue Service (SQS) for 10 messages, write them to DynamoDB, and then delete them from the SQS. This approach was found to be both scalable and highly accurate, leading to its implementation.

In addition to the throttling mechanism, the team also implemented a dynamic rate throttling feature as an optional feature that the CLI user can enable. When the infrastructure for the beekeeper is deployed and the CLI user enables dynamic rate throttling, a special lambda called the dynamic rate throttling lambda is also deployed. This lambda runs every 60 seconds and measures the response time of the final destination by pinging it. The first ping establishes a baseline latency value, which is then stored in DynamoDB. Subsequent pings are compared against this baseline, allowing the lambda to adjust the throttling rate of the conser lambdas based on the comparison.

If there is a high volume of users hitting the final destination endpoint and the latency time increases, the dynamic rate throttling lambda will reduce the rate of the conser lambdas to restrict access to the final destination. The team took care to ensure that the rate does not change too rapidly in either direction or reach unsafe levels. The dynamic rate throttling lambda has built-in logic to decrease the rate at which it can change as it moves farther in one direction. It also takes a slower approach to increasing the rate after a throttle than it does when throttling.

Moving on to the beekeeper's infrastructure capacity, the team decided to load test it using a tool called Artillery. From the results of the load test, they were able to determine the system limits. The producer lambda, with an average total duration time of 160 milliseconds, was found to be able to handle up to 6,250 requests per second. Similarly, the S3 bucket had a limit of 5,500 get requests per second imposed by AWS.

If the number of visitors to the API Gateway exceeds 6,250 requests per second, both the producer lambda and the S3 bucket will experience throttling. To address the bottleneck of the producer lambda, it is necessary to contact Amazon to raise the lambda concurrency limit. As for the S3 bucket bottleneck, distributing the requests and S3 objects to multiple prefixes can help alleviate it.

Another potential bottleneck involves the API Gateway, which has a limit of 10,000 requests per second imposed by AWS. Depending on how often the visitors in the waiting room pull the API Gateway, it could become a bottleneck. The team also expressed a desire to implement additional features in the future, such as a branded waiting room experience that allows organizations to upload their own logos and static assets. They could also enable custom domains through AWS Route 53, which would eliminate the complications of protecting the waiting room URL from spam and improving the wait times for genuine visitors.

Currently, the team uses polling to check the DynamoDB from the waiting room every 20 seconds to determine if the visitor is allowed to be redirected to the final destination. However, this polling can be wasteful, resulting in unnecessary requests and response cycles. To address this, they plan to implement websockets. By establishing an open connection to the waiting room, they can push information through it, resulting in a more efficient process with only a single request and response cycle to establish the initial connection. However, they will need to consider the cost of maintaining potentially open connections.

The team expresses their gratitude to the audience for watching the presentation and introduces themselves as the developers of Beekeeper. They also invite the audience to ask any questions during the Q&A session that follows.

During the Q&A session, the first question asks whether the architecture of Beekeeper ended up being the same as initially envisioned or if there were major changes throughout the development process. The team explains that the initial architecture involved an S3 bucket for the waiting room static assets, with the URL representing the static assets being the first point of entry. However, they eventually shifted to the current architecture, which is close to the initial idea but with several iterations and improvements made to the throttling mechanism.

The next question inquires about the team's teamwork experience. They respond positively, highlighting the absence of personal conflicts and the smooth progression of the project. The team also mentions that most of the project was done through pair programming, with individuals only working individually on smaller tasks towards the end.

The team is then asked if they have any interesting bug or problem-solving stories from the development process. They share that they faced various challenges and had to overcome issues such as cross-origin resource sharing (CORS) problems and cookies. They also recall a particularly challenging bug related to Amazon not picking up text changes in one of their services. Additionally, they discuss the difficulty they encountered in retrieving configuration details of the deployed infrastructure using the AWS console and the workaround they devised using the CLI tool.

The next question addresses the impact of using a non-strict first-in, first-out (FIFO) queue on customer ordering. The team acknowledges that there could potentially be some impact in situations where customers are trying to purchase limited stock items. However, they believe that the impact is likely minimal, especially in scenarios where users are geographically dispersed and latency differences can naturally cause slight variations in order. They note that Amazon's "best effort ordering" is not well-defined and could be influenced more by external factors such as latency.

Lastly, the team expresses their appreciation to the viewers for attending the presentation and opens the floor for any final questions or comments. They express their gratitude once again and conclude the session. Regarding the issue of quantity, I believe it will likely not be a problem. When it comes to best effort ordering, it will probably mean that the messages are processed in the exact order they are received. It is possible, in theory, for messages to arrive out of order, but this is uncertain. If anyone has any additional thoughts on this matter, please feel free to share. That concludes my perspective. 

I agree with what has been said. Amazon's definition of best effort ordering is quite vague and leaves room for interpretation. It is unclear how messages can end up being out of order. It is likely that factors such as user latency or external influences will have a greater impact on message order than the queue itself. 

To make this point clearer, let's consider a scenario where we have users from both Chicago and Los Angeles. Imagine that retailers in Chicago have deployed the Beekeeper infrastructure, while users from USC in LA also participate. Now, let's discuss the concept of FIFO (First-In-First-Out). It is possible that the user in LA technically clicks on a link before the user in Chicago. However, due to network latency, which is beyond our control, the LA user may actually arrive at the destination later than the Chicago user. 

In this case, we suspect that the latency between Chicago and LA can be in the range of hundreds of milliseconds. On the other hand, the best ordering issues with Amazon are likely in the range of single-digit milliseconds or even microseconds. Therefore, it is extremely difficult to strictly maintain a FIFO queue when dealing with users from different geographical locations. The considerations introduced by Amazon's best effort ordering may be overwhelmed in such scenarios. 

Thank you all for joining us today. We appreciate your presence and participation. 

Thank you everyone for attending this session. 

