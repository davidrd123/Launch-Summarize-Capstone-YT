well hi and welcome everyone uh so today we'll be talking about beekeeper which is an open sourced backend as a service for a virtual waiting room and hopefully by the end of today's presentation you'll understand exactly what that means so our agenda for today my name's ryan and i'll be talking about the problem that beekeeper solves some of the possible solutions to that problem what a virtual waiting room is and then i'll introduce our solution to this problem which is beekeeper and then finally i'll be walking through the beekeeper infrastructure after i finish my other teammates are going to present aaron will be giving us a demo and then ian will be talking about some of the engineering challenges we encountered and then justin will be finishing up the discussion of those engineering challenges and he'll also be talking about the capacity of our infrastructure how much traffic it can handle what are the bottlenecks and he'll be walking through some of the load testing we did finally justin will conclude by discussing a few potential future features so what problem does beekeeper solve well we're talking about a burst of traffic from a promotion or one-off event that overwhelms an existing infrastructure and causes a denial of service a classic example might be a black friday sale so some company let's call them the seal brewing company that's just a fake company for this presentation they sell beer they're going to announce a special sale for some of their beer and they're expecting an amount of traffic much larger than their usual load and they're worried that traffic might overwhelm their site so instead of a bunch of users all being sent directly to seal brewing site seal brewing is going to tell their customers to go to a different url and that's the beekeeper virtual waiting room which can handle a heavy amount of traffic and then beekeepers going to forward that traffic over to seal brewing at some preset slower rate they can handle so what are the characteristics of the use case um well we have that heavy burst of traffic we have a one-off event with a start date that's known ahead of time and the url where that event is going to be isn't something users already know about they don't know where to go for the event until they're told about it what does that mean it just means that when seal brewing holds their event they're going to tell their customers about it somehow and they're going to do that through typical marketing channels like sending an email blast and posting on social media and we only mention this because somehow beekeeper has to get in the middle and intercept this traffic and the one-off nature of our use case is what allows this to happen we're not talking about intercepting all of seal brewing's traffic 365 days a year here we're not talking about a dns change the waiting room url can be put in these marketing emails and on the social media posts okay then the last characteristic here the event involves some sort of dynamic action on the part of the user what does that mean well it just means that the action seal brewing wants their customers to take isn't a simple read or a simple right in other words you don't need a virtual waiting room to solve every kind of bursty traffic problem the problem a virtual waiting room is trying to solve means that eventually you need the traffic to hit the final destination and do something there maybe the customer has to log in with credentials that the legacy infrastructure knows about and maybe then they have to complete a purchase there but for some reason you actually have to forward the users there eventually okay so before we jump right into saying that someone needs a virtual waiting room to solve this class a problem what else could they consider doing a couple things one is they could scale their existing infrastructure maybe vertically by adding more cpu and ram to existing hardware or maybe horizontally by adding more hardware there's a couple potential problems with trying to do that the first is you just might not be able to the existing infrastructure might have a bottleneck that's either maxed out from a vertical perspective or maybe it can't be scaled horizontally second it might be wasteful to do this seal brewing might only need this capacity a handful of times a year so buying more expensive versions of hardware or buying economical hardware but more units of it and letting that capacity sit idle most days of the year isn't going to be the best return on investment third it's going to take the engineering team some amount of time and effort to put that plan into action okay so another option would be rebuilding the current functionality but on new infrastructure perhaps with the cloud provider that can easily scale up and down that would solve the problem it wouldn't be wasteful the only problem there is now you're talking about a complete rebuild involving time and effort and all steel brewing is trying to do is hold the black friday sale they're happy with their current infrastructure most days of the year no complaints they just want to have this one larger event so at this point seal brewing might be starting to think about a solution they can tack onto their existing infrastructure like a virtual waiting room so what is it what is a virtual waiting room you probably understand this already it's just a url where traffic can queue while waiting to go somewhere else you can see on the right it acts like a throttling mechanism that flattens out the rate at which traffic is flowing so assuming seal brewing wants one of these virtual waiting rooms how would they get one the first way is they could buy a solution it turns out virtual waiting rooms are a real thing some of the known offerings out there are from queueit crowdhandler and qfair if you've ever tried to buy burning man tickets or purchase a playstation on the release day you may have been sent to a virtual waiting room that was powered by one of these guys the downsides to purchasing a solution are cost and the main one is loss of control you don't control the infrastructure that backs up the waiting room and you're reliant on the third party's implementation okay so the second way to go would be to build your own uh beekeeper has a solution we're going to talk about it here shortly so seal brewing could essentially just do what we did and build one themselves from scratch the downsides are obvious for that though time and effort their team has to study what are the common characteristics of virtual waiting rooms what are the gotchas to look out for and then once they know what features they want they have to learn a cloud provider like maybe aws and design an infrastructure using those services that match the features they want that all takes time so that leads us finally to potentially using beekeeper uh which we'll talk more about now so as we said beekeeper is open sourced and it's back end as a service backend as a service just refers to what level of abstraction we're talking about here on the one end seal brewing can own and operate all their own equipment that's no abstraction but a lot of control on the other end they could own nothing but purchase software as a service solution like cue it that's a lot of abstraction but no control so beekeeper is backend as a service which means seal brewing doesn't have to make any decisions regarding the infrastructure they just get it out of the box we make all those decisions for them yet it is interesting in that this infrastructure actually gets spun up on seal brewing's own aws account so they do control it in the end and they can make any changes they want in that way it's more flexible than what you typically think of when you're talking about a traditional example of a backend as a service okay so beekeeper is an npm package you install it it creates a command line interface tool on your system you answer just a few simple questions and beekeeper spins up aws services for a waiting room the main deliverable you get after deployment is that url that seal brewing can use to give to their customers for the event let's now take a look at the beekeeper infrastructure as i said the cli tool is going to programmatically one by one use the aws sdk to create each of these separate aws services and make them work together if you're not familiar with aws i'll try to walk through this in a way that will still make sense so we have that user they're going to click on that url that beekeeper generated and seal brewing put in their social media post that click is a get request for a resource that's handled by a route on the api gateway we call that the slash beekeeper route that route on the api gateway triggers a lambda we call these producer lambdas think of them as temporary functions with some logic that aws executes and scales up and tears down as needed to respond to traffic load so that producer land the logic runs and it does a couple things first it checks if the incoming request has a cookie belonging to us on it if it doesn't it generates a random token sets some custom response headers and sets a cookie with that token in it then the response is made in the form of a 302 redirect and sent back to the client which causes it to redirect to the waiting room under the hood the waiting room is just some static assets in something called an s3 bucket so why did we generate a random token and set a cookie a waiting room needs a way to track users so that it can decide when to forward them to the final destination there's a couple different ways we could attract users we opted for a token and a cookie okay so that's the first part of the story i'm gonna come back to it in a second but let's go back to that producer lambda the other thing that happened in that logic was the token got put in a queue remember that token represents a person so you can think of them now as being in line first come first served so to speak on the other side of that queue we have another grouping of one or more lambdas called consumer lambdas the consumer land is going to pull tokens off the queue at some preset rate decided by seal brewing so this rate of pulling tokens off the queue is how beekeeper effectively throttles the traffic flow it's worth noting we always have way fewer of these consumer lenders than producer lambdas and that makes sense if we stop to think about it it's because the producer of lambdas are the things that need to scale up to handle all that bursty traffic we call them producer lambdas because they're the ones doing all the things they're generating tokens they're putting them in the queue responding to requests the consumer land is in contrast we only need enough of them to do the work of pulling tokens off the queue at some much slower rate we call them consumer lambdas because they just consume tokens off the queue okay so after the consumer lambda pulls a token off it writes it to the dynamodb which is aws's version of a nosql database now to complete the story as i mentioned earlier we have to revisit the waiting room on our diagram here that front end code has some client-side javascript in it and it's sending ajax requests every 20 seconds to a different route on the api gateway we call that the slash polling route our cookie will be sent along with each of those ajax requests and the api gateway will access that cookie and pull the token off but instead of triggering a lambda this time this polling route will directly query the database to see if the token has been written there yet if it has api gateway responds back to the polling request with an object letting the front end know it should redirect to the final destination if not the polling continues and the user waits so hopefully you can generally understand how beekeeper works we have an infrastructure that can scale and handle a lot of traffic we put users in a queue and we only take them off that queue at a specific rate a rate that seal brewing gets to decide the users wait in a waiting room and the code keeps asking the question is it my turn yet and the answer to that question is either yes or no depending on whether that user's token has been written to a database okay finally before i hand things off to aaron for the demo i want to say one more thing about who beekeepers for what is the use case first we've established that it's for organizations with a one-off event causing bursty traffic and they're constrained in some way either their existing infrastructure can't scale or it's not a good roi because of time and effort and they want to retain control over the infrastructure of the waiting room so those are the prereqs now how much traffic can beekeeper actually handle justin's gonna explain how we arrive at these numbers a little bit later but from a hardware perspective beekeeper can handle 5500 requests per second that's quite a bit of traffic that's 300 000 people in just a single minute however the next thing to realize is that all the cute users in the waiting room are pulling the api gateway every 20 seconds and because of that if seal brewing begins to approach 200 000 cubed people in the waiting room they're starting to reach the limits of our current implementation so in summary beekeeper probably isn't for the largest of bursty traffic events like a sony playstation release but it's also not limited to just small events either it's a robust open source solution that does what it's set out to do now one of my teammates aaron is going to demo how all this works all right so the demo let's get started um getting started with beekeeper is really simple first thing that we need to do is install beekeeper as a global module with npm install g beekeeper cli installing it as a global module is important in order in order to enable the beekeeper command which allows you to create destroy and manipulate your virtual weight rates if you must install locally you'll need to run the beekeeper command with npx and once everything's been downloaded we just need to run beekeeper init which will walk us through six questions needed to set up the weight room uh the first piece of information needed will be a profile name since you can have multiple weight rooms this profile name differentiates your various weight rooms from each other and lets you choose which one to create modify or destroy i'm gonna go with tester the second piece of information is the name of your waiting room this will be publicly displayed on the online weight room so it's best to choose something that describes what the weight room is for such as the name of your company so that you know your customers know that they're in the right place if the name of your customer com company is seal brewing co choose something like seal brewing co black friday sale i'm gonna go with beekeeper example so that you know you are in the beekeeper example waiting for and third is the region that you would like to place your weight room in the location you choose can affect upload speed and customer latency and a common region is u.s east one so i'm going to go with that in north virginia us eats one fourth is the url that you want to direct people after they go through the weight room i'm gonna put in beekeeper cli github io now the fifth piece of information is the max amount of users that you want to allow into your website per minute the ideal amount really differs greatly between websites and it's best to find your ideal traffic through load testing and tracking analytics beekeeper handles a minimum of 10 and goes to the thousands so i'm going to set mine at 500 per minute and the last piece of information needed is if you want to enable dynamic rate throttling which will automatically reduce the amount of users going to your site if it detects that your site is slowing down from too many concurrent users and it's wide for yes after providing all the needed information you will be given a command to deploy your customized weight room and that is beekeeper deploy tester um remember tester was the profile name that i gave to this weight room and hit enter now after entering the deploy command beekeeper creates a master role creates an s3 bucket which contains all of the necessary weight room html css js other assets sets up the simple queue service also known as sqs which will contain cookie data for everyone in the weight room and the command also sets up a dynamodb database api gateway a few lambdas and connects them all together with all the required permissions now once it's done you're given two urls the waiting room url and the client check endpoint the waiting room url is what you pride provide to your customers and put in your promotional emails and materials now over here is the client check endpoint which can optionally be optionally be added to your own back-end to prevent people from skipping the queue i'm going to take the waiting room url and paste that into my browser now when i hit enter a lambda will intercept the request issue a cookie send the cookie value to the simple q service and redirect me to the weight room now in the weight room there are a few things to notice first is the estimated time for you to enter the site this is calculated by the lambda querying the simple q service for the estimated amount of messages combines that with the user throughput rate per minute which is provided with the init command and calculates the estimated wait time over here is a countdown timer every 20 seconds an ajax request is sent to a route on the api gateway which queries the database for your cookie value which then returns an object that contains either a truthy or falsie value and may also contain the final endpoint if your cookie value was found in the database you are rerouted to the final destination even though i was the only one in the weight room i was not redirected after the first polling cycle this was because behind the scenes there is a lambda that can only be triggered every 60 seconds this lambda retrieves messages from the simple q service and populates the database with the cookie values and you can only move past the weight room once your cookie exists in the database and now that i've gotten through the weight room i'm free to do whatever i came to this website for now let's say that the traffic traffic flood has ended your site can now handle the amount of traffic and you don't need the weight room anymore but you still have those promotional links scattered across the internet and you don't want these links leading people to a non-existent page you have the option of turning off the weight room and we can do that with beekeeper off tester just wait there it is the success message lets us know that the weight room is now turned off and after turning off the weight room anyone who visits the original link will be sent directly to the endpoint to demonstrate i'll take the original url and paste that back into the browser as you can see i was sent directly to the final endpoint if you are completely done tear down and clean up is really simple just issue the destroy command beekeeper destroy tester and all of the beekeeper aws components roles and permissions will be removed this can save a lot of headaches especially if you have multiple apps running on aws also note that the previously provided weight room links will no longer work uh if you want to create more weight rooms all it takes is another beekeeper init command and if you forget which weight room accounts you currently have enter beekeeper config now as you can see a beekeeper weight room is set up really quickly in just a few minutes and cleans up in a few seconds and that's it for the demo in this next section i'm going to talk through some of our major design decisions and take a deeper dive into how each component of the beekeeper infrastructure works so when we first set out designing beekeeper we started by building out a diagram for a simple weight room using a lambda edge function deployed on a cloudfront distribution and cloudfront is amazon's content delivery network it works by placing infrastructure elements geographically closer to a user to reduce latency this first animation demonstrates users sending requests to a globally distributed system the benefits of this infrastructure are it was simple it was fast and it was globally distributed but it became difficult to add all the features we wanted and while functional this infrastructure did not quite align with other weight room products we also began to question the need for a globally distributed system this second animation shows a regional distribution if our target audience are clients with limited infrastructure serving one-off local events such as concert sales or vaccination appointments did we really need to use cloudfront or would it make more sense to deploy the beekeeper infrastructure in a specific region we moved towards a more local solution and traded our cloudfront distribution out for an api gateway an api gateway acts as a front door to our backend services and can be thought of as a beefy reverse proxy the gateway proved a valuable tool for a number of reasons it has a very high capacity which allowed us to use it as a central hub for all of our traffic it also gives us control over requests and response formats and this control allowed us to abstract route handling logic away from our lambdas and use the gateway as a direct proxy to other services by doing this we were able to both make our lambda pool more homogeneous and simplify our overall infrastructure so the beekeeper gateway uses three different routes the main route is a proxy to the producer lambdas and the url for this route is a link that is given to users to direct them to the waiting room accessing this route will invoke a lambda and either direct the user to the waiting room if the infrastructure is turned on or to the end point if the infrastructure is turned off the other two routes are proxy routes for a database the routes are similar but each returns data in a different format based on preset mappings one route is used internally to pull the database for a user status in the queue and the other is given to the client as a means to verify a user has passed through the queue so now that we have these routes set up we needed a way of tracking users in order to determine if it was their turn to be redirected to the end point and once at that end point if they'd actually passed through the queue so one potential option was to use cookies to track the user and in this case a token is created when the user first visits the beekeeper beekeeper promotional link and that token is sent to the queue for processing but it's also stored in a user's browser in the form of a cookie the weight room then uses client-side javascript to check if it's the user's term and it does this by reading the cookie and then the api gateway takes the token stored within that cookie and uses it to query the dynamodb if it's the user's turn the dynamodbu would return a truthy value and the endpoint url this cookie approach worked but it meant we had to perform a cross-origin request between our weight room site and the api gateway and this is because our s3 bucket and our api gateway are deployed on separate origins by default cross-origin requests are typically prohibited by the same origin policy unless specific headers are present so making a request without the required credentials would result in a cross-origin resource sharing error or a course error so by default in cross-site xhr or fetch invocations browsers will not send the required credentials so to include cookies in a request a special flag or object must be included and when responding to a credentialed request the server must specify an origin in the value of the header instead of specifying a more general asterisk wildcard cookies that are set and transferred across origin are commonly referred to as third-party cookies historically third-party cookies have been used by advertisers to track users across different sites and there's a push by many browsers to do away with third-party cookies and across site tracking a browser that disables cross-site tracking would break this approach to tracking users and break the beekeeper weight room a different approach to tracking users might be to use query strings instead of setting the token in a cookie the token would be exposed as a query string parameter in a user's url the token would then be visible in the url string on the weight room site and could easily be retrieved and passed along using client-side javascript this mode of tracking can be augmented further by recording a user's ip address and by doing this we could cross-reference a token against an ip address or vice versa and comparing these two approaches the pros of using cookies are they're hidden from the user they're difficult to transfer between users and cookies are stored by a browser and retained by a browser until the browser's cookies are cleared so overall using cookies is a secure option but cookies can be difficult in that they expose this course issue and that requires specific headers to transfer furthermore safari by default is beginning to block cross-site tracking and chrome is said to follow suit soon this cross-site tracking can be manually enabled but for a novice user just entering the weight room it might be a tall ask using query strings solves our cores issue but it's not without its own downsides query strings are exposed to the user and are therefore less secure they can be easily manipulated or transferred between users and if a browser window is closed the query string will be lost and a user would need to get a new token and be sent to the back of the line so these security issues could be mitigated by using a cross reference such as an ip address but ip addresses also pose an issue because they are neither static nor unique ip addresses can change based on a user's location in the case of a mobile connection or if they renew their client lease ip addresses are also shared by users on the same network and ultimately we ended up settling on cookies because of their better security and because the cores or cross-site tracking issue could be mitigated by making a dns change in placing our aws resources on the same domain so now that we have our tracking in place we needed a way to store a large number of user tokens and process them in a first come first served manner a message queue seemed the natural approach for this and is how many other weight room services pitch their product i.e as a queue a message queue is a place to store messages until some consumer retrieves them the aws simple queue service can receive many messages and those messages are stored until they are received sorry retrieved at some specified rate by our consumer lambda and there are two types of cues provided by aws the standard queue and the fifo queue fifo or first in first out queues provide guarantees that ostensibly fit our use case they provide in-order processing they only deliver a message once and they have relatively high throughput the fifo queue does have some limitations though there is a max for the throughput a fifo q can provide and that max is around 300 messages for 3 000 messages per second our initial aim was for our infrastructure to be able to handle a load greater than this so by choosing a fifo queue we would immediately implement a bottleneck in our infrastructure a standard cube by comparison has best effort ordering and possible duplication the upside of the standard cue is that it has near unlimited throughput so we could be guaranteed that the bottleneck of our system was not our cue so we decided to implement a standard queue because of this greater capacity a few messages arriving out of order was not an issue in our mind since it was unlikely to detract from the user experience or the effectiveness of our infrastructure and tokens arriving in duplicate were also not an issue because our right events are item potent that is they can be written multiple times without changing the end result that's all for me and next justin will take over and discuss our design decisions around lambdas how we use lambdas to set a rate and how we load tested our infrastructure sweet uh thank you ian let's start off by talking about how we decided to implement beekeeper's throttling mechanism so in order to limit the number of users allowed to enter the final destination we implemented a cron job which is basically a program that runs on a schedule and we have that set up to run every 60 seconds the reason why is because aws does not allow a frequency of less than a minute so in order to run a crown job in the cloud we need to configure a lambda to self-trigger on an automated schedule based on a cloud watch event rule and for those that aren't quite sure what a cloud watch event rule is it's basically a way for you to trigger events based on the schedule you set our initial approach to throttling looks like this every 60 seconds cloudwatch sends an event which triggers the consumer lambda to execute and that lambda checks the sqs for 10 messages it will then write those messages to your dynamodb and then delete them from the sqs it's 10 messages only because sqs only allows at most batches of 10 to be consumed at once to scale this approach we looked into adding more consumer lambdas remember what i said in the previous slide about how the consumer land is they execute once every 60 seconds and then gets 10 messages from the sqs then writes it to dynamodb and then deletes the messages from the sqs we can actually increase that rate by adding more consumer lenders so for every consumer alarm that we add we can increase the rate by 10 every 60 seconds and to put this in a simple example three consumer lambdas can get dirty messages every 60 seconds but the problem here is that it takes away the number of producer lambdas we can use this means that when we add a consumer lambda that is one less producer line that we can use and this is an issue because every aws account has a regional pool of 1000 lambda concurrency limit and you probably guessed by now to allow a larger number of users into the final destination every minute we would need to add more consumer lambdas so if 100 consumer lambda is used that's going to be 900 left for the producer lambdas and just a quick note before i move on the 1000 lambda concurrency limit it's just a soft limit you can raise it by calling amazon i remember this beekeeper infrastructure diagram you know the one my teammate ryan showed in the earlier slide this diagram shows an api gateway as an entry point to the waiting room and sitting right behind it are the producer lambdas these lambdas they get triggered every time a visitor visits the waiting room endpoint on the api gateway this means the producer lambdas they're used to handle incoming traffic and so these lambdas they need to be able to scale as traffic grows and when you don't have enough producer lambdas to handle that incoming traffic those actual requests coming in those are going to throttle we were able to accurately send visitors to the final destination with this approach but it was not scalable and because of that we looked into another approach the second approach we looked into involves having the consumer lambdas run a for loop what happens is we run a for loop within the consumer lambda and every loop grabs 10 messages from the sqs writes it to dynamodb and then deletes them from the sqs and we also give the consumer lambda 60 seconds to complete executing our loops to determine the number of times the consumer lambda can loop is based on the cli rate the user sets this rate is then divided by 10 and it's 10 because it's the max number of messages allowed to get from the sqs at one time we also did a load test and found out they took on average 322 milliseconds for a single loop to finish and with that number we estimated a single consumer lambda can run up to 180 loops which equates to about 1 800 requests per minute what this means is that a single consumer lambda using the loop approach is almost equivalent to running 180 consumer lambdas every 60 seconds now say for example if steel brewing company wants to allow 3 000 users to the final destination every minute then only two consumer lambdas will be used instead of 300. we found this approach to be much more efficient but it did create issues if the cli rate is set higher than what a single consumer llama can handle in 60 seconds then there would be unexpected behaviors this approach was scalable but not accurate and because of this we explored another approach for our third and final approach we introduced a new lambda called the trigger lambda every 60 seconds cloudwatch sends an event which triggers the trigger lambda to execute the trigger lambda is responsible for calculating the number of consumer lambdas that needs to be executed it also tells them how many for loops they need to run and then invokes them so say for example if steel brewing company selects a rate of 3 300 requests per minute the trigger lambda will invoke a total of four consumer lambdas three of them will complete 100 loops each and the fourth one will complete the remaining dirty loops the consumer lambdas run run on a for loop and with every loop checks the sqs for 10 messages writes them to dynamodb and then deletes them from the sqs this approach was both scalable and highly accurate and so we decided to implement it in addition to this throttling mechanism we also implemented a dynamic rate throttling feature and it's an optional feature that the cli user can enable so how it works is that when the beekeeper's infrastructure is deployed and a cli user selected to enable dynamic rate throttling then a special lambda will also be deployed as part of that infrastructure that special lambda is called the dynamic rate throttling lambda this lambda is configured to run every 60 seconds and it pings the final destination to measure the response time the first ping is recorded as a baseline latency this latency value it's stored into the dynamodb so that all subsequent pings to the final destination can be compared against this and based on the result of that comparison it can determine how it should adjust the throttling rate of the consumer lambdas now see if a lot of users are hitting the final destination endpoint and let's say it also increased the latency time then what the dynamic rate throughout the lambda will do is reduce the rate of the consumer lambdas so that less visitors are allowed into the final destination another thing is we don't want to be making too fast of a change to a rate in any direction and we also don't want it to be zero or increase to some unsafe level so what we did was build our dynamic rate throughout long lambdas with some logic that decreases the amount it can change as it moves farther in one direction it also takes a slower approach to increasing the rate after a throttle than it does performing throttle now that we're done talking about throttling let's talk about beekeeper's infrastructure capacity we'll look into how we load tested beekeepers infrastructure some of aws limitations and some of the bottlenecks we identified from the low test we decided to load test using artillery and artillery is a load testing tool that we run on our own machine from the results of a load test we were able to find our system limits the rear load test we found that the producer lambda has an average total duration time of 160 milliseconds and just to be clear this duration time factors in both the lambda code start time and the invocation time now that piece of data we were able to estimate that the consumer lambdas can handle up to 6250 requests per second and these estimates led us to believe that the bottom neck of our system is the producer lambdas and s3 bucket again just based on our load testing results we were able to extrapolate that the producer lambdas can handle up to 6250 requests per second and the s3 bucket has a 5500 get requests per second limit imposed by aws if the number of visitors to the api gateway exceeds 6250 rps then both the producer lambda and the s3 bucket will be throttled there is a way to unblock the producer lambda bottleneck and you just got to call amazon to raise your lambda concurrency limit as for the s3 bucket bottleneck we can distribute the request and s3 objects to multiple prefixes another potential bottleneck i want to quickly mention involves the api gateway in one of the earlier slides ryan briefly talked about how our hardware limitation is 5500 requests per second which is equivalent to having 300 000 visitors in the waiting room every minute depending on how often the visitors in the waiting room pulls the api gateway the api gateway can potentially be a bottleneck because it has a limit of 10 000 requests per second imposed by aws there's also some features we'd like to implement in the future if possible something like a branded waiting room experience where we allow organizations to upload their own logo and static assets to the waiting room we can also allow them to set up their own custom domains through something like aws route 53 this would uh this would actually create some tertiary benefits as well like eliminating the complications of course requests you know specifically the part being discussed earlier by production that's where we protect the waiting room url from getting spammed with locust requests which can potentially cause long wait times for genuine visitors we can probably implement this fairly easily um aws has a service called web application firewall so what it does is it allows you to set rules like ip throttling and we can also enable websockets right now beekeeper pulls the dynamodb from the waiting room so we can check if the visitor is allowed to be to be redirected to the final destination this polling is actually done every 20 seconds so even if it's not the visitor's turn yet we're still going to make a request to find out those requests and response cycles they can be pretty wasteful so instead of doing that we can implement websockets and what that does is it maintains an open connection to the waiting room and with this open connection we can now push information through it doing it this way might actually be more efficient because it's it's just a single requested response cycle to establish that initial connection but we still have to keep in mind the number of potentially open connections because it can be expensive to maintain it um thank you all for watching our presentation today uh here are the folks that built beekeeper we're now open for q a if you have any questions feel free to drop them in the chat okay so we have a question from nicolas was the architecture you ended up with the same architecture you originally envisioned were there any major changes you had to make to the system you had in mind as you went through the development process um i think i could take this one thanks nick for the question uh the first one was a bit different uh it was an s3 bucket for the waiting room static assets um and that s3 bucket was just was on a cloud front distribution so um that means that the url that people would go to first wouldn't be the api gateway would be that url representing the static assets in that s3 bucket so that would be an edge optimized version of beekeeper right so the waiting room is now existing in all sorts of different regions across the world um after we moved away from that to our current you know infrastructure uh i'd say very very close so we got pretty close to what we have now right after that um several iterations of the way we throttled the threat that throttling mechanism via the consumer lenders you know we went through some processes there you saw three of them right the first approach it was super accurate but it didn't scale the second approach uh fixed that it scaled but then we discovered it wasn't accurate at high load and then the third one uh had you know the best of both worlds it was accurate and it scaled okay next question how was your teamwork experience i can answer that one um i think we worked really well as a team uh we didn't have any personal conflicts and the the entire project seemed to go quite smoothly um we ended up pair programming uh pair programming for most of this project i think it wasn't until the end where we had some you know kind of little pet projects or you know loose ends that people went off on their own but the core of our whole cli and everything was done pair programming okay next question do you guys have any war stories of interesting bugs or problems you solved along the way i don't know if you guys have any i mean there's tons of stuff like that though um tons of stuff we're just knocking your heads together like why is this not working you know what are we not what are we missing what don't we know um and yeah sometimes so you know at the beginning we had some issues with like cores right and these these strange issues with cookies and stuff um and there's some you know special things that you have to do and you just have to know those things and once you do those things things work right and then there's other like really difficult bugs that are sometimes you know for example just amazon related and for no reason at all you cannot figure out why they're happening they just are happening uh you guys probably remember the fact when we were trying to change some code um in one of the services like amazon just wasn't even picking up the text change that we were inputting sometimes right it would grab the first three letters and we had to like wait long enough for after making the change so it would see everything i mean that that one was particularly ridiculous i don't know if you guys have any others uh that come to mind i think also um you know getting the config configuration details of the existing infrastructure that we deployed using the aws console so that we can programmatically deploy it using the sdk you know that that was kind of hard to begin with but i think um everyone figured out how to do it using the cli tool um yeah i guess that that one specifically has to deal with uh the aws um javascript node development kit or software development kit where they they expect you to input a large amount of various information and it needs to be in a specific format but they don't show you what that format is so the only option that you can do is to first build it on the aws console and then pull everything down with the cli and then figure out where those pieces go um so that was fun okay graham is asking love the presentation and great diagrams for choosing a standard queue you mentioned that the trade-off for scalability was best effort ordering in situations where a retailer may be sending a limited stock item customers could essentially skip a few spots even if unintentional on their part and end up buying the last of the product with the traffic that beekeeper can handle is there a statistically significant impact of sqs sending customers out of order yeah i love the question graham um so you're just taking note of the fact that yes since we went with the non-strict fifo cube right the regular queue that has all that unlimited throughput the trade-off is we might get some duplicates we might get some out of orderness and your site you the example you have here is that you know that might matter in some situations like this retailer um that has limited stock um but i so when amazon says best effort ordering it's really hard to figure out what that means we looked everywhere you know we had a hard time figuring that out we suspect it only happens you know logically naturally under like very high load right for them to start tossing an occasional message in out of order or a duplicate right um so i think in your example you know the small retailer that's having an event and you know limited you know kind of quantity i think it's probably not even going to be an issue you're probably best effort ordering is probably going to mean exactly uh in order for that situation that would be my guess but you know potentially theoretically yeah that could happen i guess i don't know if anyone has anything else they want to add but that's my take on that um yeah i think that's a great answer uh yeah amazon ditto that amazon is is quite vague on what best effort ordering means um or or how exactly you would have messages coming out of order um likely user latency or um you know other external factors are going to have a greater influence than the queue itself that's a super good answer i just want to like make that more clear like if you have someone from chicago i forgot about that if you have someone from chicago and someone from la right um and you know the retailers in chicago they deployed the beekeeper infrastructure you know and usc's two um right what exactly is fifo right you could have someone in la that technically clicks on the link first but because of their latency just network latency that we can't do anything about um they're gonna get there later than the chicago person right and we suspect that you know when you're talking about user latency like that chicago la i mean you're talking about um you know hundreds of milliseconds right and we suspect that the best ordering issues with amazon are really single digit milliseconds if not microseconds um so really you know um really difficult to have a strictly fifo q when you're dealing with people coming from different geographies and that will probably swap overwhelm uh the the considerations that amazon's uh introducing via their best effort ordering okay that was the last question any final words thank you for joining us today yeah thank you everyone for coming thank you you 