all right um let's get started hi everyone thank you for joining us today my name's katherine and my teammates are leah rick and adam we've spent the past few months building tapestry as a fully remote team spread out over the united states so what is tapestry we are an open source data pipeline orchestration framework for user entity data with tapestry your user data pipeline will be automatically deployed and ready to ingest data from all your user data sources store and transform that data in a data warehouse and sync that data back to your data tools for immediate use but why would a business need a pipeline to move user data between different tools and if you did need a data pipeline why would you need an orchestration framework and what even is user data perhaps the best way to discuss these topics is by diving into a simple use case let's start with a hypothetical rain shield is a company in the umbrella space with a thriving ecommerce store that is quickly gaining traction amongst a larger user base this has attracted the interest of investors and they've raised funding that has allowed them to expand the small team that started rain shield is no longer able to wear all hats and they are beginning to staff larger departments such as sales marketing and customer support to manage the influx of new business they're experiencing as the business evolves rainshield begins to utilize various sas tools in order to engage with their user base in new ways and to meet the daily operational needs of different departments for example they began to use stripe to handle all of their online transactions the company's sales team has started using salesforce to organize and track their leads and the rainfield customer support team is now incorporating zendesk to help with managing all of the support tickets that are now being generated the marketing team is even planning on on hosting a zoom webinar on a design your own umbrella product that they're about to unveil soon prior to this growth user data was largely managed in the production database however these teams are not only requiring different views of this data to accomplish their goals but are also creating new sources of user data as they interact with customers through a variety of platforms and tools while these third-party sas tools cater to the needs of each department well user data is beginning to proliferate the organization in terms of both the data produced and collected data is becoming scattered across the different tools each team is using and these tools were not designed with integration in mind it's becoming more challenging to have a unified understanding of a single user and how they're interacting with rain shield and its product each tool has access to only a portion of the customer's information but not the whole picture and these sas tools that have increased productivity have now become data silos data goes in but it doesn't come out the industry agrees that this is a big challenge colin zema the chief analytics officer at looker says the big fear that i always have is that people are using more and more of these sas tools the average company has a ton of sas tools and all of them have data locked in so salesforce has some of your sales data but slack has chat data and you've got all these systems that are holding data sets take for example susie a rain shield customer according to her profile in salesforce susie has purchased six umbrellas for her friends and family however zendesk indicated that she called two times complaining about the color of some of her umbrellas susie is excited about picking her own umbrella color and is signed up to attend the zoom webinar unveiling the new design your own product when this data about susie lives in different tools it becomes difficult to access a composite picture of susie now we've only mentioned a few different tools that rain shield uses that store user entity data we've also only talked about one customer susie but it's easy to imagine that rain shield could use many more tools that capture different pieces of user data and that they might have thousands of different customers buying their umbrellas and what if there were more users just like susie who've bought at least a few umbrellas called customer support about their umbrella color and have signed up for the webinar when all this data can be collected in one place rangefield can begin to see patterns among their users and act on these insights and when you think about how important it is to have a complete picture of your users you realize just how critical it is for a company to integrate user data from every source in order to operate effectively so how can rangefield put all these pieces of the pi together for all of their customers and even if we are able to gather this user data for analysis in one place how are we going to use these insights to impact day-to-day operations let's take a look back at our company rain shield let's say rain shield would like to use what they know about susie and other customers that match her profile in an attempt to increase sales they believe that this group of users will be especially interested in the new umbrella colors that rain shield just ruled out and they would like to prompt these customers with a custom chat message via intercom the next time they log on but before this desired action can take place rain shield still needs to provide intercom with this specific list of customers in other words making insights actionable still requires work rain shield needs to map relevant user data to intercom in the particular format that intercom requires this process could be thought of as data syncing is there a way rain shield can automate the syncing of data so let's take a step back and recap the obstacles that rain shield and companies like it are facing important user data is being trapped in silos as the quantity of rain shield sas tools increases they would like to aggregate data from these disparate third-party sources into one location for better analysis additionally they would like to sync relevant data to other third-party destinations to drive operations based on their findings but what are the challenges of this type of data integration user data stored in sas applications is similar in structure to the data we see in traditional relational databases however unlike relational databases data and sas applications cannot be accessed with a simple query instead this data must be retrieved via unique rest apis making it difficult to determine how to communicate with each tool furthermore factors such as limited documentation rate limits on api requests managing potential network errors and ever changing api schemas can make transporting large amounts of data a challenging and slow process so how can rain shield navigate these challenges and eliminate these data silos they have four main options for arriving at the desired outcome one they could manually move files between tools two they could use pre-built connectors three they could create their own custom connectors or four they could build a complete data pipeline we'll discuss each of these in turn let's talk about the first option manually moving files between tools let's say that rainshield wanted to make sure that salesforce had all of the contacts from the zoom webinar revealing the new design your own umbrella they could simply export the list of webinar attendees from zoom to a csv file and then import that file into salesforce but this might result in duplicate data and could become tedious if you have to do this task often another possibility would be to use a company that creates these connectors for you like zapier rainshield can give them some information about their zoom and salesforce accounts and they set up the flow of data between them for you using pre-built connectors this however would not allow much flexibility regarding which parts of the data would be shared between the two apps you might still end up with duplicate data and you may not find all the apps you need to integrate together on their menu another type of pre-built connection sometimes exists in certain tools settings for example zoom has a way to integrate directly with salesforce by simply configuring your settings to export your data this isn't always the case though and more than likely rain shield will not find every tool it uses in every other tool's configuration settings to sync up the third course of action is that rachel could designate one or two software engineers to begin building custom connections to pipe data directly into all the tools that they use the benefit of this option is that you can flexibly choose what data to extract and sync however not only would these engineers have to research these tools apis to extract data but also to sync as well this might not be too bad if the number of tools the company used was very small for example if rain shield only needed to connect zoom and sales force with mailchimp then they might only have to write six connectors to ensure they all share the same data you can imagine each of these lines representing a pair of connectors for each individual tool however if your company already uses several tools or plans on growing in the future this can quickly get out of hand and that's to say nothing of the fact that these connectors would also have to be maintained if one api changed it would need to be altered for all the other tools trying to use it and the reality is that even small companies use anywhere between 10 to 50 tools that would require a lot of valuable engineering time that could be better spent on core product development perhaps the best and most complete solution is our last option implementing an end-to-end user data pipeline with a cloud data warehouse at the center from an engineering perspective this approach allows access to data ingestion and syncing tools that remove the headache of working with third-party apis while also providing you with the flexibility that custom connectors offer i.e you choose exactly what data to send placing a cloud warehouse at the center of your pipeline also provides several other benefits cloud data warehouses allow for the storage of large amounts of data with very little infrastructure management and instant scalability warehouses may also serve as a single source of truth if two departments ever had conflicting data additionally data modeling and analytics tools can be built on top of the warehouse and can aid in making important business intelligence decisions lastly the warehouse offers the ability to combine and filter data from multiple sources in order to sync with another destination this warehouse centric pipeline helps aggregate all of your data into one accessible place so you can create unified models and sync them to the tools your teams need businesses can rely on a single pipeline for both analytics and operations to better understand and leverage user data here's what the ceo of rudderstack thinks of this approach you should not have point-to-point integrations you should be bringing everything to the warehouse and then pushing it out to the destinations that is the way it should be hand it over to leah now to discuss some existing solutions for creating a user data pipeline thank you catherine when deploying this type of user data pipeline the two primary options are to use a proprietary hosted solution such as the one offered by the company rudder stack or to use open source tools to configure your own pipeline the diy option does allow for the inclusion of fully open source tools while runner stack only offers open source event streaming one other point in favor of the self-hosted solution is that it grants you full ownership over your pipeline infrastructure which means you can customize it any way you like on the other hand ruddercyc requires you to use their infrastructure leaving you with little control one benefit of rudderstack is that it has lots of out-of-the-box features whereas a self-hosted solution has none but one other advantage that runnerstech has over a do-it-yourself approach is a big one ruddercyc abstracts away infrastructure provisioning and management making it extremely easy to deploy a user data pipeline quickly in contrast building your own pipeline requires an extraordinary amount of time and effort to set up provision and configure you need to make many different decisions about which tools to use for data ingestion and syncing and which warehouse to select and that's not even mentioning all that goes into provisioning and maintaining pipeline infrastructure to say the least this is an extremely complex process this is why we built tapestry we weave together all the necessary resources to create an end-to-end user data pipeline automate the setup and configuration and let you spend your valuable time doing something more important taper street is for engineers who want full control over their data infrastructure but without having to provision that infrastructure for themselves tapestry is a completely open source framework that automates the entire pipeline deployment process we we do not however have very many out of the box features so if you were thinking about rolling your own self-hosted solution but wanted to simplify the deployment process we might be able to help tapestry automates many steps and creates a number of resources for each phase of the pipeline as you can see deploying your own user data pipeline would require at least 71 steps in the provisioning of 49 resources between both aws and the data warehouse let's see tapestry in action by walking through a demo getting started with tapestry is pretty simple if if you're the engineer you would first need node and npm also installed as tapestry is a node package additionally since tapestry is specifically configured to provision a number of aws resources an aws account along with the a along with the aws cli installed on your machine are both required finally users will also need to have a docker account and have it installed on their machines after these preliminary steps all you need to do is run mpm install tapestry pipeline with a global flag and a host of commands will be provided to you as a new user the first chemistry command that you would run is tapestry init with init you give your project a name and tapestry will provision a project folder along with an aws cloud formation template which allows you to provision and configure aws resources with code this template in particular is used to provision resources for the data ingestion phase of the pipeline what tapestry does thus provide for the sinking phase of your pipeline is dependent upon which command you run next you have a choice between between our deploy and kickstart commands once you make your selection temperature then provides all of the necessary configuration files for the data syncing phase both commands automate the deployment of a fully operational pipeline but kickstart includes two pre-configured sources zoom and salesforce along with one destination mailchimp these pre-configured third-party tools set up your pipeline to have immediate end-to-end data flow beginning with data ingestion and ending with data sinking into these tools two to better show the full flow of data through our pipeline we'll demonstrate our kickstart command prior to execution you'll you will have to own or create accounts for zoom salesforce and mailchimp kickstart then begins by prompting you with a short series of questions about the previously mentioned accounts as well as snowflake tapestries data warehouse of choice after your information has been collected kickstart continues by creating the necessary databases and tables within your data warehouse to be utilized by both your ingestion and syncing tools let's take a peek under the hood at the actual infrastructure that this command is provisioning tapestry uses the cloudformation template supplied during the init command to create a cloud formation stack provisioning aws resources specifically related to your ingestion tool such as an s3 staging bucket an ec2 instance for the tool to run on and an application load balancer to route traffic to our ec2 instance the ingestion tool is then configured to extract certain data from your zoom and salesforce accounts and send it over to your warehouse you will then be asked to carry out a few steps so the data is transformed in your warehouse using a data model that tapestry gives you essentially the data will be aggregated from both sources into one table filter for duplicates and appropriately formatted to be synced to mailchimp to complete the pipeline kickstart creates another cloudformation stack this time spinning up various aws resources for your syncing tool such as an elastic container services cluster which is used to properly run your syncing tools application and another application load balancer to route network traffic to your cluster if you are deploying a new pipeline tabistry automatically launches your very own local tapestry dashboard this dashboard contains documentation for how to use tapestry along with various pages for each section of your pipeline each page displays metrics they give you better insight into the health of each component they also include links to the uis of all the tools being used at each stage of the pipeline now let's actually see data flow through the entire pipeline with with an example let's use a pipeline we just set up with kickstart to grab our zoom zoom webinar registrants well also along with our salesforce contacts and then sync them into our email marketing tool mailchimp our goal in this example is to extract data from both zoom and salesforce and push it to our data warehouse snowflake from there we want to combine the webinar registrants from zoom and our salesforce contacts into one single list and filter out any duplicates along the way finally we want to send this new complete list over to mailchimp as you can see from this table we have several different people who have registered for a zoom webinar a particular note are diana prince barry allen and betty rubble as indicated by the green box betty rubble is a unique contact only found in zoom while the red boxes around diana and barry indicate contacts that are found in both zoom and salesforce our goal is to get all three of these entries over to mailchimp with only one entry each for diana and barry now in salesforce we have a different list of customers and in the yellow box there is a contact who is unique to salesforce jack rogers and you'll you'll remember diana and bartholomew also known as barry from a previous zoom list even though barry is going by a different first name in salesforce his name is the same across both sources or his email is the same across both sources allowing us to uniquely identify him this means that when we combine the two lists we can use the email address field as a unique key to to eliminate record duplication once the data from both zoom and salesforce have made it into our snowflake warehouse we can transform the data by combining both lists and removing duplicate entries you can see the two tables highlighted in purple the tapestry webinar registrants table has all of our zoom data the tapestry contact table has all of our salesforce data the email model table that is highlighted in blue is the newly transformed table that we will sync to our mailchimp account finally here in mailchimp we see that we've successfully synced all of our zoom webinar registrants and salesforce contacts and made sure that there is only one entry for barry and diana you can also see that our unique users betty and jack made it over as well now that we know how tapestry works let's take it over to my teammate rick to explain how he built it thanks leah before diving into the specifics of tapestries architecture let's quickly revisit the three phases of our pipeline ingestion storage and transformation and sinking the ingestion phase is where data is extracted from various sources and moves into a data warehouse once in the warehouse this raw data is then stored and is available to manipulate or transform in any way needed often transformation is needed at this step so that the data can match the schema of the final destination finally the last phase is syncing this data into external tools that can then perform designated actions in the upcoming sessions we will walk through each of these three phases in tapestry's corresponding architecture and as we do we will consider these three questions first what is required of this phase next what tools should be used for this phase and why and finally how will this part of the pipeline be deployed and automated let's start with data ingestion an effective data extraction tool will contain and manage a library of api connectors specific to each source this management of connectors extracts away the maintenance required to grab data from ever-changing api endpoints in addition this tool should allow for scheduling data extraction and keeping track of global state so that only new data is pulled now in order to make the decision regarding data ingestion it's important to consider the path by which the data travels in the past storing data was an expensive endeavor this made it more cost effective to perform any sort of data transformations before loading data into a database or warehouse to reduce the amount of data being stored this approach is known as extract transform load commonly referred to as etl however with the advent of cloud data warehouses costs of storing data have decreased dramatically this makes it more feasible to store all of your user data as raw data and to perform any transformations at the warehouse level to fit a variety of analytic and operational needs and since transformations aren't required first data can be loaded extremely fast this approach is known as extract load transform or elt and since it was vital for l pipeline to have access to all of the raw data we chose to go with an elt solution while many data ingestion tools are available like 5 train stitch and multana we ultimately went with airbite we liked that it was open source had standardized api connectors a robust ui and strong community support using airbite we're able to extract raw data from many third-party tools through its library of managed api wrappers covering the e and l of elt both the airbyte application itself as well as each of its connectors all run on their own individual docker containers so everybody provides the docker image to deploy their application and we configured our warehouse as a destination for airbite via a series of api calls so essentially the main applications container needs to be able to generate new containers as users set up more and more connections due to this necessity of a docker container needing the ability to create other docker containers therabyte recommends the use of aws ec2 instance as a virtual private server for hosting and while we prefer to use a container for a container orchestration service to horizontally scale the computing resources used by each container an ec2 instance still allows for vertical scaling of the entire instance finally placing a load balancer in front of airbyte means traffic cannot reach the ec2 instance directly network traffic must first pass through the load balancer before it's routed to the airby distance this allows us to take advantage of additional security measures and keep the ip address of the actual instance hidden this keeps the instance safe from any port scanning attacks and also takes advantage of aws built-in protection from ddos attacks while prototyping the ingestion part of our pipeline we encountered an interesting challenge initially all of our zoom data was being extracted and loaded into our warehouse without any issues but our salesforce data was consistently timing out eventually leading to a 504 server error in aws we saw that our error byte ec2 instance was becoming an unhealthy target for our application load balancer every time we attempted a sync with salesforce so we dug a little further and found ourselves in the aws monitoring dashboard searching for clues there we noticed that our cpu usage was through the roof this led us to believe that the size of our ec2 instance may be the root of the problem so we vertically scaled our instance and our error message disappeared this also led to our decision to include a cpu usage section in tapestry dashboard to monitor our aws resources as shown before in our demo now the next phase of our data pipeline is data storage and transformation we'll address data storage first we've already determined that at the center of our pipeline should sit a warehouse that is capable of handling large amounts of data from a variety of sources given our decision to host our tools on aws services a warehouse that could be seamlessly integrated with aws is preferable while there are many options for a data warehouse we chose snowflake snowflake can be built on most major cloud platforms providing valuable flexibility and snowflake also separates storage needs from query processing also known as compute this allows companies to take advantage of cost savings as well as enable us to scale those responsibilities independently and finally snowflake abstracts away the provisioning and maintenance of all the necessary resources for a cloud data warehouse initially we attempted to load data directly into snowflake from third-party tools but we found the data transfer to be particularly slow this led us to investigate using a staging area with snowflake and how this impacts data loading without this staging area airbite can only insert one row of data at a time into snowflake requiring numerous sql insert commands to copy over an entire table with the addition of a staging area airbite can achieve efficient bulk data loading to implement this staging area we provisioned an amazon s3 staging bucket between our airbite instance and our snowflake data warehouse one interesting challenge we encountered when working with snowflake was how to run sql commands for a new user once you have their credentials we needed to run a sequence of 34 commands in order to set up the warehouse with the necessary databases schema roles and permissions for the tapestry pipeline to operate we found that snowflake provides a node.js sdk which would allow us to communicate with the snowflake warehouse since the sdk only allows one sql statement to be executed at a time we needed to sort each statement as an array element and then iterate through each one individually now that we can execute each sql statement individually we ran into another problem because the sdk is callback based and runs code asynchronously to visualize this problem we added a log statement with the array index and you can see here how the statements are being executed out of order the error logs indicated that snowflake was attempting to create databases and tables before the permission and role statements have finished executed one option for solving this might be to nest each callback so that the statements are executed in the correct order however this would result in unreadable code that would be difficult to manage instead we opted to use the snowflake promise library this library is a wrapper for the snowflake sdk that provides a promise-based api this made it possible to use async await to handle multiple promises in a synchronous fashion now as you can see the logs indicate the statements executed in the right order now let's talk about the other half of the states data transformation a data transformation tool should be flexible so you can transform data to meet a variety of analytical and operational needs ideally we would like a sql-based data transformation tool that can be utilized by non-developers to create data models based on the warehouse and put that data into action more quickly finally we would like a tool that maintains a history of our data transformations documentation about existing data models and how these models relate to each other can provide better context for how data has been manipulated over time when considering these requirements for a transformation tool one option stood out because it encompassed all features we wanted and was free and open source and that tool was ddt or data build tool as previously seen in our demo dvt can be used to aggregate data enable duplicate entries other transformations you might want to perform include changing column names and copying only the particular fields you need from one table into a new table because dbt has its own cloud version tapestry doesn't need to provision any resources for it so we opted to go with the cloud version of dbt because of its ease of use and simple to understand ui and now i'll hand it off to my teammate adam to discuss the last phase of our pipeline thanks rick the last phase of our pipeline is data syncing this is where we send data back into external tools that can then act on the data much like data ingestion this requires a library of api connectors specific to each destination and the ability to schedule when you want to transfer your data however data syncing is more challenging than ingestion in that the data must conform to the destination's schema this concept of syncing data back into your tools is relatively new and has recently been coined reverse etl if you recall etl and elt are concerned with moving data from your tools into your warehouse and reverse etl moves data out of your warehouse and into your business's operational tools this term however while becoming quite common does not describe the process well which is why we prefer the term data syncing while proprietary tools exist in the data syncing space like census and high touch we opted to find one that was open source grouperoo is just such a tool does everything that we need and they have a kangaroo logo to boot making them an easy pick grouproo recommends deploying their web application stack with an application layer and a data layer let's talk about the application layer first this is where a worker server and web server will reside when a request to sync data up with external sources comes in it first hits a load balancer which directs the request to the web server from there the web server can then offload the task to the worker if the task will take a long time to complete when these slower jobs are run in the background it improves the responsiveness of the web worker the data layer houses the application database as well as the cache grouparoo recommends using redis to serve as both the cache and also as the background job queue for the worker server they also suggest using postgres as the database where your user data will be stored let's take a look next at how this application architecture informed tapestries deployment of group root of the cloud given the distributed nature of this architecture we thought it was appropriate to deploy each component as its own docker container this way each container would have only one concern and the decoupling of responsibilities would make it easier to horizontally scale in the future group rue provides a docker image that can be used for deploying the web and worker service starting with their base image we added the necessary configuration to integrate snowflake as the data warehouse for grouproot to use as its primary data source of note grouproo uses javascript or json files to store configuration details and because of this any configuration changes require the grouproo docker image to be rebuilt so we chose to push the image that we provide to a repository on aws's elastic container registry giving the user easy and private access for any future updates then we provided a generic redis and postgres docker image to run the containers for the data layer now because this is a multi-container deployment tapestry had to consider how best to handle container orchestration some popular options for container orchestration include kubernetes docker swarm and amazon elastic container service or ecs kubernetes and docker swarm are both highly configurable however the learning curve is steep so we decided to use ecs to handle the container orchestration for tapestries grouper deployment because it manages and scales containers efficiently and automatically this choice also gave us the ability to use the recently rolled out ecs and docker compose integration which simplified this process even more docker compose is a tool that allows developers to define and run multi-container applications via a yaml file with this integration we could seamlessly use this same docker-compose file to deploy the grouproot application and all its dependencies as an ecs cluster aws resources are created automatically based on the specifications in this file this works because there are built-in mappings between the docker containers defined in the file and the ecs tasks ecs not only manages these containers but the servers they live on as well this occurs via aws fargate a service that abstracts away server provisioning and handles it entirely on the user's behalf we also provide a load balancer in front of the ecs cluster for all of the same reasons security wise that we placed one in front of our ingestion tool additionally we used a load balancer additionally since we used a load balancer we are also set up nicely to horizontally scale in the future if need be once group ru is deployed you are ready to start pulling data from your warehouse and syncing it into other third-party tools like mailchimp one particularly interesting challenge when deploying grouproo was determining how to inject sensitive user inputs like api keys or passwords how to get them into docker container into the docker container for group ruse web application for group roo we needed to reference these inputs in configuration files as environmental variables soon learned that you can pass environmental variables to a container by referencing a local.env file in a docker-compose yaml file and this solved part of the problem the docker container could now access any variables we provided in this file however because we were not receiving the user inputs until runtime this env file had to be dynamically generated after these inputs were received via the cli but before the container could run on ecs our solution for doing this was twofold first we stored the user inputs in the ssm parameter store on aws this ensured that the sensitive information was secure and encrypted but also available for us to access from the aws cli as needed then we created a function to dynamically write the dnv file in the user's project folder after we had all their inputs now that we have had an in-depth look at each piece let's zoom back out to view the pipeline in its entirety as outlined before the amount of steps involved in the configuration and deployment of pipelines requires a tremendous amount of time and effort and that's exactly why we wanted to build tapestry to alleviate the pains of pipeline setup for developers and save them valuable time in the long run we also wanted the management and maintenance of the pipeline to be as easy as possible so we provide a ui dashboard for monitoring the health of your pipeline and for observability that being said there are still a few features we would like to implement in the future one thing we'd love to do is enable cross platform support such as deployment on google cloud platform we'd also like to deploy airbyte with ecs so that our user has greater flexibility in terms of scaling airbyte has indicated that this compatibility will be available sometime in the future additionally we would like to add more built-in templates to the kickstart command so that tapestry can be used out of the box for more use cases and lastly we'd like to add more advanced metrics for pipeline monitoring such as cloudwatch alarms that can send notifications when particular utilization thresholds are reached and that's tapestry thanks so much for coming out and listening today at this time we'd love to answer any questions that you might have and we'll jump back now to our architecture in case any questions relate to it okay we have first question from kyle really awesome project blown away good job tapestry question for everyone on the team if we think of this project as three having three sections data and data ingestion data transformation and data thinking is there a section that you found to be more challenging to build and configure than the others i would love to hear your thoughts i can go ahead and take that one um thanks kyle for the compliment first of all um definitely the data syncing piece um was the most challenging um for a couple of reasons um on our data ingestion side um the tool that we used airbite in order to configure um sources and destinations you you do that through either a ui or their api so once you have that piece of the pipeline kind of up and running on your infrastructure um it's a little bit easier to interact with um grouproo on the other hand has a code configuration so kind of working through how to um allow for our users to redeploy the infrastructure anytime they want to reconfigure their destinations was a bit more challenging um i would also say additionally that side was a little bit more challenging because group brew itself went through some rolled out some updates while we were in the middle of implementation that were some breaking changes and that was a we had kind of a big weekend where we were trying to figure out um what was going wrong there so that was definitely a bit of a challenge but um the end result of that was kind of interesting that we ended up interacting with some of the developers there and um they rolled out new um new changes kind of based on some of our feedback on what was happening for us okay another question from lena impressive work tapestry team how was the experience of learning so many new technologies i see you're using airbite docker snowflake gopro on top of the aws components i can uh i can take that one uh thanks so much for the compliment lina yeah it was really interesting experience i think honestly i i i go back to the the process that we always fall back on that we learn in core which is pdac and just kind of breaking up a bigger problem with the smaller little chunks just approaching it that way i think it just makes learning all these new technologies um more digestible and easier to kind of break down and furthermore once you start breaking them down into those different subcomponents if you will you're able to better kind of compare um similar tools in the in the same space so i think that that also helps a lot and um yeah i i think was super interesting um getting to learn about all the different technologies especially reverse ctl reverse ctl is a emerging technology over the last year or so so that was that was really interesting to kind of um kind of be a get an inside look at this uh pretty new uh new space new technology but um but yeah it was it was a very educational experience and uh yeah i hope that answered the question so what was your favorite or least favorite part of working on this i guess that can be a question for everyone sure um i can definitely answer first so your favorite or least favorite part of working on this um i would say that my favorite part was after we had chosen our data ingestion and data syncing tools figuring out um how they worked specifically with group ru our data syncing tool um it was a bit tricky to figure out how the mapping worked from tables in snowflake to jump into the mailchimp um basically what like when you're mapping data over you kind of have to have the right table names um you have to clean the data you might even need like certain columns that are required in mailchimp that are not required when you're describing the data and that was definitely challenging and interesting for to figure out the code configuration for for the data syncing component yeah i think my favorite part was probably the first time that we got data all the way through the pipeline we're like hey it works we did it um that felt pretty good um but yeah to lina's point earlier it was it was a lot of different tools to learn and uh on top of aws it was it was a pretty big learning curve a big experience of figuring some new stuff out but but also fun on the other end of things so yeah i'd say my least favorite part as catherine alluded to was the night that grouper rolled out some changes that we were unaware of and everything broke on the sinking side of things and we couldn't figure out why for a while um and just debugging that was that was pretty painful but yeah it was an awesome project to get to work on okay we have a final question and it's how you landed in this idea justin asks thank you justin um i can take this one so kind of as we've come as like overall we understand that companies are adding more and more like data marketing tools and even kind of seeing companies out like ourselves we like we know that that the aspect of user data is kind of becoming like a a larger and larger aspect of how a company works um and after we kind of realized that that was such a huge priority of companies like to focus on their data data analytics um then we kind of found our found ourselves looking into like how data was transferred from third-party tools and and then how companies were able to use that data um and that's kind of how we fell upon like data pipelines um and we also saw that data pipelines were generally like more focused on how could a company use like analytics tools and intelligence tools to kind of like grab insights from combining all the data to kind of make business business decisions um like how should they price their products what new products should come out next and we also kind of saw that there was almost like a missing piece or like kind of a new part of the pipeline that came out like that we refer to as data syncing or reverse ctl um a lot of open source solutions and even proprietary solutions stopped once they got data in like into the warehouse um so once we saw that grabbing the data from the warehouse back into those tools um to actually be utilized in marketing campaigns and other and other tools like to be operationalized was new um we we kind of found a hole in the market to have that as an open so to have that as an open source solution so it looks like we don't have any more questions so any final words just say thanks everybody for coming out um hope you enjoyed it and if you have any other questions or want to chat we're on slack thanks 