Thank you for joining us today. My name's Katherine, and my teammates are Leah, Rick, and Adam. We've spent the past few months building Tapestry, an open-source data pipeline orchestration framework for user entity data. With Tapestry, your user data pipeline will be automatically deployed and ready to ingest data from all your user data sources, store and transform that data in a data warehouse, and sync that data back to your data tools for immediate use.

So why would a business need a pipeline to move user data between different tools? And if you did need a data pipeline, why would you need an orchestration framework? Let's dive into a simple use case to discuss these topics.

Let's start with a hypothetical company called Rain Shield. They have a thriving ecommerce store and are quickly gaining traction amongst a larger user base. They've raised funding and expanded their team. Rain Shield is now utilizing various tools like Stripe for online transactions, Salesforce for lead tracking, Zendesk for support tickets, and Zoom for webinars.

With different tools being used by different departments, user data is becoming scattered and fragmented. Each tool only has access to a portion of the customer's information, leading to data silos. For example, Rain Shield might have a customer named Susie who purchased brellas, called customer support, and signed up for a webinar. But this data is spread across different tools, making it challenging to have a unified understanding of Susie and her interactions with Rain Shield.

To overcome this challenge, Rain Shield needs to integrate user data from every source into one place. This would allow them to see patterns among their users and act on these insights. It's critical for a company to have a complete picture of their users in order to operate effectively.

Now, let's discuss the options Rain Shield has for integrating their user data. They could manually move files between tools, use pre-built connectors, create custom connectors, or build a complete data pipeline.

Manually moving files between tools is a possibility, but it can result in duplicate data and be tedious if done frequently. Pre-built connectors, like those offered by Zapier, provide some automation but may not offer the flexibility Rain Shield needs.

Creating custom connectors would allow Rain Shield to choose what data to extract and sync, but it would require a significant amount of engineering time, especially if Rain Shield uses multiple tools.

The most complete solution is to build an end-to-end user data pipeline with a cloud data warehouse at the center. This approach provides access to tools that remove the complexity of working with third-party APIs while offering flexibility in data extraction and syncing. Cloud data warehouses also offer benefits like storing large amounts of data, serving as a single source of truth, and enabling data modeling and analytics.

By implementing this warehouse-centric pipeline, Rain Shield can aggregate all their data into one accessible place, create unified models, and sync them to the tools their teams need. This allows them to rely on a single pipeline for both analytics and operations, leading to a better understanding and leverage of user data.

Now, let's explore some existing solutions for creating a user data pipeline. The two primary options are using a proprietary hosted solution like the one offered by RudderStack or configuring your own pipeline using open-source tools.

A self-hosted solution offers the advantage of full ownership over the pipeline infrastructure, allowing customization. However, it requires a significant amount of time and effort to set up and maintain. On the other hand, RudderStack provides an easy-to-deploy user data pipeline but offers limited control over infrastructure.

RudderStack has out-of-the-box features and abstracts away infrastructure provisioning and management. This makes it quick and easy to deploy a user data pipeline. However, a do-it-yourself approach with open-source tools allows for more customization but requires extensive knowledge and effort.

To simplify the deployment of a self-hosted solution, we built Tapestry. Tapestry is an open-source framework that automates the entire pipeline deployment process. It is designed for engineers who want full control over their data infrastructure without the burden of provisioning and managing it.

Tapestry automates many steps and creates resources for each phase of the pipeline. Using Tapestry, deploying your own user data pipeline becomes easier and more efficient compared to building it from scratch.

To get started with Tapestry, engineers need to have Node and npm installed, along with an AWS account and the AWS CLI. Docker is also required. After these setup steps, running "npm install tapestry pipeline" will provide a list of commands.

The first command to run is "tapestry init" to give your project a name and provision a project folder along with an AWS CloudFormation template. This template will help set up the necessary resources for the pipeline.

Setting up your own user data pipeline would require dozens of steps and resource provisioning. Tapestry streamlines this process and makes it more manageable.

In conclusion, building a user data pipeline is essential for businesses to integrate data from various sources and gain a unified understanding of their users. Rain Shield's hypothetical case exemplifies the challenges and options available for data integration.

While manual data movement, pre-built connectors, and custom connectors have their drawbacks, building a complete data pipeline with a cloud data warehouse at the center offers the most benefits. It simplifies the integration process, allows for customization, and provides additional advantages like scalable storage, data modeling, and analytics.

Both proprietary hosted solutions like RudderStack and self-hosted approaches using open-source tools have their pros and cons. To address the need for flexibility and automation, we created Tapestryâ€”a fully open-source framework that automates the pipeline deployment process, giving engineers control over their data infrastructure without the hassle of provisioning and managing it.

By using Tapestry or similar solutions, businesses can create end-to-end user data pipelines that enable better data analysis, improved operational efficiency, and enhanced decision-making based on unified user data. The DIY option allows for fully open source tools, while the Runner Stack only offers open source event streaming. Self-hosted solutions grant full ownership over pipeline infrastructure, allowing for customization. However, RudderStack requires the use of their infrastructure, providing little control. RudderStack has many out-of-the-box features, while self-hosted solutions have none. RunnerStack has an advantage by abstracting away infrastructure provisioning and management, making pipeline deployment quick and easy. Building your own pipeline requires a significant amount of time and effort. Tapestry was created to automate the setup and configuration of an end-to-end user data pipeline, providing engineers with full control over their data infrastructure without needing to provision it themselves. Tapestry is an open-source framework that automates the entire pipeline deployment process but has limited out-of-the-box features. It automates many steps and creates necessary resources for each phase of the pipeline.

To get started with Tapestry, if you're an engineer, you need to have Node.js and npm installed as Tapestry is a Node package. Additionally, you need an AWS account, the AWS CLI installed on your machine, and a Docker account with Docker installed. After these preliminary steps, running "npm install tapestry pipeline" with a global flag will provide a host of commands. The first command to run is "tapestry init," which allows you to give your project a name and provisions a project folder along with an AWS CloudFormation template. This template is used to provision resources for the data ingestion phase of the pipeline. The next step in Tapestry is to choose between the "deploy" and "kickstart" commands. Both automate the deployment of a fully operational pipeline, but "kickstart" includes pre-configured sources and destination tools like Zoom, Salesforce, and Mailchimp. Kickstart prompts users with a series of questions to set up the necessary configuration files for the data syncing phase. Tapestry automates the creation of resources for both the ingestion and syncing phases of the pipeline.

To demonstrate Tapestry in action, let's walk through a demo. Setting up Tapestry requires Node.js, npm, an AWS account with the AWS CLI installed, and a Docker account with Docker installed. After running "npm install tapestry pipeline," the first command to run is "tapestry init," which creates a project folder and an AWS CloudFormation template. The next step is to choose between the "deploy" and "kickstart" commands. Kickstart is selected, which includes pre-configured sources (Zoom and Salesforce) and a destination (Mailchimp). After providing the necessary account information, Kickstart creates the required databases and tables within the data warehouse (Snowflake). Tapestry uses a CloudFormation template to provision AWS resources for the ingestion tool (S3 staging bucket, EC2 instance, application load balancer) and configures it to extract data from Zoom and Salesforce accounts. The data is then transformed in the warehouse using a data model provided by Tapestry, aggregating, filtering duplicates, and formatting it to be synced to Mailchimp. Another CloudFormation stack is created for the syncing tool, providing AWS resources like an ECS cluster and an application load balancer. A Tapestry dashboard is automatically launched, displaying documentation and metrics for each component of the pipeline. Data flows through the pipeline, extracting Zoom webinar registrants and Salesforce contacts, combining them into a single list, filtering duplicates, and syncing them to Mailchimp.

Now let's discuss how Tapestry was built. The pipeline consists of three phases: ingestion, storage, and transformation, and sinking. For the ingestion phase, a data extraction tool is needed to manage API connectors and schedule data extraction. Tapestry uses Airbyte, an open-source solution with standardized API connectors and a robust UI. Airbyte runs on Docker containers, and Tapestry configures the warehouse (Snowflake) as the destination. An EC2 instance is recommended for hosting Airbyte, and a load balancer is used for routing traffic. In the storage phase, Tapestry uses Snowflake as the data warehouse, which provides flexibility, scalability, and easy integration with AWS. To optimize data loading, a staging area is implemented using an S3 bucket between Airbyte and Snowflake. This allows for efficient bulk data loading. Tapestry encountered challenges with CPU usage and adjusted the size of the EC2 instance accordingly. A CPU usage section is included in the Tapestry dashboard to monitor AWS resources.

In conclusion, Tapestry is an open-source framework that automates the deployment of an end-to-end user data pipeline. It provides engineers with full control over their data infrastructure and automates many steps, creating necessary resources for each phase of the pipeline. Tapestry uses tools like Airbyte for data ingestion and Snowflake for data storage and transformation. The pipeline can be easily customized, and a Tapestry dashboard provides documentation and metrics to monitor the health of each component. The implementation involves provisioning AWS resources, configuring Docker containers, and utilizing AWS services for hosting and routing traffic. With Tapestry, engineers can simplify the deployment process and focus on more important tasks. In this project, we focused on enhancing the security and efficiency of our data pipeline. To achieve this, we implemented a load balancer in front of the Airbyte EC2 instance. This ensures that network traffic must pass through the load balancer before reaching the instance, providing additional security measures and hiding the actual instance's IP address. This protects the instance from port scanning attacks and utilizes AWS's built-in protection against DDoS attacks.

During the prototyping phase of our pipeline, we encountered an issue with the ingestion of our Salesforce data. While our Zoom data was being extracted and loaded into our warehouse without any problems, the Salesforce data consistently timed out, resulting in a 504 server error. Upon investigation, we found that the Airbyte EC2 instance was becoming an unhealthy target for our application load balancer whenever we attempted a sync with Salesforce.

Through further analysis in the AWS monitoring dashboard, we discovered that the CPU usage of our EC2 instance was extremely high. This led us to believe that the instance's size was causing the issue. To resolve this, we vertically scaled our instance, and the error message disappeared. As a result of this experience, we decided to include a CPU usage section in the Tapestry dashboard to monitor our AWS resources.

Moving on to the data storage and transformation phase, we determined that a warehouse capable of handling large amounts of data from various sources would be the central component of our pipeline. Given our decision to host our tools on AWS services, we opted for Snowflake as our data warehouse. Snowflake provides flexibility by being compatible with major cloud platforms and separates storage needs from query processing, allowing for cost savings and independent scalability. Additionally, Snowflake abstracts away the provisioning and maintenance of the necessary resources for a cloud data warehouse.

Initially, we attempted to load data directly into Snowflake from third-party tools. However, we found that the data transfer was slow. To improve efficiency, we implemented a staging area using an Amazon S3 bucket. This staging area enabled efficient bulk data loading into Snowflake. It allowed Airbyte to insert multiple rows of data at once, avoiding the need for numerous SQL insert commands.

Working with Snowflake also presented challenges when running SQL commands for new users. We needed to perform a sequence of 34 commands to set up the warehouse with the required databases, schemas, roles, and permissions for the Tapestry pipeline to operate. To overcome the limitations of the Snowflake SDK, which only allows one SQL statement to be executed at a time, we utilized the Snowflake Promise library. This library provided a promise-based API, allowing us to handle multiple promises in a synchronous fashion using async/await, ensuring the correct order of statement execution.

In the data transformation phase, we sought a flexible SQL-based tool that could be used by non-developers to create data models based on the warehouse. We also wanted a tool that maintained a history of data transformations and documentation about existing data models. To meet these requirements, we chose DBT (Data Build Tool), an open-source tool that supports aggregation, duplicate elimination, and various other transformations. DBT's cloud version streamlined our deployment process by eliminating the need to provision additional resources.

Moving to the data syncing phase, we needed a library of API connectors specific to each destination and the ability to schedule data transfers. Data syncing is more challenging than ingestion as the data must adhere to the destination's schema. We opted for an open-source tool called Grouparoo. Grouparoo fulfilled all our requirements and provided a user-friendly UI for configuration. It recommended deploying the application layer with a worker server and a web server. The worker server handles long-running tasks, enhancing web server responsiveness. The data layer consists of an application database and a cache, with Redis serving as both the cache and the background job queue for the worker server, and PostgreSQL for storing user data.

To deploy Grouparoo, we utilized a distributed architecture approach. Each component was deployed as its own Docker container, allowing for decoupling of responsibilities and easy horizontal scalability. We used Grouparoo's Docker image as the base and added the necessary configuration for Snowflake integration as the primary data source. The image was pushed to an AWS Elastic Container Registry for easy and private access for future updates. We also provided generic Docker images for Redis and PostgreSQL to run the containers for the data layer.

For container orchestration, we chose Amazon Elastic Container Service (ECS). ECS efficiently manages and scales containers, and its integration with Docker Compose simplified the deployment process. Docker Compose allowed us to define and run multi-container applications using a YAML file. This file served as the basis for deploying the Grouparoo application and its dependencies as an ECS cluster. AWS Fargate, which abstracts away server provisioning, managed the ECS cluster and servers.

To ensure the security of our deployment, we included a load balancer in front of the ECS cluster, providing the same benefits as with the Airbyte instance. This setup also allows for horizontal scalability in the future.

Throughout the pipeline, we faced challenges in handling sensitive user inputs, such as API keys and passwords. We developed a solution that stored these inputs securely in the AWS SSM Parameter Store, allowing us to access them via the AWS CLI. We then dynamically generated an environment file during runtime to populate the necessary variables for the Grouparoo Docker container to run on ECS.

In conclusion, building and configuring each section of the pipeline presented its own set of challenges. However, the data syncing phase proved to be the most complex due to the code-based configuration and updates in Grouparoo's framework. Overall, our aim with this project was to simplify pipeline setup, improve security and efficiency, and provide monitoring capabilities through the Tapestry dashboard. In the future, we plan to expand cross-platform support, add more templates, and enhance pipeline monitoring with advanced metrics and alerting. Tapestry, our coding Capstone project, was developed with the aim of alleviating the time-consuming and effort-intensive process of configuring and deploying pipelines. We recognized the need to simplify pipeline setup for developers and save them valuable time in the long run. As part of this, we wanted to make pipeline management and maintenance as easy as possible, which led us to offer a user-friendly UI dashboard for monitoring pipeline health and ensuring observability.

While Tapestry already offers valuable features, there are some additional enhancements we plan to implement in the future. One significant goal is enabling cross-platform support, including deployment on the Google Cloud Platform. This will provide our users with greater flexibility and options for scaling their pipeline. We also plan to deploy Airbyte with Amazon Elastic Container Service (ECS) to further enhance scalability. Although this compatibility is not available at present, we anticipate its availability in the future.

In addition to cross-platform support, we aim to expand the selection of built-in templates within Tapestry's kickstart command. This will enable users to easily utilize Tapestry for a wider range of use cases without the need for extensive configuration. Lastly, we plan to incorporate more advanced metrics for pipeline monitoring. This includes the integration of CloudWatch alarms that can send notifications when specific utilization thresholds are reached. By doing so, we enhance the monitoring capabilities of Tapestry.

We are pleased to have had this opportunity to present Tapestry. We appreciate your time and interest, and now we are ready to address any questions you may have. If any of your questions relate to the architecture, JP is available to provide further insights.

Kyle, thank you for your kind words and your question. When it comes to building and configuring the three sections of our project - data ingestion, data transformation, and data syncing - we found the data syncing section to be the most challenging. This is primarily because of the differences in configuration between the data ingestion tools we used. Airbyte, our data ingestion tool, offers a user-friendly UI or API for configuration, making it relatively straightforward to interact with. On the other hand, Grouparoo, our data syncing tool, requires code configuration. We had to tackle the challenge of allowing users to redeploy their infrastructure to adapt to reconfigurations of their destinations. Additionally, Grouparoo went through some updates during our implementation, which presented some breaking changes. This required us to work closely with the Grouparoo developers to address the issues, making it a particularly challenging part of the project.

Lena, thank you for your compliment and your question. Learning and working with multiple new technologies was indeed an intriguing experience. To effectively navigate this process, we applied the problem-solving approach known as PDAC, which involves breaking down complex problems into smaller, manageable tasks. This approach made it easier to learn and understand each technology. It also allowed us to compare similar tools within the same space, aiding in our decision-making. The emerging technology of reverse CTL, in particular, was fascinating to explore. Overall, the project provided an educational experience, expanding our knowledge of various technologies and their application.

Given your interest, we'd like to know your favorite and least favorite aspects of working on this project. We welcome input from everyone.

Thank you for asking, Catherine. My favorite part of working on this project was when we successfully passed data through the entire pipeline for the first time. Seeing the project come together and realizing that it was functioning as intended was a great moment of accomplishment. On the other hand, learning all the new technologies, especially within the AWS ecosystem, presented a significant learning curve. It required dedicated effort to understand and implement them effectively. Specifically, the least favorite part was when Grouperoo introduced unexpected changes that caused our data syncing to break. Debugging and resolving the issue was challenging and time-consuming. However, despite the challenges, working on this project was an overall gratifying experience.

Thank you, Justin, for your question. Our idea for Tapestry came about from recognizing the increasing significance of user data for companies and their focus on data analytics. We observed the growing number of data marketing tools and the need for companies to transfer data from third-party tools and utilize it effectively. While many existing solutions focused on utilizing analytics and intelligence tools for business decisions, we identified a gap in the market regarding data syncing or reverse CTL. This involves transferring data back from the warehouse to the marketing tools, enabling its operationalization. Identifying this missing aspect prompted us to develop Tapestry as an open-source solution to address this need.

We appreciate your participation and the opportunity to share our project with all of you. If you have any further questions or would like to discuss anything related to Tapestry, please feel free to reach out to us on Slack. Thank you once again for your time and interest.