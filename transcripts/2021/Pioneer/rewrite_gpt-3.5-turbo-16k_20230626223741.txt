Welcome and thank you for joining us today. My colleagues (Kyle, Jimmy Zheng, and Elizabeth Tackett) and I are the creators of Pioneer. Pioneer is a self-hosted feature flag management tool. Today, we will discuss the problems Pioneer solves, introduce the concept of feature flags, provide an overview of our application, discuss the technical decisions and challenges we faced while building Pioneer, and touch on future work.

Adding new features to an application can range from simple UI changes to fundamental architectural changes. In this presentation, when we mention adding new features, we specifically refer to organizations that want to migrate from a monolith to a microservices architecture.

To understand the challenges faced by organizations in making this architectural change, let's take a hypothetical company called Harvest Delivery as an example. Harvest Delivery is a mid-sized retailer that provides an online grocery shopping service. As they grew, they started experiencing strains on their monolithic architecture. Tightly coupled code, slow development due to multiple code base updates, increased traffic, and availability issues were some of the challenges they faced.

Harvest Delivery decided to extract some of their logical components into separate microservices. They wanted to add three microservices to their application: shopping cart, payment processing, and live customer service chat. The migration strategy they adopted is known as the strangler fig pattern.

One common challenge faced by teams adding new features to an application is how to deploy the updated code. Deployment can be time-consuming and prone to errors, and a deployment issue can result in application downtime. Two deployment approaches are commonly used: a slow but safe approach and a quick but risky approach. Unfortunately, both approaches have disadvantages.

To address these challenges, we propose using the canary deployment strategy, which involves routing a portion of the traffic to the new microservice deployment while keeping the original deployment running. However, this approach has limitations, such as lack of granularity and control when rolling back microservices.

To overcome these limitations, we introduce the concept of feature flags as an alternative solution. Feature flags allow granular control of individual microservices without the need for additional infrastructure. They can be thought of as representing the active state of a microservice. Using the SDK, the application code can be engineered to handle the routing based on the flag's state, eliminating the need for a load balancer.

Feature flags offer several advantages: they minimize interference with existing infrastructure, allow for percentage rollout, and provide a convenient way to toggle flags via a UI. By toggling the flag state, the application can switch between microservices or execute internal monolith code.

To implement feature flags, a key-value pair like a dictionary is used, where the key represents the microservice's active state, and the boolean value determines the flow control. The flag's state is checked, and based on that, the corresponding microservice or internal monolith code is executed.

In summary, Pioneer provides a self-hosted feature flag management tool that solves the challenges of adding new features to an application while minimizing downtime and providing granular control. The canary deployment strategy and the use of feature flags offer efficient and flexible solutions. We are excited to share our application with you and discuss the technical decisions and challenges we faced while building Pioneer. Solution to Canary Deployment:
The solution presented here, known as the canary deployment, addresses the previously mentioned problems by providing several benefits. Firstly, it enables independent toggling of multiple flags, allowing for granular control over the state of each flag. Each flag represents the active state of a microservice. The canary deployment also eliminates deployment downtime, which will be explained further. Additionally, this deployment strategy minimizes interference with existing infrastructure, as it operates at the application code level rather than the infrastructure level. It only requires embedding an SDK into the existing code base, without the need for an extra environment or load balancer. Lastly, it retains the benefits of percentage rollout, which can be adjusted through a UI instead of configuring a load balancer.

Overview of Canary Deployment:
To provide a high-level overview of how canary deployment works, let's consider a scenario where a monolith and a microservice extracted from that monolith are in place. When a request comes in from the internet, the application handles it by referencing an SDK. This SDK checks if a flag is toggled on, representing the active state of a specific microservice. If the flag is on, the request is routed to that microservice. If the flag is off, the request executes the internal monolith code. 

Efficiency and Convenience of Feature Flags:
To highlight the efficiency and convenience of feature flags, let's consider a scenario where microservice A goes down. In this case, the engineer only needs to toggle off the flag corresponding to microservice A. Once the flag is toggled off, the SDK is notified of the change. Consequently, future requests are routed to the internal monolith code instead of microservice A. Importantly, this does not affect the activity of other microservices.

Understanding Feature Flags:
A feature flag can be thought of as a key-value pair, like a key-value pair in a dictionary. It is used in the code base as an if statement with two possible choices. The boolean value of the flag determines the flow control. For example, the get feature method takes a key name as an argument, such as "can," which represents the active state of microservice A. If the method returns true, the new feature, routing the request to microservice A, is executed. If the method returns false, the original code, representing the internal monolith's code, is executed.

Options for Adopting Feature Flags:
When considering options for adopting feature flags, there are two main choices: creating and maintaining a custom feature flag rule set or using third-party solutions. For smaller and medium-sized companies like Harvest, factors such as accessibility, ease of setup, and affordability are crucial due to limited resources. Enterprise solutions like LaunchDarkly are closed-source and come with monetary costs, typically requiring more configuration and engineering effort. Open-source solutions like FeatureHub offer flexibility but may require additional time and effort to configure. Considering these factors, Pioneer was developed for Harvest, providing the best fit for their use case.

Introduction to Pioneer:
Pioneer is a self-hosted, open-source feature flag management tool specifically designed for small to medium-sized organizations transitioning from a monolith to a microservices architecture. It allows for the controlled rollout of microservices without extensive configuration or additional infrastructure like load balancers. Users have the flexibility to deploy Pioneer on their chosen infrastructure, be it AWS VPC, DigitalOcean droplet, or an on-prem server. Being open-source and self-hosted, it allows users to fully customize the application to their unique needs.

Pioneer's Architecture:
Pioneer's architecture consists of multiple components, namely Compass, JetStream, and Scout, all running in a Docker network. Compass, a React application, serves as the primary dashboard for managing feature flags. JetStream, a third-party message streaming service, facilitates communication between Compass and Scout. Scout, a Node.js application, acts as the distributor of flag data. The user's application communicates with Pioneer through an SDK embedded in its code base.

Compass: Feature Flag Management Dashboard:
Compass serves as Pioneer's primary application for managing feature flags. Its user interface allows users to create, update, and delete feature flags, which have titles, optional descriptions, and assigned rollout percentages. Flags can be toggled on or off with a single click, and changes made in Compass are published to JetStream and received by Scout. Scout then pushes the updated flag data to connected SDK clients, enabling real-time updates without redeployment.

JetStream: Message Streaming Service:
JetStream, an open-source message streaming service, facilitates communication between Compass and Scout. It provides asynchronous, fault-tolerant messaging with guaranteed delivery. Pioneer uses a "publish-subscribe" messaging pattern, where Compass publishes updated feature flag data to JetStream, and Scout, as a subscriber, receives and acknowledges the message. JetStream stores the most recent flag data until Scout acknowledges its receipt.

Scout: Distributor of Flag Data:
Scout acts as the interface between Compass and the SDK embedded in a user's application. It distributes flag data to SDK clients through a persistent HTTP connection. Connected SDK clients receive flag updates as server-sent events (SSE). Pioneer currently offers server-side SDKs in Node.js, Ruby, and Go, allowing users to evaluate flags within their applications. The SDKs integrate with Google Analytics for collecting analytics events related to new features.

Out-of-the-Box Usage of Pioneer:
To quickly get started with Pioneer, users need to clone the GitHub repository and navigate to the pioneer directory. Running the command "docker-compose up" will start all components of Pioneer's architecture. Once up and running, users can create and modify feature flags using the Compass dashboard. Changes made in Compass are transmitted from Pioneer to the user's application. The flag data is distributed to SDK clients through JetStream and Scout, enabling real-time updates without redeployment.

Engineering Decisions and Challenges:
During the development of Pioneer, several key engineering decisions were made to balance benefits and trade-offs. One decision concerned how to communicate feature flag data from Pioneer to user applications. Websockets were not chosen due to their bidirectional nature, while polling did not deliver real-time updates. Pioneer ultimately utilizes JetStream, a lightweight message streaming service that supports asynchronous, fault-tolerant messaging. The "publish-subscribe" pattern of JetStream allows flag updates to be distributed to connected SDK clients through Scout.

Another decision involved the design of server-side SDKs for implementing gradual feature releases. Pioneer currently offers SDKs in Node.js, Ruby, and Go, which can be installed in the user's application code with a single command. The SDKs evaluate flag data within conditional statements, allowing seamless code execution based on flag values.

When formatting feature flag data for transmission, Pioneer's architecture efficiently utilizes server-sent events (SSE). The flag data is sent from Scout to SDK clients through a persistent HTTP connection, ensuring real-time updates without excessive network traffic.

To ensure cross-platform performance and flexibility, Pioneer is designed as a self-hosted application. Users can deploy Pioneer on their preferred infrastructure and fully customize it to meet their specific needs. This flexibility allows users to adapt Pioneer to their unique requirements, adding additional components or modifying existing ones if desired.

In conclusion, Pioneer provides Harvest, and similar organizations, with an efficient and customizable solution for feature flag management. It offers advantages such as granular control over flag toggling, real-time updates without redeployment, and flexibility in deployment and customization. With Pioneer, Harvest can smoothly transition from a monolith to a microservices architecture, enhancing their development workflow and enabling more controlled feature releases. Our three SDKs evaluate feature flags within conditional statements to determine code execution. Feature flags are evaluated at the code level, eliminating the need for load balancers or additional deployed environments. Let's use the example of the Harvest Delivery hypothetical to understand how feature flags work.

One logical component that Harvest Delivery plans to extract into a microservice is their payment processor. In the code example provided, we see a conditional statement that evaluates the payment processor flag in Harvest Delivery's code base. This flag can be toggled on or off, or its rollout percentage can be updated, all without making any changes to the code. No redeployment or additional engineering work is necessary to see the change reflected in the application's behavior.

Since feature flags are controlled in a granular manner, changes made to the payment processor flag will not impact any other feature flags, such as the flag for an inventory catalog. It's a simple and efficient way to manage feature changes in an application.

Now, let's discuss the engineering decisions and technical challenges we faced while building Pioneer, our feature flag management system. These decisions involved careful consideration of the benefits and trade-offs.

One key decision was how to communicate feature flag data from Pioneer to users' applications. We considered options such as web sockets, polling, and web hooks. However, we found that server-sent events (SSE) were the best fit for our needs. SSE connections provide persistent HTTP connections between the server and clients, allowing near real-time communication of flag updates from Pioneer to connected SDKs.

We also had to consider the level of security for SSE connections. We chose to authorize all SDKs requesting to connect to Pioneer as SSE clients. This additional layer of security ensures that only authorized SDKs can access feature flag data. SDK clients initiate an SSE connection with Pioneer by sending an HTTP request to Pioneer's specified endpoint. Pioneer verifies the SDK's authorization key before opening the SSE connection and transmitting feature flag data to the SDK.

Another challenge we faced was designing server-side SDKs that could implement gradual feature releases, specifically percentage-based rollouts. Instead of relying on infrastructure-level solutions like canary deployments, our SDKs handle the logic of the percentage rollout strategy at the application level. The SDKs require context about the user to determine if they should receive the feature flag rule. This context, such as a customer ID, is passed into the SDK, which then evaluates if the user falls within the rollout percentage. If they do, the user is routed to the microservice; otherwise, they are routed to the monolith service.

For transmitting feature flag data, we decided to send the entire rule set as JSON objects. This ensures that all SDKs have the most up-to-date flag data at all times. Although transmitting only the updated sections of the rule set would reduce data size, it could lead to outdated flag data for some SDKs due to network issues. By prioritizing data accuracy and considering the manageable size of the data being transmitted, we chose to send the entire rule set each time an update is made.

Lastly, we focused on ensuring cross-platform performance and flexibility by containerizing Pioneer. This allows users to deploy Pioneer across different platforms without the need for additional configuration. By using Docker containers and a Docker network, the behavior of the application remains consistent regardless of the deployment platform.

Looking towards the future, there are several potential areas for Pioneer's further development. Currently, a single instance of Pioneer supports only one application, but it could be expanded to support multiple applications by allowing for multiple distinct rule sets. Additionally, the rollout strategy offered by Pioneer is limited to simple percentage rollouts. Future roll-out strategies could include targeting internal users, beta testers, or specific market segments for more tailored feedback.

Lastly, flag exploration could be implemented to help define clear testing or rollout windows for adding new features or microservices. Setting an expiration date on a feature flag would ensure that the conditional logic related to that flag does not live indefinitely in the user's application codebase.

Thank you for joining us today. If you would like more information on our Pioneer project, please visit our case study for a detailed write-up or explore our GitHub repository. We are now open to any questions you may have. The process of updating your code with new functionalities requires the deployment of updates. This means that you need to embed the necessary switches and if-else statements in your code beforehand. The Pioneer tool allows you to toggle between different states, but if you need to update a microservice itself, you still have to redeploy it.

Regarding the challenges faced during the setup of container-to-container communication, there is some configuration involved when running things within a Docker network. However, we made the decision to extract certain variables into a .env file. This centralizes the modification process, as users can simply modify the .env file instead of having to locate and modify different parts of the Pioneer components within the application. In the end, the configuration process was manageable and aligned with standard Docker practices.

In terms of choosing Future Flags as our project topic, we explored several ideas initially. Each team member proposed different project topics, and we found the concept of feature flags to be particularly intriguing and fun to work on. Additionally, there weren't many capstone projects in this area, making it an interesting space to explore and contribute to.

We received a compliment on our project from Lena, and she asked what our favorite challenge was while working on this project. Each team member was given the opportunity to answer. We acknowledged that there were many challenges throughout the project, with no individual component being straightforward. The distributed nature of our team, with a six-hour time difference between certain members, presented its own unique challenges. However, it was a valuable experience in learning to work effectively as a distributed team.

Mark posed a question about using a proxy like Nginx for container-to-container communication. We mentioned that our vision for the project involved all Docker containers running on the same server. Therefore, we did not anticipate the need for a proxy in the communication between microservices. While it is possible to implement such a proxy, additional configuration would be required, which we have not accounted for in the development of Pioneer.

Graham asked if the Compass UI provides collaboration features, specifically in the context of hosting public Docker containers. We clarified that the Compass UI is designed to be self-hosted. As such, it does not provide any authentication mechanisms beyond controlling access to the UI through the assigned address.

A participant acknowledged our work and asked where the code that determines traffic routing between services resides in a Pioneer user's architecture. We explained that for a Pioneer user, the code responsible for handling requests from the internet is typically located in the backend architecture, such as a framework like Express. Here, the user embeds an SDK and includes an if-else statement to determine traffic routing based on a defined percentage. This is where the functionality of routing a specific percentage of traffic in one direction and the remaining percentage in another is implemented.

Another participant inquired about how we divided the project tasks amongst the team. We responded that initially, each team member took on a specific portion of the project and conducted code spikes and tests. However, as we progressed, we began working collaboratively on different aspects and stages of the project to ensure everyone gained familiarity with the overall operation.

Pair coding was also a significant part of our collaboration strategy. Kyle and I worked together closely and found it to enhance productivity. We also had larger group sessions where all four team members would come together to work on core pieces, such as the functionality of Scout, the flag data distributor. This approach helped us resolve any issues efficiently and improved our overall understanding of how all the components fit together.

Lastly, we thanked everyone for joining the session and expressed our appreciation for their questions and feedback. We apologized for any technical difficulties we might have faced and expressed gratitude for their patience and attention throughout the presentation. We acknowledged the support received and emphasized the importance of their presence in making it an excellent project.