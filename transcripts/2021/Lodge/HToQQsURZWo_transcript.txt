uh thank you for joining us today uh in our presentation we are going to talk about lodge lodge is an open source self-managed logging framework for small distributed applications lodge allows users to ship transform store visualize and monitor their logs in this presentation we're first going to give a brief narrative based on a fictitious company we've invented for the purposes of outlining the ideal use case for lodge then we will conclude this narrative with a comparison of existing solutions to the problems it has introduced once we've shown where lodge fits in in the context of the existing solutions we'll transition to an overview of lodge's architecture followed by some demonstrations of how it works from a user's perspective in the final sections we will discuss some of the key design decisions we made as well as an implementation challenge we faced along the way let's start things off with our use case this is boardwalk boardwalk is a small but fast growing online retailer that sells handcrafted board games their product has become a huge hit on social media which has exponentially increased the traffic on their website this is an exciting time for boardwalk but also a challenging time since their growth will require a complete overhaul of their architecture their monolithic code base has worked great up until now but their increasing number of users are demanding an increasing number of new features so their development pace needs to keep up and their architecture needs to facilitate this new pace this is why they've decided to break up the monolith and migrate to a microservice architecture in a monolith architecture the business logic of the application is run entirely on one machine this means that different functional components or modules of the application can communicate by simply calling each other's methods for example if a user wants to check out their cart they will trigger a request from the web server to the checkout endpoint of the app server which will then call the check inventory method of the inventory module which sends a sql query to the database and so on a microservice architecture introduces network complexity between these modules now they must communicate by making api calls over an inherently unreliable network the user checkout event now begins with a request to the checkout service which fetches the user's cart contents from its database then initiates a request to the inventory service for it to verify that the product is available in its database and so on this complexity is not just added for fun though there are significant benefits gained from using this architecture remember boardwalk is aiming to increase development turnover on new features so if they want to update their checkout service they can do so independently of the rest of the services and of course boardwalk is growing fast so now they can accommodate this growth by horizontally scaling these services as needed so now the boardwalk team is set up to focus all their time and engineering effort on new features right well maybe not there's a problem users are reporting issues with the checkout system what does the team do in their previous architecture all they needed to do was ssh into their application server and maybe their database server to examine the logs now they have to ssh into multiple instances examine their logs and try to piece together the entire application state from a series of isolated records is the checkout service running is the payment service running are they both running but the network is failing fortunately the number of nodes is still low enough that it's possible but as it increases they can see that this process will soon become untenable so the team begins researching a solution it's important to note here however that their users have not stopped using their application and the backlog of features these users have requested including a more streamlined checkout service and the ui for customizing their board game orders has not suddenly disappeared they need a solution but every minute they spend working on it is a minute they are not spending developing their application the team soon finds out that their problem has a name observability the need for observability is universal across all distributed systems and the literature devoted to this subject is enormous the type of observability solution that fits boardwalk's use case is going to need to involve some sort of log aggregation that is they need to collect the logs from all the individual servers in their system and send them to one centralized location this usually involves a collection agent that taps into the log files of whatever machine it's installed on detects changes and sends those changes over the network to another location however they also discover that since these log files contain raw text without much structure just piling them all in a database as is is not helpful the logs need to be searchable a mature observability solution needs to store the data in a way that is queryable and visualizable in sql database terms this process would entail dividing up the relevant characteristics of the logs such as the timestamp the source and the message and organizing the tables and columns of the database using these characteristics that way instead of visualize visually scanning through thousands of lines of text they can leverage all the power of sql queries to find and analyze the logs they're looking for in addition to being searchable the data returned from these queries can now be fed into a visualization tool to create interactive dashboards this is the path for getting from raw text stored in multiple locations to an interactive dashboard summarizing all of the data in one location there is a bit more left to learn about logs though before the team can decide on a solution one important factor they need to consider is the time series nature of log data this means the time the log occurred is a primary component in organizing it if the team needs to diagnose a failure that occurs at 4 30 pm they need to see the logs generated in the narrow window of time leading up to and immediately after that time furthermore time series data has a shelf life as it ages it tends to become less and less relevant so with storage requirements evolve over time the most recent logs need to be both written to and read from disk fast for real-time analysis whereas tuesday's logs for instance will not need to be written to on wednesday and the likelihood that the team will need urgent access to logs from a year ago is next to none the other important factor to consider is that logs are bursty while things are running well logs tend to be generated in a fairly steady stream however when something goes wrong systems can generate as much as five times the normal amount if a logging solution is not prepared to handle those bursts it can fail when it is needed most in summary the team needs a solution that collects the logs from all nodes in their architecture normalizes the logs stores them in a central location queries and visualizes them manages the storage of the logs based on relevance and handles bursts the team determines that their options are to buy operate or build a solution they quickly throw out building as they barely have time to research let alone build a solution what remains is buying versus operating buying is the fastest and simplest option but also the costliest there are great log management services in the marketplace such as log z log dna and scalar however their convenience comes with a steep cost and the potentially sensitive data being sent to them is no longer owned by their users boardwalk's cloud provider aws like most cloud providers does offer a ready-to-deploy logging pipeline consisting of managed services for ingesting storing and visualizing logs this solution allows users to have the convenience of a managed solution while maintaining data ownership but each component in the pipeline charges per log that passes through it which as log traffic increases can potentially be even more expensive than the third-party managed solutions operating entails installing an open source solution on their own infrastructure and maintaining it themselves there are multiple open source solutions available but the one that is by far the most widely used is the elastic stack formerly known as the elk stack which stands for elastic search logstash and kibana logstash is used to transform and index the data into elasticsearch which is a document database whose shards contain instances of the full text search engine called apache lucene this design is in contrast to the sql database paradigm we outlined before when discussing log aggregation in general there are no tables for data to be normalized into instead there are indexes which elastic recommends organizing by time stamps not log source that means that logs from all sources generated in a specific window of time will be written to the same index then once the specified time has elapsed a new index will be created and no more logs will be written to the old one no transformation is actually required to index the logs since elasticsearch stores text but it does help the readability of the logs to do some formatting before they are stored this data stored in elasticsearch is then queried and visualized by cabana which uses elasticsearch's rest api rather than sql sqlite query syntax is an available feature of cabana but the rest api is actually what's being used under the hood logstash in addition to parsing and indexing data used used to serve as a collection and shipping agent but the elastic team has since replaced it with their lighter weight tool beats the inclusion of beets in the elk stack is why elastic now refers to it as the elastic stack which is the term we will use going forward the boardwalk team was able to easily install and use a small development mode stack for testing but in researching the next steps required for production configuration they found themselves in a deep and time-consuming territory of indexes shards jvm heap and garbage collection multi-availability zone clusters grok patterns and message queues this configuration difficulty leaves the team with a dilemma they don't have the budget to pay for the convenience of a managed solution but they don't have the time to figure out how to set up a robust self-managed solution either in addition they're not comfortable with giving up ownership of data for the convenience if they could just get a viable elastic stack deployed with a ui that makes the management sensible they would be prepared to make the trade-off of the added time needed to manage the cluster for the ownership of data and the financial expense saved by not using a managed solution this is where lodge fits in lodge provides an opinionated pre-configuration for the elastic stack that allows users to leverage elastic's benefits while eliminating the engineering overhead of setting up the stack lodge does not eliminate the maintenance burden but does provide a ui for facilitating that maintenance users still get to own their data this trade-off is that lodge only provides pre-configuration for a small subset of log types so users needing to ship other log types will not get a streamlined of an experience and finally lodge does use one managed service for log backup and archiving but this actually both simplifies and even decreases storage costs for reasons we'll get into later so hopefully this hypothetical use case and existing solutions comparison has given you an understanding of why lodge was built next we'd like to give a high-level overview of lodge's architecture as well as some demos showing how it works thank you sam now let's see how lodge works this is what lodge looks like from a user's perspective on a high level the user has deployed lodge on the network so now all the applications in that network can ship logs to the stack using filebeat a subset of beats specifically for collecting and forwarding log data the user can then view these locks from the lodge dashboard that is deployed with the stack next let's zoom in on launch and take a closer look at its individual architectural components logis infrastructure is comprised of a series of technologies that work together to ship transform store visualize and monitor your log data you've recognized some of these from a previous discussion of the elastic stack lodge uses these components as well as some additional supporting ones all of which we'll go over before we demonstrate how lodge works first where filebeat this component is not actually part of the lodge deployment but it is the shipper that lodge automatically generates configuration files for which the user then imports onto their servers filebeat is a lightweight shipper for forwarding and centralizing log data before filebeat logstash was used both as a data collector as well as a data transformer but that was one issue logstash requires jvm to run and this dependency coupled with the implementation in ruby became the root cause of significant memory consumption the introduction of beats opened new doors as file beats are now used as lightweight agents installed on different servers and your infrastructure for shipping logs filebeat can actively monitor the log files or locations specified collect log events and forward them to a specific location in this case your data buffer kafka kafka is the first supporting component we have introduced locks and lock volumes are unpredictable in nature following a production incident and precisely when you need them the most locks can suddenly search and overwhelm your locking infrastructure in order to protect lockstash and elasticsearch against such data bursts lodge incorporated a buffering mechanism to act as message brokers to flatten the curve when there is bursty locked traffic kafka is usually deployed between the shipper and the indexer acting as an entry point for the data being collected we'll discuss in more detail later in the scientific decisions section on why this buffer is needed but for now just note that it's there to buffer incoming logs for lockstash after kafka we have lockstash to supplement what was mentioned before about lockstash lockstash is a server-side real-time data processing pipeline that ingests data from multiple sources simultaneously transforms it and then sends it to a stash like elasticsearch and amazon s3 in the case of lodge logstash first ingests data log data from specific kafka topics performs parsing and transformation and sends lock over to two different storages in lodge this leads us to the storage layer which consists of elasticsearch and amazon s3 we have already talked about elasticsearch but as a recap elasticsearch is a distributed restful search and analytics engine that centralizes your log data so you can search index and analyze log data of all shapes and sizes one might notice that now we also have the amazon s3 bucket receiving data from logstash storing logs in elasticsearch can be very costly both in terms of monetary value as well as in terms of engineering hours you have to put in to retrieve them back s3 is supporting elasticsearch here by acting as a backup for data currently in elasticsearch as well as long-term archive for data no longer needed in elasticsearch what exactly s3 is and why we're using it instead of just storing data in elasticsearch we'll also cover in the design decisions section in between elasticsearch and s3 we have lodge restore as we mentioned previously s3 stores data that is no longer needed in elasticsearch but what if the user changes their mind for that purpose we built an application to retrieve data from s3 and re-index it back into elasticsearch cabana is the next elastic stack component we use on lodge cabana is a ui that's built on top of the elastic stack it allows you to visualize and analyze data within elasticsearch the interface is accessible through a browser and a lodge dashboard with a built-in web server the kibana server communicates with the elasticsearch cluster to retrieve its data cabana also stores all of its data within elasticsearch indices this is convenient because we don't have to manage a database for these data and handle things like backup finally we have the lodge dashboard which serves as a unified dashboard for using cabana and lodge restore downloading file beat configurations for different supported log types and managing kafka and zookeeper clusters this is the high level overview of what the lodge dashboard looks like now let's take a look at how lodge works here's the demo walkthrough of how you can install and initialize lodge first the user installs logic cli once the installation is done the user can run launch init to initialize launch and prepare to be deployed on aws here are some instructions for installing and deploying lodge i'll briefly go for the process and demo the installation and deployment process afterwards the first step the user takes is installing the launch cli then they can run login it to initialize the application and prepare it to be deployed once the initialization is finished they can run lodge deploy which after a few questions will deploy the entire lodge infrastructure next is the demo walkthrough of how you can deploy lodge once the initialization is done the user can run lodge deploy which after a few questions it'll deploy the entire lodge infrastructure based on a series of cloudformation templates that are generated from the lodge cloud development kit code onto either a new amazon web services virtual private cloud or an existing virtual private cloud of the choice as a reminder this is a large deployment with a lot of components it might take some time to finish deploying this is how large this infrastructure looks like on aws after the launch deployment is finished first we have the bastion host it is a special purpose server instance that is designed to be the primary access point from the internet and access a proxy to the other ec2 instances then we have the elasticsearch cluster with the master nodes data nodes and the volt only node next up are the zookeeper and kafka cluster instances then we have the kibana instance the logstash instance in an auto scaling group and lastly the webtools instance here we can see that lodge is deployed on the same vpc as the boardwalk app board block is currently using engine access their web server and filebeat is already installed onto the instance we copy the public ip address of the boardwalk instance to ssh into it we then go to the path location etc filebeat to locate the filebeat.yaml configuration file simply remove the configuration file and create a brand new one based on the configuration generated from the lodge dashboard specifically from the shippers section follow the instructions and pick the module you would like to be incorporated into your filebeat config and download the module configuration file here is an example of the configuration that is dynamically generated by lodge based on what module option you have selected copy the content over to the new filebie.yaml configuration file to start shipping logs to ensure that the module is working as expected filebeat has to be restarted you should also verify the status of filebeat after restarting as well we can see that filebeat is active again after restarting to test that you can start using the launch logging pipeline simply send a car request to the nginx server to generate an nginx axis log to view the nginx access log that we just generated go to the kibana tab in the lodge dashboard click on analytics within the embedded cabana ui and then discover in order to view the logs you first have to create an index pattern index patterns that are generated by logstash they're prefixed with the keyword logs and the timestamp of when logstash creates the log from a specific kafka topic once you're done with creating the index pattern your nginx access lock is now ready to be viewed launch restore is a service that allows you to retrieve archival log data from amazon s3 given a specific date range entry index to lock data back into elasticsearch to be visualized in cabana first the user defines the start and end date to retrieve the archive logs from s3 a list of all the log files that are inserted into s3 during that time frame will be fetched and lists under log files once we see the success message it indicates that the logs are re-indexed into elasticsearch and are ready to be visualized in cabana you can also download the log files and here is the raw log text file that is downloaded going back to the kibana tab we'll have to repeat the steps mentioned before for viewing the logs in kibana and create a new index based on the predefined index pattern for the restored logs to distinguish between the restored logs and the logs that are shipped into elasticsearch in real time this restore logs this index is prefixed with the keyword restored and the timestamp is when the log was inserted into s3 once the index pattern for the restored logs are created in cabana we can then head back to the discover dashboard to check out the restored logs by choosing the specific filter restored in the lodge dashboard we have built-in monitoring tools to manage kafka and zookeeper kafka cowl is a management tool that would allow the user to see a list of all the topics dynamically generated by filebeat when the ships logs to kafka within the consumer tab consumer group tab we can see that lockstash belongs to a consumer group that is consuming logs from kafka topics and in the brokers tab we can see all the kafka brokers heading over to zoo navigator it is a management tool that allows us to manage both kafka and zookeeper in one place we zoom into kafka with access to all things kafka for example the softkafka brokers consumers config etc that concludes our demo of lodge here's a review of all the lodge components filebeat for shipping logs to lodge kafka for buffering the logs lockstash for parsing transforming and indexing the logs elasticsearch and s3 for storing the logs and the large dashboard for managing kafka re-indexing archive locks back into elasticsearch from s3 and using cabana which is there to query and visualize the locks in elasticsearch so what's left for the remainder of the presentation we're first going to look at the supporting components we added to the elastic stack to build lodge then discuss an implementation challenge we faced along the way now i'm going to pass it over to rana and regina to talk about these remaining topics thank you justin the engineering decisions we are going to cover are the decision to add kafka and s3 to the elastic stack let's start with kafka and why we need it we have already mentioned that logs are bursty in nature that is when something goes wrong systems can all of a sudden generate a burst of locks much larger than the normal traffic it generates when everything is running smoothly logstash instance can only handle so much data and horizontal scaling doesn't happen instantly so when this burst occurs a large version of the log data we would want can be lost or worse log stash can crash and shut down the pipeline's ability to index any logs at all a solution for this problem is to use a data buffer so logstash can pull log data of the end of the queue at its own pace regardless of the rate at which the logs are generated there are three message queue options that we consider for pipeline the first is wrapped mq which is a simple broker solution however it has to enqueue large amounts of data its responses to dq requests from logstash slow down considerably redis is also an option since it's an in-memory cache that can read and write enormous amounts of data at an incredible speed keeping data in memory allows it to be even faster than kafka but also makes it less reliable since it doesn't process data when it shuts down or crashes so we decided to go with kafka for multiple reasons durability we wanted our logs to be durable and kafka uses the disk for storing data if a node crashes we don't lose the data availability because we can we can have multiple replicas of each partition if a broker goes down its data is still accessible from other replicas performance kafka is able to process a very high volume of messages very quickly which makes it able to handle a burst and logs when they occur the drawback for the user is that it adds complexity and management overhead to the pipeline let's look at how we set up kafka in more detail our kafka deployment consists of zookeeper cluster and kafka cluster where the zookeeper cluster used for configuration management and kafka brokers are used to store data and topics divided into partitions taking into consideration best practices for setting up a cluster in a distributed system first we set up the cluster over three availability zones because when an issue that causes an instance to go down occurs there is a good chance that the same problem will affect the other instances in the same availability zone like network and power outages another decision we made was to separate zookeeper nodes from kafka brokers they run on a different machines so they don't compete for resources and if a broker goes down it will not affect zookeeper nodes and vice versa regarding the size of the cluster we decided to go with the three zookeeper nodes and three kafka brokers which will allow one node to go down without affecting the cluster for example in kafka cluster there would be enough brokers left to choose a new a new leader for the partition that can continue writing loss to desk also using multiple brokers increases the performance of kafka because the data for a topic is spread between partitions and different progress which means less disk space is used per broker and producers and consumers requests are spread out between different machines finally separating zookeeper and kafka instances and deploying them over different availability zones give the user flexibility to scale the instances as needed in the future so now with kafka in a place logstash is able to pull the logs of the queue at its own pace regardless of the first in uh locked traffic now i will pass it to regina to talk about another design decision we made thanks serena the other supporting component we'd like to discuss is s3 s3 is a file storage service managed by aws before we talk more about s3 though let's talk a little bit about elasticsearch elasticsearch has an amazing built-in lifecycle management system with tiered storage not only is there a dedicated data node rule for reads and writes but there are various specializations within the data role including search and machine learning optimized roles the roles we considered using for our cluster were the roles specific to time series data hot warm cold and frozen the hot tier is where the most computing resources are needed as this is where new logs are being written to the current index while also being read by kibana for real-time analysis once new logs are no longer being written to an index the index can be moved to the warm tier where the data is still relevant enough that it needs to be read fast so that it can be optimized for reads then as the tiers get colder the indexes get smaller and slower to read instead optimizing for storage this system is great and as much of a pleasure as it would have been to use it in our pipeline it would have required deploying four times the number of servers we really needed for our use case furthermore calculating the required ssd instant storage needed per node was not a decision we wanted to make for our users as there are too many variables instead we opted to use s3 as both cold storage for older logs and as a backup for new logs even though s3 charges storage fees storing logs as json formatted text files in an s3 bucket is more space and cost efficient than storing them as json documents in elasticsearch charts using local instance storage trade-off is speed this trade-off is why we only want to keep the most recent logs in elasticsearch and move everything else to s3 if the user needs to see older logs it will first have to re-index them into elasticsearch from s3 using the lodge restore tool in the dashboard this design also provides a smaller and simpler starting architecture for their elasticsearch cluster which we will discuss in more detail next since we decided not to deploy the tiered data node architecture for elasticsearch we could start with only two generic data nodes and allow the user to scale from there otherwise we were still able to follow elastic's recommendations for designing an available and partition tolerant production cluster some of the specific recommendations we implemented included separating master eligible and data node rules to dedicated instances so that the tasks of managing cluster state and disk input output operations do not compete for resources and so that they can scale independently scaling the data nodes horizontally with primary shards in one availability zone zone and ripple class in the other we implemented this by deploying the data nodes in an auto scaling group across across both availability zones and configuring the cluster to allocate shards across the two availability zones using a fixed odd number of master eligible nodes elasticsearch recommends three and scaling them vertically to keep up with the cluster state as it grows three master eligible nodes only one per availability zone requires of the resources to perform the master role in the event that it is selected the third node can use minimal resources and only serve as a tiebreaker in the event of a partition between the availability zones where a master election is held in both halves of the cluster if this partition and subsequent master election occurs the half of the cluster in the availability zone with no vote only node will not elect its master eligible node as master and instead we'll go offline until a connection is established with the other half which will remain online during the partition this configuration allows half of the cluster to remain available while preventing a split brain scenario where there are two nodes containing two different cluster states that both think they are the single source of truth for the cluster with this architecture in place users are able to choose how long they want to have immediate access to their logs from kibana and delete the data from elasticsearch once that time has elapsed they can rest assured knowing that if they change their mind later they can always re-index the data back into elasticsearch for visualization with kibana this concludes our designs our design decisions section in the next section we will discuss an implementation challenge we faced in building lodge in this final section we're going to discuss a challenge we faced in resolving circular dependencies while automating lodge deployment for automating launch deployment we used aws cloud development kit or cdk cdk is their infrastructure as code library that provides an interface for generating cloud formation templates in a cdk application values that are used within the application but are not actually known until after the infrastructure is deployed values such as instance ids or ip addresses are assigned arbitrary tokens until their real values are resolved if a component is dependent on the resolved token value of another component in the same stack then it will throw an error this occurred for example when our automate is set up for logstash dependent on knowing the ips of the kafka brokers and the elasticsearch master eligible nodes another example is lodge dashboard which needed the ips of other components in the infrastructure to configure the management utilities the solution to a unidirectional dependency such as this is simple we separated the deployment into several stages mapped based on the dependencies they have on each other so no stack would need to use an unresolved token value from the same stage but as you can see from the diagram the zookeeper stage still has a circular dependency problem in our code for deploying zookeeper the configuration of every node depending on the ip addresses of all other nodes and the node itself to better explain the problem let's look at how we set up a zookeeper node in more detail as we mentioned before each node needs to be configured with all the clusters ips on the other hand cdk code only creates cloud formation templates so you cannot execute commands on existing ec2 instances therefore to setup and configure zookeeper we added scripts to the user data section which is provided as a part of the instance creation arguments not after from here we can see that the script in the user data needs to contain the resolved id of the instance being created before it is created which is a circular dependency beside besides that it also needs to contain the eyepiece of the other nodes which means the other nodes mods already been deployed and here's the problem which nodes can we deploy first therefore we could not separate them out into multiple stages based on the dependencies they had because these dependencies were circular we determined that the problem was where uh was where both depending on aws to assign ibs to zookeeper instances and trying to use that these ips in zookeeper configuration so there were two possible solutions either remove the the need for aws to assign ips or perform the zookeeper configuration using some automation tools like ansible after cdk had finished creating those instances you may wonder why this wasn't the problem for elastic's search cluster it's because for elasticsearch we didn't need to use the ipswide building that clustered in the cdk code since we were able to use an auto discover plugin that allows the clusters nodes to automatically configure themselves after everything has been deployed to solve the zookeeper issue we decided to tackle the problem by eliminating the need for aws to assign ips to the instances we needed to determine the ips ourselves before deployment this meant that we first needed to dynamically generate those ips based on the cider plot of the subnets that the clusters would be deployed in where these cider blocks came from we depended on how large is deployed which can be a new vpc created during deployment or an existing vpc and subnets specified by the user for a new vpc we create new subnets within the cider block provided by the user and calculate the first i available ips from there this task was simpler than with an existing vpc as we could assume that the subnet was empty other than the few ips reserved by aws for an existing ppc where the user selects the subnets we had to iterate through the range of ips in a given cider block and verify that each potential ip was not already assigned as we could not assume that an existing existing subnet was empty we accomplished this by using this aws cli to programmatically check if and any network interfaces were attached to that id once we found the available ips we needed we assign them to the nodes in the cluster and would have the needed values to use in the configurations to conclude lodge is an open source self-managed logging framework that allows users to ship transform store visualize and monitor their logs it's built for reliability and scalability requires minimal configuration and is a comprehensive observability solution for small teams operating a distributed system before we move on to the q a section at the end we'd like to leave you with this complete architecture diagram of lodge that we're hoping you're able to digest after going through this presentation there's still more work we have left to do on lodge to make it suitable for more use cases and relieve more of the user's management burden we'd like to start by adding more support for more log types then we'd like to add monitoring and alerts for lodge and implement intelligent auto scaling for elasticsearch finally we'd like to make lodge deployable on any cloud environment thanks for spending your time with us today to learn about lodge we're a remote team collaborating across the us and canada and we're here to take any questions you might have about our project so our first question is from lena and she she also says the visuals were very impressive and the scope of our project was impressive so good job team um what question sorry the question is what did your day-to-day look like in order to build such a polished framework which i think any of us can probably answer but since i'm already talking it was day and night it was pretty much lodge i think for the most part um there was a lot to learn in attempting this um and yeah it was it was a lot but i think we were able to come away feeling pretty proud of what we accomplished uh does anyone else want to speak to this uh yeah i can chime in as well i felt that um our team did a pretty good job in meeting regularly and kind of catching up with each other in regards to where we're at when it comes to onboarding new technologies um and i think overall as a team we work pretty good and yeah i think so i think that's about it for me yeah i would agree with that as well it was a lot of work and and we had to you know we went into this project um you know with no experience with uh with the elastic stack and so we learned about the stack and and learned how to you know be a proficient user of it and even and pre-configure it and you know it was a lot of we had a lot of ground to cover so the next question is um we tied a lot of different technologies together what were some of the more challenging aspects of this feat [Music] reina do you want to feel this one you did a lot in terms of kafka and some of the other solutions yeah sure um so in terms of the challenges like the most challenging thing is to learn what is the best practices to uh create or build the clusters that in our project and uh find the best way to automate uh these instances in the way that fit our project yeah i would add to will um to what rayna said one of the challenges i think in tackling this project was that the breadth like the scope of what lodge required was quite wide but then in terms of the level of understanding required to really use some of the tools like kafka like elasticsearch even just understanding grok patterns for filebeat that also required a depth of knowledge so we were both stretching ourselves to income encompass and learn everything we needed to and also balancing mastery with just-in-time learning which was a big challenge but i think we did a good job in pulling it off so the next question is from katrina and she says great presentation and the project since you worked remotely have you encountered any communication challenges and if so how did you deal with them i would say at least communication wise um the i i would answer in this way like a takeaway that i got after having this experience was the importance of just um you know you know when you meet just being very organized about decisions that you make you know you write them down and just have everyone talk particularly writing down um summaries of meetings and just being more organized about that this experience has made me i think just better at that i would agree i think too another thing is learning to balance i think we're all very passionate in our work and learning as much as we can and i know for myself something i struggled with was learning to balance um [Music] contributions but also wanting to contribute and putting effort into things when all also being able to pull back and take things in a different direction if needed sort of the this dual requirement of being able to really put yourself into something and being able to walk away from it if that's what's best for the overall project to add to that i think being honest to both yourself and your teammates really help our team like move forward in a lot of sense i feel like um in the beginning of capstone we're kind of like not like strangers not like complete strangers but it was hard a little bit hard for us to communicate in a team setting but as time goes on we kind of figured out what each other this community way of communication is and then it just went a lot more smoother as it goes so i guess the biggest takeaway for me is to be completely transparent and be more honest with your team pretty much that'll be my takeaway for this question so will has another question and it was uh was testing the project difficult it seems like you'd need a non-trivial app to generate meaningful logs that is a good observation will maybe we can kind of trade off in responding to this um so at the end of the day elasticsearch does have a testing tool called rally which is a feature we would love to be able to integrate lodge with if we had more time and would continue to work on this project our baselines for testing lodge mostly came about through understanding the the constraints of kafka using it as the message bus and understanding the constraints of elasticsearch and lodge and the different components and while we were able to do some baseline testing we weren't really able to simulate the the types of sort of log ingest you would encounter in a production environment yeah that's what i would say that we weren't able to actually simulate production traffic but what we set up though like in the kafka cluster was there at least for for this one it's there's 30 gigabytes of storage per broker and so and it has with a retention time of seven days so that's really your limit is um that 90 gigabytes over seven days basically is the max that it can in its current state can hold um and at one time it can fill up and kafka can take is extremely fast and so all of that could get used at once um and that that's really your that's your limit is the physical limit of what we put in kafka and that being said our use case um from what we looked at it seemed like for a small company with a distributed architecture it could be reasonable to assume they would generate about 50 gigs of logs a day which with the three separate uh kafka brokers at 30 gigabytes each that um still falls it could allow for bursts of traffic um without overwhelming the system for our use case yeah also i will add to that with the default launch deployment we have the log stash into a auto-scaling group so it will grow and shrank depending on how much work is doing pulling logs from the the queue so we the the cluster can handle around 90 gigs of logs uh within a seven days period awesome we'll give it a few more seconds for any last questions and thank you for your kind comments you guys we appreciate that you appreciate our project um so i think uh it's safe to wrap up there we don't have any more questions so thank you everyone for your time and for coming out to learn about lodge and hear us talk about our project we're really excited to share it with you we're really proud of what we built and we'd like to thank launch school and our mentor nick for helping us achieve this and just great job team so thank you everyone yeah thank you thank you thank you 