Thank you for joining us today for our presentation on Lodge. Lodge is an open-source, self-managed logging framework designed for small distributed applications. With Lodge, users can ship, transform, store, visualize, and monitor their logs.

In this presentation, we'll begin by sharing a short narrative based on a fictional company called Boardwalk to illustrate the perfect use case for Lodge. We'll then compare existing solutions to the challenges introduced by Boardwalk's growth. Once we establish where Lodge fits alongside these solutions, we'll provide an architectural overview of Lodge and demonstrate its functionality from a user's perspective. Finally, we'll delve into some key design decisions we made during the development of Lodge and discuss an implementation challenge we faced.

Let's start with the use case. Boardwalk is a fast-growing online retailer that specializes in handcrafted board games. Their products have gained significant popularity on social media, resulting in a substantial increase in website traffic. While this is an exciting time for Boardwalk, it also poses challenges as their architecture needs to adapt to their growth.

Currently, Boardwalk operates on a monolithic codebase, where the entire business logic runs on a single machine. This architecture allows different components/modules of the application to communicate by calling each other's methods. For example, when a user wants to check out their shopping cart, a request is made from the web server to the app server's checkout endpoint, which then calls the check inventory method in the inventory module, and so on.

To accommodate their growing user base and meet the demand for new features, Boardwalk decides to transition from a monolithic architecture to a microservice architecture. In a microservice architecture, the communication between the modules becomes more complex, as they need to make API calls over an inherently unreliable network. For example, when a user initiates the checkout process, the checkout service fetches the user's cart contents from its database and then requests the inventory service to verify product availability in its database.

While this network complexity is necessary to support increased development turnover and scalability, it introduces challenges. For instance, when issues arise with the checkout system, the development team can no longer easily examine logs by directly accessing application and database servers, as they now need to SSH into multiple instances and piece together isolated log records to understand the system's overall state.

To address this problem, the team researches observability solutions—tools that collect logs from multiple servers and consolidate them in a central location for easy analysis. They find that the ideal solution for Boardwalk's use case is log aggregation, which involves collecting logs and sending them to a centralized location for storage and analysis.

However, simply storing logs as raw text in a database is not helpful. To make logs searchable and analyzable, a mature observability solution must store data in a structured format. This entails organizing logs based on their characteristics, such as timestamps, source, and message, and using SQL queries to find and analyze the logs instead of visually scanning through thousands of lines of text. Additionally, the solution should offer visualization tools to create interactive dashboards summarizing the log data.

Another important consideration is the time series nature of log data. Logs need to be organized based on when they occurred, especially when diagnosing failures that happen within a specific time window. As time progresses, the relevance of logs decreases, so the storage system must handle the ongoing storage of recent logs for real-time analysis while archiving older logs that are less likely to be needed urgently.

Furthermore, log data tends to be bursty. During normal operation, log generation follows a steady stream, but when issues occur, log output can potentially increase by up to five times the usual amount. A logging solution must be able to handle these bursts effectively to prevent failures when logs are needed most.

After researching available options, the team narrows down their choices to buying or operating a solution. Building their own solution is quickly dismissed due to time constraints. Buying a managed solution, while convenient, comes at a significant cost and raises concerns about data ownership. Operating an open-source solution on their own infrastructure allows them to maintain ownership of their data but requires time and expertise for setup and maintenance.

One widely used open-source solution is the Elastic Stack, previously known as the ELK stack, which comprises Elasticsearch, Logstash, and Kibana. Logstash, a server-side real-time data processing pipeline, ingests data from multiple sources, transforms it, and sends it to Elasticsearch—a distributed, document-based database that uses Apache Lucene as its search engine. Kibana is a visualization tool that utilizes Elasticsearch's REST API.

However, deploying and configuring the Elastic Stack for production use can be complex and time-consuming, involving various concepts such as indexes, shards, JVM heap, garbage collection, multi-availability zone clusters, grok patterns, and message queues.

This is where Lodge comes in. Lodge provides an opinionated pre-configuration for the Elastic Stack, simplifying the setup process while still leveraging the benefits of Elasticsearch. Lodge does not eliminate the maintenance burden but provides a user interface (UI) to facilitate that maintenance. Users retain ownership of their log data, and Lodge allows them to manage a subset of log types effectively.

Lodge also incorporates Amazon S3 as a managed service for log backup and archiving. This not only simplifies the storage process but can also reduce storage costs. Now that we've provided an overview of Lodge's purpose and comparison with other solutions, let's delve into the architecture of Lodge and demonstrate how it works.

From a user's perspective, Lodge comprises several components that work together to handle log data. The user deploys Lodge on their network, enabling all applications in the network to ship logs to the stack using Filebeat—an agent specifically designed for collecting and forwarding log data. The user can then access and view these logs using the Lodge dashboard.

Now, let's examine Lodge's individual architectural components. Lodge leverages technologies from the Elastic Stack, as well as additional supporting components. One such component is Filebeat, a lightweight log shipper that Lodge automatically generates configuration files for. These files are then imported onto the user's servers, allowing Filebeat to actively monitor log files or specified locations, collect log events, and forward them to a designated location, known as the data buffer.

To accommodate bursty log traffic and flatten data bursts, Lodge incorporates Kafka as a message broker between Filebeat and Logstash. Kafka acts as an entry point for the collected log data, buffering incoming logs before forwarding them to Logstash. This buffering mechanism helps protect Logstash and Elasticsearch from being overwhelmed during peak usage.

Logstash, the next component, ingests log data from specific Kafka topics, performs parsing and transformation on the data, and sends it to Elasticsearch and Amazon S3 for storage. Elasticsearch is primarily a distributed, document-based database that is optimized for search. By leveraging Elasticsearch's REST API, Kibana can query and visualize log data.

Meanwhile, Amazon S3 serves as a managed service for log backup and archiving. This component simplifies storage management while reducing costs.

In summary, Lodge enables users to ship, transform, store, visualize, and monitor log data. By providing pre-configured settings for the Elastic Stack, Lodge eliminates much of the engineering overhead involved in setting up the stack. However, bear in mind that Lodge focuses on a specific subset of log types, and users needing to ship other log types may experience a less streamlined experience.

In the next sections, we will discuss some of the key design decisions made during the development of Lodge and describe an implementation challenge encountered along the way. By understanding Lodge's architecture and its benefits, we hope you now have a clear understanding of why Lodge was built and how it can simplify log management for small distributed applications. In this coding Capstone project video, we will be demonstrating how Lodge works. Firstly, let's take a look at Lodge from a user's perspective. Lodge is deployed on the network, enabling all applications in that network to send logs to the stack using Filebeat, a subset of beats designed for collecting and forwarding log data. The user can then view these logs from the Lodge dashboard, which is deployed with the stack.

Now, let's dive deeper into Lodge and examine its individual architectural components. Lodge's infrastructure consists of various technologies that work together to ship, transform, store, visualize, and monitor log data. Some of these components have been discussed before in relation to the Elastic Stack, and Lodge utilizes these components along with additional supporting ones. We will go over these components before demonstrating how Lodge works.

Filebeat is the first component we will discuss. Although not part of the Lodge deployment itself, Filebeat is the shipper that generates configuration files automatically for Lodge. The user then imports these configuration files onto their servers. Filebeat is a lightweight shipper used for forwarding and centralizing log data. Previously, Logstash was used for both data collection and transformation. However, Logstash's dependency on JVM and its implementation in Ruby led to significant memory consumption. With the introduction of Beats, Filebeats are now used as lightweight agents installed on different servers in the infrastructure for shipping logs. Filebeat actively monitors specified log files or locations, collects log events, and forwards them to a specific location, in this case, the data buffer Kafka.

Kafka is the first supporting component introduced. Logs and log volumes can be unpredictable, particularly during production incidents when logs are needed the most. Logs can suddenly surge and overwhelm the logging infrastructure. To protect Logstash and Elasticsearch from such data bursts, Lodge incorporates a buffering mechanism using Kafka as a message broker. Kafka acts as an entry point for data collected by Filebeat and is deployed between the shipper and the indexer. It flattens the curve during bursty log traffic, buffering incoming logs for Logstash.

Moving on to Logstash, which supplements what has been mentioned about it previously. Logstash is a server-side real-time data processing pipeline that ingests data from multiple sources simultaneously, transforming it, and sending it to storage, such as Elasticsearch and Amazon S3 in the case of Lodge. Specifically for Lodge, Logstash ingests log data from Kafka topics, performs parsing and transformation, and sends the log data to different storage locations.

This brings us to the storage layer, which consists of Elasticsearch and Amazon S3. Elasticsearch has been previously discussed, but as a recap, Elasticsearch is a distributed RESTful search and analytics engine that centralizes log data for easy searching, indexing, and analysis. Storing logs in Elasticsearch can be costly financially and in terms of engineering hours required to retrieve the logs. Amazon S3 supports Elasticsearch by acting as a backup for data currently in Elasticsearch and as long-term storage for data that is no longer needed in Elasticsearch. The reasons for using S3 instead of storing all data in Elasticsearch will be covered later in the design decisions section.

Between Elasticsearch and S3, we have Lodge Restore. Lodge Restore is an application specifically built to retrieve data from S3 and re-index it back into Elasticsearch. This allows users to retrieve log data from S3 if they change their mind about archiving it.

Now let's talk about Kibana, another Elastic Stack component used in Lodge. Kibana is a UI tool built on top of the Elastic Stack that allows users to visualize and analyze data within Elasticsearch. Accessible through a browser, Kibana communicates with the Elasticsearch cluster to retrieve data. Kibana also stores its data within Elasticsearch indices, eliminating the need for a separate database and simplifying backup processes.

Lastly, we have the Lodge Dashboard. The Lodge Dashboard serves as a unified platform for using features such as Kibana and Lodge Restore. It provides functionalities such as downloading Filebeat configurations for different supported log types, managing Kafka and Zookeeper clusters, and offering a high-level overview of the Lodge infrastructure.

Now, let's take a closer look at how Lodge works by demonstrating the installation and initialization process. To begin, the user installs the Lodge CLI. Once installed, they can run "lodge init" to initialize the application and prepare it for deployment on AWS. Detailed instructions for installing and deploying Lodge are provided, which will be briefly covered here, followed by a demonstration of the installation and deployment process.

The installation process starts with the user installing the Lodge CLI. After installation, they run "lodge init" to initialize the application and prepare it for deployment. When initialization is complete, they can run "lodge deploy", which, after a few questions, will deploy the entire Lodge infrastructure. This process is based on a series of CloudFormation templates generated from the Lodge Cloud Development Kit code. The user can choose to deploy on either a new Amazon Web Services (AWS) virtual private cloud (VPC) or an existing VPC of their choice. Note that this is a substantial deployment with multiple components, so it may take some time to complete.

Once the Lodge deployment is finished, the infrastructure on AWS includes several components. The Bastion host is the primary access point from the internet and acts as a proxy to the other EC2 instances. The Elasticsearch cluster consists of master nodes, data nodes, and a dedicated voting-only node. Additionally, there are Zookeeper and Kafka cluster instances, a Kibana instance, a Logstash instance in an auto-scaling group, and a web tools instance. In this example, Lodge is deployed on the same VPC as the Boardwalk app, which utilizes an NGINX server with Filebeat already installed. 

To configure Filebeat, the user copies the public IP address of the Boardwalk instance and uses SSH to access it. They navigate to the "/etc/filebeat" directory to locate the "filebeat.yaml" configuration file, remove the old configuration file, and replace it with a newly generated one from the Lodge Dashboard. The Lodge Dashboard provides instructions for selecting the desired Filebeat module and downloading the corresponding module configuration file. The content of this file is then copied over to the new "filebeat.yaml" configuration file. To ensure the module is working as expected, Filebeat needs to be restarted, and its status should be verified.

To test the Lodge logging pipeline, the user can send an HTTP request to the NGINX server to generate an NGINX access log. To view this log, they navigate to the Kibana tab in the Lodge Dashboard, click on analytics within the embedded Kibana UI, and then select "Discover" to view the logs. Before logs can be viewed, an index pattern needs to be created. Logstash generates index patterns prefixed with the keyword "logs" and a timestamp based on when the log was created from a specific Kafka topic. After creating the index pattern, the NGINX access log is ready to be viewed.

Lodge Restore is a service that allows users to retrieve archival log data from Amazon S3 based on a specific date range and re-index it into Elasticsearch for visualization in Kibana. To retrieve the archive logs, the user defines the start and end dates, and a list of log files inserted into S3 during that time frame is fetched and displayed under "log files". When the re-indexing process is successful, a success message is shown, indicating that the logs have been re-indexed into Elasticsearch and are ready for visualization in Kibana. Users also have the option to download the log files.

In order to manage Kafka and Zookeeper, Lodge provides built-in monitoring tools accessible through the Lodge Dashboard. Kafka Cowl is a management tool that allows the user to see a list of all topics dynamically generated by Filebeat when it ships logs to Kafka. In the "Consume Group" tab, Logstash belongs to a consume group that consumes logs from Kafka topics. In the "Brokers" tab, all Kafka brokers are displayed. Zoo Navigator is another management tool that enables the user to manage both Kafka and Zookeeper from a single interface. It provides access to various Kafka components, including Kafka brokers, consumers, and configuration settings.

To summarize, Lodge incorporates several components in the Elastic Stack, including Filebeat, Kafka, Logstash, Elasticsearch, and Kibana. Filebeat is responsible for shipping logs, Kafka acts as a buffer to handle bursty log traffic, Logstash processes and indexes logs, Elasticsearch and S3 store the log data, and the Lodge Dashboard provides centralized management and access to features like Kafka monitoring, re-indexing archived logs, and utilizing Kibana for log analysis. The decision to add Kafka and S3 to the Lodge infrastructure was driven by the need to handle bursty log traffic and to provide durability and availability for the log data.

In the next part of the presentation, we will discuss implementation challenges we faced during the development of Lodge. This written document presents a condensed version of a coding Capstone project video. It discusses the implementation of a solution for indexing logs using a data buffer. The challenge is addressed by utilizing a message queue, and three options are considered: wrapped mq, Redis, and Kafka. After weighing the advantages and disadvantages, Kafka is chosen based on factors such as durability, availability, and performance.

The Kafka deployment is explained in detail, consisting of a zookeeper cluster and a kafka cluster. The clusters are set up in three availability zones to account for potential issues affecting individual instances. Zookeeper nodes and Kafka brokers are run on separate machines to prevent resource competition, and the cluster is designed to allow for one node to fail without impacting overall functionality. The separation of zookeeper and kafka instances across availability zones offers scalability options in the future.

With Kafka implemented, the next component discussed is Amazon S3. Elasticsearch's built-in lifecycle management system is described, outlining different tiers (hot, warm, cold, frozen) based on data relevance. However, the project decides against deploying the tiered data node architecture and instead opts for using S3 as both cold storage for older logs and as a backup for new logs. This approach proves more cost and space-efficient, with the ability to re-index data into Elasticsearch for visualization with Kibana when needed.

The architecture of the Elasticsearch cluster is then explained. Master eligible and data node roles are separated onto dedicated instances, allowing for optimal resource allocation and independent scaling. Scaling is achieved by deploying data nodes in an auto scaling group across availability zones, with fixed numbers of master-eligible nodes. This configuration ensures availability during potential partitions by having half the cluster remain online. Overall, the design aims to provide flexibility and options for users while minimizing complexity.

The transcript proceeds to discuss an implementation challenge faced during the automation of lodge deployment. The challenge involves resolving circular dependencies when using the AWS Cloud Development Kit (CDK) to automate the process. For example, the deployment of Logstash and Zookeeper required knowing the IP addresses of Kafka brokers and Elasticsearch master-eligible nodes. The solution is to separate the deployment into multiple stages, avoiding unresolved token values. However, the zookeeper stage still faces circular dependency issues due to configuration requiring IPs of all other nodes, with no clear deployment order.

The challenge is further explained by focusing on zookeeper node setup. Since CDK code only creates CloudFormation templates without executing commands on existing instances, the script in the user data section of zookeeper instances needed to contain resolved instance IDs and IP addresses, resulting in circular dependencies. Unable to solve this by separating into stages, two potential solutions are discussed: removing the need for AWS to assign IPs or using automation tools like Ansible for zookeeper configuration after CDK finishes instance creation.

Ultimately, the solution involves determining the IPs beforehand, either by creating new subnets within a provided cider block or by iterating through IPs in an existing subnet to find available ones. This eliminates the circular dependencies and allows for successful deployment.

In conclusion, Lodge is an open-source, self-managed logging framework designed for reliability and scalability. It offers capabilities for shipping, transforming, storing, visualizing, and monitoring logs. The project aims to support various log types, implement monitoring and alerts, and introduce intelligent auto scaling for Elasticsearch. The team acknowledges the remaining work but is confident in providing a comprehensive solution for log management. This is a transcript of a coding Capstone project video, discussing an existing PPC (Pay Per Click) framework called Lodge. In this project, users are able to select subnets and iterate through the range of IPs in a given cider block. The goal is to verify that each potential IP is not already assigned, as they cannot assume that existing subnets are empty. To achieve this, the team used the AWS CLI (Command Line Interface) to programmatically check if any network interfaces were attached to a given ID. Once the available IPs were found, they were assigned to the nodes in the cluster, providing the necessary values for configurations.

Now, moving on to Lodge itself. Lodge is an open source self-managed logging framework designed to help users ship, transform, store, visualize, and monitor their logs. It is built with reliability and scalability in mind, requiring minimal configuration. It offers a comprehensive observability solution for small teams operating in a distributed system.

In the presentation, the team shared a complete architecture diagram of Lodge. They hoped that the audience would be able to digest the information presented. However, they also acknowledged that there is still work left to be done to make Lodge suitable for more use cases and relieve more of the user's management burden. Their future plans include adding support for more log types, implementing monitoring and alerts for Lodge, and enabling intelligent auto-scaling for Elasticsearch. Lastly, they want to make Lodge deployable on any cloud environment.

The team expressed their gratitude to the audience for spending their time learning about Lodge. They mentioned that they are a remote team collaborating across the US and Canada, and they are available to answer any questions about their project.

During the Q&A session, Lena commended the team on the impressive visuals and the scope of the project. She then asked about the team's day-to-day activities in building such a polished framework. The team members took turns answering, emphasizing that it was a lot of work and required a steep learning curve. They mentioned the importance of regular meetings and catch-ups to keep everyone informed about their progress. Overall, they felt proud of what they accomplished.

The next question came from Katrina, who asked about any communication challenges the team faced while working remotely and how they dealt with them. The team recognized the initial difficulties in communication due to being somewhat unfamiliar with each other. However, they found that transparency, honesty, and organization were key in improving their communication as time went on. They also mentioned the importance of writing down decisions and having clear summaries of meetings.

Will asked the team about the challenges they faced in tying together different technologies. The team members shared their experiences, with Reyna mentioning the challenge of learning the best practices for building clusters and automating instances. Will added to her answer, explaining that the scope of the project required them to understand various tools deeply, such as Kafka, Elasticsearch, and grok patterns for Filebeat. They had to balance broad knowledge with deep understanding, which was a challenge, but they felt they handled it well.

Regarding testing the project, Will asked if it was difficult as they needed a non-trivial app to generate meaningful logs. The team responded that while they weren't able to simulate production traffic, they did perform some baseline testing. They mentioned using Elasticsearch's testing tool called Rally, which they would have liked to integrate with Lodge given more time. They also mentioned the physical limits of their Kafka cluster in terms of storage and retention time, which determined the maximum amount of logs the system could handle.

Eventually, the Q&A session came to an end with no further questions from the audience. The team expressed their gratitude to everyone for attending the presentation and for their kind comments. They thanked Launch School and their mentor, Nick, for their guidance and support throughout the project. Overall, they were excited to share their work and proud of what they had achieved.