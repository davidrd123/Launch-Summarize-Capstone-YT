Hello everyone, thank you for joining our presentation. I'm Juan, one of the creators of Horus. This presentation will be divided into six sections. First, I will introduce the problem encountered by Evil Tees, an e-commerce site, using a real-world scenario. Then my colleague Rich will provide context on the engineering area needed to solve the problem: observability and telemetry. Next, Kali will discuss existing solutions and explain why they are not perfect for our use case. She will also demonstrate Horus in action. Afterwards, Jose will describe the architecture of Horus and demonstrate how easy it is to deploy with Docker and npm. Then, I will discuss some implementation challenges that arose during the development of the project. Finally, I will touch upon future work that could be done for Horus.

Let's start by introducing Evil Tees. It is an e-commerce site that has recently experienced a surge in popularity. What began as a one-person site hustle has outgrown its monolithic architecture, prompting the need to transition to a microservices architecture for better organization. Oliver, the site's creator, has led a team of engineers in building Evil Tees over the years, and he continues to oversee each sub-team in their transition to microservices.

One day, Oliver wakes up to find that the site has been flooded with complaints from customers who are unable to buy products. Some customers even declare that they will never use the site again. Oliver is perplexed, but the CEO demands answers. The team attempts to replicate the problem by making purchases themselves, but they encounter inconsistencies - some can complete the purchase without errors, while others experience issues. It becomes challenging to pinpoint the problem due to these inconsistencies.

As a last resort, Oliver sifts through the logs of their microservices, hoping to find useful information. However, the logs are overwhelming and difficult to parse. Oliver is frustrated as he tries to make sense of the data. He may spot mentions of errors here and there, but it remains unclear what is happening and why. Oliver realizes that he needs an observability tool to gain insights into the workings of the application.

Before delving further into Oliver's situation, let's gain a better understanding of observability, telemetry, traces, and metrics. Observability refers to the ability to measure the internal state of a system solely based on its outputs. It allows developers to effectively investigate and resolve issues by examining data related to the system's performance and behavior.

Telemetry, on the other hand, refers to the collection and transmission of data for monitoring. It encompasses three main data types: metrics, logs, and traces. Metrics are values that express some data about the system and are often aggregated or calculated over time. They provide a broader picture of an application's overall state and performance.

Logs, on the other hand, are structured or unstructured lines of text emitted by an application in response to events. They offer detailed information about the system's actions and can be extremely useful when diving into the specifics of the application's behavior. Finally, traces provide a view of a request's journey through a system, capturing each step and the time it takes. They provide contextual information that helps pinpoint issues in a distributed system accurately.

To achieve observability, we need to understand the three stages of a telemetry pipeline: emission, shipping, and presentation. In the emission stage, telemetry data is generated in production through various sources such as production code, hardware, or external software. The data is collected and aggregated using SDKs. The shipping stage involves receiving the raw telemetry data, processing it, and storing it for later retrieval. The presentation stage transforms the telemetry data into understandable visuals and metrics for stakeholders to make informed decisions.

Now, let's return to Oliver and Evil Tees. The team faced issues with customer complaints and the inability to consistently replicate errors. Observability could have helped them overcome these challenges. By implementing an observability system alongside their application, Oliver's team would have access to correlated metrics and traces. This would allow them to quickly identify errors without relying solely on customer feedback.

Using metrics, such as requests per second and errors per second, the team could detect errors in real-time. However, this high-level perspective might not provide enough information to pinpoint the root cause of the problem. That's where traces come in. By examining the correlated traces, the team could analyze the user's journey and understand the specific actions that led to the error. This would empower them to quickly and accurately resolve the issue.

Oliver and his team recognize that they need an observability system, but they don't want to invest time in mastering the intricacies of observability. Instead, they are seeking an all-in-one solution, a SaaS platform that provides all the necessary components and is easy to set up and use.

Now that we understand the problem faced by Oliver and the potential of observability, let's explore existing solutions. There are two major SaaS solutions available: Solution A and Solution B. Both offer a range of observability features, including the generation of correlated metrics and traces. However, upon closer examination, it becomes clear that neither of these solutions is ideal for Evil Tees.

Solution A lacks certain critical features required by Evil Tees, making it unsuitable for their specific use case. Additionally, Solution A has limitations that affect its scalability and performance, making it less effective for a rapidly growing e-commerce site like Evil Tees.

Solution B, on the other hand, is a powerful and feature-rich solution. It provides all the necessary components for observability and offers excellent scalability and performance. However, Solution B also comes with a hefty price tag that may not be feasible for Evil Tees at this stage.

Given these limitations, Oliver and his team decided to develop their own observability solution: Horus. Horus is an open-source observability tool specifically designed for microservices. It allows users to generate, store, and visualize correlated telemetric data, such as metrics and traces, in real-time. With Horus, Oliver's team can easily find bugs and reverse engineer the application's behavior.

In the next section, Jose will delve into the architecture of Horus and demonstrate how to deploy it seamlessly using Docker and npm. But before we move on, I want to touch upon some implementation challenges that arose during the development of Horus. These challenges include ensuring compatibility with a wide range of programming languages and frameworks, managing the storage and retrieval of large volumes of telemetry data, and handling the real-time visualization of the data.

Despite these challenges, the team successfully overcame them, and Horus is now a powerful tool that can be utilized for observability in microservices. Looking to the future, there are several areas where Horus can be further improved. These include enhancing its compatibility with additional programming languages and frameworks, implementing advanced analytics and anomaly detection features, and integrating with other observability tools and platforms.

In conclusion, Evil Tees encountered difficulties in identifying and resolving issues due to a lack of observability. By implementing an observability system like Horus, Oliver's team can gain real-time insights into their application's behavior, detect errors, and understand the specific actions that led to those errors. Horus provides a comprehensive solution for observing microservices and can be easily deployed using Docker and npm. While challenges were encountered during its development, Horus is now a valuable tool for improving the observability of microservice architectures. In the future, Horus can be further enhanced to cater to specific programming languages and frameworks, provide advanced analytics, and integrate with other observability tools. Evo T-shirts was facing issues with its website. Some customers described their problems as inaccurate shopping carts or inability to complete checkout transactions. Others simply complained that the website was not functioning properly. Although some team members were able to recreate certain issues, such as inaccurate shopping carts, they were unable to consistently replicate the problem in order to pinpoint the root cause. This made it difficult for them to fix the issues because they couldn't be sure if their attempted fixes actually solved the problems or if they just couldn't reproduce them. The lack of understanding of the problem left them with frustrated customers.

This is where observability could have made a significant difference for Oliver and his team. By implementing an observability system alongside their application, they could have generated telemetry data that would have helped them identify and resolve issues. With metrics and traces correlated, they would have been able to easily identify errors as they occurred, without having to wait for customers to report them. They could have seen errors in real-time and gained insights into the user journey that led to those errors. This information would have allowed the developers to efficiently locate and fix the issues, regardless of their nature, such as 500 errors, 404 errors, or issues with payload in a POST request.

However, Oliver and his team, being software engineers, were not interested in mastering observability itself. Their focus was on using observability to address their current challenges. So, the most suitable option for them would be an all-in-one service that provides everything they need without requiring them to build and maintain the entire system themselves. The two major players in this domain, New Relic and Datadog, offer such solutions. They provide comprehensive observability platforms out of the box, eliminating the need for complex configurations. All that's required is copying a few lines of code into their application files, and the system starts working seamlessly. Additionally, these platforms abstract away complexities such as database management and querying, freeing the developers from having to deal with those details directly. The developers interact with the data through polished UIs that offer a wide range of features.

Despite the benefits of these SAS (Software-as-a-Service) solutions, there are some immediate drawbacks for Oliver and his team. Firstly, the cost is prohibitively high, ranging from $2,000 to $11,000 per month for a small engineering team. This cost is beyond what Evo T-shirts can afford at the moment. Secondly, opting for the free version of these platforms would mean giving up ownership of their data. If they were to switch to another platform or stop using these services, they would have to leave their data behind. Furthermore, the free versions limit the data retention period to eight days, making it impossible for them to perform deeper analysis beyond that. Lastly, the sophisticated UIs provided by these platforms require time and effort to fully comprehend and utilize, which is something Oliver and his team are not willing to invest in right now.

Considering these factors, Oliver finds the SAS approach unsuitable for Evo T-shirts. He wants to explore a DIY (Do-It-Yourself) approach, leveraging multiple technologies and open-source projects in the observability space. However, he realizes that such an approach comes with its own challenges. Integrating and making these various technologies work together seamlessly requires a deep understanding of each component. For instance, not all technologies can effectively collaborate with each other. This means spending significant time and effort to master these technologies and properly integrate them, which Oliver and his team are not willing to prioritize.

Hence, the current situation for Oliver and his team is that they appreciate the advantages of both SAS solutions and a DIY approach. They like the idea of DIY as it offers flexibility and affordability, and allows them to retain control of their data. On the other hand, the ease of deployment and the comprehensive features of SAS solutions are also appealing. However, the cost of SAS is a major obstacle for them. They need a solution that aligns with their requirements and limitations.

Fortunately, there is a solution that can address these concerns for Oliver and his team: Horus. Horus is an open-source observability tool designed for microservices. It enables users to generate, store, and visualize correlated telemetry data, providing real-time insights into the health of their applications. Being open-source, Horus offers free access to its code, ensuring transparency and allowing for customization if needed. The simplicity of Horus makes it easy for developers to understand and integrate into their existing systems without requiring in-depth knowledge of observability technologies.

Horus achieves its functionality by leveraging two key components: open telemetry for generating telemetry data and time scale for storing it. Open telemetry, a widely accepted industry standard, allows for the generation of various types of telemetry data, including metrics and traces. Time scale, a powerful time-series database, provides efficient storage and retrieval capabilities for this data.

The Horus user interface serves as the primary interaction point for users to visualize their telemetry data. It displays metrics such as request count, error count, and latency health on the main page, allowing users to monitor the real-time health of their application. The UI also provides a view of the correlated traces, enabling users to filter and sort them based on their requirements. Clicking on a specific trace provides a detailed breakdown of the spans, categorized as HTTP or non-HTTP. Users can explore individual spans to see attributes such as action, status code, and method, providing insights into the specific details of each trace. The UI allows for easy navigation between different traces and returning to the main page to start a new analysis.

In summary, Horus offers a solution that fits the needs of Oliver and his team. Being open-source, it provides the benefits of a DIY approach while eliminating the high engineering costs associated with building and integrating multiple technologies. It offers a user-friendly UI that includes all the necessary features without overwhelming the users. The clear correlation between metrics and traces facilitates effective issue identification and resolution. While Horus may not provide every feature offered by commercial SAS solutions, it meets the specific requirements of Evo T-shirts at a fraction of the cost. With Horus, Oliver and his team can gain the desired observability without compromising their limited budget and resources.

Now, let's delve into the architecture of Horus and understand how it is built and deployed. The high-level overview of Horus consists of two main components: the microservice application on the left and the Horus infrastructure on the right. The microservice application comprises a root service and three additional services, which handle various operations requested by customers. These services generate telemetry data, which is then sent to the Horus infrastructure for processing and visualization.

The Horus infrastructure runs on a single VPS (Virtual Private Server) and utilizes a Docker network. Within this network, four Docker containers work together to handle different aspects of the telemetry data pipeline. These components communicate with each other to ensure the smooth flow of data throughout the system.

To enable the generation of telemetry data, Horus utilizes a package called Horse Agent. Horse Agent is an NPM package developed using open telemetry tools. It integrates seamlessly into the microservice application, allowing it to generate the necessary telemetry data required by Horus. By adding just a few lines of code, the Horse Agent instates the necessary exporters and instrumentation to generate metrics and traces. The instrumentation is lightweight and easy to install, taking no more than five minutes per service. For the root service, it involves less than 10 lines of code, while for the dependent services, only two lines of code are needed.

With the Horse Agent added to the microservice application, the HTTP request cycles now generate the required telemetry data, which is seamlessly transmitted to Horus. This empowers Evo T-shirts to unlock the benefits of observability and gain insights into the health and performance of their application.

The Horse Agent comprises two distinct agents: the metrics agent and the tracing agent. The metrics agent, installed with ultra-instrumentation, generates metrics at regular intervals. Horus generates three types of metrics: total request count per 10 seconds, error count per 10 seconds, and latency health. These metrics provide a high-level overview of the application's performance and health. Users can select different time frames to analyze or refresh the data to ensure up-to-date information.

The tracing agent, on the other hand, generates detailed traces of the user journeys within the application. These traces capture the entire path followed by a user's request, allowing developers to thoroughly analyze and understand the behavior of the application. The Horus UI provides filtering and sorting options to explore and navigate through the traces based on specific requirements. Clicking on a trace reveals its underlying spans, categorized as HTTP or non-HTTP. Users can further inspect individual spans, revealing attributes such as action, status code, and method.

Horus offers a comprehensive observability solution for Evo T-shirts, combining the benefits of open telemetry and time scale within a user-friendly interface. Its simplicity and affordability make it an ideal choice for Oliver and his team. With the deployment of Horse Agent and the utilization of the Horus infrastructure, Evo T-shirts can effectively monitor, analyze, and troubleshoot their application's performance in real-time. The correlation between metrics and traces ensures a holistic understanding of any issues or bottlenecks faced by their users.

In conclusion, Horus is a viable solution that allows Evo T-shirts to achieve observability and address their challenges without breaking their budget or requiring extensive engineering efforts. By leveraging open-source technologies, Horus strikes a balance between customization and ease of use, providing a highly accessible solution for microservice-based applications. With Horus, Oliver and his team can effectively monitor and optimize their application's performance, resulting in improved user experiences and customer satisfaction. The microservice application consists of a root service and three services to the left. These services post telemetry data to the Horse infrastructure on the right. The Horse infrastructure runs on a single VPS with a Docker network. Within this network, there are four Docker containers, each communicating with each other. The goal is to break down and understand how this architecture works.

Starting with Evil T-Shirts as an example, the customer sends an HTTP request to the root service, which uses other services to fulfill the customer's operations. However, the HTTP request cycle doesn't generate the required telemetry data. This is where the Horse Agent comes in. The Horse Agent is an NPM package developed using OpenTelemetry tools. OpenTelemetry is chosen due to its rising popularity and ability to generate different types of telemetry data, such as metrics and traces, using a single technology.

After installing the Horse Agent NPM package, it helps to instrument and install the exporters and instrumentation required to generate the data. The code snippet for instrumenting the root service is less than 10 lines and takes about five minutes. Dependent services require even fewer lines of code to generate traces.

By adding the Horse Agent to the Evil T-Shirts application, the HTTP request cycles now generate telemetry data that can be used by Horse. There are two types of agents: metrics agents and tracing agents. Metrics agents are installed with OpenTelemetry instrumentation using the NPM package. They generate metrics every interval, including requests per second, errors per second, and latency per request. These metrics are then exported and sent as a POST request to the Horse connector.

Tracing agents, on the other hand, are not only installed in the root service but also in the dependent services. This is because spans, which represent operations in a service, happen in other services as well. To ensure all spans are captured, the tracing agent is installed in every service. These traces and spans are not generated continuously but are queued to be sent as a batch. This batching allows for higher request throughput and resource efficiency.

The Horse infrastructure on the VPS consists of four components. The first is the connector, which is an Express server that receives POST requests for metrics and traces. It parses and validates the data, enforces UTC timestamps, and allows associating metrics with traces. The second component is the database, which stores the data. TimeScale was chosen as the time series database for its ability to sort, organize, and work with time-series data efficiently.

The remaining two components are the UI server and the UI client, running in separate Docker containers. The UI server is an Express app that queries the database, while the UI client is an XJS app with React components that provide a user interface to interact with the data. Decoupling the UI client and server allows for easier development and future extensions. The UI client includes custom React components for database administration.

Horse follows a telemetry pipeline that consists of three stages: mission, shipping, and presentation. The mission stage involves the customer's request traveling between the root service and other services, where telemetry data is generated by the Horse Agent. The shipping stage includes the services themselves, which export the data they generate. Both metrics and traces are exported through the Horse connector to be processed and stored in the database.

Finally, the presentation stage involves the UI server and client, which allow users to interact with the database. Horse can be easily deployed using Docker. A Docker Compose file is provided to spin up the necessary containers and make them work together seamlessly.

In the development process, several challenges and trade-offs were encountered. Connecting metrics and traces together created time zone differences. To maintain the relationship between metrics and traces, a time-based connection was used. Metrics cover a time interval, and traces falling within that interval are queried and associated with metrics. However, time zone inconsistencies led to incorrect correlations between metrics and traces. To fix this, timestamps in the connector were consistently set to UTC.

Another challenge was the visualization translation. The UI components of Horse presented several challenges, such as querying the database for the right traces and maintaining connection with metrics. By implementing a time-based connection and handling time zone inconsistencies, the visualization was improved. In developing our coding Capstone project, we encountered several challenges that required innovative solutions. One issue we faced was the potential storage of hundreds of IDs per second, which could quickly become overwhelming. Storing such a large amount of data in real-time posed a significant storage problem.

Another challenge we encountered was related to the complex relationship between metrics and traces. In some cases, a metric could be related to multiple traces, or a trace could be related to multiple metrics. This "many-to-many" relationship made it difficult to store and query the data effectively.

To address this issue, some platforms store only one important trace per metric data point. If a trace meets certain criteria, such as having a response code of 500 or 400, it is considered the most important and stored. However, this approach leads to a loss of information as other relevant traces are discarded.

To overcome this limitation, we adopted an organic connection based on time intervals. Since metrics cover a specific time interval, we only need to query traces that fall within that same interval. This ensures that the correct traces are supplied while avoiding the need to store all traces.

However, this introduced another challenge related to different time zones. Correlating metrics and traces became problematic due to inconsistencies in time zones. When testing the database, we discovered that sometimes metrics and traces were not well correlated. We found that the timestamps generated by the connector varied depending on the time zone of the machine executing the code.

To rectify this, we ensured that the connector always generated a timestamp in UTC, regardless of the machine's time zone. This consistency in timestamps resolved the time zone-related issues and resulted in the correct correlation between metrics and traces.

Another challenge we faced involved visualization. Initially, we planned to use Grafana, an open-source dashboard monitoring solution, to integrate the presentation stage into our application. However, when implementing Grafana, we discovered that visualizing traces would require using tracing solutions different from the open telemetry standard.

After evaluating various tracing options, we decided to use Jager. However, Jager had its drawbacks. First, it used its own instrumentation to collect tracing data, necessitating instrumentation with both open telemetry and Jager. Additionally, Jager only supported storing tracing data in either Cassandra or Elasticsearch, which meant adding another database to our project.

To resolve these issues, we modified our architecture. Traces in Jager format were sent to Elasticsearch for storage, while metrics generated by open telemetry were stored in Timescale. Our application queried Timescale directly for metric data and used Elasticsearch as a data source for retrieving trace data. This altered architecture required two virtual private servers to handle the resource-intensive Elasticsearch.

Despite solving some challenges, this architecture had its drawbacks. The resource-intensive nature of Elasticsearch caused server crashes, and the use of two databases introduced potential time synchronization issues between metrics and traces. Additionally, we were not comfortable with the increased potential for failure that a complex architecture entailed.

After careful consideration, we decided to develop our own user interface (UI). This decision offered several advantages, including avoiding the aforementioned challenges and enabling total customization of the user experience. With our own UI, we had complete control over the look, feel, and functionality of Horus, improving its suitability for potential users.

The development of the UI presented its own set of challenges. We had to make deliberate design decisions and learn new libraries and frameworks. We used the Tailwind library to streamline CSS, Nevo for metric charts, Chart.js for waterfall charts, and Next.js as a frontend framework, similar to React. Despite the additional engineering effort, the results of developing our UI were well worth it, enhancing the elegance and usability of Horus.

While our Capstone project fulfilled our initial goals, we recognize that an open-source project is never truly finished. In future iterations, we would like to develop more pages for the UI, allowing users to choose custom time frames and view all the activities that occurred within those frames. We also aim to support additional languages apart from JavaScript when generating open telemetry data. Lastly, we plan to further test the project to determine the hardware requirements for hosting Horus on larger systems.

In conclusion, our Capstone project encountered various challenges that required innovative solutions. We addresses issues relating to data storage, complex relationships between metrics and traces, time zone inconsistencies, and visualization. Despite planning to use Grafana initially, we ultimately developed our own UI, bringing numerous advantages and enabling customization. While Horus fulfills our current needs, we have identified areas for future development and improvement. We are proud of the results we achieved and remain open to questions and further discussion. Thank you. During the project, we had to make some design decisions, and I'll discuss why we made those choices. One major decision was whether or not to work on the user interface (UI) component. Initially, we were hesitant about tackling this aspect of the project. We estimated that it would take up around 30% of our total project time. However, as we progressed, we realized that we needed to create our own UI. This decision added complexity to the project since we wanted to ensure that the UI was visually appealing, which would require a significant amount of time. Overall, I would say that the UI development took up around 30 to 50% of our total project time.

Now let's address a question from Katarina about why we decided to work on the observability solution and what other ideas we considered. We chose to focus on observability because we found it to be an area with a lot of potential. The concept of a telemetry pipeline and the challenges associated with it intrigued us. We considered various problems within the observability space but ultimately decided to develop a full-stack application performance management (APM) solution. This decision allowed us to work on both the backend and frontend, which was something that appealed to us. Additionally, we wanted to create our own UI instead of using existing tools like Grafana. Although we had other ideas in mind, one that stands out is the connection between observability and security. We explored the possibility of detecting security breaches through metrics or traces but were unable to find adequate support for this idea. Nonetheless, observability remained a clear choice due to its importance and complexity.

Moving on, we received a couple of questions about why we opted for Next.js as our technology stack. The decision to use Next.js was relatively straightforward. We chose it because someone recommended it, and upon further investigation, we found that it was similar to React but considered easier to work with. Given our previous experience with React, Next.js seemed like the most comfortable choice. It felt familiar enough that we didn't feel disoriented or confused when working with it. In fact, I was able to start building our UI with other teammates within just two hours. Using Next.js simplified the UI development process by eliminating certain steps required in React. This streamlined approach was particularly beneficial since we were already hesitant about creating the UI.

Let's address another question: what challenges did we encounter while working on this project as a distributed team, and how did we overcome them? Working as a distributed team presented its challenges, given the geographical and time zone differences. Initially, we were concerned about how well we would be able to collaborate. However, we found ways to overcome these challenges and maintain effective communication throughout the project. One key aspect was having regular meetings and check-ins to ensure that everyone was on the same page and aware of each other's progress. Despite being physically apart, we created a sense of connection and unity. Additionally, we made accommodations for the different time zones, ensuring that everyone had a comfortable work schedule. We were mindful of each other's well-being and found ways to adapt to the remote working environment. Overall, the team dynamic and deliberate efforts to address these challenges contributed to a successful collaboration.

Now let's discuss any trade-off decisions that, in hindsight, may not have been ideal solutions. One notable trade-off decision relates to Dockerizing Next.js. This process proved to be quite challenging for us, primarily due to the time it took and the difficulties with handling environment variables. When Dockerizing Next.js, we experienced significant delays, with each image generation taking approximately 15 minutes. This hindered our progress towards the end of the project. Furthermore, working with environment variables in Next.js and Docker was not as straightforward as we had hoped. These challenges were not thoroughly considered before committing to Next.js, and if given the opportunity, we would explore alternative front-end frameworks that offer easier Dockerization.

I hope this adequately addresses all the questions raised during the presentation. If you have any additional inquiries, please feel free to reach out to us via Slack. We appreciate everyone's participation and hope you enjoyed our presentation. Thank you once again for joining us. We realized that dockerizing Nexus was not an enjoyable experience. Therefore, we decided to explore alternative front-end frameworks that are easy to work with and offer excellent performance. We hope that this addresses all of your questions. We understand that sometimes questions may arise later on, so please feel free to reach out to us on Slack if you have any further inquiries. We are more than willing to assist you. Thank you once again for joining us today, and we hope you found our presentation enjoyable. Thank you to everyone for being here today.