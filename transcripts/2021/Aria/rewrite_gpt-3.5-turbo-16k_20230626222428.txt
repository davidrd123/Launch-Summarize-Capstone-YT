Hello, everyone! Thank you for joining us today. My name is Yue, and together with my colleagues Caleb, Heath, Sam, Graham, and Seville Hammerman, we have created Arya, a manual canary analysis tool. Today, we will cover several topics. First, I will introduce a hypothetical company called Chart Health and discuss the deployment challenges they face. Then, I will explain various modern deployment options, highlighting the pros and cons of each. Finally, I will dive into why a team might choose canary deployment over other modern deployment types.

Now, let's talk about Chart Health. Chart Health provides electronic medical record (EMR) software that manages sensitive patient information. Their main feature is reading and writing to EMRs. Recently, Chart Health's service priorities have changed due to new regulations and their expansion to include hundreds of medical facilities nationwide. They now want to ensure the high integrity of medical records and achieve zero downtime during deployments to push out more updates and gain a competitive edge.

Currently, Chart Health's deployment pipeline involves taking down the host server, updating the application all at once, and bringing the server back online. However, this traditional or big bang deployment doesn't align with their new service priorities. Any issues with the updated software would affect the entire user base, potentially compromising the integrity of EMRs. Additionally, there is service downtime with each update.

To research deployment types for modern service-based architectures, Chart Health reached out to real-world companies Vamp and Harness, which provide continuous integration and continuous deployment products. Customer surveys conducted by Vamp and Harness revealed that the most popular deployment type is rolling deployment, followed by blue-green deployment, big bang deployment, and canary deployment, which is the least popular.

Let's explore these deployment types in more detail. Traditional or big bang deployment involves replacing the old version of the application with the new version all at once, resulting in service downtime. Rolling deployment, on the other hand, gradually replaces the older version of the application with the updated version in different instances within the architecture. This allows for targeted deployments and better control, but it takes longer to complete.

Blue-green deployment requires two production environments: a current production environment (green) running the old version and a replica environment (blue) running the new version. Traffic is routed to the green environment until the blue environment is ready, at which point the traffic is switched over. This approach allows for instantaneous rollout and rollback, but it requires duplicate production resources.

Finally, we have canary deployment. This technique involves analyzing a new or updated software, called the canary, alongside the stable version in the same production environment. A load balancer routes a minority of network traffic to the canary, while the majority continues to be served by the stable version. This allows for high fidelity analysis using real network traffic. If the canary performs well, the traffic can be gradually increased until it can handle the full load, at which point the stable version is replaced. Canary deployment minimizes risk exposure and provides tight control over testing the canary, but it requires careful configuration and extended deployment time.

Based on their deployment priorities, Chart Health has narrowed down the options. They need a deployment type that allows for high-fidelity analysis of the updated software, impacts a minority of users in case of bugs, and ensures zero service downtime. Canary deployment is the only technique that fulfills all three criteria, so Chart Health is interested in pursuing canaries for subsequent deployments.

Now, let's discuss why existing canary solutions wouldn't entirely meet Chart Health's needs. They find that automated canary analysis, which is the latest generation of canary analysis, routes a minority of traffic to the service revision and a copy of the current production service. However, they prefer a manual approach for promoting new software into their production environment. Additionally, Chart Health lacks the necessary DevOps experience to set up and manage complex solutions.

They explore different types of canary solutions, such as platform services, orchestrator plugins, and standalone services. Platform services offer flexibility but require extensive configuration and management. Orchestrator plugins are easy to work with but are limited by the features of the control plane product. Finally, standalone services are expensive and time-consuming to set up, but they offer advanced analysis and integration capabilities.

Chart Health decides to go with a standalone solution because they want to take advantage of advanced analysis while maintaining control over the promotion process. They also prefer a tool that can self-provision its own infrastructure. After comparing various existing standalone solutions, they consider Vamp, Spinnaker, and our application, Arya.

Vamp is a SAS solution that offers release observability but is expensive. Spinnaker is an open-source solution that self-provisions but lacks release observability. Finally, Arya is a self-hosted open-source canary deployment tool that provides advanced analysis, self-provisions the infrastructure, and offers release observability. Considering their needs, Chart Health chooses Arya.

Let's take a closer look at the architecture and life cycle of an Arya canary deployment. Arya consists of a client user interface app and a backend server. The user configures a new deployment and provides Docker images for the baseline production service and the revised service. After submission, the Arya server coordinates with AWS to provision the necessary resources and setup monitoring and analysis tools.

Once the resources are provisioned, the traffic is routed to the canary infrastructure, and the user can monitor the performance of both the baseline and canary instances. Advanced analysis can be configured, and when ready, a destroy command is invoked to remove the traffic routing rule and tear down the Arya infrastructure, restoring the production environment to its initial state.

To provide a more detailed understanding of the infrastructure, let's return to Chart Health's production environment. Their EMR service is horizontally scaled using a load balancer, which ensures high availability and distribution of traffic across instances. Arya integrates within this environment to perform canary deployments.

In conclusion, we have introduced Arya, a manual canary analysis tool developed by our team. We explored different deployment types, including traditional, rolling, blue-green, and canary deployments. Chart Health, a hypothetical company, narrowed down their options based on their deployment priorities and chose canary deployment for subsequent deployments. Existing canary solutions didn't entirely meet their needs, leading them to select Arya, a standalone canary deployment tool that provides advanced analysis, self-provisions infrastructure, and offers release observability. The architecture and life cycle of an Arya canary deployment were also discussed. Thank you for joining us today, and we hope you found this information valuable. The chart health medical records development team needs a solution for advanced analysis of medical records accuracy. They want to utilize a standalone solution because they don't trust automated software promotion into their production environment. They prefer a manual approach, even though most advanced analysis tools are designed for automation. Additionally, they want a tool that can self-provision its own infrastructure and eliminate the need for DevOps experience.

Let's compare the existing standalone solutions for advanced statistical analysis. Vamp, while feature-rich, is not self-provisioning and quite expensive. Spinnaker is an open-source solution that can self-provision, but it lacks release observability, requiring additional tooling. Finally, there's Aria, specially tailored for canary deployments, offering advanced analysis, self-provisioning, and release observability.

Now, let's delve into Aria's lifecycle for a canary deployment. Aria consists of a client user interface app and a back-end server. The user configures a new Aria deployment by providing two Docker images, one for the baseline production service and one for the revision. Once the deployment is submitted, the Aria server coordinates with AWS to provision resources, install the user-provided software, and configure monitoring and analysis tools. After provisioning, a traffic segmentation rule is created, directing a portion of the incoming traffic to the newly deployed Aria canary infrastructure. The user can then monitor baseline and canary instance performance, configure advanced analysis, and eventually invoke a destroy command to revert the production environment back to its initial state.

Now, let's examine the infrastructure within a chart health production environment during an Aria deployment. The chart health EMR service is horizontally scaled with a load balancer. When Aria is deployed, a fraction of the traffic is routed to both the baseline and canary EC2 instances. The baseline instance mirrors the same version as the production service (version 1.0), while the canary instance represents the updated version (version 1.1). Each deployment also includes a third instance for monitoring and analysis services, unique to that Aria canary deployment.

The Aria user interface provides an application for easy configuration and deployment of resources, access to monitoring and analysis tools, review of status and event logs, and destruction of resources. User configuration involves defining existing resources to coordinate with, precise traffic routing, and providing Docker images and associated compose files. Aria communicates with AWS, deploys the resources, and allows users to access deployment information or destroy it.

The monitoring and analysis tooling used by Aria includes Prometheus, Grafana, Cayenta, and Referee. Prometheus is a time-series database that collects metrics from the baseline and canary instances and stores them as time series data. Grafana is a graphical interface and dashboard builder that allows users to visualize and analyze the collected metrics. Cayenta is a statistical judge that accurately determines the health of the canary application by comparing metrics from the canary and baseline instances. Referee is the graphical front-end for Cayenta, providing users with a user-friendly interface to configure and analyze the canary performance over a given time period.

Now, let's dive into the technical aspects of Aria's implementation. Aria leverages AWS CloudFormation, an infrastructure-as-code tool that provisions and configures resource stacks based on developer-defined templates. Aria uses the AWS SDK to interact with various AWS services programmatically, complementing the CloudFormation capabilities. The SDK enables Aria to perform tasks that may not be easily achievable with CloudFormation alone, such as configuring existing load balancers for traffic routing.

When it comes to traffic routing, Aria uses AWS Application Load Balancers (ALBs). ALBs distribute incoming network traffic across multiple servers running the production application. Aria configures ALBs to route a fraction of the traffic to the canary infrastructure by creating a rule on the ALB listener. Conditional routing based on HTTP requests and sticky session support can also be implemented.

As for the computing resources, Aria deploys the baseline and canary versions of the target application to AWS Elastic Compute Cloud (EC2) instances. These instances are automatically configured with Docker and Docker Compose using user-provided Docker image files and compose setups. Aria also installs the Node Exporter application on each instance, which collects hardware and operating system metrics for monitoring purposes.

The monitoring and analysis toolkit in Aria consists of Prometheus and Grafana. Prometheus collects metrics by pulling from HTTP endpoints and stores them as time series data. Grafana provides a graphical interface to visualize and analyze the metrics collected by Prometheus. Together, they enable users to monitor the performance of baseline and canary instances.

In addition to Prometheus and Grafana, Aria incorporates Kayenta and Referee for canary analysis. Kayenta is a statistical judge that compares metrics from the canary and baseline instances to determine the health of the canary deployment. Users can configure analysis requests via the Kayenta API or use Referee, the graphical front-end for Kayenta, to visualize and analyze the canary performance.

Overall, Aria provides a comprehensive solution for canary deployments, offering self-provisioning, release observability, advanced analysis, and a user-friendly interface. With its unique combination of tools and infrastructure management, Aria enables the chart health medical records development team to ensure the accuracy and quality of their medical records analysis. In this video, we will discuss the metrics data generated by the applications in our canary infrastructure and the analysis tools that process these metrics to provide insight and recommendations regarding the canary. Specifically, we will examine the monitoring tools Prometheus and Grafana.

Prometheus is a popular open-source time series database used to log metrics from targets. It operates on an HTTP pull model, scraping metrics from targets at configured intervals through HTTP requests. The metrics are stored as time series data on disk and can be queried and visualized using the Prometheus front-end and the PromQL query language. Prometheus can identify targets through service discovery and automatically configure itself to start collecting metrics from deployed baseline and canary EC2 instances.

In addition to Prometheus, our system also installs Grafana, which is a graphical interface and dashboard builder often used alongside Prometheus. With Grafana, users can configure rich visualizations of multiple queries and compare baseline and canary instances at a glance. The integration of Prometheus and Grafana allows users to effectively monitor and analyze the metrics generated by their canary applications.

Let's delve into how Prometheus works. It collects metrics by pulling from HTTP endpoints, which can exist on two types of targets: standalone exporters and instrumented applications. Arya supports both. A standalone exporter is an application that collects metrics and exposes them for Prometheus to use. In our setup, node exporter is automatically installed on both the baseline and canary instances. Node exporter is an exporter that exposes hardware and operating system metrics monitored by Unix-based systems.

On the other hand, instrumented applications are applications that incorporate a Prometheus client library. This allows them to collect and calculate metrics, which can then be exposed on an HTTP endpoint. For example, a web application may calculate and expose metrics about request response time, the number of requests received, or the rate of request failure. It is the responsibility of users to instrument their applications, but Arya can help configure Prometheus to scrape the endpoints of these instrumented applications automatically.

Moving on to the analysis tool, Kayenta, and its graphical front-end, Referee. Kayenta is a statistical judge used to determine the health of a canary application by comparing metrics from the canary and baseline instances. While a human engineer can assess a canary by examining metrics themselves, using a statistical judge can help filter signal from noise and provide a more objective assessment.

Kayenta interacts with metrics provided by Prometheus. Users can configure analysis requests and send them to the Kayenta API or use the graphical front-end, Referee. Kayenta was originally part of Spinnaker, a continuous delivery platform developed by Netflix. However, while Spinnaker uses Kayenta to automatically judge and promote canaries to production, Arya offers Kayenta as a supplemental tool for users who want to explore advanced canary analysis and gain more confidence in their deployment.

To configure an analysis, a user selects one or more metrics to examine over a given time period and can weight the relative importance of these metrics. For example, a user may consider memory usage to be more important than request response time or request error rate. The user also sets a threshold for a pass score. Kayenta performs the analysis and returns the results, including a numerical score that can be used to deliver a verdict on the canary's health, such as healthy or unhealthy.

Now that we have covered the technical deep dive, let's discuss some of the engineering challenges faced during the development of Arya, as well as the trade-offs made.

First, let's talk about the limitations of the AWS CDK and how we overcame them. We initially chose to use the AWS CDK because of its ability to express AWS infrastructure as code. However, we discovered that the CDK was primarily designed to be used as a standalone tool rather than programmatically. It lacked the capability to interact with existing production environments and fetch information about existing resources. To address this, we incorporated the AWS SDK into our codebase, allowing us to access important configuration information from the user's AWS resources.

Another challenge we encountered was configuring and deploying the canary and baseline instances. We wanted to work with the user's existing AWS resources and enable the launch of multiple canary deployments simultaneously. However, the CDK's assumption of one project corresponding to one resource stack posed a challenge. To overcome this, we created a wrapper library around the CDK core library, extending its functions and enabling the acceptance of additional configuration parameters.

Installing the user's applications onto the EC2 instances also presented challenges. We required users to provide their canary and baseline applications in the form of Docker images. Transferring these images onto the EC2 instances posed a challenge, as the images could be stored in different Docker registries or may not be stored in a registry at all. To address this, we utilized the Docker save and Docker load commands, converting the images into tar archives and transferring them onto the instances.

Additionally, we faced challenges in monitoring and analyzing the metrics. We chose to use the official node exporter, which exposes system-level metrics related to disk, CPU, and network. While this choice provided valuable system-level insights, it did not include HTTP-related metrics that may be crucial for some applications.

To deploy the monitoring and analysis toolkit, we decided to use Docker images on a single EC2 instance. This approach simplified communication between the different tools and allowed us to specify dependencies, ensuring correct ordering during startup. However, the configuration and setup of these tools proved to be frustrating at times, highlighting the importance of our configuration scripts in streamlining the process for users and reducing troubleshooting efforts.

While we are proud of the features and capabilities of Arya, there are areas we would like to improve in the future. Firstly, we aim to enhance the ability to adjust canary traffic dynamically, allowing users to increase or decrease traffic allocation after the initial deployment. Additionally, we plan to support applications that use an API gateway, as many microservices employ this pattern for routing traffic. Lastly, we intend to expand Arya's compatibility to include other cloud providers such as Microsoft Azure and Google Cloud.

Thank you for watching the presentation on Arya, and we appreciate your support. Here is the dedicated team that built Arya.