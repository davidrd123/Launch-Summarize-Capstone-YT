hi everybody welcome and thank you for joining us today my name is yue and i along with my colleagues caleb heath sam graham and seville hammerman created arya a manual canary analysis tool let's take a look at the topics we'll be covering today first i'll introduce a hypothetical company chart health and walk you through deployment challenges they will face then i will go through several modern deployment options and the pros and cons of each finally i will explain why a team might choose canary deployment over other types of modern deployments next sam will go over why existing canary solutions would not entirely meet the team's needs before introducing a solution that will our application aria caleb will go into the technical deep dive of arya's architecture followed by seville who will give insight into the technical decisions and engineering challenges we face during development and finally you will touch on some great ideas we have for future work should we continue developing aria before we get started i'd like to introduce you to a hypothetical company chart health chart health provides an electronic medical record software which manages sensitive patient information the product's main feature is reading and writing to electronic medical records or emr for short the team makes new software updates also known as revisions on a daily basis recently chart health's service priorities changed due to new regulations and expansion to include hundreds of medical facilities nationwide now they want to ensure that medical records are as high integrity as possible and to have zero downtime during deployments so they could push out more updates and have a competitive edge over their competitors when there are new revisions or new changes to a software the software in production is replaced by a process called deployment currently chart health's deployment pipeline looks like this first they take down the host server which means their application will be down for the duration of the update second they update whole or large parts of an application in one go or in one big bang and third they bring the server back online the problem with traditional or big bang deployment is that it doesn't align with chart health's current service priorities emr service updates go out to everyone which means that the entire user base will be exposed to all issues if there are any problems with the revision thus potentially compromising the integrity of emrs there is also the issue with service downtime with each and every update as part of their research chart health contacted vamp and harness who are real world companies that provide continuous integration and continuous deployment products to see what deployment types their customers use in modern service-based architectures stamp and harness then provided customer surveys broken down by deployment types both surveys indicate that the most popular is rolling deployment with usage at over 40 percent the second most popular is blue green with usage at over twenty percent the third most popular is big bang with usage at around eight percent and the least popular is canary with usage at around five percent in the upcoming slides i will describe all of these deployments with the context of choosing canary in addition to describing traditional deployments the term big bang can also be used to describe deployments in modern service-based architectures the concept is similar except that the deployment is isolated to all instances of the service in this example a service is briefly taken offline during the deployment the old application version is replaced by the new and the service is brought back online to service requests the pros are it is very easy to deploy using this technique the cons are similar to those in traditional deployments there is exposure of all users to problems with the updated software if there are any and service downtime the second deployment type is rolling deployment it is a technique where an updated software is introduced by incrementally replacing the older application version on each of the multiple instances within their architecture the pros are it lets you target specific instances according to region ip address and other types of traffic differentiators roll out status transparency and control meaning it's easier to get a clearer picture of how successful the deployment is going because rolling deploys are so slow and the risk is limited to a subset of users the cons are there is a an extended deployment time frame and it is not suitable for all or nothing changes which means either everybody sees a change or nobody sees a change the third deployment type is blue green deployment it requires two production environments a blue and a green the environment to the right green is the current production environment running the old application version all network traffic is currently being routed to it the environment to the left blue is an exact replica of the green except it runs the newer application version zero network traffic is being routed to it once the blue environment is ready to receive traffic the load balancer routes all network traffic from the green to the blue the traffic router can redirect all network traffic to and from the blue or green on an on an as needed basis depending on how well the updated software is doing if it is doing well then it can maintain routing to the blue and destroy the green the pros are instantaneous roll out and roll back and simple disaster recovery this is due to the ability to route all traffic to and from the two environments the cons are having duplicate production resources to make two environments and the exposure of all users to service revision issues and finally the last deployment type is canary deployment it is a technique that allows a new or updated software called the canary to be analyzed alongside the old called the stable version all in the same production environment it has a load balancer that routes a minority of the incoming network traffic to be served by the canary while the majority of the traffic continues to be served by the stable version that way the performance of both versions can be observed by using real network traffic if the canary is doing well then the amount of traffic can be incrementally increased and its performance monitored until the developer is confident that the canary can handle the full traffic load then they can replace the stable version with the canary to complete the deployment process the pros are there is minimized risk exposure to a minority of users and a high fidelity analysis of the canary's performance data due to the ability to test the canary by using tight segmentation control the cons are onerous configuration extended deployment time frame and complex disaster recovery because both the stable and canary instances exist in one production environment this is in contrast to blue-green deployment where disaster recovery is as simple as routing all traffic back to the stable version now that we understand how the four deployments work and the pros and cons of each let's revisit chart health's deployment priorities one medical records should be as high integrity as possible and two there should be zero downtime during updates with this understanding in mind chart health narrowed down the deployment types based on three criterias the option to have high fidelity analysis of the updated software's performance minority users impacted if there are bugs to the revision and zero service downtime during the deployment as you can see from this chart canary deployment is the only technique that fulfills all three criterias so chart health is interested in using canaries for subsequent deployments in the upcoming section sam will continue with our chart health narrative to re to review existing canary solutions thanks uh now that chart health has decided to implement canary deployments the company realizes that there are many different ways to implement this technique two approaches can be derived based on the historical progression of the technique let's review the traditional canary deployment approach we'll refer to this as manual canary analysis a canary deployment routes a minority of network traffic to a revision of the production service performance monitoring tools then gather data relative to the canary performance traditionally the performance data or metrics are reviewed by an engineer who decides whether or not the revision is fit to succeed the current production service the latest generation of canary analysis termed automated canary analysis routes a minority of incoming traffic to the service revision and a freshly initialized copy of the current production service one way to think about this is that automated canary analysis is like a scientific study or experiment where a control group in this case a copy of the production service is routed the same amount of traffic as an experiment group the service revision we'll refer to the baseline the copy of the production service as the baseline going forward the performance of each of the instances is monitored with the same monitoring tooling that would be used in manual canary analysis an automated analysis is then performed on the time series data of all the metrics collected within this automated analysis a comparison is made among the metrics data using advanced statistical techniques resulting in a score if the score is above a threshold then the new service revision is promoted to production chart health then looks to the existing solutions that are on the market they begin researching existing products that would help them implement canary deployments chart health discovers that the existing solutions can be divided into three categories which we will refer to as platform services orchestrator plugins and standalone services we will briefly explore platform services and orchestrator plugins and then focus our attention on standalone platform services are solutions constructed from managed services offered by infrastructure providers the providers abstract away the low-level details of managing the infrastructure but ultimately it's the responsibility of the user to define and manage the infrastructure the major benefit of this class of solutions is that it can be easily tailored to the development team's needs but this is also a very diy approach the major downside of these solutions is that implementation is tedious and requires a devops skill set it's not uncommon that companies lack devops experience and chart health is one of these and therefore they decide not to utilize platform solutions orchestrator plugins are another category orchestrator plugins work with control plane products or orchestration products like kubernetes or istio because they're using these control plane products the pro the canary deployments that can be achieved are rather primitive because they're limited by whatever features the control plate products may have major benefit of this class of solutions is that configuring deploying and observing deployments is nearly effortless the major downside of this class of solutions is that a control plane or orchestration solution must be implemented and used to coordinate the entire production environment the development team decides against orchestrator plug-in solutions because their architecture isn't that complex and doesn't rise to the level of complexity that's needed or justifies a control plane or orchestrator solution the standalone category refers to software as a service or open source platforms these products can integrate with control plane and orchestration solutions or they can perform canary deployments independently canary releases are not the focus of but rather one of many features offered by these products most standalone solutions aren't opinionated in the sense that they can support interfacing with most major observability and deployment tools whatever features a given standalone product lacks it can easily interface with another solution which provides that feature several products in this class implement automated canary analysis which is unique also unique to this class the biggest drawback for these solutions is that they are expensive in time to set up or they are expensive in cost the chart health medical records development team decides to utilize a standalone solution because in order to ensure medical record accuracy they would like to take advantage of that advanced analysis of this category however chart health doesn't trust the automated promotion of new software into their production environment so they would like to use a manual approach although most advanced analysis tools are built to work with an automated approach lastly because they laugh like devops experience they would like a tool that self-provisions its own infrastructure let's compare the existing standalone solutions which offer advanced statistical analysis techniques the first is vamp this tool isn't self-provisioning it's a sas solution so it's rather expensive it builds in release observability and it's very feature rich spinnaker is an open source solution um it it self provisions unlike vamp but it doesn't include release observability you need to implement your own tooling for that it's also very feature-rich and then our tool aria provides advanced analysis it's self-provisioning and release observability but it's tailored specifically for canary deployments introducing aria aria is a self-hosted open source canary development tool arya self-provisions the infrastructure which enables high fidelity analysis of service provisions deployed within the user's production environment ari allows users to customize to focus on customization that is beneficial to the canary analysis itself while abstracting away much of the infrastructure definition deployment management and monitoring configuration that doesn't add benefit to a canary analysis in order to better understand the overall structure and purpose of aria we'll briefly walk through the life cycle of an aria canary deployment the aria application itself is composed of a client user interface app and a back-end server the user configures a new aria deployment the user interface and provides two docker images to the client containing the baseline production service and the revision of that service after the user submits the deployment the aria server coordinates with aws to provision the resources for our the aria deployment install the user provided service baseline and revision software and configure the monitoring and analysis tooling to observe the baseline and canary after aria resources have been provisioned the aria server then coordinates with the existing infrastructure to create a traffic segmentation rule such that a minority of the incoming traffic is routed to the newly deployed aria canary infrastructure once traffic has been routed the user can now monitor baseline and canary instance performance and configure advanced analysis when ready the user invokes a destroy command which results in arya coordinating with aws to first remove the traffic routing rule and then tear down the ra infrastructure restoring the production environment to its initial state to better understand the infrastructure than entails in aria deployment let's return to chart health and inspect the infrastructure at a more detailed level within the production environment chart health's emr service is horizontally scaled via a load balancer once aria is deployed a minority of traffic is routed from the load balancer to each of the baseline and canary aws ec2 instances in this case the baseline instance is represented by version 1.0 which is the same version of the service running in production the canary instance is represented by version 1.1 once a third instance is also provisioned within which each of the monitoring and analysis services are configured and executed the monitoring and analysis services are persistent and unique to each aria canary deployment the aria user interface provides an application that allows the user to easily configure and deploy resources access monitoring and analysis tooling review status and event logs and destroy resources here the user is configuring a new deployment by defining which existing resources to coordinate with and then defines precise traffic routing to the new aria deployment finally the user provides the docker images that contain their service revisions and the associated docker compose files which detail how those services should be initialized aria communicates with aws and after the resources are deployed the user can then access information about the deployment and destroy it the monitoring and analysis tool utilized by arya includes well before i say that let me say that the user's provided links to each one of the monitoring and analysis tooling applications that are running on the re infrastructure for each release these are independent of one another so if there was a second aria deployment listed on this page uh and the user accessed those links they would be independent so the tooling is not like grafana and referee and prometheus are not communicating with all of your aria deployments they are dedicated instances for each are a deployment prometheus is used to collect metrics from the baseline and canary instances and store them as time series data grafana is used to view and analyze those metrics independently meaning that rafana either analyzes the metrics from the canary instance or from the baseline instance and k enta which is an analysis tool comparing performance of canary to baseline instances this is a test-based tool where the user sets up tests to compare the two over a given time period in the next slides caleb will take a deeper look at how arya works under the hood okay thank you sam all right we're now going to move into the technical deep dive into the technologies used by arya to carry out a canary deployment we'll start the section by looking at the aws tools that we incorporated into aria that allow it to deploy and configure the canary infrastructure we'll then examine how aria implements traffic routing using load balancers after that we'll look at the computing resources deployed by arya to run the baseline and canary versions of our target application we'll finish the deep dive by looking at the monitoring and analysis toolkit configured by arya these include the monitoring tools prometheus and grafana and the analysis tools caienta and referee so arya uses several technologies to configure deploy and destroy aws resources aws cloud formation is an infrastructure as code tool that allows for the provisioning and configuration of resource stacks based on developer defined templates while you can build these templates by hand write them out you can also use the aws cloud development kit or cdk to abstract away cloud formation and express your desired resource stack in terms of code this is what aria does the code is turned into cloud formation templates and the templates are then used to deploy the resources defining the canary infrastructure aria also uses the aws software development kit or sdk because it's a highly flexible tool that can interact with a variety of aws services through code while the cdk is specialized for creating and destroying resources there are some things that it can't do easily or at all one example of a task for which arya uses the sdk is configuring a user's existing load balancers so that they can route traffic to aria's canary infrastructure we're going to discuss load balancers in more detail right now a canary deployment requires that some amount of traffic be split between the production version of an application and the canary version traffic splitting or traffic routing is typically accomplished using a load balancer which is an application that receives network traffic and then redistributes it equitably between a number of servers running the production application aria specifically implements traffic routing by making use of the capabilities of an aws application load balancer or alb for short aria is designed to work with aws production environments that have an existing application load balancer routing traffic to instances of a production application the alb routes traffic at the application layer and so it has a rich feature set for routing http https traffic application load balancers consist of one or more listeners and a listener is defined by a combination of a port and a protocol in the example here you can see a listener that detects http traffic on port 80 each listener also has one or more rules and when a listener receives a piece of traffic of the appropriate protocol on the port it listens in it goes down the list of rules if a piece of traffic meets the condition to trigger a rule a specific action is then triggered the action in this example is to forward any traffic to the production servers this is a default rule meaning it always executes arya allows a user to create a new rule on an existing application load balancer listener by setting this rule at a high priority it can supersede the normal behavior of the listener traffic that triggers the reo rule is forwarded to three target one target group contains the production version of the application one target group contains the baseline version of the application and one contains the canary version of the application the baseline and canary targets both receiving the same amount of traffic another feature aria implements is that it configures the listener to perform a health check on a path specified by the user what this means is that the canary infrastructure created by arya will not receive traffic until it is ready and responsive so no requests will ever be lost when arya destroys a canary deployment all it has to do is destroy the rule put on the application load balancer listener which then resumes the routing behavior it exhibited prior to the aria deployment the rules that aria creates can be conditionally applied so canary traffic can be limited by filters such as http requests method http header value or url path pattern sticky sessions can also be implemented ensuring that users from a given ip address will always return to the version of the application they were originally routed to if a user is routed to the canary application and they refresh their browser they'll be taken back to the canary application and not instead be shunted into the production version we'll move on to the computing resources the instances which run our canary applications first we'll discuss the role of the baseline instance in a canary deployment canary deployments require a new canary version of an application but it is also considered good practice to deploy a baseline version of the application as well the baseline application is identical to the production application but the instances running the baseline are created alongside the canary instances and this minimizes any time sensitive variants in performance between the old and the new versions of the software so you get a better analysis comparing the baseline in the canary than you would get comparing the production instances and the canary aria deploys the baseline and canary applications to aws elastic compute cloud ec2 instances these instances are automatically configured with both docker and docker compose for both the baseline and canary versions the user prepares docker image files in the form of tarball archives and docker compose setup files and then both of these assets are transferred to the baseline and canary instances and used to initialize the applications aria also installs an application called node exporter on each of these instances this is going to be described in more detail soon but for now just know that it's tool that helps collect data about the canary deployment the final resource deployed by arya is the monitor instance this is a larger ec2 instance that holds a collection of the monitoring and analysis tools that can help a user investigate their canary application like the baseline and canary instances arya also installs docker and docker compose on the monitor instance all of the tools in this instance run as part of an interconnected docker network and are immediately ready to use the tools fall into two categories monitoring and analysis monitoring tools collect store and display metrics data generated by the applications in our canary infrastructure in contrast analysis tools process those metrics to provide insight and recommendations regarding the canary we'll now examine the monitoring tools prometheus and grafana prometheus is a popular open source time series database that's used to log metrics from targets the prometheus server operates on an http pull model scraping metrics from targets at configured intervals by making http requests the metrics are stored on disk as time series data and can then be queried and visualized in the prometheus front end using the promql query language prometheus can identify targets through service discovery and arya automatically configures prometheus to allow it to begin collecting metrics from deployed baseline and canary ec2 instances immediately in addition to prometheus aria also installs grafana a graphical interface and dashboard builder which is often used side by side with prometheus an aria user can configure grafana to display rich visualizations of multiple queries and compare the baseline and canary instances at a glance a bit more about how prometheus works prometheus collects metrics by pulling from http endpoints these endpoints can exist on two kinds of target a standalone exporter or an instrumented application aria supports both an exporter is a standalone application that collects metrics and exposes them for prometheus to use recall we said earlier that arya automatically installs something called node exporter on our baseline in canary instances node exporter is an exporter and it exposes hardware and operating system metrics that are monitored by unix-based systems the other kind of prometheus target is an instrumented application developers can incorporate a prometheus client library into their application it can then collect and calculate metrics and expose them on an http endpoint for example an instrumented web application may calculate and expose metrics about the time required to service a request the number of requests it's received over a given time or the rate of request failure users have to instrument their applications themselves but arya can configure prometheus to scrape the endpoints of such an already instrumented application moving on to the analysis tool calenta and its graphical front-end referee kayenta is a statistical judge which can be used to accurately determine the health of a canary application by comparing metrics from the canary and baseline instances while a human engineer may be able to judge a canary by examining the metrics themselves say in a grafana dashboard by using a statistical judge they can help filter signal from noise and get a more objective assessment of the canary cayenneta interacts with metrics provided by prometheus users may configure analysis requests and send them to the kayenta api or they can make use of the graphical front-end referee carrienta was originally created as a component of spinnaker a continuous delivery platform created by netflix while spinnaker uses kayanta to judge and promote canaries to production automatically without human intervention arya instead offers cayenneta as a supplemental tool for users who wish to explore the advanced realm of canary analysis and gain more confidence in these tools a user configures an analysis to examine one or more matrix over a given time period they may also weight the relative importance of those metrics for example a user may consider an application's memory usage to be much more important than its request response time or its request error rate the user then sets a threshold for a pass score kayenta performs an analysis returns the results and these results include a numerical score which can then be which can be used to deliver a verdict of healthy or unhealthy for the canary so with that we've concluded the technical deep dive we're going to go now to seville who is going to discuss some of the engineering challenges we faced while building arya and some of the trade-offs that we had to make thank you caleb we'll first talk about the limitations we faced with the cdk and how we got around those limitations we'll then look at some of the challenges with regards to configuring and deploying the canary and baseline instances lastly we'll explore the decisions we made regarding setting up all the monitoring and analysis tools to talk to each other we decided early on that we wanted to use the aws cdk due to its ability to easily express aws infrastructure in terms of code however although it was easy to create and destroy resource stacks we ran into several difficulties and limitations when we tried to interact with the existing infrastructure of the production environment we discovered that there were several assumptions about how the ecdk was intended to be used that were not true for our use case the cdk was designed to be used on its own as a command line tool not programmatically it's really more of a standalone tool for defining infrastructure than something you can incorporate into another codebase and invoke another assumption we found was that each cdk project is meant to correspond to a single stack of resources which it can deploy destroy and modify it has no native capability to fetch information about existing resources much less interact with them these assumptions ran counter to how we wanted to develop aria we wanted aria to have a ui with a rich ability to configure and monitor canary deployments for this we needed aria to work with a user's existing aws resources furthermore we wanted aria to be able to launch multiple canary deployments at any given time which ran counter to the cdk's assumption of one project corresponds to one resource stack we did manage however to solve these problems and have aria fulfill our original expectations we just had to get creative first we incorporated the aws sdk into our codebase the sdk allows programmatic access to a variety of aws services while the cdk is specialized for creating and destroying resource stacks we were able to use the sdk to fill in missing features including getting the user's aws profile information from the local environment using that profile information to look up existing production resources and retrieving important configuration information from those resources and creating and destroying traffic routing rules on a user's existing load balancer second the classes used to define resource stacks in the cdk didn't have all the features we needed so we created a wrapper library around the cdk core library the wrapper library we created incorporates the sdk and contains functions which extend the cdk core library functions and does not utilize the cli commands this made it possible for the resource stacks to accept the additional configuration parameters needed to make the user's production and canary infrastructure compatible with each other this also made it easier to deploy multiple canary deployments from our app next we'll discuss some challenges we face in installing our users applications onto the ec2 instances rear requires users to upload their canary and baseline applications in the form of docker images which is the industry standard for containerized applications this introduced us to the challenge of how to transfer our users docker images onto the ec2 instances if the images were stored in a public registry it would be simple to add scripts to the ec2 instances to pull the images at launch time however there are many different docker registries also our users images may be stored on private registries or not be stored in a registry at all to solve this issue we utilize the docker save and docker load commands docker save converts an image into a tar archive while docker load converts that archive back into an image now our users are able to provide aria with tar files created from docker images and we can convert those files back into images on the ec2 instances however we were faced with a further challenge of how to transfer the user's tar files onto the ec2 instances while the cdk allows users to run a user script at the time of an ec2 creation there is no easy way to transfer a file onto the instance the way we solve this was to add the user's docker car files to a temporary aws s3 storage bucket that way in the cdk we could configure the ec2 instances to download those tar files from the s3 bucket we also included commands in the ec2 user scripts to load the docker images from the tar files and run containers from those images at launch time next we'll look at how we chose node exporter there are many different types of exporters including those related to hardware database http traffic logging and other types the challenge we faced was that the type of metrics that are important will vary from application charts to application some applications may prioritize latency or http errors while other applications may prioritize what is happening at the database and other applications may not even be using the http protocol but instead grcp which is a popular protocol used with microservices because system level metrics are relevant no matter the type of application we decided to use the official node exporter node exporter exposes system level metrics like those related to disk cpu and the network a downside however is that you don't get http related metrics next we'll discuss our monitoring and analysis toolkit our decisions about deploying the monitoring and analysis toolkit were driven by our experiences in installing and configuring those tools we decided to install all the tools on one large ec2 instance using docker images to avoid dependency issues and to ensure a smooth installation this had other benefits as well because our tools were all dockerized on the same machine we could use a docker network to simplify communication between them we could also specify that some tools depend on one another making sure that each one spins up in the correct order to avoid errors our experience installing and configuring the monitoring and analysis tools was marked by many frustrations the configuration scripts we developed to overcome these hurdles proved to be an even more important part of aria's value proposition than we had first thought enabling users to make use of tools like kayanta without getting stuck on troubleshooting we're proud with what we built with arya however there are certain areas that we would like to add support for in the future first is the ability to increase and decrease existing canary traffic currently users can set how much traffic a canary receives only in the initial deployment we would like to add the ability to adjust the traffic of an existing canary second many applications that implement microservices use an api gateway an api gateway sits in front of a user's microservice application and forwards traffic to the proper destination we would like to support those applications they use an api gateway third and last aria currently works with applications deployed with aws we would like to add support for other cloud providers like microsoft azure and google cloud that's the end of our talk thank you for coming here is the team that built arya you 