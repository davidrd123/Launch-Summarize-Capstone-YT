all right um thank you everyone for coming and welcome to the tackle box presentation um my name is juan i'm here with kyle kevin and armando and we're a fully remote team spread across the us as you already know we're here to talk about tackle box so what is tackle box tacobox is an open source serverless framework that offers webhooks as a service now don't worry if that seems like a mouthful because it is so we'll try to unpack the meaning behind that phrase throughout the presentation tackle box as a project includes a cli tool a management ui four client libraries which are written in javascript ruby python and go and a webhook service which comprises a restful api and aws infrastructure now an overview of the whole presentation is as follows first we'll give you some context around the world of web books then we'll explain why building web hooks well is hard next we'll explore some of the already implemented solutions in the real world after that we'll talk about what tackle box is all about then we'll get into the nitty-gritty stuff of how we actually got to the final implementation of tackle box and last but not least we'll talk about some future ideas so without further ado let's jump right in in this first section to give you context around web hooks we're going to talk about a couple of things first we'll define what the word webhook means next we'll talk about current users of this tool and lastly we'll talk about things to consider when building web hooks to understand what web hooks are let's begin with an example using a tool that we all know github imagine you are part of a team you have one repo for the project the team will work on and you want to do something programmatically when a branch is created by a teammate like for example post a message on slack or something like that the flow would look something like this first one of your teammates creates a new branch locally then the change is pushed to the repo next the branch is actually created on the repo and finally you want to do something programmatically as a result of this branch creation so the question then becomes how can you automate what you want to do once the event happens which in this case is a branch being created well you basically have two options polling and web hooks in the context of web apis the party interested in data which in this case it's you is called the consumer and the party that provides the data of interest which in this case is github is called the provider with polling you basically send an http request to github every so often to see if there's any updates in this case you happen to send a couple requests first with no updates from from github and then a third one where there is an update with webhooks however instead of having github wait for incoming requests in order to handle them now github proactively sends an http request to you every time an event you care about occurs to be more specific github sends an http request every time something of interest happens to a url that you register with them in this particular case a branch was created so github sends an http request to your endpoint and you then acknowledge the request in other words a web hook is nothing more than an http request sent when an event happens now up to this point we have only talked about webhooks from the perspective of the consumer let's switch gears a bit to talk about webhooks from the perspective of the provider which in the example was github in other words let's put ourselves in github's shoes following the same example if we are github and a consumer is interested in receiving a web hook every time a branch is created the workflow would look something like this first you receive a subscription request with a url to send web books to which will be the url on the right of the slide which reads send requests here dot io this subscription request also specifies the event type that the consumer cares about which in this case is a branch creation then you'll persist the subscription after that a branch is created at some point in time then you check to see who is subscribed to what event type since there may be more interested parties and lastly you send a web hook to the url of each consumer subscribed to that particular event type which in this case for simplicity's sake is just the consumer from step one to show you how web hooks look and look like in real applications here's a screenshot of the ui github offers for webhooks up to the right you can see an add button which will be used to request a subscription after clicking that button you get to the page where you specify the url where you want to receive web books the event type you're interested in among other things and once you register a url to receive webhooks you can see all the webhooks that github has sent their headers their payload etc so far when talking about webhooks we've only talked about github however it turns out that github is not the only company that offers web books some examples of companies that also offer web books are stripe which can send you a webhook when for example a payment is processed twitter which can send you a webhook when for example you receive a direct message into it the company that owns products like turbotax and quickbooks can send you a webhook when for example a budget is created and i mean i can keep going the point is webhooks are everywhere now that we have some context around webhooks we can start talking about things to consider when building them as a provider webhooks offer a few benefits for example you can provide a better experience to your users that want or need webhooks and also you can lower the amount of requests your infrastructure needs to handle over time however webhooks are not perfect if you want a very simple webhooks implementation it may not take you too long to implement however if you want a robust implementation you have to spend a lot of time and effort building it now the reason you have to spend a lot of time and effort building webhooks well is because building webhooks well is not easy we're going to talk about why building webhooks well is not easy in the next section in this section we're going to explain why a robust web box implementation can get really complicated really fast first we'll talk about the first challenges that may come to mind next we'll show you what it looks like to implement web hooks well in one app then we'll show you what it looks like to implement webhooks well in multiple apps and lastly we'll talk about abstracting away this implementation now you the provider want to build web hooks in your app however implementing webhooks well is not as trivial as it may seem at first glance since we can go down a rabbit hole here we decided to talk about four different areas or categories that may present a significant amount of challenges the first area that comes to mind when thinking about building web hooks is failed messages or endpoint failure for example what happens if a consumer url fails or is unreachable do you try to send a message again and if you do how often and how many times do you retry before you stop the second area that comes to mind is security and authentication for instance do you want to make sure the appropriate consumer controls an endpoint before you send messages to it how do you allow consumers to verify the integrity of messages and how about allowing them to verify the origin of messages the third area revolves around monitoring for example how do you identify frequently failing endpoints and how do you monitor historical usage usage patterns and last but not least we have the area of consumer experience for instance do you want to allow your consumers to manually trigger events for testing or other purposes like for example aside from testing do you want to allow them to resend a message manually and if you do how would you implement that in the simplest scenario building webhooks in one app would look something like this now for simplicity we have two consumers only but there could be more and all these concerns we have would fall into this new web book implementation in this single app but what if you need webhooks in more than one app well if you implement webhooks in more than one app it would look something like this now you have all the concerns from before times the number of apps that you have and on top of that more questions come up in this scenario for example how do you avoid redundant work while implementing and maintaining web hooks across all your apps how do you make sure that web hooks are consistent across all your apps and lastly is governance or compliance critical to your apps and if so how do you enforce control over your data at this point the need for building a robust webhooks implementation as its own service becomes clear now all these concerns are abstracted away by this service and this would give it would give you many benefits for example first you would have a single place to maintain code and add features also you would have a single place to monitor the service performance and other metrics aside from that you would have one place to manage related infrastructure also you could have consistency across all the apps aside from that now the apps don't have to be aware of who is consuming their web books allowing more focus on business logic and last but not least now you would have centralized control over data in order to enforce governance or compliance if those are important for your apps now i'm going to pass it over to kyle to tell you a bit about current web hook implementations thanks juan so for this section i'm going to give a high-level overview of how a couple of companies implemented their web hook solution i'll walk through a couple of options to go with if you decide that you don't want to implement web hooks yourself i'll then dive a bit deeper into the pros and cons of the commercial versus diy options and finally i'll talk about some specific use cases that our framework is geared toward so if you're thinking about implementing a web hook solution yourself the question that naturally arises is how do i do that thankfully as we saw earlier many companies have already implemented their own in-house solution and posted their learnings online so there are quite a few blueprints out there we took particular inspiration from how square went about implementing this feature themselves some key features that we liked were how they used a cue for messages waiting to be sent to subscribers how they used lambdas to dq messages and send them to subscribers and how they went about storing successful or failed messages in a separate queue or database we also liked wise engineering's web hook service momentation they utilized amazon's simple notification service to handle consumer subscriptions this meant that in regards to figuring out which subscriptions needed to be notified when an event happens sending messages to subscribing endpoints when the event is actually triggered and handling the retry logic for failed messages all of these operations were extracted from the main app logic and were handled by a single service sns as we mentioned earlier trying to build web hooks and integrate them even in just one app can be a very complex complex thing to do so as you think about trying to expand that out to multiple different apps in your ecosystem that can be very difficult and time consuming let's say for instance that you decide that you don't want to do that because of complexity and you'd rather offload that to a third party provider generally you would have two options either a commercial solution or an open source solution unfortunately there currently isn't an open source solution that provides webhooks as a service that so that really only leaves us with the commercial solutions or building it ourself if you decide to go the commercial route there are a couple of companies that offer webhooks as a service the general workflow when using the commercial solutions would be as follows you the provider communicate with their service via api calls to set up subscriptions create event types send messages when an event is triggered and access message history the commercial solutions handle the data storage message sending processes and general infrastructure management the trade-offs that arise when using commercial solutions would be the fact that you'd be sharing data with a third-party vendor and there would be a lack of software customization because it isn't open source as we saw earlier there are a number of large existing companies that offer web hooks to their customers like github stripe twitter and square they had to build their own solutions in-house and manage their own infrastructure but by accepting those trade-offs they were able to have control over their data and customize their solution to better suit their business needs as they changed by examining the pros and cons of each approach we set out to create an open source framework that's easy to deploy and manage and that's where tackle box comes into play so who is tacklebox geared toward a good way to find out is to ask yourself these four questions do you want to be able to easily integrate this feature across multiple applications do you want to have control over your data is your app small to medium sized and is your is your application's core competency anything but webhooks if you answered yes to all of the above questions then tacklebox would likely be a good fit for you in this section i'm going to talk about the goals of our project and how to deploy each piece i'll highlight how each feature accomplishes the affirmation goals i'll walk through a brief demo to give you some ideas of how to use tackle box and finally we'll look at how to tear down tackle box if you no longer need it so we had three goals in mind when creating tackle box we wanted to make it easy to provide web hooks across multiple applications we wanted to make it easy to deploy and manage its infrastructure and we wanted to make it easy to integrate with an application so to realize the goal of easy deployment we created our framework in such a way that deployment is abstracted to one simple command tackle box deploy this project process usually takes upwards of 20 minutes but is sped up for demo purposes as you can see here the output of the deployment has two parts first the api host which is the base url you'll use to communicate with the tackle box api and second the api key which is the key the tackle box api uses to authenticate you these two pieces of information are all you need to communicate with our api and you don't have to leave your cli to retrieve them after running tackle box deploy you can run the management ui on your local server with one command tackle box ui this management ui ui ties back to making it easy to provide web hooks across multiple applications instead of having to make api calls to figure out the kpis of your webhooks feature for one of your apps you can just view it all in a convenient dashboard which looks like this to make our framework even easier to manage across your apps we provided client libraries in four languages to make you using the tackle box framework standard and dry now we're going to show a brief demo of tackle box in action so imagine that you're a software engineer at a consumer electronics company and you're tasked with enabling automatic notifications from the purchase order app to several other supply chain apps the initial workflow when using tackle box is to register a service by providing the name of it in a json body this service in this case is going to be order our order app this is the app that's going to be providing web hooks now that we've created a service we can use its id and the path parameters to register an event type that users can subscribe to in this case you want notifications to be sent whenever a purchase order is created so we'll call the event type order created next you can create users in order to group their endpoints in one place and ensure that they only receive messages intended for them something to note here as well is that users aka entities that are interested in being notified of an event occurrence can be thought of in a few different ways depending on the use case in our example the users are other supply chain apps like our invoice app here that the invoice app wants to be notified when whenever a purchase order is created for github their users are repositories for a to do app the user could be the actual user who signed up just something to keep in mind here so in the same way that we use the service id to create event types we can use the user id to register endpoints after providing the user id and the path parameters we also include the url and event types that this endpoint will be subscribed to in this case the invoice app only cares about being notified when a purchase order is generated so that is the only event type that we list here so after the initial setup we can view our management dashboard for the service we just registered currently we are viewing the dashboard for the order app and there's only one user and one endpoint from here we are able to view more granular details about the user event type and endpoint that was just created so here on the left we have our purchase order app and on the right we have an endpoint that our user registered for notifications so let's say a purchase order for five computers is created this triggers a message to be sent to the subscribing endpoints which can then be which can then view the webhook content and take action accordingly so going back to our management dashboard after the triggered event we see the corresponding data has been has updated accordingly with one event being triggered and one message being sent from here we are also able to view more granular details about the recently triggered event as well as the corresponding message that was sent if you decide that you no longer need the web books feature then you can simply remove the infrastructure with one simple command tackle box destroy this pulls down the entire infrastructure of the entire tackle box framework now i'm going to pass it over to kevin to talk about how we went about building tackle box thanks kyle okay so in this section i'm going to go over how we built tackle box uh and hope that this will make a little more sense now that we've shown how it actually works uh and we're also going to go into some of the implementation decisions that we made along the way while we were why while we were building this so it'd be difficult to discuss our architecture without first laying out what we believe to be required features in a web hook service so we'll spend a bit of time reviewing those after discussing required features we thought it would be helpful to spend some additional time talking media about the event message life cycle which we we consider to be the core of our service next we'll give a brief overview of tackle box's architecture and how it relates to the event message life cycle and finally we're going to discuss a few of the challenges that we faced along the way and how they influenced our implementation so first feature requirements since consolidation is a key part of our use case allowing multiple applications to use tackle box simultaneously was important to us we wanted to allow providers to specify event types when setting up their web hook service so recalling that github example their web hooks feature would be less useful if it was an all or nothing approach allowing granular control over these event types is important next we wanted to provide easy access to message history so this includes two distinct parts persisting every message sent by tackle box along with delivery statuses and providing an interface to view that message history finally we wanted to send messages to consumers when events occur again this is the central piece of tackle box without this it's not a web hook service and as a side note here i want to mention that while this message delivery often occurs shortly after an event notification is sent to tackle box delivery speed was not a primary goal for us the core functionality of a webhook service is sending messages to endpoints when events occur that process might include the following steps an event occurs in an application and that application notifies the webhook service the webhook service looks at the event type and gathers a list of subscribers that need to be notified the webhook service sends a message to every valid subscriber letting them know that an event has occurred if any message delivery fails the webbook service attempts to resend the message either until it succeeds or until it reaches a set number of attempts each message and its result gets saved to the database regardless of whether or not it was successful so with this lifecycle in mind we're going to map it onto our final tackle box architecture so again i said final tackle box architecture on this slide we're going to talk about what our final implementation looked like and a lot of this may not make sense without context but we'll get into the context afterwards when we discuss challenges so in our architecture notification notification intake is handled by a pair of services api gateway and lambda functions api gateway serves as the entry point into tackle box and passes incoming notifications to an intake lambda the next two steps in the life cycle gathering consumers and sending messages are handled by the simple notification service the last step saving messages to the database is taken care of by a combination of a log service cloudwatch logs and a lambda that transfers message information from those logs to our database so now even though we're going to be focusing almost exclusively on the event message life cycle as we discuss our architecture moving forward i did want to add a quick comment about how our architecture handles basic processes like adding updating and deleting entities generally speaking almost all of our crud operations are handled by our intake lambda which communicates directly with the database as needed because of our focus on the event message lifecycle we're going to call our database the message history moving forward but this is really a store for all of our persistent data including applications users event types etc also there are a few processes that require touching both the database and sns like creating or updating a subscription and in those cases the lambda again orchestrates the process okay so now that we've shown the end result for our architecture we're going to talk about some of the challenges we faced along the way again these challenges influenced our implementation and are how we went from our initial approach to that final architecture these challenges are how we dealt with wasted uptime how we handled the message fanout process and how we persisted message history our initial implementation of tackle box used a single application server it could have worked for our use case but we saw an opportunity to optimize our infrastructure web hook usage patterns are often unpredictable can have moments of high use as well as periods of minimal usage as a result with the single app server architecture we may have had a lot of unused compute resources to address this we decided to use aws lambda functions lambda functions which are aws's function as a service offering allow us to only run code when needed lambdas of course come with trade-offs so first of all cold start times can increase latency and cold start time refers to the time that it takes to load the function into memory before it can be invoked lambdas are stateless so accessing persistent data requires a call to another service and finally each lambda has an execution time limit currently this is set to 15 minutes we were okay with these trade-offs so we decided to use lambdas going with lambdas though meant that each lambda would have to do all of the following tasks every time it runs they would need to intake a new event and persist it to the database gather subscriptions and create messages for each consumer send messages to all consumers and wait for their responses and write responses to the database finally they respond back to the event producer after all of these tasks are done so this is a lot of responsibility and potentially a lot of execution time for a single function so while we believed lambdas improved our architecture they also added some additional challenges that we wanted to address to explain these remaining challenges and how they influenced our architecture i'm going to go ahead and pass it off to armando awesome thanks kevin uh so as kevin just mentioned our lambdas are currently doing a lot of jobs and the message fan out process in particular is expensive so in our current design when an event comes in a lambda function gets fired up and creates a list of messages that need to be sent to each consumer listening for that event type now the lambda not only has to send the messages but also has to wait for a response from everyone before it can finish its work by writing the results to the message history and because web hooks are typically application or user specific the number of consumers needing to be notified is typically pretty low however there are use cases where the number of consumers is high as the number of consumers increases so do the number of http requests and the probability that one of the consumers endpoints is going to fail and when we think about consumer endpoints that fail as in they time out or are unreachable the burden still lies with the same lambda retrying failed messages is a key part of a web book service so in addition to sending its original set of messages the lambda would also need to repeatedly try to resend failed messages before it can write to the database and finish its execution and using exponential backoff which is a common algorithm used in retry logic that increases the amount of time between successive retries means that with every failed attempt the lambda is going to have to run even longer until it reaches the maximum number of retries or lambda's 15 minute execution time cap so in order to reduce the strain on our lambdas we could have done a couple things so we could have added a second type of lambda which we'll call a messenger lambda which would allow us to separate the message creating which is what the intake lambda does and message sending which is what this new lambda would do with this approach when an event comes in our intake lambda gathers the list of consumers that need messages and calls a separate messenger lambda to actually send each message this way our messenger lambdas can all run at the same time and don't have to wait for each other to finish so retries would be difficult to implement though because where do you put messages that need to be retried we could have the intake lambda spin up a new messenger lambda for each retry but then the intake lambda would be again waiting for all retries to finish we could potentially put them all in the database also but that's a lot of writing reading and deleting for data that's only meant to be temporary ideally we'd have something that can buffer our outgoing messages and have a mechanism to quickly pull them off in order and that's exactly what a queue would give us and the queue would also have the added benefit of centralizing our entire outgoing message list in one place which simplifies debugging so amazon's simple q service or sqs for short does exactly this sqs gives us a reliable place to put both our original and retried messages into which is great because our intake lambdas can now just focus on creating messages for an event sqs doesn't send messages to consumers itself so we keep our messenger lambdas in place where they are and continue to use them to send messages the messenger lambdas check sqs for new messages and pull them off as soon as they appear so our messenger lambdas don't have to wait for retries we have them immediately put failed messages back into the queue so that they can be pulled out at a later time by an entirely different lambda invocation so by adding sqs we've increased the complexity of our implementation a bit but each piece does fewer things and we've sufficiently separated our concerns across services suited for their jobs and this is a perfectly reasonable implementation however we wondered if we could eliminate ironically enough for a web hook service the polling in our architecture so our messenger lambdas use polling to constantly check sqs to see if new messages have been added and as we've mentioned web hook events can be intermittent and all of those polling requests in between events can add up under the hood it gets even more interesting because when using lambdas with sqs amazon requires that a minimum of five parallel lambdas pull sqs so the number of requests is higher still when your q is constantly full all of this pulling is a real benefit because it allows you to dq more quickly but with our use case there will likely be periods where this is wasteful if we could eliminate that polling entirely we could potentially make our implementation even more efficient and reduce latency in the message sending process so we were able to remove polling from the message sending process by using amazon's simple notification service or sns sns is a messaging service that when given an event sends a message to every subscriber to that event it also automatically handles retries so in other words it does the same job that the combination of sqs and messenger lambda previously did for us rather than use polling sns uses a pub sub model where new messages are published to topics and anyone who subscribed to that topic automatically receives a message in hook parlance every consumer that's listening for that event type receives a message it's important to note that sns itself stores subscription data so that's one more job sns also handles for us that our lambdas were previously handling with sns there's no longer a need to pull consumer subscriptions when an event comes in sns already knows who needs to be notified switching from sqs to sns simplifies our architecture and removes polling at a key step however there are some tradeoffs with this approach sns has a max retry delay of one hour compared to sqs is 12 hours so it's got less flexibility to implement large retry windows and because sns itself keeps track of subscriptions we now have to ensure that the subscription data in our main database matches what's in sns and when changes to subscriptions are made we have to make the change in two places other operations like adding new event types also require double maintenance so both sqs and sns approaches meet our project goals and both definitely have their benefits in the end we chose sns because it better fits our use case and it includes features like rate limiting and customizable retry logic out of the box which leads us to our third challenge persisting messages to our message history now you may have noticed that in the last sns slide there wasn't a line between sns and our database or our message history that's because sns doesn't have a built-in way to write message data to our database and in a way this kind of makes sense it's main job is to fan out messages to consumers and as far as it's concerned once it delivers a message or makes every effort to with retries its job is done so our entire event message lifecycle currently ends the moment sns sends a message to a consumer and receives a response the only thing that's actually been saved to our database at that point is the original event itself which the intake lambda did but no message data has been saved that means no record of who would send a message when what the payload was what response sns received back etc so we needed to find a way to get message data from sns into our message history after the message send process completed so our first thought was to see if we could tack on some post process to sns like a function that executes immediately after it receives a response back from a consumer sns doesn't have a built-in way to do that but it does allow lambda triggers to be set to run whenever a message is published inserting lambdas here though would add more complexity because the lambda would again be responsible for sending the message to the consumer and at this point we're essentially back at our previous sqs implementation so instead we found a way to use a service that sns was already integrated with sns stores logging data in amazon's cloud watch log service which is commonly used for things like debugging and a variety of other tasks sns can be configured to log specific events and in this case we wanted it to log every time it received a response back from a consumer using cloudwatch logs we now had our persistent message store but it wasn't in the same place as the rest of our data which is the message history so we needed to find a way to get it from cloudwatch logs into our message history the cloudwatch logs can be set up to asynchronously invoke a lambda function so we set cloudwatch logs to invoke the lambda when it received a new sns log and once invoked the lambda transfers the message data in the log to the main message history now in light of these added steps and services sns definitely added a bit of complexity to our architecture however we still thought that overall its feature set best fit our implementation and provided a few additional nice-to-haves out of the box and as kevin showed earlier this is the implementation we chose in the end so hopefully you've got a bit more context now as to how and why we ended up with this architecture and we'd like to finish by just calling out a few features we could potentially add moving forward so tackle box currently does two things by default that while really useful are not features all providers might want the first thing is it requires consumers to confirm their endpoints meaning they have to verify they're in control of it by taking some action before our service will send any web of notifications to them and the second thing is it includes aws metadata in messages sent to consumers that isn't really relevant to tackle box it's just kind of extra information so because these are both features of sns itself and sns is the piece of our infrastructure that communicates directly with consumers we can make both of these optional by putting some logic in between sns and consumers so one way we might do that would be to use a proxy server which would handle the auto confirmations and stripping of metadata um it would allow us to change headers do a couple of other things but using a typical server would cause us to lose some of that uptime efficiency we gained by going with lambdas in the first place speaking of which as you guessed we could use lambdas here also to accomplish this but as we saw in the last section that extra set of lambdas again adds complexity to the architecture for a feature that only a small percentage of our users would probably use another few things we could add would be the ability to dynamically set and change settings like retry policy and rate limiting on a per consumer basis a consumer ui that potentially allows them to view their own message history and do things like resend failed messages a dead letter queue for efficient processing of messages that still after the max number of retries failed to be delivered to their consumer endpoint and a feature that detects when a consumer endpoint has passed some threshold for successive failures and automatically removes that consumer's subscription from our service and that's tacklebox we hope you enjoyed our presentation and know a little more about the simple yet quite complicated world of web hooks as well as the use case that tackle box fills in this space thank you okay awesome let's start with questions so first is actually not a question but a comment so one attendee loves your diagrams and animations it's really helpful to make things clear for us watching your presentation nice work and now let's get to questions so can you talk more about implementing the several client libraries how difficult was that to implement and what do you remember about the decision making process i will i will start that one and i'm going to let juan pick that up because the basic flow that we took was um i built the initial library in javascript and then juan was the was the wiz who translated it into the other three languages so i'll let him talk about maybe the process of translating it but as far as building it um it was kind of hard to find some information online about a good way to to approach building a client library like this there was obviously a lot of information about using client libraries there were some internal launch school resources that we used to work with apis in general but i think at the end of the day providing a consistent interface for people to use um was challenging but definitely doable within you know a fairly short amount of time and i'll let juan sort of talk about how maybe what the process was like translating into other languages because i'm sure that came with its own challenges uh yeah definitely um so this was a very good example of you know the power of just in time learning uh myself i had never worked with python before and writing this library took me one day um at least the python one um and that's where i realized how important it is to have strong fundamentals to understand basic programming concepts really well even if it's in different languages because when you try to translate it to another language it's just a matter of syntax at that point in time um another part that was really challenging was the publishing part because each language has its own ecosystem its own you know flow and that part was sold was also really hard to pick up but in general it was very gratifying and it was well worth it okay awesome so rodney says love the animation seems like you had a lot of different data types of data to keep track of what went into deciding or how did you decide what kind of database you needed uh that was an interesting question um and definitely one that we spent a fair amount of time discussing uh it didn't make it into this presentation but uh if you if you love pain and want to go read our write-up after watching this whole presentation we are going to put it in there uh some notes about that so um i'll just say that we obviously the initial question was do we go with something that's nosql something that's unstructured or do we use something like postgres which is eventually what we ended up with and our initial thought was we're providing this service to an unspecified number of people who might use this because it's open source and with web hooks you know uh each consumer might choose or each provider might choose to put different types of payloads in there it could be sort of endless but at the end of the day after we sort of dug into it we realized that of all the data that our service handles the only thing that's unstructured is the payload and we can sort of still use a relational database to do that and and put the payload into like a json b column and and still have it work uh and so the benefits that we would have got with going with uh with a nosql database which we definitely could have um you know we try out trialled this with dynamodb which also worked um there there just weren't enough reasons to go to nosql and we felt fine keeping it in the postgres area okay did you end up using the aws cdk to build this out seeing the multiple languages that must have been challenging yes we did um so the the deploy and teardown steps are both done with the cdk um and so our client libraries were actually deployed separately so we didn't have to to worry about packaging those up within with our cdk code but we ran into a lot of other problems by using the cdk so it was sort of like a double-edged sword there it it made the tear down a lot simpler and it seemed to make setting it up a lot simpler but we ran into some some strange permissions errors and and race conditions um when trying to to apply um iam permissions to things that we had just created so um yeah it added some challenges but it was not the multi-language challenge okay ronnie just said that they also or his group also had a lot of pain as well a lot of discussion with the database choice and we have a final question which is what was experience going from wanting to do something to making it work with aws oh that's a big that's a big question um i think in the end it was really gratifying i i do i wanna i'm gonna keep my answer short because i kind of want to hear what some of the other guys have to say about that but um yeah it's really cool just to finally see something work that first time you see a message come through you kind of do a little fist pump in the air you're like all right you know this is happening um but uh learning aws definitely took some time but as with most of these things once we learned it it did not seem as daunting for sure yeah i'll just figure back on that and say that aws if you ever never worked with it before it has a very steep learning curve especially at first but like armando said once you get the hang of it it's all you know everything falls into place yeah and just to add a little bit onto that as well it was definitely a daunting experience but going from wanting to do it to actually putting into aws i think something that really did help us was uh some initial advice we got was actually implementing it on a smaller scale like in one of our own local apps uh just a simple web hook feature and just kind of getting an idea of what it looks like on an elementary level uh and then just building from there so that's definitely a very helpful experience doing that okay actually we have more questions so what did you guys use to build the management ui was it tailwind yeah i could speak to that um yes it was tailwind um it was not these were not all bespoke components uh tailwind has a library called tailwind ui that they offer which offers like react specific components that you can use as a template and then sort of build out however you want so that's saved us a whole lot of time with like the visual layout and then we were able to make the tweaks and spend more time actually connecting it behind the scenes with redux to to our api okay so almond says love the dashboard it seems like a really useful feature for into like this how much work was it to create data visualizations in your dashboard and what tool did you use for rendering them so the most of the time um creating those visualizations was finding a library to do it i thought for like maybe an hour that i was gonna do it myself um which was really naive and and i mean i would have liked to see how it turned out because it would have been a disaster but i went with chart js in the end and there was a react specific library that sort of packaged up this chart.js library and made it easier to use okay and the final question what was your favorite part of working on textbox yes that's question for all um for me i really liked working on the the cdk to try to get the automatic deployment up and running i felt like it was really fun to do because i felt like i was able to see from start to finish like the different resources um that needed to be provisioned and just kind of the interest intricacies that were involved in in working with each one um and just finally seeing seeing it come together into one simple command to get it all deployed was really fun yeah i i definitely agree with that um i think after after having built this i think it's it's interesting to note i thought they were i feel really proud of what we built uh and at the same time you start to zero in on these like really really tiny insignificant things like oh i gotta fix this thing like you know this this there's an extra space here like i gotta handle that um and so i think you know maybe it's like a parent thing or something but once you once you release that thing into the world you're like you always worry about it i think and so it's it's the same with this like it's the job's never done i think we're super proud of what we got here and it definitely meets all of our goals but uh i don't know you can always do more you know since since no one else has said it i'll say the the relationships i made along the way um which which is true i mean it was it ended up good um learning to work in a group was was just this really um this piece that i underestimated uh and it was challenging in many many ways but i thought it was cool how um you know you learned you learned that everybody wants the same thing and even if you have different ways to go about it if you can just be patient and kind with each other that it's going to turn out better for everybody in the end and if you try to rush ahead on your own and get like 10 more finished but you sacrifice those relationships on the way it's probably not worth it and you're not going to look back on it very happily and i'll piggyback on what kevin just said coming from core which can be a very isolating experience very tough very full of stress um and just a path that you have to walk on mostly like alone if you you know don't have the time to meet with others or just be part of the community as much like it was my case because i had a full-time job and well circumstances life circumstances being part of a team where everyone had strong fundamentals when you could where you could have um discussions about very complex topics but very clearly always being respectful kind with one another um it's something that i have never experienced before it was really really gratifying and it was also very challenging because you know working with people who are you know just great at what they do brilliant is is just it's it's a lot but and you basically have a high bar set um but you know with the preparation that we get from core it was definitely doable and here we are we did it yeah awesome work any final words i just want to say thanks again for for everyone who popped in yeah thank you yeah thank you guys thank you 