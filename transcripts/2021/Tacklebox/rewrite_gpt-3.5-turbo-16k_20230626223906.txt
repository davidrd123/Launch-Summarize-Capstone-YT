Thank you all for joining us today for the Tackle Box presentation. My name is Juan, and I'm here with Kyle, Kevin, and Armando. We are a remote team spread across the US. Our project, Tackle Box, is an open-source serverless framework that provides webhooks as a service. Throughout this presentation, we will explain what Tackle Box is, how we developed it, and discuss some future ideas.

To give you some context on webhooks, let's start by defining what they are. Webhooks are HTTP requests that are sent from a provider, such as GitHub, to a consumer, in response to a specific event. They allow for real-time notifications and automate actions based on those notifications. Many companies, including GitHub, Stripe, and Twitter, offer webhooks as a way for developers to integrate their services programmatically.

Building webhooks can be challenging and time-consuming. There are several factors to consider, such as handling failed messages, ensuring security and authentication, monitoring endpoints, and providing a good consumer experience. Implementing webhooks in one application can be complex, and scaling it to multiple applications increases the complexity even further.

While some companies have implemented their own webhook solutions in-house, there are also commercial solutions available. These commercial solutions handle the storage, message sending processes, and infrastructure management. However, using a commercial solution means you have less control over your data and limited software customization options.

To address these challenges, we developed Tackle Box, an open-source framework that makes it easy to deploy and manage webhooks. Tackle Box is specifically designed for small to medium-sized applications that want to easily integrate webhooks across multiple applications and have control over their data.

The goals of our project were to provide a centralized solution for managing webhooks, simplify the integration process, and ensure data consistency across applications. We achieved these goals by developing a CLI tool, a management UI, client libraries in JavaScript, Ruby, Python, and Go, and a webhook service with a RESTful API and AWS infrastructure.

Deploying Tackle Box is straightforward. First, you need to set up the infrastructure using the CLI tool or the management UI. This will provision the necessary resources on AWS for the webhook service. Then, you can use the client libraries to integrate webhooks into your applications. The client libraries handle the subscription process, message sending, and retry logic.

Let me give you a brief demo of Tackle Box. In the management UI, you can configure the webhook URL, specify the event type, and manage subscriptions. Once a webhook is triggered, the service will send an HTTP request to the subscribed consumer endpoints. The UI also provides detailed information about the webhook requests, including headers and payload.

If you no longer need Tackle Box, you can easily tear down the infrastructure using the CLI tool or the management UI. This ensures that the resources are cleaned up properly and no longer incur any costs.

In conclusion, Tackle Box provides a comprehensive solution for managing webhooks. It simplifies the integration process, ensures data consistency, and gives developers control over their data. With Tackle Box, you can easily add webhooks to your applications, automate actions based on real-time events, and provide a seamless consumer experience.

In the future, we plan to enhance Tackle Box by adding new features, such as webhook throttling, message deduplication, and payload transformations. We also aim to expand the list of supported client libraries to accommodate more programming languages.

Thank you for your attention, and we welcome any questions you may have. In this coding Capstone project video, we will discuss the retry logic for failed messages and the integration of web hooks across multiple applications. To simplify this complex task, we extracted these operations from the main app logic and implemented them in a single service called SNS.

Building and integrating web hooks in just one application can be a challenging and time-consuming process. However, when expanding it to multiple apps within your ecosystem, it becomes even more complex. In an attempt to address this complexity, you have two options: utilizing a commercial solution or an open source solution.

Unfortunately, there is currently no open source solution that offers web hooks as a service. This leaves us with the choice of using a commercial solution or building our own. If you opt for the commercial route, there are several companies that provide web hooks as a service. The general workflow for using these commercial solutions involves setting up subscriptions, creating event types, sending messages when events occur, and accessing message history. These commercial solutions handle data storage, message sending processes, and infrastructure management.

However, using a commercial solution has trade-offs. Sharing data with a third-party vendor and limited software customization are two challenges you may encounter. On the other hand, large companies like GitHub, Stripe, Twitter, and Square have built their own in-house solutions to overcome these trade-offs. By doing so, they have gained control over their data and customized their solutions to better suit their changing business needs.

In order to address the pros and cons of both approaches, we set out to create Tackle Box, an open source framework that aims to make web hook deployment and management easier. Tackle Box is particularly useful if you want to integrate web hooks across multiple applications, have control over your data, and your application is small to medium-sized. If your application's core competency does not include web hooks, then Tackle Box is likely to be a good fit for your needs.

The primary goals of our project were to make it easy to provide web hooks across multiple applications, to simplify the deployment and management of the infrastructure, and to facilitate integration with an application. To achieve these goals, we designed our framework to be easily deployable with a single command: "tackle box deploy". Although the deployment process typically takes around 20 minutes, we have sped it up for the purpose of this demo.

Upon successful deployment, you will receive the API host URL and API key, which are essential for communicating with our API. With this information readily available in your CLI, there is no need to switch to another interface or tool.

We also developed a user-friendly management UI for Tackle Box. Instead of making API calls to gather information about your web hook's performance, you can view all relevant data through a convenient dashboard. This dashboard provides an overview of the performance of your web hooks across multiple applications, eliminating the need to navigate through multiple interfaces.

To further streamline the management of Tackle Box across different applications, we created client libraries in four different languages. These libraries help standardize and simplify the use of the Tackle Box framework.

Now, let's take a look at a brief demo of Tackle Box in action. Imagine you are a software engineer at a consumer electronics company, and your task is to enable automatic notifications from the purchase order app to other supply chain apps. The first step is to register a service, in this case, the order app, which will provide the web hooks. Once the service is created, you can register specific event types, such as "Order Created", to which users can subscribe.

Users are entities interested in receiving notifications when an event occurs. They can be defined in various ways depending on the use case. For example, in our scenario, the users are other supply chain apps like the invoice app. By registering an endpoint, the invoice app ensures it receives notifications whenever a purchase order is created.

After setting up the initial configuration, you can access the management dashboard to monitor the performance of your web hooks. This dashboard provides detailed information about users, event types, and endpoints, allowing you to track the performance of your web hook infrastructure.

For instance, when a purchase order is created, a notification is triggered, and a message is sent to the subscribing endpoints. These endpoints can process the webhook's content and take appropriate actions. The management dashboard reflects the triggered event and sent message, providing valuable insight into the performance of your web hooks.

If you decide that you no longer need the web hooks feature, you can easily remove the entire Tackle Box infrastructure with a simple command: "tackle box destroy". This command shuts down all components of the Tackle Box framework, freeing up resources for other purposes.

Now, let's discuss how we built Tackle Box and the implementation decisions we made along the way. To effectively explain our architecture, it is necessary to outline the required features of a web hook service. By understanding these requirements, we can better understand the architecture we developed for Tackle Box.

First and foremost, Tackle Box allows multiple applications to use it simultaneously, consolidating web hook functionality. Providers can specify event types, enabling granular control over which events trigger notifications. Furthermore, Tackle Box provides easy access to message history by persisting every message sent, along with delivery statuses, and offering a user-friendly interface to view this history. Lastly, Tackle Box ensures the delivery of messages to consumers when events occur, which is the core functionality of any web hook service.

To support these features, Tackle Box's architecture is centered around the event message life cycle. This life cycle encompasses the entire process that occurs when an event triggers a notification, leading to the delivery of a message to subscribing endpoints. The architecture consists of multiple components, each responsible for a specific stage of the life cycle.

Notification intake is handled by API Gateway and lambda functions. API Gateway acts as the entry point for Tackle Box and passes incoming notifications to an intake lambda function. The intake lambda function is responsible for gathering subscribers and creating messages for each consumer, which are then sent using the Simple Notification Service (SNS). The process of saving messages to the database is performed by a combination of a log service (CloudWatch Logs) and a lambda function that transfers message information from logs to the database.

Our architecture also addresses basic processes, such as adding, updating, and deleting entities, by relying on the intake lambda, which communicates directly with the database. Additionally, there are processes that require accessing both the database and SNS, such as creating or updating a subscription. In these cases, the intake lambda efficiently coordinates the process.

Throughout the development of Tackle Box, we encountered several challenges that influenced our implementation. The first challenge was optimizing our infrastructure to avoid wasted uptime. We discovered that web hook usage patterns can be highly variable, with periods of high usage followed by moments of minimal activity. To address this, we decided to leverage AWS Lambda functions, which allow us to only run code when needed. This approach minimizes the use of compute resources during low-activity periods.

However, using Lambdas also introduced some challenges. Cold start times, which refer to the time it takes to load a Lambda function into memory before it can be invoked, can increase latency. Lambdas are stateless, so accessing persistent data requires calls to other services. Additionally, each Lambda has an execution time limit, currently set at 15 minutes. Despite these trade-offs, we believed Lambdas enhanced our architecture, so we chose to embrace them.

The second challenge we faced was the message fanout process, which proved to be expensive in terms of execution time for a single Lambda function. This process involved gathering subscribers, creating messages for each consumer, and sending those messages. To optimize this process, we focused on allocating these tasks to multiple Lambda functions, thereby reducing the workload assigned to each function.

Finally, we needed to persist message history effectively. To accomplish this, we utilized a combination of CloudWatch Logs and a Lambda function to transfer message information from the logs to the database. This approach allowed us to store all relevant data, including applications, users, event types, and messages, in a single unified store called the "message history."

In conclusion, Tackle Box is an open source framework designed to simplify the deployment, management, and integration of web hooks across multiple applications. It offers a user-friendly management dashboard, client libraries for easy integration, and the necessary infrastructure to handle the event message life cycle. Our architecture leverages AWS Lambda functions to optimize resource usage and offers an efficient solution for processing notifications and delivering messages to consumers. Despite the challenges we encountered, we successfully developed Tackle Box to fulfill our goals and provide a valuable tool for developers in need of an easy-to-use web hook solution. Our focus on the event message lifecycle led us to name our database "message history." It serves as a store for all persistent data, including applications, users, and event types. Some processes require interaction with both the database and SNS, such as creating or updating a subscription. In such cases, the lambda function orchestrates the process.

Now that we have shown the end result of our architecture, let's discuss the challenges we encountered along the way. These challenges influenced our implementation and guided us from the initial approach to the final architecture. The challenges we faced included wasted uptime, handling the message fanout process, and persisting message history.

Initially, our implementation of Tackle Box used a single application server. While it could have worked for our use case, we saw an opportunity to optimize our infrastructure. Web hook usage patterns are often unpredictable, with moments of high usage and periods of minimal usage. With a single app server architecture, we might have had a significant amount of unused compute resources. To address this, we decided to utilize AWS Lambda functions.

Lambda functions, AWS's function as a service offering, allowed us to run code only when needed. However, lambdas come with trade-offs. Cold start times can increase latency, as it refers to the time taken to load a function into memory before invocation. Additionally, lambdas are stateless, so accessing persistent data requires a call to another service. Moreover, each lambda has an execution time limit, currently set at 15 minutes. Despite these trade-offs, we opted to use lambdas due to the benefits they provided.

Using lambdas introduced the challenge of having each lambda perform multiple tasks every time it runs. These tasks included intaking a new event and persisting it to the database, gathering subscriptions, creating messages for each consumer, sending messages to all consumers, waiting for their responses, writing responses to the database, and finally responding back to the event producer. This immense responsibility and potential execution time made us consider the challenges associated with this approach.

The message fanout process was particularly resource-intensive. In our design, when an event arrives, a lambda function creates a list of messages to be sent to each consumer listening for that event type. This lambda not only sends the messages, but it also waits for a response from each consumer before completing its work by writing the results to the message history. As the number of consumers increases, so does the number of HTTP requests and the probability of endpoint failures.

To alleviate the strain on our lambdas, we could have introduced a new type of lambda called a messenger lambda. This separate lambda would handle the task of sending messages, while the intake lambda focused solely on creating the messages for an event. However, implementing retries would have been challenging as we would need to address where to store messages that needed to be retried.

One potential solution was to use Amazon Simple Queue Service (SQS), which allows us to have a reliable place to store both original and retried messages. With SQS, we could separate the creation of messages in the intake lambda and the sending of messages in the messenger lambdas. Retries could be implemented by immediately putting failed messages back into the queue for future processing.

While this solution increased the complexity of our implementation, it provided clear separation of concerns and centralized our outgoing message list for easier debugging. However, we still faced the issue of constant polling in our architecture, as messenger lambdas had to continuously check SQS for new messages. This polling could accumulate unnecessary requests during periods of intermittent web hook events.

To completely eliminate polling, we transitioned from using SQS to Amazon Simple Notification Service (SNS). SNS is a messaging service that sends a message to every subscriber when given an event. Instead of polling, SNS uses a publish-subscribe model, where new messages are published to topics, and all subscribers automatically receive the message. This simplified our architecture and removed the need for constant polling.

While using SNS simplified our architecture, it also introduced some trade-offs. For instance, SNS has a maximum retry delay of one hour compared to SQS's 12 hours, limiting our ability to implement large retry windows. Furthermore, SNS stores subscription data, requiring us to maintain consistency between our main database and SNS. Adding new event types or making changes to subscriptions required double maintenance.

Despite this, we chose SNS because it better suited our use case and offered features such as rate limiting and customizable retry logic out of the box. This final implementation accounted for the challenges we encountered throughout the project.

One remaining challenge was persisting messages to our message history. SNS does not have built-in functionality to write message data to our database. We explored options such as adding a post-process to SNS or using lambda triggers, but these approaches added complexity to the architecture. Instead, we discovered that SNS is integrated with Amazon CloudWatch Logs, which logs specific events. By configuring CloudWatch Logs to invoke a lambda function, we were able to transfer message data from the log to our main message history.

While this introduced additional steps and services, it allowed us to effectively store message data. Although CloudWatch Logs and the message history were not in the same location, we were able to bridge the gap by using lambda functions. Despite the added complexity, this implementation using SNS and CloudWatch Logs proved to be the best fit for our project.

In conclusion, our architecture, called Tackle Box, aims to simplify the implementation of web hooks. We have considered additional features that could enhance Tackle Box, such as making endpoint confirmation and including AWS metadata optional. Further improvements could involve dynamically setting and changing settings on a per-consumer basis, implementing a consumer UI for message history and retries, incorporating a dead letter queue for failed messages, and automatically removing subscriptions for failing consumers.

We hope this presentation has shed light on the intricate world of web hooks and provided insight into the use case filled by Tackle Box. Thank you for your attention.

Now, let's move on to the question and answer segment. In light of the added steps and services, SNS has definitely added complexity to our architecture. However, we believe that its feature set best fits our implementation and provides some additional benefits out of the box. As Kevin showed earlier, this is the implementation we chose. Hopefully, this provides more context on how and why we ended up with this architecture.

Moving forward, there are a few features we could potentially add to Tacklebox. Currently, Tacklebox requires users to confirm their endpoints and includes AWS metadata in messages sent to consumers. These may not be desired features for all providers. To address this, we can make both of these optional by putting logic between SNS and consumers. One approach could be to use a proxy server to handle the auto confirmations and stripping of metadata. However, using a typical server would result in some loss of uptime efficiency gained from using lambdas.

Another potential solution could be to use lambdas to accomplish this, but it would add complexity to the architecture for a feature that only a small percentage of our users may use. Additionally, we could add the ability to dynamically set and change settings like retry policy and rate limiting on a per consumer basis. This would provide more flexibility in tailoring the service to individual consumers' needs.

Another feature we could add is a consumer UI that allows them to view their own message history and perform actions such as resending failed messages. Additionally, implementing a dead letter queue for efficient processing of messages that couldn't be delivered to the consumer endpoint after reaching the maximum number of retries would be beneficial. Lastly, a feature that detects when a consumer endpoint has experienced a threshold of successive failures and automatically removes that consumer's subscription from our service could be implemented.

To answer some questions from the viewers: one attendee mentioned loving the diagrams and animations in our presentation, finding them helpful in understanding the content. We appreciate the feedback and are glad that they were effective in conveying the information. Regarding implementing client libraries, it was a challenging but rewarding process. Building a consistent interface for users was difficult at first, but with the help of internal launch school resources and research on working with APIs, we were able to achieve it.

Translating the initial library into different languages posed its own set of challenges, but having a good understanding of programming concepts made it easier. Initially, publishing the libraries in different languages had its own difficulties due to the different ecosystems and workflows. However, overall, the process was gratifying.

A question was raised about our choice of database. We considered using NoSQL databases for the unstructured payload, but after further analysis, we found that using a relational database like PostgreSQL with a JSONB column to store the payload would suffice. This choice aligned with our goal to provide a service to various providers without imposing specific data structures.

Regarding the use of AWS CDK, we did utilize it for the deployment and teardown steps. However, the client libraries were deployed separately. While CDK simplified the tear down process, we encountered some challenges with IAM permissions and race conditions when applying permissions to freshly created resources.

Viewers also asked about our experience with AWS. Admittedly, it had a steep learning curve initially, but as we gained familiarity with it, the process became more straightforward. Starting from the desire to build something and then taking the journey to make it work on AWS was a gratifying experience.

A comment was made on the dashboard and its usefulness. Creating the data visualizations in the dashboard involved finding a suitable library, and we ended up using Chart.js along with a React-specific library for ease of use and rendering.

When asked about our favorite part of working on Tacklebox, we all had different answers. The process of working in a group and building relationships was a highlight for some of us. It taught us the importance of patience, kindness, and collaboration. The technical challenges, such as implementing the AWS CDK and the client libraries, were also fulfilling for others.

In conclusion, we are proud of what we built with Tacklebox and feel that it accomplishes our goals. However, there are always more possibilities for improvement. We appreciate everyone who joined us for the presentation and hope that this project sheds light on the complex world of webhooks and the specific use case that Tacklebox addresses. Thank you for your time and attention. During the development process, we came across a couple of issues. For instance, there was an extra space that needed to be handled. It seemed like a small matter, but as developers, we worry about every detail. Even after releasing our project, we still feel a sense of responsibility towards it. The job is never truly done. Nonetheless, we are extremely proud of what we have accomplished. Our project has successfully met all of our goals. However, we always feel that we can do more. It's a constant drive for improvement.

One thing that I want to acknowledge is the relationships we formed along the way. This has been a valuable learning experience for me. Working in a group was something I underestimated. It presented many challenges, but it was amazing to see how everyone was striving for the same goal. Despite our different approaches, we learned that patience and kindness towards one another leads to better overall outcomes. It's not worth rushing ahead individually and sacrificing these relationships. In the end, looking back, we will not be happy about it.

I can relate to what Kevin mentioned earlier. Coming from a background where I was mostly on my own in a stressful and isolating professional path, joining this team was eye-opening. Working with individuals who have strong fundamentals and expertise was both rewarding and challenging. The bar was set high, given the exceptional skills of my teammates. However, with the knowledge and preparation we gained from our previous experiences, especially from the Core program, we were able to meet and exceed expectations.

Before we wrap up, I want to express my gratitude to everyone who supported us throughout this project. Each person who contributed deserves recognition for their valuable input and time. Thank you all for popping in today and being a part of our journey. Your support means a lot to us.