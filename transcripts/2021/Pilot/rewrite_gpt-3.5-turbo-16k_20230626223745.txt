Welcome, everyone! Thank you for joining us today. My name is Graham, and together with Carl Vince, I am excited to present our project "Pilot." Pilot is an open-source multi-cloud framework designed to provision an internal platform as a service (PaaS) with a workflow-agnostic build, deploy, and release pipeline. In this presentation, we will first explore the problem domain that Pilot addresses. Then, we will demonstrate how Pilot works and discuss its underlying architecture. Next, we will cover some of the design decisions we made during the development of Pilot. We will also discuss a few implementation challenges we encountered. Finally, we will touch on some of the future features we plan to implement in Pilot.

To illustrate the problem domain, let me introduce you to Acme, a small company in the private airline industry. Many small airlines are using Acme's application, but their business is growing faster than their monolithic infrastructure can handle. Acme's application is currently hosted on a bare-metal server, and all the business logic resides within a monolithic codebase. While this approach has served Acme well, it has limitations, such as physical bottlenecks and scalability challenges. In light of these challenges, Acme's CTO has set two essential goals for their digital transformation: breaking apart their monolith into microservices and adopting a multi-cloud or hybrid cloud strategy.

Breaking apart the monolith involves extracting distinct logical services from the monolithic application. Each service has its own responsibility and can scale independently. This microservices approach allows for improved scalability and resource utilization. The second goal, multi-cloud adoption, involves utilizing services from multiple cloud providers. This strategy helps Acme avoid vendor lock-in and ensures operational resiliency. It also allows them to take advantage of emerging technologies and overcome limitations imposed by specific cloud providers.

However, adopting a microservices and multi-cloud strategy can be challenging, particularly for smaller companies. Respondents in a recent survey expressed concerns such as lack of in-house skills, hybrid cloud complexity, networking challenges, and the absence of tools within the multi-cloud space. Acme, facing similar challenges, wants to extract an authentication service from their monolithic application and deploy it to a cloud provider to improve performance. They also need a solution that allows them to migrate their payment service to another cloud provider without vendor lock-in.

Before we delve into the implementation of these changes, let's understand the typical deployment pipeline for a multi-cloud approach. A software development team selects a programming language such as Go, Ruby, or Node.js. They then create their application's source code, which undergoes a build process to produce a deployable artifact. This artifact could be a containerized version of the application or compiled code. The application is then deployed to the appropriate hosting solution, such as Google Cloud Run or AWS EC2. Finally, any necessary resources for public consumption are configured, such as load balancers or content distribution networks.

To simplify this complex process, Platform as a Service (PaaS) offerings have emerged. PaaS abstracts away the complexity of the build, deploy, and release cycle, allowing developers to focus solely on their code. Examples of PaaS include Heroku, Google's App Engine, and Amazon's Elastic Beanstalk. While PaaS offers ease of use, it often comes with trade-offs in terms of control and configuration.

Acme decides to extract their authentication service and deploy it using a PaaS solution to improve performance. They choose Google's App Engine, which handles the build, deploy, and release cycle for their service. However, they also face a challenge with their payment service running out of disk space. Instead of using Google's cloud storage and getting locked into a single vendor, Acme decides to migrate their files to AWS S3. This migration requires modifying their payment service and setting up a data pipeline that sends invoice data to an AWS Lambda for preprocessing before storing it in S3. As a result, their application topology becomes increasingly complex, with services running on both Google Cloud and AWS.

At this point, the development team at Acme realizes that managing these separated deployment pipelines is becoming a cumbersome task. They have seen larger companies, such as Netflix and Atlassian, build their own internal platforms to simplify multi-cloud deployments. However, building a platform from scratch requires significant expertise and time. Acme begins to search for an alternative solution, and that is when they discover Pilot.

Pilot is an open-source, cloud-agnostic, and self-hosted multi-cloud framework that provisions an internal platform with a workflow-agnostic build, deploy, and release pipeline. With Pilot, Acme can deploy their services to multiple cloud providers using a unified platform. This eliminates the need to manage separate deployment pipelines and provides the ease-of-use benefits of PaaS without vendor lock-in. Pilot is not as feature-rich as some existing solutions like Cloud 66 or building a platform from scratch, but it offers the necessary features Acme is looking for, is cloud-agnostic, and has no additional platform fees.

Now, let's dive into the technical details of Pilot's architecture and see how easy it is to set up and deploy applications using Pilot. Acme, having already extracted their authentication and data pipeline services, is ready to use Pilot to deploy these services to Google Cloud and AWS. To get started, they first provision the Pilot server, which serves as the core of the Pilot framework. Once the server is set up, deploying a service is as simple as running a single command, "pilot up." This command is executed in the local Pilot CLI, which sends the deploy request to the Pilot server. The Pilot server then handles deploying the service to the desired cloud provider.

In Acme's case, they use Pilot to deploy the authentication service to Google Cloud Run and the data preprocessor Lambda function to AWS. Both deployments use the same process, making it easy to deploy to multiple cloud providers. All of this is achieved through a combination of built-in plugins and Pilot's cloud-agnostic approach.

This concludes our overview of Pilot and its capabilities in simplifying multi-cloud deployments. Pilot allows you to set up an internal platform that can deploy applications to multiple cloud providers. It provides a unified platform experience while remaining cloud-agnostic and self-hosted. With Pilot, you can avoid vendor lock-in, simplify your deployment pipeline, and focus on developing your applications.

Thank you for your attention, and we hope you find Pilot as valuable and useful in your own multi-cloud deployments as we do. If you have any questions, please don't hesitate to ask. The purpose of this Capstone project is to create a workflow agnostic build, deploy, and release pipeline. We will compare the options available and discuss the essence of this project. One option is Pilot, which may not be feature-rich but has all the necessary features that Acme is looking for. Pilot is cloud agnostic, self-hosted, and open source, meaning there are no additional fees. Throughout this presentation, we will focus on the concept of ownership in platforms and the difference between a platform-as-a-service (PaaS) and an internal PaaS.

A PaaS manages everything for you, taking away control and configurability in order to provide ease of use. The platform provider, such as Heroku, manages the infrastructure themselves, often using an infrastructure-as-a-service provider. For example, Heroku builds on top of AWS services. On the other hand, an internal or self-hosted PaaS allows you to control and configure the platform, as long as you have the expertise. It still provides sensible defaults so that novice users don't have to worry about the underlying services. In the case of Pilot, developers can provision the internal PaaS on either Google Cloud or AWS. The cloud provider still manages the underlying hardware, while Pilot manages the necessary infrastructure-as-a-service offerings that we utilize.

Getting Pilot up and running in your own cloud is straightforward. We will now demonstrate how easy it is with a demo from our team member, Carl. He will walk you through Pilot's architecture and show how a developer would set up and deploy an existing application. Acme's desired architecture includes a main application and a local server, with the goal of deploying an authentication service on Google Cloud and a data pipeline service on AWS.

Using Pilot, the first step is to provision the Pilot server, the core of Pilot where applications will be deployed. Once that is done, all you have to do is call a single command, "pilot up," which will deploy a service to the desired cloud provider. The local Pilot CLI will send the command to the Pilot server, which will then deploy the service. In the demo, you can see that the authentication service is successfully deployed to Google Cloud Run using a built-in plugin. By calling "pilot up" again, this time for the data pipeline service, the data preprocessor Lambda function is deployed to AWS. As you can see, with Pilot, deploying to different cloud providers is as simple as using the same deployment process.

Now let's delve into how the Pilot server accomplishes this. To quickly recap, a traditional platform-as-a-service can be built by containerizing the application with Docker, orchestrating it with Kubernetes, ensuring a robust network within the chosen cloud provider, provisioning and managing a database, integrating with GitHub, and building a UI to manage it all. However, this approach takes a significant amount of time and resources, especially for a small team like ours. Our goal is to deploy to multiple cloud providers, and building the platform this way would further extend the time frame. We needed a tool to expedite this process, and that's where HashiCorp's Waypoint comes in.

Waypoint is a workflow agnostic tool that can build, deploy, and release a variety of applications. It abstracts away the complexity of deploying to different cloud providers. However, there is still a lot of configuration needed to deploy an application using Waypoint, which we will discuss in more detail later. It is essential to note that Waypoint is a tool specifically for the deployment pipeline and not a replacement for a platform.

Now let's take a closer look at the architecture of the Pilot server. First, we provision a virtual machine, in this case, an AWS EC2 instance, although a Google Compute Engine instance could also be used. Next, we install Docker, which will run and manage our containers. Finally, we run our custom Waypoint server and runner. We had to build a custom Waypoint image to ensure that the runner has everything it needs to handle the deployment process. More details on this will be covered in the engineering decisions section.

To deploy applications, the Pilot server contains a custom Docker image that Waypoint uses to run both a server process and a runner agent. The server handles incoming requests, serves the UI, and caches project metadata. The runner handles the deployment lifecycle execution. When we send the command "pilot up" to deploy our application, the server processes the request, recognizes it as a deployment command, and tells the runner to deploy the application.

Now that we understand the Pilot server's components, let's see how to deploy an application using Pilot. In our demo, we will deploy a simple to-do list app connected to a database to Google Cloud Run. There are two main steps: setting up the Pilot server and deploying the application. To set up the local environment, we run the command "pilot init." This downloads the necessary binaries, such as Terraform and Waypoint, and sets up the remaining files for Pilot to run correctly.

The next step is provisioning the Pilot server, which is done by running the command "pilot setup." This command provisions a virtual machine in the chosen cloud provider, in this case, an AWS EC2 instance. However, it is just as easy to set up a Compute Engine instance in Google Cloud by using the "pilot setup" command with the GCP flag instead of the AWS flag. This step may take a few minutes as it includes tasks like virtual machine and database provisioning, software installation, and configuration to work with the local environment. Once the setup is complete, we are ready to deploy an application.

It is worth noting that the server's location, whether an EC2 or Compute Engine instance, does not affect our ability to deploy to multiple cloud providers. To configure this, we use the command "pilot configure" with either the AWS or GCP flag, which sets up an IAM service account on the respective cloud provider for the Pilot server to use. At this point, we have a virtual machine with the Pilot server, which can deploy new or existing applications to different cloud providers. This concludes the first phase of setting up the Pilot server, which only needs to be performed once.

Now we move on to the next main step, deploying the application. We need to create a project to deploy on the Pilot server, which is done by running the command "pilot new project." This command prompts us for the project name, in this case, "pilot-todos," and creates the project in the Pilot server. We can view the project through the UI by running the "pilot UI" command. The UI provides project management and deployment configuration.

To configure our existing application with the project, we have a two-step process. First, we create the app in our project by running the command "pilot new app." This command asks for the necessary information to set up the application, such as the cloud provider, application name, and other relevant details. The command generates a manifest file named "waypoint.hcl" that needs to be pushed into the app repository.

The second step is to configure the app settings through the Pilot UI. From the UI, we select our project and go to "manage settings." Here, we can link our application repository by providing the Git source URL and other important information like the Git source path, authentication usage, and waypoint.hcl file location. After clicking "apply," our application is set up and ready to be deployed.

To deploy the app, we use the command "pilot up" with the project name and app name as arguments. This command looks at the waypoint.hcl file and goes through the three phases of deployment: build, deploy, and release. In the build phase, Pilot builds an image based on the source code using cloud-native buildpacks and pushes it to the cloud container registry. The deploy phase configures the necessary resources in Google Cloud Run, and the release phase sets up the Cloud Run load balancer and prepares the application for use. After the command finishes, the app is deployed and can be accessed via the provided URL.

In summary, we have explored the architecture of the Pilot server and learned how to deploy an application using Pilot. Acme can easily deploy both the authentication service on Google Cloud and the data pipeline service on AWS. To do this, they initialize their local environment with the "pilot init" command, provision the Pilot server with the "pilot setup aws" command, and configure Google Cloud credentials using the "pilot configure gcp" command. Once the Pilot server is set up, they can configure and deploy their applications. Acme can create projects and apps through the CLI using "pilot new project" and "pilot new app" commands, respectively. They need to link the app repository through the Pilot UI and then use the "pilot up" command to deploy the application.

Throughout the development of Pilot, we had to make engineering decisions and trade-offs. One integral decision was to extend Waypoint to expedite the process. Waypoint provided a solid foundation, allowing us to focus on CLI and infrastructure development. It also had an existing user interface, and the Waypoint SDK enabled us to add static web hosting as a deployment option using custom plugins in Go.

Extending Waypoint allowed us to leverage the trustworthiness of HashiCorp as an open-source company. Waypoint uses the HashiCorp configuration language, which is widely known and used, making it easy for developers familiar with HashiCorp's products to adapt to Pilot.

We also discussed the challenges we faced in building Pilot. One of the major challenges was handling remote Waypoint operations. Integrating Waypoint into a platform would require significant provisioning of infrastructure, installation and configuration of dependencies, networking setup, permissions configuration, and local environment configuration. We believed that this process could be streamlined.

In conclusion, we have seen the potential of Pilot as a workflow agnostic build, deploy, and release pipeline. By utilizing and extending Waypoint, we can simplify the deployment process and deploy to multiple cloud providers with ease. Acme can take advantage of Pilot's capabilities to deploy their services while owning the cloud resources they rely on. Throughout the development process, we made engineering decisions and faced challenges, but ultimately achieved a tool that can expedite deployment while providing configurability and control. In this coding Capstone project, the developers create an app by calling "pilot new app." They then configure the app and link it to the app repository through the UI. Although the app itself is not shown for brevity, they can deploy it using "pilot up." The deployment goes through the build, deploy, and release lifecycle, and after a few moments, the authentication service is deployed.

Since the Pilot CLI and Pilot Server are already set up to deploy the data pipeline service, the developers can start by creating a new project in the Pilot Server and deploy it with "pilot up." As shown, Acme was able to easily deploy to multiple cloud providers by extracting each service into its own standalone application, leaving Pilot to handle the rest. By using Pilot, they have ownership of the cloud resources hosting their services.

Now, Vince will discuss the engineering decisions, trade-offs, and challenges they faced while building Pilot. One of the key decisions was to extend Waypoint, a new product in the open-source deployment space. This allowed them to focus on CLI and infrastructure development, as Waypoint already had an existing user interface.

Using the Waypoint SDK, they were able to develop custom plugins in Go, which enabled them to add static web hosting as a deployment option. Waypoint, developed by HashiCorp, is a trusted open-source company with products widely incorporated into multi-cloud strategies. Using Waypoint also means leveraging HashiCorp's configuration language, which is easy to read and write.

Waypoint's design intentions aligned with what the developers wanted for Pilot, particularly the idea of collaboration. However, integrating Waypoint into a platform proved challenging. Setting up a remote Waypoint server involved provisioning infrastructure, installing and configuring dependencies, setting up networking and rules, and granting proper permissions. This process appeared cumbersome and could be streamlined.

To address this, they decided to create a remote Pilot server. While a simple Waypoint configuration is easier to set up locally, a remote Pilot server provides increased flexibility for users. It can be provisioned on supported cloud providers, facilitating collaboration and eliminating the need to configure local machines. It also offers centralized management for project and application deployments, as well as streamlining the installation of local dependencies.

They had to choose between Docker and Kubernetes for the installation platform of the Waypoint server. While Kubernetes offered benefits like high availability, scalability, and self-healing capabilities, it introduced extra overhead and complexity, especially for smaller development teams without dedicated DevOps resources. Therefore, they opted for a single Waypoint server using Docker, which proved sufficient for a small team.

Next, they discussed the challenges of developing Pilot, starting with plugins. Waypoint plugins are binaries implemented in Go that act as middleware for the deployment lifecycle. While Waypoint had built-in plugins for deploying full-stack or back-end applications, the team saw an opportunity to create plugins specifically for deploying static assets. Fortunately, Waypoint provided an SDK that enabled them to develop these custom plugins.

Creating plugins posed challenges, as the documentation mainly focused on the build phase. Additional components required referencing source code and trial and error. SDK support varied depending on the cloud provider, with the AWS SDK proving robust and user-friendly. However, the GCP SDK had limitations, requiring the team to use a combination of the SDK and the gcloud CLI to achieve the desired functionality. They ended up creating a wrapper for gcloud commands, creating a dependency on Waypoint containers to have gcloud installed for cloud CDN deployments.

Another challenge was building a custom image to handle Waypoint operations. By default, Waypoint uses HashiCorp's Waypoint image, but the team needed to include their custom plugins and their dependencies in the image. However, Waypoint lacked a community plugin system for custom plugins, so they reverse-engineered Waypoint's default image and source code to understand the starting process and internal packages. They found a Google image with gcloud and used it as a base, significantly reducing the image size.

In terms of future work, the team mentioned several features they would like to add to Pilot. They aim to support a wider range of cloud providers, as they now only support AWS and GCP. They also plan to develop a user interface for database management to provide centralized control. Simplifying the configuration of environment variables through a UI component is another goal. Lastly, they want to enhance support for other infrastructure as a service offerings during application initialization, allowing users to choose between various deployment options.

In conclusion, the developers presented Pilot, a powerful tool for deploying applications to multiple cloud providers. They discussed their engineering decisions, challenges, and future plans. Pilot provides a streamlined deployment experience, leverages Waypoint's foundation, and offers flexibility, collaboration, and ownership of cloud resources. The team appreciated the variety of tools they worked with and the opportunity to provision cloud resources for the first time. They highlighted the advantages of multi-cloud strategies, including avoiding vendor lock-in and adding redundancy. The presentation concluded with the team's gratitude for the positive feedback and their willingness to answer any further questions. During the project, we extensively studied the source code of Waypoint to understand their approach and how it could guide our own plugin development. We incorporated similar design decisions from Waypoint into our project, which was a very enjoyable process. 

As for how we found this domain and decided to work on this project, it happened by chance. We discovered containerization, specifically cloud-native build packs, which allow containerizing applications based solely on source code. This inspired us to create a more robust project. However, realizing that it was too much to implement in the given timeframe, we started exploring Waypoint. We found that Waypoint shared similar design intentions and decided to experiment with it. Our project then evolved based on how people were using Waypoint and how we could utilize it.

Regarding the advantages of one cloud provider over a hybrid cloud or vice versa, it ultimately depends on the specific use case. However, using a single cloud provider reduces context switching and simplifies the learning process as you only need to understand one ecosystem. Small teams often find it challenging to adopt a multi-cloud approach due to its complexity. On the other hand, a multi-cloud strategy can help avoid vendor lock-in and provide a more modern approach. It prevents any single provider from monopolizing the market. Additionally, it offers redundancy in case one cloud provider experiences downtime.

In terms of how a user configures Pilot, they need to enter their credentials for AWS or GCP. For AWS, users must install and configure the AWS CLI to obtain the credentials. Pilot retrieves the credentials from local files and passes them to the Pilot server. The same process applies to GCP using the gcloud CLI.

During the exploration and context-gaining phase, we uncovered pain points in our infrastructure that we would like to extend, modularize, or replace with other technologies. One potential improvement is transforming the Pilot server into a Kubernetes cluster. This would offer significant benefits in terms of performance and dynamic resource allocation. However, we acknowledge the challenge of making it manageable for users while maintaining its advantages.

Regarding branding, a fellow Launch School student named Karis designed our logo and color palettes. I handled the presentation branding based on her designs.

The most challenging technology we worked with, besides Waypoint, was Terraform. While it was enjoyable and had robust documentation, there were challenges in using a tool called cloud-init to bootstrap the server with the required software. Initially, we had a manual process but later discovered we could automate it with cloud-init. Another challenge was working with Docker and running it from within another container. We had to ensure it was done properly without compromising security.

One of the major challenges we faced was developing plugins within Waypoint's ecosystem. We had to conform to their design decisions, which limited flexibility. It required using the Go language and their SDK, which was a unique challenge. We also had to learn about technologies like gRPC and understand how Waypoint functioned internally.

Finally, we would like to thank everyone for their support and for attending the session. We appreciate all the great questions and will continue to be available on the Logical Slack channel to answer any additional queries.