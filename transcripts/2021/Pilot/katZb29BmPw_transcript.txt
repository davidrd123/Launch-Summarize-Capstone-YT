hey everyone thanks for coming my name's graham and carl vince and i are excited to talk to you about our project pilot which is an open source multi-cloud framework for provisioning an internal pass with a workflow agnostic build deploy and release pipeline today we'll start out with exploring the problem domain that pilot is operating within then we'll demo pilot and discuss the underlying architecture afterwards we'll cover some of the decisions we made when designing it followed by a couple implementation challenges we'll also cover some features we'd like to implement in the future now i'd like to introduce you to acme acme is a small company in the private airline industry many small airlines have been using their application but business is growing faster than their monolithic infrastructure can keep up with let's take a look at their application with a monolithic application all business logic is handled within the same code base and in acme's case on a bare metal server that they own this has been acme's tried and true approach for a while now however this infrastructure has limits with a single server they're limited by physical bottlenecks like the server's memory and networking capabilities because of this acme's cto has been working on their digital transformation plans and generally the term digital transformation means improving business processes by modernizing the company's technology and operations as part of their digital transformation the cto has informed the development team of the two most important goals breaking apart their monolith into microservices and adopting a multi-cloud or hybrid cloud strategy let's take a quick look at the first goal if we look at their monolithic application in more detail we can see that they've got some distinct logical services already in a monolithic application these could be classes in an object-oriented language but otherwise they're just portions of the code base with a distinct responsibility and all this is hosted on that bare metal server that they own adopting a micro service approach would mean that rather than portions of the same code base services are discrete and are only concerned with their own processes while still being able to communicate with other services one of the primary benefits of this approach is that it allows services to scale and consume resources independently of one another now let's discuss their next goal a multi-cloud approach means utilizing services provided by multiple cloud providers hashicorp one of the leading companies in the multi-cloud space conducts a yearly survey to gain more insight into how companies are adopting multi-cloud and according to their most recent state of the cloud survey 76 of respondents said that their companies have already adopted a multi-cloud strategy with projections predicting this number reaching 86 percent in just two years these are just a few reasons that companies are adopting a multi-cloud strategy avoiding single vendor lock-in companies don't want to be dependent on any one cloud provider another common issue is operational resiliency ensuring that applications can remain online even if a service offered by one cloud provider goes offline security and governance can come into play especially for companies who have to operate in certain localities that might not be covered by specific cloud providers emerging technologies can also play a part a cloud provider that is the first to offer a tool in a specific field such as artificial intelligence is much more likely to be utilized by companies if they're already operating with that cloud provider however adopting this strategy can be difficult especially for smaller companies some of the inhibitors that respondents brought up were lack of in-house skills small companies might not have dedicated devops teams to manage their infrastructure another is hybrid cloud complexity learning the ins and outs of just one cloud provider can be daunting let alone multiple especially since there's not a lot of exact one-to-one mappings of services tied in with that is the complexity of networking and finally absence of tools within the multi-cloud space so now that we're familiar with acme's digital transformation plans let's see how the development team goes about implementing these changes the first thing they're going to do is identify which service to extract out of the monolith they want their users to be able to log in to third-party websites with their acme accounts however this means that a lot of requests coming in just for authentication could degrade their overall applications performance to solve this they'll extract the auth service out of the monolith and they'll want to deploy it where the service will be highly available with that in mind let's take a look at what the deployment pipeline could be with a multi-cloud approach first a software development team plans architects and develops an application go ruby and node.js are just a few languages that they could use then source code goes through some sort of build process to produce a runnable artifact that could be containerizing the application to provide a slim low footprint environment for the application to execute it using docker or it could be compiling code from a front-end framework like react into static files using a package manager like yarn the artifact from the build phase is then used as the application is deployed to the proper hosting solution there are many solutions out there such as google cloud run for container orchestration aws ec2 for virtual machines and aws s3 for static file hosting finally any necessary resources for public consumption are configured to interface with an existing deployment such as setting up an application load balancer to direct network traffic or configuring a content distribution network like aws cloudfront to distribute static files stored in s3 all this complexity can be difficult to both learn and manage especially for a small development team with little devops experience this is why platforms as a service have arisen a platform as a service or paths abstracts away the complexity of the build deploy and release cycle it allows developers to simply focus on their code examples of apas are heroku and cloud provider-specific platforms like google's app engine and amazon's elastic beanstalk and when you use a pass you're generally trading control and configuration away for ease of use so let's look at paths compared to other options like on-premise servers and infrastructure as a service with on-premise servers you have the most control and configurability of your environment from your application all the way down to the bare metal server however with that comes a lot of responsibility with physical servers come physical maintenance ensuring your server room is properly ventilated and secured is very important to ensure your services are stable next there's infrastructure as a service as the name implies the service you're paying for is the management and control of those physical servers you no longer need the overhead of a server room in maintenance simply spin up a virtual machine and go from there however for simply deploying an application there's still a lot of management and configuration overhead and this is why platforms exist developers can concern themselves with their applications and data while the platform handles the infrastructure under the hood let's revisit the auth service acme wants to deploy they're allowing their users to log into third-party websites with their acme accounts however they know it needs to be highly available since they can't predict the usage patterns of that third-party website acme has decided they want to use a pass for ease of use they've decided on google's app engine which will handle the build deploy and release cycle for their authentication service under the hood google uses their own cloud run container orchestrator which is known to be highly available and scalable but now acme has another issue as part of their payment logic they process payment data generate invoices and store them on a local file server however their local file server is running out of disk space and they don't want to keep buying hard drives while they could you use google's cloud storage they don't want to get locked in to a single vendor so early on in their service extraction process so they've decided to migrate their files to amazon's s3 however this also means they need to modify their payment service they've decided to design a data pipeline the application will send invoice data to an aws lambda which will perform any pre-processing before sending it to firehose which will then stream it into their s3 buckets as you can see with just these two services extracted the application topology is beginning to become more and more complex they've got one service running on google cloud run via app engine and their data pipeline on aws the development team at acme is getting frustrated with the separated deployment pipelines and wish they could manage all of this in a unified platform they've seen companies build their own platforms before and are toying with the idea themselves companies like netflix and atlassian have built their own platforms the reasoning behind that is as you initially set up infrastructure as a service offerings you think that managing them will be as simple as setting up some load balancers pointing them to an auto scaling group of virtual machines which then have access to your resources however the reality is that especially as your application grows this topology becomes increasingly complex to manage it's difficult to track down exactly which application is running on which resource which deployments might be stale and ready to tear down and overall takes a dedicated team to truly manage in an efficient manner both netflix and atlassian are large enterprises that can afford to dedicate teams to their platforms acme isn't however they're still interested in building their own platform so let's take a look at what it takes to build your own paths it could look like a lot of things however in today's environment it most likely looks like this containerizing your application using docker along with maintaining a container registry managing your deployments with a container orchestrator like kubernetes this offers high availability and scalability of your container deployments plus learning cloud provider specifics when it comes to networking along with database provisioning and management with at least one database such as postgres maybe you want to trigger deploys on a pull request or enable git polling perhaps even integrate with github actions don't forget building your own user interface and your favorite front-end framework such as react all of this and more goes into building a platform like heroku and that's just for a single cloud provider because of this acme decided to shop around a bit more and stumbled upon cloud 66 a multi-cloud pass however it has its own limitations so let's compare it to their diy solution cloud 66 does have many useful features but it's currently limited to node.js and rails applications however it is easy to set up and it is cloud agnostic the downside is that acme would prefer an open source solution and they would be paying platform fees on top of infrastructure fees the other option as we discussed is building it themselves this comes with its own learning curve and configuration headaches because of this they might not have time to implement implement many features it's not a very easy or simple process to get set up but it is cloud agnostic and open source if they want it to be and while they may not be paying platform fees they're still spending precious development time on building out their platform acme is now wondering is there a solution out there to easily set up an internal platform that can deploy to multiple cloud providers and that is why we built pilot which is an open source multi-cloud framework that provisions an internal platform with a workflow agnostic build deploy and release pipeline so now that pilot is in the picture let's compare it to our other options pilots not feature rich however it has the necessary features acme is looking for it's also cloud agnostic and self-hosted and it's also open source so no extra fees and since these concepts will be discussed throughout the remainder of the presentation let's take one more look at the spectrum of ownership when it comes to platforms and focus on the difference between a path and an internal pass apaz manages everything for you taking away control and configurability to provide ease of use the platform provider such as heroku manages the infrastructure themselves potentially even using an infrastructure as a service provider in heroku's case they build on top of aws services an internal or self-hosted pass still provides the ability to control and configure the platform as long as you have the expertise while still providing sensible defaults so that novice users don't have to worry about the underlying services in pilot's case developers can provision that internal pass on either google cloud or aws so the cloud provider still manages the underlying hardware and pilot manages the necessary infrastructure as a service offerings that we utilize which we will discuss in the next section as i've said it's very straightforward to get pilot up and running in your own cloud i'll pass it along to my team member carl to show just how easy it is all right thanks ram so i'll be going over pilot's architecture and showing a demo of how a developer would set up pilot and deploy an existing application to the cloud before we get into the demo let's revisit acme's desired architecture they have their main application and a local server and they want to deploy two services to different cloud providers an authentication service on google cloud and a data pipeline service on aws what would it look like if they use pilot to deploy these services now let's assume that acme already extracted both services into their own applications and are ready to deploy to aws and google cloud the first thing they need to do is provision the pilot server it's the core of pilot and it's what we'll be deploying our applications once that's done all they have to do is call a single command pilot up which will deploy a service into its desired cloud provider the local pilot cli will send the command to the pilot server which then deploys a service as you can see the authentication service is now deployed to google cloud run through a built-in plugin calling pilot up again this time for the data pipeline service will deploy the data preprocessor lambda to aws so we're able to easily deploy to both aws and google cloud run with the deployment process being the same now let's dive deeper into how the pilot server does this but a quick recap here's an example of how a pass can be built and you can reference the image that graham showed earlier so you might containerize your application with docker orchestrate it with kubernetes ensure your network is robust within the desired cloud provider provision and manage a database integrate with github and finally build a ui to manage it all if this seems like a lot well it is we could have built pilot using these tools but much like acme we're a small team and it would take a very long time to properly build the paths this way furthermore our goal is to be able to deploy to multiple cloud providers and doing it this way would augment that time frame even more we need a tool that can help us expedite this process so is there a tool out there to do that well hashicorp recently released waypoint a workflow agnostic tool that can build deploy and release a variety of applications it abstracts away the complexity of deploying to different cloud providers however there's still a lot of configuration needed in order to deploy an application and we'll expand upon this later furthermore it's simply just a tool for the deployment pipeline not a replacement for a platform so let's take a look under the hood and see what makes the pilot server first we provision a virtual machine we chose aws ec2 here but it can be a google compute engine instance as well then we installed docker which runs and manages our containers and finally we run our custom waypoint server and runner we had to build a custom waypoint image so that the runner will have everything it needs to handle the deployment process vince will talk about this further in the engineering decisions section now let's zoom in on how the pilot server deploys applications the pilot server contains our custom docker image that waypoint uses to run both a server process and a runner agent the server handles incoming requests serves the ui and caches the project metadata when a user visits the app management ui they're interacting with the server the runner handles the deployment lifecycle execution if we send the command pilot up to deploy our application the server will first process the request recognize that it's a deployment command and tells the runner to deploy the application now that we know what the pilot server is made of let's see how one might deploy one of their applications so the app we're going to deploy today is a simple to-do list app and it's where you can keep track of your to-do's it's a full stack application that's connected to a database for storage so instead of hosting it on our local server what we're going to do is deploy it to google cloud run now there's two main steps we need to take in order to have our app deployed to cloud run first setting up the pilot server this consists of the command's pilot init and pilot setup and second is actually deploying the application the command pilot new project then linking the existing project through the pilot server ui and the command pilot up will take us there so first setting up the local environment to do that we run pilot init this command downloads the binaries we need like terraform and waypoint and also sets up the rest of our files for pilot to run properly once our local server is initialized the next step is to provision our pilot server to do that we run the command pilot setup this command provisions a virtual machine in the chosen cloud provider in this case an aws ec2 instance although we could just as easily set up a compute engine instance in google cloud by calling pilot setup with the gcp flag instead of the aws flag that you see here this step this step normally takes a few minutes because it has a few things it needs to do like provisioning a virtual machine provisioning a database installing all the necessary software for our remote server and then configuring that server to work with our local environment but once setup finishes they're ready to deploy an application it's important to note that it doesn't matter whether our server sits in an ec2 instance or a compute engine instance we're still able to deploy to both google cloud and aws all we'll have to do is run the command pilot configure with either the aws or gcp flag which configures an im service account on the respective cloud provider for the pilot server to use so at this point we have a virtual machine configured with a pass which is the pilot server that can deploy new or existing applications to different cloud providers now that concludes the first phase which is setting up the pilot server we only have to perform this step once and we'll be able to deploy to multiple apps from here now we're ready to move on to the next main step which is deploying the application let's see what this process looks like first we need to create the project to deploy on the pilot server it's as simple as calling pilot new project this command will prompt us for the name of our project in this case pilot to dose and once we press enter it will create the project in the pilot server and give us a hint on what to do next which is creating the application we can view the project through the ui by calling pilot ui and it will look something like this let's zoom in a little bit here if you look closely you might have noticed that this is the app management ui that waypoint provides us we can create projects here as well and manage our current projects and their deployments so now that we have a project created we need to configure our existing application with it this is a two-step process that consists of creating the app in our project and then linking our app repository to it so first creating the app we run the command pilot new app the command will prompt us for the necessary information it needs to set up the application here is where we configure general information about our app like what cloud provider we're planning on deploying to our application name and various other information but after that's configured pilot will generate a manifest file named waypoint htl and that will need to push into our app repository so that finishes step one of configuring our app now for step two we'll need to configure our app settings through the pilot ui from our ui we click on our project then go to manage settings and this brings up our project settings we can link our application repository here by putting the git source url and other pertinent information like the git source path whether or not we're using authentication and where the waypoint hdl file is located but once we hit apply we'll have our applications set up to be deployed we can now deploy the app using the command pilot up so we call that pilot up with the project name and app name as arguments it will look at the waypoint hcl file and go through the three phases of deployment build deploy and release so first the build phase the build face lets pilot know to build an image based off of the source code using cloud native build packs and then push the image to the cloud container registry then the deploy phase lets pilot know to configure google cloud run resources and finally the release space sets up the cloud run load balancer and ready the application for use after the command finishes the app is deployed and can be visited using the provided url full deployment can sometimes take a few minutes depending on where we're deploying to but once it's fully deployed we're able to visit the url and see that our application is live now that we have a better understanding of the pilot architecture and we just saw how to deploy an app to the cloud using pilot let's quickly recap how acme would deploy both the auth service and the data pipeline service so first they initialized their local environment with pilot init then they provisioned their pilot server with pilot setup aws since they're deploying to both aws and google cloud they need to configure gcloud credentials as well they do this with pilot configure gcp and once the pilot service is provisioned they can configure and deploy their application at this point they're also able to open the ui where they can create and configure the app they'll create the project in the cli this time though using pilot new project they'll call the project off svc then they create the app by calling pilot new app they'll simply call the app off and we won't show it here for brevity's sake but they also have to link the app repository through the ui so after it's configured they can deploy the application using pilot up it will go through the build deploy and release lifecycle and after a few moments the authentication service is deployed since the pilot cli and the pilot server are already set up to deploy the data pipeline service they can just start from creating a new project in the pilot server and deploy with pilot up as you saw acme was able to easily deploy to multiple cloud providers all they had to worry about was extracting each service into their own standalone applications and pilot did the rest and by using pilot they owned the cloud resources that their services are hosted on i'll now hand off to vince to talk about the engineering decisions and trade-offs we made as well as the challenges that we face while building pilot thanks carl an integral decision for pilot was to extend waypoint it's a fairly new product in the open source deployment space but it did give us a solid foundation to work off of we could focus on cli and infrastructure development since waypoint has an existing user interface the waypoint sdk allowed us to develop our own custom plugins in go which enabled us to add in static web hosting as a deployment option it's also developed by hashicorp which we consider to be a trusted open source company with products that many other companies have incorporated into their multi-cloud strategies developers that have used hashicorp's products are also familiar with their hashicorp configuration language as it is easy to read and write lastly the design intentions of waypoint really mess with our own around pilot in using waypoint ourselves we like the idea of collaboration but found that aspect difficult to implement if a team wanted to integrate waypoint into their platform this is what they would have to do first they would need to provision the necessary infrastructure such as an aws ec2 virtual machine to allow remote operations then install and configure dependencies such as waypoint and docker at a minimum we have to set up networking and rules to allow their developers access to this remote waypoint server the waypoint server would also need the proper permissions for all cloud providers that they want to deploy to they probably also have to configure docker and docker for the waypoint runner to handle building and pushing images and finally they would have to configure their local environment to communicate with the remote pipeline so we believe this process could be more streamlined we talked about our reasoning behind extending waypoint and what the steps might look like to set up a remote waypoint server next we'll go into handling remote waypoint operations and whether we chose docker or kubernetes for our server installation platform and that brings us to one of our early design decisions we decided to use a remote waypoint server to serve as the backbone of our project so why did we choose to create a remote pilot server in contrast a simple waypoint configuration is much easier when set up locally in this diagram we have the waypoint binary and a server container running on a local machine here a runner isn't necessary the waypoint binary will serve as a runner in this type of configuration a remote pilot server would enable us to provide increased flexibility for our users the pilot server can be provisioned on supported cloud providers to easily allow collaboration with a team users won't have to configure their local machine to be accessible on a network and you get a centralized management server for project and application deployments we also take away the concern of installing local dependencies for the deployment lifecycle so there's no need to install docker pack or any additional tooling since it's already available on the pilot server but how should we install the waypoint server to our virtual machine waypoint can install servers and runners using docker kubernetes and even hashtag corp's nomad orchestration service for us the options came down to docker and kubernetes as an installation platform for our containers with kubernetes orchestration we could spin up a cluster that would give us the benefits of high availability and scalability having multiple runners set up on a cluster would be able to handle concurrent deployments of applications cluster also has self-healing capabilities in case the containers were ever to go down that all sounded great but kubernetes has some drawbacks it introduces extra overhead for smaller development teams it's difficult to understand and implement especially if you want to get running quickly and clusters can be difficult to manage without a dedicated devops person or team member with devops experience with pilot we decided to have the pilot server virtual machine provision a single server and run a container using docker through our testing a single waypoint server and runner was enough for a small team this makes it easier for users as they will only have to manage a single virtual machine with a couple of containers and not a kubernetes cluster on top of it runners are considered an advanced configuration but you could spin up more on the pilot server if needed with documentation available on the waypoint site now we move on to some implementation challenges of developing pilot let's talk a little bit about plugins plugins are binaries written in go that implement one or more waypoint components you can think of them as middleware for the deployment life cycle when we started learning about waypoint we realized that many of the built-in plugins were geared towards deploying full stack or back-end applications we felt there was an opportunity for plugins that were able to deploy static assets so we were happy to see waypoint provides an sdk that allows developers to create their own plugins you can create plugins that execute for the entire build deploy release cycle or individual parts of it for pilot we created several plugins a yarn plug-in that executes during the build phase to bundle front-end applications into static files an amazon cloudfront plug-in that interfaces during the deploy and release phases this plugin will upload static files to s3 and release a static site to a cloudfront distribution and finally a cloud cdn plugin for the gcp for gcp that works like the cloudfront plugin by uploading static files to cloud storage and releasing on cloud cdn developing these plugins introduced their own challenges for one the documentation for creating plugins was a bit light on context it focuses mainly on the build phase the information for additional components is more conceptual so we reference source code of the built-in plugins and custom plugins and trial and error to get the functionality that we wanted another problem was sdk support depending on the cloud provider we found the aws sdk to be robust and easy to work with anything we needed to provision for deploying the cloudfront could be done using it the gcp sdk however was limited in some ways creating cloud storage buckets and uploading assets was supported or provisioning the resources needed for a cloud cdn release was not deleting certain resources from the sdk was also partially supported we had to use a combination of the sdk with the g cloud cli if we wanted the same functionality as the cloud plugin we ended up creating a wrapper for g-cloud commands to have similar usability as an sdk and it worked out for our needs this created a dependency on our waypoint containers to have gcloud installed for deployments using our cloud cdn plugin so we went over some challenges of creating plugins how sdk support came into play and why we needed a gcp wrapper we'll go up it off with we'll cap it off with our last challenge building a custom image to handle waypoint operations so by default weakling uses hashicorp's waypoint image which can be found publicly available on docker hub it includes the waypoint binary and built-in plugins but this introduced a problem for us we wanted to include our custom plugins for yarn builds and cloud cdn and cloudfront deployments waypoint does not currently have a community plug-in system for making custom plugins generally available our plugins are also have their own dependencies like node.js and yarn to build static files or g-cloud cli for cloud cdn provisioning in the end we decided to create our own custom waypoint development and release images that can be pulled from docker hub to do this we had to reverse engineer waypoint's default image and source code to figure out how weak one starts the server and runner containers but also what packages the waypoint image had internally we found it was pretty much a bare-bones linux distribution and you were limited to the bourne shell this meant using hazard corp's image as a base would require installing multiple standard linux packages in our docker file instead we started off with a standard node image and only needed to install gcloud as a dependency this worked fine but the resulting image was unnecessarily large the node image has other packages installed we really didn't need so we discovered an image that google maintains with gcloud and use it as a base for our custom image let's cut down the bloat by about 15 50 percent which increase the performance of our waypoint server installs using our image we've covered a bit of ground today and are coming to the last section of our presentation lastly is the future work portion we'll wrap things up to a close so there's many features we would like to add to pilot but we have a key few we want to add support for a wider range of cloud providers as we currently only support aws and gcp a user interface for database management as this would provide central management rather than having users go through our cloud providers management console and we'd also like to make configuring environment variables of applications easier through a ui component currently environment variables can be set within a waypoint hcl file or using a waypoint cli on an application or project level and lastly we would like to support more infrastructure as a service offering is during our application initialization process with aws for example we default the generated configuration file to use vargate deployments on ecs but you could also use ec2 or land the deployments if you wish so that concludes our presentation the pilot and here's our team i'm glad you all could come out today and we'll be sticking around a bit longer if you have any questions about pilot okay we have three comments incredible job that was awesome an excellent presentation we're going to wait a couple more minutes to see if we have any questions thank you everyone for that for the awesome compliment okay we actually do have a question and it's from leah she says great job creating pilot what was your favorite part about building pilot everyone can answer if they like oh man that's hard i think i liked it because like we we covered a lot of different things so like uh not only did we work with within waypoints ecosystem using go and grpc but we also touched tools like terraform and got to work with some interesting technologies there so i just like the variety of tooling that we got to work with and and integrate into our uh project yeah i would i want to double that but also um being able to provision like a cloud resource for the first time is pretty exciting i thought yeah for me it was definitely um just looking into waypoint's source code and just trying to figure out like how they were doing things and uh how we should go about doing things with our like with our plugins for example um so we just tried to kind of incorporate the same design decisions that they did with waypoint so that was actually really fun okay we have another question how did you find this domain and decide to work on this project um i can answer that one i guess because we stumped we stumbled upon this kind of domain by looking into containerization uh specifically cloud native build packs which we mentioned briefly but essentially that allows you to containerize applications based just on source code and when i read about those i was actually inspired to create a much more uh let's say a robust project um and it was just way too too much to be able to implement in this amount of uh time but it kind of directed us towards waypoint and once we discovered waypoint we realized it had a lot of the same kind of like design intentions that we were talking about um and that's kind of how we uh we played around with it and it just kind of evolved from there into you know how are people using waypoints you know how can we use waypoint and just evolve from there okay we have another question from ricardo he says excellent presentation and any advantages and advantages of one cloud provider over a hybrid cloud or hybrid cloud over one provider um i can i can handle that one or anyone else okay um yeah so it'll come down to your specific use case and um and whatnot but generally like with a single cloud provider that's just less context switching uh it comes down to just you only have to learn that ecosystem and so that's why for small teams it can be hard to adopt a multi-cloud approach um for multi-cloud uh some of the reasons that we mentioned earlier so like avoiding single vendor lock-in can be important so i you know we've heard stories of certain cloud providers trying to strong-arm people into into uh hiking up prices um because their environments are so locked into their services um so single vendor lock-in was a big one for us um and but ultimately it does just come down to your specific situation but a lot of companies are are shifting to it just to um have a more modern approach and not let any one provider provider dominate the market uh and i'll just add to that there's also the redundancy aspect of it uh just because in case like if you were on aws and that was ever to go down for whatever reason then you would still be able to spin up your services on gcp as an example and while that would still be up i think we've got some questions in the chat actually so i can uh there are questions in the chat okay i think starting with malik yeah so the first question what inspired the decision to target smaller dead teams and operations yeah uh it it honestly came down to personal experience at least for me i was um uh you know on a small dev team a while ago um and so yeah there was just a lot of complexity when it came to uh cloud providers and whatnot so some of it was from personal experience but also just uh the three of us uh working together um we've worked throughout launch school in various aspects and um and so we uh there were a lot of pain points that we personally experienced as a small dev team um with with not a lot of devops experience uh so it was a it was a pretty personal um decision for us um but it it was validated by us you know seeing other teams out there with with the same pain okay points you talk a little more about how a user configures pilot as far as they need process entering their credentials for aws jsp is there a config file involved along the way and if so what is the formal format of that like and what does it contain i can take this one um so it's a prerequisite for for pilot if you're if you're deploying to aws or gcpf if you're deploying to aws um you would have to install and configure your aws cli so that's where you would get the credentials um it passed it just grabs it from your local files and then passes it on into the pilot server that's the same with gcpngcloud here we have more questions having gone through the exploration and context gaining phase are there any pieces or parts of your infrastructure that you would like to extend or modularize or replace with other technologies that you became aware of also how did your branding well for branding um another launch school student karis i forgot her last name but she she did the the logo and like color palettes and stuff as far as the presentation um that was me uh i i did all the presentation branding but the branding is all based on the designs that she gave us um and what was the other the actual question um let me pull it up real quick uh let's see okay yeah so uh the exploration and context gaining phase any pieces or parts of infrastructure that we would like to extend or modulate modularize or replace with other technologies um for me i would say uh yes and no um like there are definitely certain uh benefits that we talked about like with container orchestration like kubernetes um so i think that of anything transforming the pilot server into a kubernetes cluster would be the biggest boom if we could figure out a way to still have it be manageable for for users um because i think that would that would just be the biggest return on investment as far as uh performance and um you know dynamically spinning up runners for for deployments and and all of that so i think that was probably you know the biggest thing for me that i think we could uh improve upon yeah i don't know about him sir carl though but okay we have violent question and it is what was the most challenging piece of technology you work with besides waypoint it sounds is sounded like you work with a lot of new tools like terraform yeah i can start this one um terraform was actually pretty fun to work with the documentation is pretty robust and um it was there's it's widely used so um there's a lot of people out there with uh articles and problems that they saw already but as far as working with terraform a challenge that we encountered was using this tool called cloud init and just basically provisioning or bootstrapping the server with the software that we needed we had a lot of manual processing uh at first and then we figured out that we can do it with cloud init so that made it a lot easier for us um and then i think for me uh it was working just working with uh some aspects of docker um we didn't get into it here but we'll get into it in our write-up just kind of the challenges of running docker from another container and how to just go about doing that properly without you know it also being a security flaw in someone's infrastructure but yeah that was a fun one yeah and for me i i agree with both of those but i think the the the difficult and probably most fun challenge was just the plug-in development so kind of like having to conform like to you know someone else's design decisions you know it's a is a very important thing when working within with a tool and so like not having a lot of flexibility there was a challenge so like we if we were to work within waypoints ecosystem we had to use go and their sdk um and so that was a challenge but it was also very uh interesting and and and unique challenge of learning about things like grpc and and how waypoint internally works it was very very interesting okay awesome work once again we don't have any further questions so any final thoughts um i'd just like to thank everyone for for coming out and uh being supportive and asking a lot of uh really good questions um you know we'll be on the logical slack for for those of you there and be able to answer even more questions 