Good afternoon everyone! My name is Jordan, and today my teammates Tim, James, and I will be presenting on Guardrail, an open-source tool we have been building over the past two months. In a nutshell, Guardrail is a tool that generates regression tests for microservices using captured HTTP traffic. Throughout this presentation, we will delve into the intricacies of microservice testing, the challenges faced by developers in creating these tests, and how we developed Guardrail as a solution.

Let's start by discussing why developers want to test microservices. To illustrate the need for testing, let's imagine a scenario where Aaron is responsible for maintaining an online store. The architecture of this store includes an API gateway, a store service, and a shipping service. Whenever a user makes a request, it first goes through the API gateway, then to the store service, and finally to the shipping service. If Aaron wants to update the store service to improve performance or introduce new technology, they need to ensure that any changes made do not disrupt the overall functionality of the system.

The traditional approach to testing microservices is to perform tests either in production or before deployment. When testing in production, a common strategy is to use a canary deployment. In this method, the updated version of the microservice is deployed alongside the existing version, and a small portion of the incoming traffic is directed to the new version. By monitoring for errors, Aaron can determine if the new version behaves as expected and decide whether to fully replace the old version. However, testing in production can be risky, especially in heavily regulated industries where even a small number of mishandled requests can lead to compliance issues or financial losses.

Alternatively, regression tests can be used to validate changes made to a microservice before deployment. Regression tests aim to confirm that both the old and new versions of a microservice handle the same traffic in the same way. By generating requests and comparing actual responses with expected responses, developers can ensure that the updated microservice is production-ready. Although regression tests do not cover all potential issues, they are a suitable choice for validating changes without interacting with the production environment.

However, implementing regression tests comes with its own set of challenges. Firstly, generating requests and responses manually can be time-consuming and prone to error, especially when traffic is complex and unpredictable. The internal network traffic of a microservice can be affected by user actions or other frequently changing upstream services. Secondly, accessing downstream dependencies, such as the shipping service in Aaron's case, is crucial for comprehensive regression testing. If these dependencies are internal to the application, they can be spun up alongside the service under test in the testing environment. However, when dependencies are operated by third parties, direct interaction is not possible, requiring alternative solutions.

To address these challenges, we propose traffic replay as a method for regression testing. Traffic replay involves using recorded production traffic as test requests and comparing the actual responses with the expected values. By using actual production traffic, developers are relieved from the task of predicting internal network traffic and can focus on validating the microservice. This approach is particularly suitable for complex and unpredictable traffic.

Traffic replay also offers a solution for making downstream dependencies available to the service under test. By recording all outgoing requests from the microservice in production and their corresponding responses, developers can "virtualize" each dependency. This means that instead of spinning up full-fledged microservices in the testing environment, small and lightweight processes are used to simulate the behavior of the dependencies. Virtualizing dependencies is beneficial when there are numerous downstream dependencies or when testing microservices relying on non-deterministic third-party dependencies.

Currently, the main enterprise option for traffic replay is a tool offered by Speedscale. This tool offers a wide range of features, including load testing and chaos engineering, but it is not open-source and lacks customization options. On the other hand, there are open-source tools like Go Replay and StormForge's VHS for recording and replaying upstream requests, and WireMock, Mountebank, and Hoverfly for mocking downstream dependencies. While these tools are well-supported individually, combining them to work together seamlessly requires significant effort from developers.

This is where Guardrail comes into the picture. Guardrail is an open-source tool that addresses the need for traffic replay in regression testing microservices. While not as feature-rich as enterprise options, Guardrail provides a more accessible solution for small development teams with limited resources. It eliminates the complexities of integrating multiple open-source tools and provides an intuitive and user-friendly interface for recording and replaying production traffic.

In conclusion, Guardrail is a tool designed to generate regression tests for microservices using captured HTTP traffic. It tackles the challenges faced by developers in testing microservices and offers a more accessible alternative to enterprise options. By utilizing traffic replay, Guardrail enables developers to validate changes made to microservices without interfering with the production environment. With Guardrail, developers can ensure the production-readiness of their microservices while saving time and effort in creating comprehensive regression tests.

Thank you for your attention, and we look forward to any questions or feedback you may have. In regression testing, when production traffic is complex and unpredictable, using traffic replay can be challenging. One major constraint is that recording production traffic can introduce complexity into the application's architecture. For example, if Aaron's store service handles hundreds of requests per second, creating a record of those request-response pairs would require significant additional infrastructure to handle the storage of that data.

Traffic replay also addresses the challenge of making downstream dependencies available to the service under test. This approach involves recording the downstream production traffic, which includes all outgoing requests from the store service in production and all the corresponding responses it receives from those dependencies. This recorded data is then used to virtualize each dependency.

Virtualization means that for each dependency, a process is started in the testing environment. When this process receives an HTTP request, it searches the recorded downstream production traffic for a similar request that occurred in production. Once it finds the request, it issues the corresponding response that the store service received during production. This eliminates the need to spin up an entire microservice in a testing environment, simplifying the process.

Virtualizing dependencies is particularly useful for testing microservices that rely on multiple downstream dependencies. It also enables testing of microservices that depend on non-deterministic third-party dependencies. With virtualized dependencies, the response for a given request will be the same every time that request is issued.

Currently, the main enterprise option for traffic replay is a tool provided by a company called Speed Scale. While it offers a rich feature set, including load testing and chaos engineering, it is not open source and lacks customization options. Alternatively, developers can create their own traffic replay tool using a combination of existing open source tools such as Go Replay or StormForge's VHS tool for recording and replaying upstream requests, and WireMock, Mountebank, or Hoverfly for mocking downstream dependencies. However, making these tools work together in production and produce easily interpretable results requires significant developer effort.

For small teams that do not require advanced features or lack time to develop their own tooling, Guardrail provides a good fit. While it may not be as feature-rich as enterprise options, it is easier to deploy and well-suited for simpler testing scenarios.

Now, let's dive into the implementation of Guardrail and how it is used. Guardrail consists of two main parts: capturing traffic in production and replaying the captured traffic in a staging environment. To capture traffic in production, instrumentation is inserted on both sides of the microservice. This requires minimizing interruption and changes to the existing architecture. The captured traffic is then stored in a data directory.

To replay the captured traffic in a staging environment, Guardrail uses tooling like Go Replay and Mountebank. Go Replay captures HTTP traffic using packet capture, serializes the intercepted requests and responses, and stores them in a log file. Mountebank, on the other hand, creates virtual services that mimic the behavior of the real downstream dependencies by matching requests to recorded responses.

To tie everything together, Guardrail includes a reporting service. This service receives the replayed data, checks for differences in responses, and generates a report. The report allows Aaron to detect potential problems early on by comparing performance metrics and identifying requests with different responses. Guardrail provides a simple but effective way to run regression tests on new versions of microservices.

Guardrail does face some challenges during implementation. One challenge is mismatched traffic during replay. Sometimes, responses become associated with the wrong requests, which can lead to incorrect results. This is a concurrency problem, as requests processed simultaneously can cause traffic mismatches. To address this, Guardrail introduces the use of correlation IDs to tie together related upstream and downstream traffic, preventing mismatches.

In summary, Guardrail is an open-source traffic replay tool that simplifies the process of testing microservices. It captures production traffic, replays it in a staging environment, and provides comprehensive reports for regression testing. It allows developers to detect potential issues early on and helps ensure the stability and reliability of microservice architectures. The problem we're facing is that when replaying traffic, our store service is sending responses intended for Bob to Alice, and vice versa. This is happening because requests from Bob and Alice are arriving at around the same time, causing a concurrency issue. We can't simply limit the service to one user at a time because that's not acceptable. Upon deeper examination, we realized that our downstream service virtualization tool, Mountebank, patterns incoming requests to stubs, which then generate responses. If there are multiple recorded responses for a request, Mountebank iterates through them. Our traffic mismatch problem occurs when two identical requests are replayed closely together and Mountebank has more than one recorded response for those requests. To fix this, we need a way to tie together related upstream and downstream traffic using correlation IDs.

A correlation ID is a unique identifier attached to each incoming user request and subsequent related requests and responses. This allows us to identify every piece of a request-response cycle, providing a comprehensive view of the network traffic. We discovered that popular web servers like nginx and apache already support correlation IDs. All we have to do is notify our users to configure correlation IDs or request IDs and ensure that their services pass along these headers throughout the entire request-response cycle.

Implementing correlation IDs solves our traffic mismatch problem and allows Aaron and Guardrail to have a better understanding of their network traffic. However, we now face the challenge of distilling the disparate traffic data into a meaningful and comprehensive format that Aaron can use. We want to present three pieces of data side by side: the recorded user request, the recorded response, and the replayed response from the staging environment. Currently, this data is spread out in different places and formats. To address this, we need to tie all the pieces together and parse and compare the results of our traffic replay tests.

To achieve this, we introduced the concept of triplets, which combine the three pieces of data into a single unit for evaluation. However, we realized that Go Replay, our upstream recording and replaying tool, doesn't natively support correlation IDs. So, we need to rearrange the Go Replay traffic files based on correlation IDs. A naive approach would be to read each traffic file into memory and group the entries into triplets. However, we opted for a more scalable solution using a document store like MongoDB, as our traffic data is already represented as JSON and doesn't have strong relationships beyond correlation IDs.

To avoid parsing the traffic files, we developed a middleware for Go Replay. This middleware parses the components (HTTP requests and responses) as they arrive and forwards them to our reporting service. The reporting service assembles the components into triplets, compares the expected and actual responses, and generates the test report. This approach eliminates the need to parse traffic files and allows us to scale efficiently.

Working together as a team was both challenging and rewarding. It took time to get into the flow of things, but daily communication and discussions helped us align our perspectives. We found the drawing board and visual communication to be effective tools. Trusting and relying on each other's strengths was essential. Overall, the experience was enjoyable, and we hope our project can deliver significant testing value to engineering teams working with microservices.

In conclusion, we have addressed the challenges of traffic mismatch and data organization in our Guardrail tool. By implementing correlation IDs and using triplets as a unit of evaluation, we have improved the accuracy and comprehensiveness of our testing. We believe Guardrail is a valuable open-source tool that simplifies the implementation and maintenance of regression tests for microservices. Our hope is that Guardrail can help prevent production incidents and contribute to the success of budding microservices projects. Thank you for attending the presentation, and we welcome any questions you may have. In the chat, there is a question about working together as a team, specifically for Marcos. The question asks how long it took to get into the "flow" of things and if there is any advice on things learned while working as a team.

Marcos responds to the question by stating that working well together as a team was both challenging and rewarding. He shares that for him, getting into the flow of things was a daily process. He believes that one of the reasons a project of this scale is successful is because multiple people are working together. Each team member sees things from a different perspective and can provide input that the others may have missed. Marcos mentions that there were many instances where they all read the same thing, but he missed obvious points that James caught, or vice versa. Communication played a crucial role, and the team had to learn how to have discussions rather than arguments. Marcos emphasizes that this skill is something that needs to be adjusted and fine-tuned when working with new groups or individuals. Overall, Marcos finds working as a team to be rewarding and enjoyable.

Tim adds his thoughts to Marcos' response. He mentions that they learned to love using the drawing board as a communication tool. Every morning, the team would gather and discuss the entire project, utilizing the drawing board to visually represent their ideas. Tim believes that this approach ensures that everyone is on the same page and has a clear understanding of the project. He doesn't have much to add to Marcos' response but mentions that visual communication was beneficial and emphasizes the importance of trust and reliance on each other.

James follows up on the previous responses by mentioning that they also played to each other's strengths, which was a fun aspect of working together. He highlights that there are no more questions in the chat and thanks those who attended the session. The team expresses their gratitude for everyone's time and hopes that the session has piqued the attendees' interest and shed light on interesting topics or concepts.