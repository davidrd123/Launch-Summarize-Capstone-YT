good afternoon everyone my name is jordan today my teammates tim and james and i will be presenting on guardrail which is a tool we've been building together over the last two months in brief guardrail is an open source tool that generates regression tests for microservices using captured http traffic don't worry if this description doesn't quite make sense yet we'll spend the rest of the presentation breaking down these concepts we'll first talk about some of the different ways microservices can be tested and the challenges developers encounter as they create those tests then we'll talk about traffic replay which is an approach to testing microservices that addresses those challenges finally we'll talk about how we made guard guard rail which is an important implementation of traffic replay all right so first let's talk about why developers want to test microservices we'll base our conversation around this hypothetical scenario we have a developer named aaron who is responsible for maintaining an online store with the service oriented architecture diagrammed here as you can see in this architecture requests from users first hit an api gateway which then issues requests to the store service and then the store service issues requests to the shipping service as the maintainer of the app aaron decides to update the store service in order to improve its performance or perhaps integrate a new form of technology they run their unit tests on the changes they've made to the code and then deploy those changes into the production environment if the updated microservice handles production traffic as expected then the update was a success and the store continues to operate without interruption however there's also the possibility the updated code once it's integrated with the other pieces of the microservice introduced unforeseen changes into the microservice as a whole and the service starts to mishandle traffic in the production environment in fact according to devops research and assessments state of devops study 15 of deployed changes caused an incident in production and we obviously want to avoid these production incidents because they degrade application performance cause revenue loss for the business and lead to system-wide outages here's what a system-wide outage might look like for aaron's e-commerce site if the application has a low fault tolerance mishandled requests could lead to other services failing and the entire system crashing from this scenario we know that aaron needs to confirm that the new version of the microservice as a whole operates as it is expected to another way to say this is we need to test the microservice now aaron needs to decide how to go about doing that test and there are two broad categories for testing microservices those that are done in production which means the new code is deployed and tested using real traffic and those done before the service is deployed in production often using manually scripted traffic let's start with what it means to test in production the principle of testing in production can be applied in many different ways but one of the most common ways that it's applied is through a canary deployment so taking our example of aaron's ecommerce application if aaron were to release the new version of the store service using a canary deployment they would deploy to production like our previous example except this time they would also keep the older version of the service up and running and then the api gateway would then split requests meant for the store service between the old and new versions only sending a small portion of that traffic to the new version by canary deploying the new version aaron is able to confirm whether or not the service as a whole behaves as it is expected to if after canary deploying it the error monitoring system does not notice new errors aaron can replace the old version of the store service with a new version if their error monitoring system does notice new errors then aaron simply needs to change the api gateway so that production traffic is no longer being sent to the mishape behaving new version and the production system will have returned to its original functioning state while testing production is a good way to test the behavior of an updated microservice there are cases where it introduces more risks than benefit for example in highly regulated software industries like financial or medical applications a canary deployment mishandling a few requests could put a company in non-compliance or even an example of aaron's e-commerce store even if aaron rolls back the canary deployment soon after it starts to mishandle requests those few mishandled requests could lead to a few customers being overcharged and the business might might not be willing to risk that happening so going forward let's say that is the case and testing and production is not a good fit for aaron this means we need to find a way to test the microservice without interacting with the production environment and in order to figure out what that test would look like let's think about what aaron is trying to do aaron is wanting to replace an old version of a microservice with a newer version of that same microservice and if you think about that this means we want to prove that any given request whether it's issued to the original version of the microservice or the new version result in the same response if that's the case we can say the new version of the microservice is production ready and this is called a regression test we're not testing a new additional function of the microservice we're assuming that the old and new versions of the service will be handling the exact same traffic and if one is swapped out for the other the system as a whole won't notice the change regression tests don't provide full confidence that a new version of a microservice won't introduce instance into production there are bugs and performance issues that a regression test won't catch for example we haven't confirmed the service can handle spikes in production traffic this isn't a load test but for aaron's use case this is a good fit so regression tests are a good way to confirm that changes being made to the store service are safe to deploy without having to interact with the production environment as one would with a canary deployment now that we've established that we need to test the behavior of the store service 2.0 as a whole we need to run those tests outside of production and we've identified regression tests as a way to do that we need we can talk about the challenges developers encounter when they implement those regression tests those challenges are generating the requests to issue against the service under test and accessing downstream dependency services we'll go further into detail about what those challenges mean first generating requests to be issued against the updated service that is what will these requests be and what will their responses be one common approach to this challenge is for the developer to manually script the requests based off of their understanding of the application's internal network traffic and the microservices behavior this is a brief example of what that look like looks like the developer comes up with a list of requests to issue against the service under test and a response they expect the service under test to respond with then they issue those requests against an instance of the microservice that they spun up in a testing environment and compare the actual responses received with the expected responses here's another way to represent what this type of testing looks like the store service is spun up in the testing environment the developer issues requests against it and then compares receipts response with expected response this approach is adequate for applications that have simple predictable internal network traffic however the traffic a microservices handling could be unpredictable which might be the case if the traffic is generated by user actions or if it's generated by upstream services that are frequently changing in addition the network traffic could be complex it could have large amounts of data in various formats which make it difficult for the developer to create requests that accurately represent production traffic the second challenge that arises from doing regression tests outside of production is making downstream dependency services available to the service under test and this is what we mean by a downstream dependency service if we return to aaron's e-commerce store you'll notice the shipping service needs to be available to the store service in order for the store service to handle its traffic in this case we refer to the shipping service as a downstream dependency of the store service and the way to address this challenge depends on whether or not the downstream dependency is internal or external to the application if the dependency is internal to the application as is the case with aaron's e-commerce store the dependency could be spun up in the same environment as a service under test to accomplish this aaron would need to coordinate with the team responsible for that shipping service in order to gain access to the most up-to-date version and then get it up and running in their testing environment and if the dependency is operated by a third party which is the case in this example architecture the approach changes because the third party is owned by because the dependency is owned by a third party aaron can't spin it up locally instead they can interact with the actual third-party dependency from their test environment so when it comes to dependencies that are within the applications architecture this approach is a good fit where there are only a few simple dependencies downstream of the service under test when this is the case it won't take up too much additional effort to spin those up in the testing environment it's not a good fit for architectures with many complex dependencies because spinning up many microservices in the same testing environment makes that environment brittle and secondly it requires those teams to coordinate with one another one of the main advantages of having a distributed architecture is allowing teams to advance independently and requiring them to coordinate in order to run tests reduces each team's independence in the case of third-party dependencies this solution won't work if the dependencies responses changes over time so for example if the dependency returns the exchange rate of us dollars to euros the data being used by the store service will change every time it is tested making it impossible to predict what the expected response should be for that set of tests now we'll examine a different approach to regression testing called traffic replay and address the same two challenges we just went over so when it comes to generating the requests and the expected responses to test the microservice with traffic replay uses recorded production traffic as those requests and responses so that means instrumentation is added to the production environment which creates a record of two things all of the requests sent to the store service and the store services corresponding response for each of those requests that record of requests and corresponding responses is then used as a request to be issued to the service under test and the expected value for each response so the regression test is similar to before the updated microservice is spun up in a testing environment and issues requests are issued against it the only difference is this time those requests are from recorded production traffic not from something developers scripted and by using recorded production traffic the developers spared the effort of predicting what internal network traffic looks like and how exactly the service under test should respond this makes traffic replay a good fit for regression tests when production traffic is complex unpredictable one of the main constraints of using traffic replay is that recorded traffic production can recording traffic production traffic can introduce complexity into the application's architecture and if aaron's store service is handling hundreds of requests per second creating a record of those request response pairs could require significant addition to the production infrastructure to accommodate the storage of that much data traffic replay also addresses the challenge of making downstream dependencies available to the service under test similar to traffic replace approach to the first challenge the approach here is to record downstream production traffic that is all outgoing requests from the store service in production and all the corresponding responses it receives from that downstream dependency that data is then used to what is called virtualize each dependency what virtualization means in brief is that for each dependency a process is started in the testing environment when that process receives an http request it searches the recorded downstream production traffic for a similar request that happened in production and then when it finds that request it issues the corresponding response that the store received when in production now instead of having to spin up an entire micro service in a testing environment which can be a long complicated process the developer simply spins up small lightweight process for each downstream dependency virtualizing dependencies is a good fit for testing microservices when that service has many downstream dependencies um in addition it makes it possible to run tests on microservices that rely on non-deterministic third-party dependencies um with virtualized dependencies the response for a given risk quest will be the same every time that request is issued now we're going to cover what options currently exist for developers who want to use traffic replay to test their microservices currently the main enterprise option for traffic replay is made by a company called speed scale by charging for their services they're able to offer a rich feature set including load testing using recorded traffic and chaos engineering where the virtualized services are programmed to drop or mishandle requests however it's not open source so it's not customizable if a company did want didn't want to pay for the traffic replay tool they could create the one themselves using a combination of already existing open source tools go replay or stormforge's vhs tool can be used to record and replay upstream requests and wiremock mounted bank or hoverfly can be used to mock downstream dependencies and while these open source tools are well supported making them work together in production and produce easily interpreted results requires a good deal of work from the developer so if an application is maintained by a small team they might not need the advanced features of an enterprise traffic replay tool and they might not have the time to develop their own and this is the use case for guardrail it's not as feature rich as enterprise options but it's easier to deploy than the diy options making it a good fit for small teams that wouldn't have a very complicated testing scenario and don't have much time to build their own microservice testing tooling now now that we've discussed what traffic replay is and why it's useful to have an open source traffic replay tool james will talk about how we built guardrail and how it's used thank you jordan so we are capturing traffic in production then we have to reapply it in in staging that means we have to build for two different environments so i'm going to show you the overview of guardrail it's two parts and then go into detail with a demo and then i'm going to show how aaron can detect potential problems early on so again this is aaron's simplified store service in reality any service in production may have any number of arbitrary connections and to make things easier i'm going to abstract away the gateways and upstream the shipping service as a downstream dependency you can imagine the difficulty in adding the recording instruments in an architecture that's already built and running and by the way by instrument i mean a process or tool that captures traffic so first we need to insert instrumentation on both sides of the microservice and then doing that we have to minimize interruption and changes we also have to consider the fact that capturing is only temporary so we need a way to connect and disconnect easily and then the last concern is um inserting an instrument in the path of a request is hard to automate unless we integrate into an orchestration system if the system has any so we opted to add the instruments on the host where the service is running to coordinate the recording doing it this way we can take advantage of packet capture from the operating system and then have the ability to use scripting languages to manage the instruments this brings us to the three different parts guardrail on the production side it orchestrates the recording instrumentation for capturing traffic on the testing side it replaced the recording using the same tooling and the recorded and replayed data are combined to make them digestible then they are sent to the reporting service for processing the reporting service is the one that checks for each responses are different and it generates a report it then provides a way for aaron to see the difference and that's guardrails three parts first let's look closely at capturing from two different sides of the microservice and how the tooling are inserted i have to mention though that for capture to work it's got to be plain http and not encrypted otherwise we can't capture on the upstream side the tool we are using is called go reply it is built for capturing http traffic and it runs passively without redirecting traffic it uses a technique called packet capture and with this technique with this technique ip packets are duplicated on the network interface it intercepts the traffic on a lower level though so go replay reassembles the payload to get to the http messages once captured it serializes the intercepted requests and responses into a log file along with some metadata it's also capable capable of doing the reverse that means you can later read a log file and generate requests for testing the ability to record and replay along with the support for a middleware mix is a good choice for guardrail that's capturing traffic on the upstream side let's cover virtualizing a downstream dependency with mounted for mocking service dependencies we opted for mountibank because it is platform independent it is well documented and has a good api this tool creates virtual services over the network a virtual service mimics the real downstream dependency by first gathering data in proxy mode in that mode it creates a record for every unique request that matches a pattern we have configured we we have configured the pattern to uniquely um identify a single request so that it can be matched for responses later on so it stubs request in proxy mode by matching a request to the list of different responses and then during reply it responds by using that capture data so in that mode you can think of it as a simple server where you send it a request and it gives you the recorded matching response sending it the same request again will give it will give you the next recorded matching response doing it this way it can mimic the real dependency now this example shows virtualizing a single dependency but a service may have multiple so multiple proxies are needed the list of dependencies are not known in advance so a json list has to be provided on init guardrail starts a proxy for each of them now you ask how do we insert this proxy well by convention the url for the dependency is set on a configuration file so to use these proxies aaron will have to change the configuration of the store service manually it has to be done manually because the configuration for each microservice is different then after loading the changes the traffic is now going through the proxies so both tools are connect coordinated in the same host and the output is a data directory that contains upstream traffic that can be used for testing and data that can be used to virtualize services let's see how aaron would use it for capturing first aaron declares a list of dependencies in a json format then he runs in a command this will start the proxies ready for service mocking then aaron updates the configuration of the source service to use those proxies and then once updated he can now start recording the duration of capture is up to him and it will depend on his traffic volume that's capturing traffic in production let's look at replying the capture traffic in a staging environment to understand what's going on consider how it would look like to run a single test we arrange the data for both instruments then we send one request through and capture the response having both the recorded response and replayed response from the same request we can now make the comparison to replay all the traffic the replay service runs all the recorded traffic and captures all of the responses then sends it to the reporting service which makes the individual comparisons the reporting service receives the replay data from the replay service and it receives those using an api we built using node and express that in turn persists data in mongodb for a short duration and then the node again creates a report from it finally react display something for aaron to investigate let's let's walk through how aaron would run the test and see the results in a staging environment aaron copies over the data directory once then he starts the store service 2.0 with the same virtual services then the replay command runs the rest the same data can be replayed to multiple versions of the microservice for rapid development and for faster feedback of course traffic replay can be used alongside other tests as well to get more coverage if any regression is detected guardrail shows a simple report this is how aaron can catch errors at the top is a few metrics for performance parity he can see how the new service compares he can tell if it's timing out more if it's returning more error codes or if it's longer if it's running longer than average at the bottom there's a list of requests that has different responses they are identified by the correlation id also it shows the deep comparison of the data in the body in case you'd like to investigate further and that's how guardrail helps aaron run regression tests to a new new version before it can cause cascading problems in the entire system with that tim is going to talk about a few challenges that we have encountered thank you james so now that we've covered the problem space of testing microservices and why we would want to do that in the first place and what's so complicated and tedious about that uh and we've talked about our implementation of guardrail and the the benefits and improvements that we think it brings to testing microservices we'll look at some of the challenges that we faced as we implemented our design of guardrail specifically we'll look at a challenge with mismatched traffic as well as a challenge uh kind of at the end when we went to tie everything together so mismatched traffic we noticed that when replaying traffic in a staging environment responses would sometimes become associated with the wrong request hypothetically this could look something like two you know pretend users alice and bob using microservice our store service at about the same time but bob might get a response that's actually meant for alice and alice will get a response that's actually meant for bob for bob and that's obviously not not correct so we need to fix this now bear in mind this is at least when replaying traffic so we're in a staging environment we're not we're not responding incorrectly to real users thankfully but the point of guardrail is testing and our testing solution can't just mix and match requests and responses however it pleases they're they're not interchangeable so so we need to fix this this is important so what do we do so what could be going on here there's a key phrase in the problem description and that phrase is at the same time this fundamentally is a concurrency problem so as long as requests are spaced out enough there is no traffic mismatch uh so we just need to limit our service to one user at a time right no that's that's obviously not acceptable so what else can we do let's look a little more deeply at what's causing this problem so recall how mountebank our downstream recording and service virtualization tool recall how that works when it's replaying traffic when it's virtualizing services specifically that it pattern matches an incoming request to a stub a stub just being a set of configuration that it uses to generate a response so the matching stub then generates a virtualized response from a list of one or more recorded responses so in this diagram this would be on the right the r1 r2 r3 etc if there's more than one recorded response for a given request mount to bank on each subsequent matching request will iterate through its list of recorded responses starting over once it reaches the end and so here with each request for my address mounted bank will return r1 and if there's another my address request it will return r2 and if there's another one it will return r3 and so on until it reaches the end and then it will loop back to r1 so with this behavior in mind let's reexamine our traffic mismatch problem a traffic mismatch problem can occur if there are two or more identical identical requests that are replayed in close sequence about the same time and mount to bank are downstream service virtualization tool has more than one recorded response matching those requests in such a case the mounted bank test double the virtualized service might return a mismatched response so we've got to fix this how can we prevent this from happening really what we need is a way to tie together related upstream and downstream traffic so that guardrail recognizes those as a single contiguous thread conveniently there's already a common pattern in service oriented architectures that can help with this and that pattern is called correlation ids so what are those well for every incoming user request for every upstream request we add an x correlation id http header that header's value is a unique request identifier so this identifier on the initial incoming request is then attached to any related downstream requests and responses as well as the upstream response back to the user this way aaron and guardrail can identify every piece of a full request response cycle including upstream and downstream traffic so this seems useful but uh maybe intricate kind of kind of detailed kind of complex so how do we do this well good news thankfully um popular web servers like nginx and apache both already support this feature though the naming does vary slightly sometimes it's like a request identifier or correlation id uh so this isn't something we actually have to implement ourselves uh sort of we'll put a little asterisk there and come back to that later so all we have to do is notify guardrail users like aaron to ensure that they have correlation ids or request ids configured and also that their services won't strip those correlation id headers out of their requests but actually continue to pass them along the entire request response cycle both upstream and downstream so that any piece of traffic flowing through their micro service whether left to right or right to left to ensure that it has a correlation id header attached to it so that solves our traffic mismatch problem with correlation ids in place um we've solved traffic mismatch we've enabled aaron and guardrail to have a more comprehensive view of their network traffic so what's next well now we've encountered another challenge we've got a lot of moving parts here that generate a lot of traffic data and that data is spread out over several different places different environments different directories different files so on and so forth correlation ids certainly help but there's still more work to do in order to distill all of this disparate traffic data and do something that is comprehensive and meaningful that aaron can actually use to better serve uh better serve his users so how do we actually tie all these pieces together and then parse and compare the results of our traffic replay tests as with our architecture let's let's kind of take a step back and look generally consider what exactly we want to accomplish with guardrail ideally we'd like three pieces of data side by side number one is the recorded user request or upstream request from production number two is the recorded response to the user from production and number three is the replayed response from from staging from our testing environment basically what our test service responded with when we replayed number one when we replayed the user request for guardrail's use case none of this data is very useful in isolation if we have just the request or just the response or just the replayed response or even you know two out of the three it's not that helpful there's not a lot that we can do with that we really need all three of these pieces combined into a single unit that we can then evaluate and as james and jordan and i continued to work through this project we began referring to this three-part unit as a triplet once again we have we have some help so conveniently these three components of a triplet are actually exactly the information that is included in go replays traffic files are our upstream uh recording and replaying tool uh however so so all the three pieces are there and go replace traffic files but they are spread out all over the place so mount to bank our downstream tool is smart enough to match and group traffic based on correlation ids but go replay our upstream tool is is unfortunately not so for guardrail to work we we need to do something we need to turn go replays upstream traffic files into a list of triplets that we can then iterate through and evaluate individually so on one hand we thankfully didn't have to implement correlation ids ourselves but on the other hand we still have to do some work with correlation ids to parse and rearrange go replace traffic files based on correlation ids so how do we do that one naive approach would be maybe to read each of the traffic files into memory and group the entries into triplets but we wanted something a little more scalable a little more efficient and so because all of our data all of our traffic data it's http requests and responses and those are already very commonly represented with json and because our data isn't very strongly related just beyond correlation ids you know correlation id is the key relation there's not much relation beyond that a document store like mongodb proved to be a perfect fit also thanks to go replay's extensibility um i i was kind of making fun of it for not being able to do anything with correlation ids but on on the bright side it's it's highly extensible so we were able to avoid parsing traffic files entirely by writing a go replay middleware and as james covered our middleware parses components as they arrive components being http requests and responses our middleware parses those as they arrive and forge them to our reporting service where the components are assembled into triplets and then those triplets are evaluated basically the expected response and actual response are compared and then our reporting service uses that data to generate uh the the test report that james showed so we've covered a lot of material and most of it has been quite dense i want to thank all of you for your patience and your attention to recap we examined some of the challenges we encountered in trying to wrangle these two sides of traffic into a meaningful cohesive whole and we looked at some of the solutions that we implemented to overcome these challenges in what we believe is an efficient and scalable manner we believe we built guardrail into an open source tool that is well positioned to deliver significant testing value to engineering teams in charge of budding microservices the core value delivered by guardrail is that it abstracts and automates away much of the complexity and tedium involved with implementing and maintaining good accurate regression tests for microservices our hope is that guardrail can help you prevent your deployed changes from being in that 15 percent that cause a production incident thank you any questions all right so we've got a couple coming in um lena asked uh great work guardrail team thank you lena um how did we decide on this domain this problem domain james and jordan feel free to please correct me if i get this wrong but my memory is that we were looking into like like load testing but there seemed to be a lot of really good kind of simple easy to use solutions for that and just as we continue to research load testing we encountered traffic shadowing and then i think it was jordan that actually found the idea of traffic replay which uh we don't have to get into the differences there that's that would be a whole nother presentation on its own but but but that's my memory is that we kind of started with load testing and and wound up here um got another one from vahid great presentation thank you vahid what was the most fun part of working on this project that's a good question james or jordan do you want to answer that one yeah i'll take a stab at it for me um i think it was going from a pretty abstract idea to conversations with um people who were working in software engineering uh like uh our mentor max and derek and julius and them kind of affirming that this is yeah this is something they could see being useful um and that was it was fun to go from something that just felt very abstract to something that seemed like it could potentially be used yeah i'll take a step i i found it pretty fun to to do all the research yeah we've all we did a lot of research and we we read a couple of books just to learn you know for example how how service virtualization works and that was fun for me for me there's there's so many different things um pro probably one of the most fun moments was because there are so many moving pieces to this the the first time like we ran everything together and it worked was was just magic and a lot of that uh came down to uh docker actually which i didn't have any experience with prior to capstone and and james was was definitely the driving force behind getting docker set up and working correctly for us and being able to just run everything and see it work was was awesome uh we've got another question in chat it's uh on working together as a team uh it's for marcos thank you marcos on working together as a team how long did it take to get in the quote unquote flow of things and do you have any advice on things you learned working as a team that's a fantastic question and uh that was probably one of the most both challenging and rewarding parts of this is figuring out how to work well together and getting to know each other better at least for me i'd say that the getting into flow of things was uh honestly was a daily it was a daily process for me at least um and you know it's i i think in my opinion one of the things that enables a project of this scale is the is that you have multiple people working together and everybody sees these things a little bit differently and is able to give input that you know somebody else or everybody else in the group might have missed and i could i could i could list off several examples of you know we can all go all three of us could go read the same thing and come back and talk about it and there were you know seemingly very obvious things from what we read that i missed completely that you know james caught or or vice versa um so just a lot of communication every day um and and learning how to do that you know well how to um you know have have uh i guess discussions you know rather than rather than arguments not that thankfully i don't think we had many issues with that but that's that's a skill and it's when it's one that when you um every time you're in a new group or working with somebody new you kind of have to tweak and adjust but it's also very rewarding a lot of fun yeah that's great i'll follow up with with tim um yeah you've we've we've learned to love the drawing board um so every morning would sit around and meet and talk about you know the entire project uh the drawing board and um that's it's it's a really good communicating communication tool and to get everyone you know looking at the same subject and have the same meaning yeah i don't have much to add to those two but yeah using i think visual communication was very helpful and um just sort of trusting each other and relying on each other for sure yeah we also played we we also played to each other's chance which is which was fun okay looks like there are no more questions any final words thank you guys for attending absolutely we really appreciate it we hope uh maybe it peaked some interest or revealed some some topics or concepts that might might look interesting or appealing and we we really appreciate your your time 