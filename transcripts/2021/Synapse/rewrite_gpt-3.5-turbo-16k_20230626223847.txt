Welcome to our presentation on Synapse, a GraphQL gateway. Synapse is a framework that deploys a GraphQL API server, allowing users to connect various backend service APIs through a single endpoint. Today, we will introduce a fictional scenario that reflects a real-world company. Let's meet Chatter, our make-believe social media startup.

Chatter was created as an ad-free alternative to existing social media platforms. It has gained immense popularity, and in order to streamline development, Chatter has decided to adopt a microservices architecture. This will enable specialized development teams to work on different parts of the backend services without disrupting the entire application.

One challenge Chatter has faced after transitioning to microservices is the need for multiple network requests to retrieve data from various services. This issue is particularly pronounced when using the app on mobile devices with slow or unstable connections. Making multiple requests significantly slows down the app's performance. This problem is a result of the over or under-fetching of data through REST APIs.

To illustrate, previously, Chatter's architecture consisted of a monolithic server that interacted with various services such as user management, message handling, location services, photo storage, video clips, and payments. However, retrieving all the necessary data required making multiple requests to different endpoints, leading to performance issues.

Under-fetching occurs when a REST API endpoint fails to return all the data needed to fulfill a client's request. This necessitates additional requests to other endpoints, resulting in slower performance. Over-fetching, on the other hand, happens when a response contains more data than required, resulting in unnecessary data transmission. For example, while retrieving a specific user's posts, multiple requests are made, one for the user's ID and another for their posts. This results in over-fetching, where more information is returned than needed, like the user's address and birthday.

To overcome these challenges, Chatter has a few options. They could redesign all their API endpoints to meet the current demands, but this would be a time-consuming process. Alternatively, they could create new endpoints for specific data, leading to a growing API backend. However, our proposed solution is to combine the backend services into a single endpoint using GraphQL.

What is GraphQL? It is a query language for APIs developed by Facebook in 2012. Its purpose is to improve mobile app performance by reducing the need to prepare and parse data on both the server and client sides. With GraphQL, data availability is organized by a schema, which defines the types and fields representing objects. This ensures that clients can request specific data, avoiding over-fetching and under-fetching issues.

GraphQL allows clients to customize queries, fetching only the required data at any given time. It minimizes network requests by aggregating multiple queries into a single request and prevents over-fetching by only returning the requested data. Nested queries enable retrieving data from various resources with just one request, further optimizing performance.

The adoption of GraphQL has been widespread since its release by Facebook in 2015. According to the 2021 State of JavaScript report, the percentage of developers using GraphQL has risen from 6% in 2016 to 47% in 2021. GraphQL receives a high developer satisfaction rating of 94%. Additionally, 84% of developers either show interest in learning GraphQL or plan to use it again.

So, what are the main benefits of implementing GraphQL for Chatter? First, it allows clients to retrieve precise data, improving performance and efficiency. Second, it reduces over and under-fetching of data by customizing queries. Third, it minimizes the number of network requests made by the client to the API. Finally, GraphQL provides a unified and optimized API endpoint, enhancing the overall experience for Chatter's users.

Now, let's address the challenge of transitioning Chatter's existing APIs to a GraphQL gateway. One approach is to overhaul all the service endpoints and redesign them to support GraphQL. However, this would be time-consuming and require substantial resources. Another challenge is integrating third-party APIs that Chatter does not own or control.

Our proposed solution is to use a GraphQL API server as a gateway that sits in front of the existing API endpoints. This server acts as a mediator between the client and the underlying services, regardless of the API types used. For example, REST APIs, OpenAPI, JSON schema, or direct database integration like MongoDB or SQL/PostgreSQL.

To facilitate this, we have two implementation techniques: stitching and federation. In federation, the underlying services have knowledge of each other's data and logic, allowing them to work harmoniously. The gateway combines the requested data by reading the schemas of each underlying service. Federation focuses on translating and integrating data, ensuring cross-service interactions.

On the other hand, stitching treats the company schema as a centralized responsibility. The underlying services remain unaware of each other, leaving them unaltered. The gateway takes on the task of loading and combining the sub-schemas, merging data from multiple services. Stitching focuses on aggregating and combining data.

By adopting a GraphQL gateway, Chatter can achieve several benefits. It simplifies the querying and accessing of data by providing a single endpoint. It eliminates over and under-fetching issues, improving both speed and efficiency. Furthermore, a gateway reduces the attack surface, as security can be enforced at a single point rather than in individual services.

In conclusion, transitioning to a GraphQL gateway offers significant advantages for Chatter. The use of a unified API endpoint, customizable queries, reduced network requests, and optimized performance all contribute to a superior user experience. Although there are challenges involved in implementing the gateway, choosing between stitching and federation allows Chatter to find the most suitable approach for their specific needs. Overall, GraphQL is a widely adopted and highly regarded technology that can transform the way Chatter's app operates.

Thank you for joining us in this presentation on Synapse and the utilization of a GraphQL gateway for enhancing Chatter's performance and user experience. In this coding Capstone project video, we will discuss the implementation of a GraphQL API gateway. The goal of the gateway is to solve the problem of under-fetching by providing a single endpoint for all underlying APIs. By doing so, it reduces under and over-fetching of resources. From the client's perspective, this means they can customize their queries to only retrieve the exact data they need. This improves the speed and efficiency of mobile applications by reducing the number of network requests. Additionally, the gateway also offers a reduced attack surface in terms of security, as it allows for security enforcement at the gateway level rather than at each service's API.

To create a GraphQL gateway, there are two main approaches: stitching and federation. Federation treats the company schema as a distributed responsibility. Each underlying service acts as a puzzle piece, aware of others and designed to fit together. To implement federation, services need to be aware of each other's data and contain the logic for communication between services. The gateway acts as a generic agent, combining requested data by reading the schemas of each underlying service, known as subschemas. Federation focuses on translating resources for querying using GraphQL and interacting with other necessary resources for crossing or nesting data.

On the other hand, stitching treats the company schema as a centralized responsibility. Underlying services are like individual pieces of fabric with no knowledge of being stitched together. The gateway acts as the seamstress, responsible for stitching the subschemas together. Unlike federation, the underlying services in stitching remain unaware of each other, allowing them to remain unaltered. Stitching focuses on combining and translating sub-schemas within the gateway to fetch requested data correctly. Stitching uses vanilla GraphQL, while federation requires knowledge of Apollo's federation specifications, making it more complex to implement.

Let's compare the advantages and disadvantages of federation and stitching. Federation offers faster development by allowing teams to work on different services in parallel, simplifies the gateway layer by moving the logic for combining sub-schemas away from the gateway, but it alters the underlying services and has a significant learning curve due to Apollo's federation specifications. On the other hand, stitching leaves underlying services unaltered, abstracts logic away from the services to the gateway, has a smaller learning curve since it uses vanilla GraphQL. However, stitching increases the gateway's logic, making it a more critical part of the architecture, and may require increased coordination between services.

Considering Chatter's existing microservices architecture, our team believes that stitching is more suitable for implementing the GraphQL gateway. It allows Chatter to leave their existing services unaltered and provides a beginner-friendly solution. With a few additional features, we believe Synapse can be a viable solution for a small business like Chatter.

Let's explore the existing GraphQL gateway solutions for Chatter. Integration platform as a service, such as AWS AppSync, provides a feature-rich platform. However, it requires manual setup and may lead to vendor lock-in. Open core solutions like GraphQL Portal offer different versions with core features accessible freely, but advanced features require a paid subscription. DIY or do-it-yourself solutions require the Chatter team to embrace the learning curve of GraphQL and may take additional engineering time. While federation is an option, it would require a redesign of their services.

Synapse aims to address these requirements for Chatter. Its main priority is to provide a simple way to unify legacy APIs into a single GraphQL endpoint. We identified three core features for Synapse: a simple and intuitive way to configure the gateway using a frontend GUI, streamlined deployment to quickly create a production-ready gateway, and a monitoring dashboard to analyze incoming GraphQL requests and identify issues with underlying services.

Comparing Synapse to existing solutions, it offers flexibility in hosting on any platform like GraphQL Portal. The GUI simplifies configuration, and the monitoring dashboard provides valuable insights. Additionally, Synapse offers an optional feature to automatically deploy the gateway on an AWS server, which is unique compared to other solutions. Being open source, Chatter can also extend Synapse in the future.

Now let's dive deeper into how Synapse works. It can be divided into four phases inspired by GraphQL Portal. The first phase is downloading and setting up Synapse on the local machine. This can be done by running a command to configure Synapse and start it up. Once running, the developer can test the gateway on their local machine.

The second phase is configuring the Synapse GraphQL gateway. The Synapse dashboard, provided as a GUI, allows the developer to add data sources to the gateway. They can easily configure multiple data sources using intuitive forms and tool tips. The dashboard updates the gateway's configuration through the local file system, and changes are reflected when the gateway container is restarted.

In the third phase, the developer can test the configured gateway on their local machine. Running the gateway locally allows for testing and verification before moving to deployment.

The fourth phase is deployment. If desired, the developer can deploy the Synapse gateway onto AWS. During deployment, the Synapse architecture changes slightly, and a fresh MongoDB database is used. This ensures a clean separation between the configuration and production states, preventing data interference. In the production state, the gateway can be managed by users, and the monitoring dashboard becomes available to analyze incoming GraphQL requests.

To summarize, implementing Synapse as a GraphQL gateway for Chatter allows them to unify their legacy APIs into a single endpoint. It offers a simple configuration process, streamlined deployment, and a monitoring dashboard. Compared to existing solutions, Synapse provides flexibility, ease of use, and the option for automatic deployment. It is designed using stitching to leave Chatter's existing services unaltered. With these features, Synapse can be a valuable solution for small businesses in need of a GraphQL gateway.

Now that we've discussed the concepts, options, and implementation of the GraphQL gateway, Chatter can evaluate Synapse as a potential solution for their needs. This is a transcription of a coding Capstone project video. In this video, we will discuss the process of setting up and deploying Synapse, a GraphQL API Gateway.

Synapse is designed to manage users, and when deployed, it utilizes a fresh MongoDB database. This ensures that the production gateway data is separate from any testing data collected during the configuration phase. In the deployment process, Synapse wipes the data from the configuration phase and seeds the new database with the credentials of the root user.

Let's now go through a step-by-step walkthrough of the four phases involved in setting up and deploying Synapse.

Phase 1: Downloading and Setting up Synapse
To start, the developer must download and configure Synapse to run on their local machine. This can be done by running the command "mpx at synapse team start synapse." This command prompts the user for a few inputs and sets up Synapse on their local machine.

After downloading and configuring Synapse, the developer can run the command "synapse up" to start using Synapse on their local machine. This command launches and runs the Synapse architecture in its configuration state.

In this configuration state, Synapse consists of three components: the GraphQL gateway, the dashboard, and MongoDB. Each of these components is containerized using Docker files.

Phase 2: Configuring the Synapse GraphQL Gateway
When the "synapse up" command is executed, it instantiates a containerized instance of the dashboard on the developer's local machine. The dashboard serves as a GUI interface that allows the developer to configure their gateway and add additional features.

The dashboard interacts with the gateway through the developer's local file system. Changes made to the file system are reflected in the gateway when it is restarted. The dashboard provides a user-friendly interface to configure the gateway, enabling the addition of multiple data sources using intuitive forms and tool tips.

The dashboard leverages an open-source tool called GraphQL Mesh, which automatically creates a unified GraphQL schema and automated resolvers for the configured data sources. This automation reduces the time and effort required to manually write schema and resolver functions, resulting in a more seamless integration of data sources.

Phase 3: Testing Synapse on the Local Machine
To test the gateway, the developer can access the GraphQL Playground sandbox through the dashboard. Here, they can test sample queries and errors to ensure the gateway is functioning as expected.

Synapse also allows the developer to add custom logic to the gateway, such as custom queries. Any changes made to the gateway can be updated by running the command "synapse restart."

The dashboard provides monitoring of request latencies and errors for testing purposes. This monitoring feature displays the slowest requests and provides valuable information on errors, including the origin of the request and the original query that caused the error.

Phase 4: Deploying Synapse on AWS
Once the gateway has been thoroughly tested and configured, the developer can proceed to the deployment phase. This starts with stopping the locally running Synapse instance using the command "synapse down" and then deploying Synapse on AWS using the command "synapse deploy."

The production state architecture of Synapse on AWS is similar to the configuration state. The three components: the dashboard, the gateway, and MongoDB, are containerized using Docker. Additionally, AWS provisions an AWS Fargate instance for each component and sets up a load balancer in front of the ECS clusters.

In the production state, the functionality of adding data sources and configuring the gateway is replaced by the user management tab, which is only accessible to admin and root users. Admin and root users have the ability to view, delete, and create users, assigning them different roles.

With Synapse deployed, Chatter, the company using Synapse, can start using the gateway to query data from their deployed services. They can transition their client applications to interact with the gateway instead of directly accessing individual services.

Synapse supports integrating various data sources, including REST, GraphQL, MongoDB, and PostgreSQL. As Chatter grows and develops new services, they can easily add them to the gateway through a redeployment process.

In summary, Synapse simplifies the process of setting up and deploying a GraphQL API Gateway. Developers can download and configure Synapse, use the dashboard to configure the gateway, test it on their local machine, and then deploy it on AWS. Synapse provides user management, allows for easy addition of data sources, and offers monitoring capabilities for testing and production purposes. Running the command "synapse deploy" deploys synapse in the production state, allowing us to examine the architecture more closely. In the production state, synapse is similar to its configuration state, with all three components containerized using Docker. The GraphQL API gateway functions and appears the same as in configuration. The main difference is that the deployed dashboard in production is different from the configuration dashboard. In production, the functionality of adding data sources and configuring the gateway is replaced by a user management tab accessible only to admin and root users. These users can view authorized users, delete users, create new users, and assign admin or non-admin roles.

After deployment, the synapse architecture on AWS consists of each part of synapse (dashboard, gateway) being put on AWS Elastic Container Service (ECS). For each part, AWS provisions an AWS Fargate instance. Fargate is a technology that allows running containers without managing servers or clusters of EC2 instances. Additionally, a load balancer is added in front of the ECS clusters that contain the three containerized applications. This architecture enables synapse to handle high volume and traffic using AWS's automatic scaling and descaling capabilities.

Now that synapse has been deployed, let's explore how the synapse gateway looks in production for Chatter. When the gateway is initially deployed in production, it can be queried and connected to Chatter's existing services. Chatter can then transition its client applications to query the gateway instead of the individual services. Synapse enables Chatter to introduce a GraphQL API endpoint and take advantage of the client-side benefits of GraphQL. If Chatter expands in the future and develops new services, it can easily add them to the gateway through a simple redeployment.

Now, let's dive deeper into some of the details regarding how synapse provides certain features. Synapse is capable of monitoring request latencies and error data to aid in testing and production traffic monitoring. This monitoring data is stored in MongoDB. To collect monitoring data from each request, synapse uses two plugins passed to the Apollo Server in the gateway.

The first plugin is the "use timing" plugin from the open-source tool GraphQL Envelop. It allows synapse to retrieve and record request latency data by providing events throughout the lifecycle of a GraphQL request. Two events, "on resolver measurement" and "on execution measurement," provide data on the total request latency and latency times for the execution of each resolver. Callback functions are created at these events to extract the needed data and store it directly into MongoDB.

The second plugin is a custom Apollo Server plugin that captures error data. Synapse uses Apollo Server's "did encounter errors" event to grab error data. By combining these plugins with the schema and resolvers from GraphQL Mesh, any request hitting the gateway is recorded for monitoring purposes, regardless of whether the response is successful or returns an error.

Now let's discuss the deployment process and how synapse can be deployed on AWS with a single command. Synapse uses AWS Copilot, a CLI tool, to simplify the deployment process. Copilot takes Docker files and provisions the necessary resources to deploy the containers directly to AWS infrastructure. Under the hood, Copilot registers the Docker files to AWS Elastic Container Registry, similar to Docker Hub. Once registered, AWS generates a CloudFormation template based on user inputs. Copilot then deploys the containers onto Elastic Container Service and provisions other resources defined in the template for scalability and production readiness. Copilot achieves this by executing API calls to the AWS CLI, so the only prerequisite is configuring the AWS CLI with the developer's credentials. The Synapse CLI provides commands that simplify the deployment process using Copilot. Running the "synap deploy" command automatically provisions default settings in the manifest files for the AWS deployment and provides the necessary Docker images for deployment to Elastic Container Registry.

During the implementation process, we encountered challenges with reconfiguring an existing Synapse gateway deployed in the cloud. Docker container encapsulation prevented easy communication between containers, requiring additional linking to send files or commands. Data persistence was also an issue as Docker containers themselves do not persist data, requiring an external service. In our deployment architecture, built using Docker images, any changes made to the gateway container would be lost upon restart. In the local environment, we overcame these challenges by instructing Docker Compose to share the host's local file system, enabling communication and data persistence between containers. However, replicating this communication pipeline in the cloud was more complex. We opted to remove the configuration interface from the deployed application entirely, focusing on smooth redeployment. Configuration changes are made locally and the updated Synapse is redeployed to AWS. Only the updated container image of the gateway needs to be redeployed, thanks to removing the need for a connection between files in the dashboard and gateway containers. AWS Copilot handles swapping out the running gateway container with the updated one.

Now, let's discuss the future direction of Synapse as an open-source project. One goal is to implement a simplified way to configure cross-API resolvers through the dashboard interface. Synapse primarily focuses on service unification, but currently does not support populating fields from multiple APIs or creating new fields pulling data from multiple sources. This can be achieved through custom resolving scripts but could be simplified with an interface that allows GraphQL-savvy developers to configure this easily.

Another planned feature is built-in support for securing portions of the unified gateway using GraphQL management capabilities. This would allow restricting parts of the schema to only authenticated users with specified roles. Providing a graphical interface to facilitate these configurations is a desired improvement.

Lastly, improving the tracking and updating of deployed Synapses is important. Currently, users can only generate and deploy one Synapse per AWS account via the Synapse CLI. The goal is to remove this limitation, allowing multiple Synapses to be deployed. This would involve extending the Synapse CLI to generate the necessary files and directories for additional Synapses, tracking them based on unique names provided by the user, and handling edge cases such as mismatches between local and AWS naming.

In conclusion, this is a summary of the status and future plans for Synapse. It is a powerful tool for deploying GraphQL gateways and orchestrating service unification. Synapse offers monitoring capabilities and smooth deployment processes through AWS Copilot. However, there are opportunities for enhancement, such as simplifying cross-API resolvers, incorporating secure schema restrictions, and implementing improved tracking and deployment of multiple Synapses. With continued development and community contributions, Synapse can become an even more versatile and robust tool for GraphQL deployment. In the video, we observe that the response received from the gateway matches the structure of the initial query. When we retrieve a JSON object, it contains nested objects that correspond to the fields of the initial request. This is possible because we are working within well-designed APIs, where our separate services have appropriate foreign keys.

For example, we can see the author's ID variable acting as a foreign key. Despite this, we still obtain the desired return data. However, the fields themselves are not properly nested. As a result, the front-end code that receives this response would need to do more work to nest the code for use in client-side components.

In situations where a field returns an array of responses, combining these objects can become computationally burdensome if large amounts of unnested data are returned from the server.

To address these challenges, we can use cross-API resolvers, which allow us to nest queries. This involves adding additional resolving logic that extends the unified schema. By doing so, APIs become aware of the object types available in other APIs. This is similar to the schema federation solutions discussed earlier.

Implementing cross-API resolvers in a tool like Synapse could involve allowing developers to manually write the additional resolving logic directly into the gateway manager. With this additional resolver logic, we would be able to make the "get author" field aware of the return type of the "get author's book" field. As a result, we would be able to nest the queries, and the gateway would return an object with similar nesting.

In the example provided, the "books" field, which is the original call to the "get author's books" service, is now embedded as an attribute on the returned author object. This eliminates the need to manipulate and combine data further in the front-end logic.

Additionally, we would like to leverage another built-in GraphQL feature, which is management support for securing portions of the unified gateway. GraphQL can restrict parts of the schema to only authenticated users with specified roles. To facilitate this, we aim to provide a graphical interface that allows easy configuration of these security settings.

Finally, we want to implement a way to easily track and update deployed synapses. Currently, users are limited to generating and deploying one synapse per AWS account using the Synapse CLI. This means that common workflows, such as having a staging architecture deployed in parallel to the production version, are not currently feasible. To overcome this limitation, we propose extending the Synapse CLI to generate the necessary files and directories for additional synapses. These directories would be tracked based on unique names provided by the user. We would also need to handle edge cases where local naming does not match what AWS Copilot sees on the AWS infrastructure.

These are the main points we wanted to convey about Synapse, an exciting project we have been working on as a remote team of four across the US and Canada. We appreciate your time in learning more about Synapse. Special thanks to our project mentor, Sienna, for her invaluable guidance, and to Surgeon for organizing this presentation. We are now open to answering any questions you may have. Please type them into the chat or the Q&A section.

One question we received is about Hasura. The person is asking if we have considered using Hasura as a GraphQL gateway. Hasura is a managed GraphQL gateway solution that focuses on connecting directly to databases. While Hasura has its strengths and can be combined with tools like GraphQL Mesh, we did not include it in our discussion because our emphasis is more on integrating with databases like PostgreSQL or MySQL. However, it is indeed possible to use Hasura alongside open-source tools like GraphQL Mesh, but it requires the developer to manually integrate the two solutions.

Another question is related to error handling. If a GraphQL call is made using Synapse and it stitches together data from three services, but one of those services is down and returns an error, will the data from the two successful services still be returned? The answer is yes. Each request that goes through Synapse has its own event cycle, and the response or the documents created in that event cycle are specific to that request. Therefore, even if one service returns an error, it will not prevent the successful data from the other services from being returned.

Thank you all for joining us today. We hope you found Synapse interesting and useful. We wish you all a happy holiday season.