welcome to our presentation today about synapse a graphql gateway i'm sure you are asking what is synapse well synapse itself is a framework that creates and deploys a graphql api server allowing the user to connect a variety of different backend service apis that will be accessible via a single api endpoint using graphql so i'm going to introduce a fictional scenario here that could easily translate to a real world company and i'd like to introduce you to our new make-believe social media startup named chatter chatter was created to be an ad-free alternative to many of the current social media platforms our fictional company chatter has become immensely popular over the past few months and recently decided to redesign its architecture to a microservices platform so it can focus development teams on specific parts of its backend services without triggering massive changes throughout the entire chatter application anytime a change is made in a particular service and here you can see their old architecture was just simply a sort of what you might call a monolithic server and it was interacted with a rest api endpoint and they have but they have a lot of services contained in this so they they have services for users messages locations photos video clips and even payments one problem that our app has had since its conversion to microservices has been multiple network requests to get data from the various services especially when using a mobile device on a network that may be slow or have poor connection stability having to make multiple requests for data slows the app's performance down significantly and this can be due to the problem caused by rest apis of under or over fetching data in single request here on the screen you can see that the client has to make requests to all different backend services to finally get all the data it needs to provide a response to the request one problem let's talk about the problem of over and under fetching data using rest apis so when one rest api endpoint does not return all the data that is needed to fulfill the client's request this results in under fetching as the client still requires more data from another endpoint causing additional network requests overfetching data on the other hand may also occur in slow transmission speed when a response contains more data than was actually needed due to the nature of what the rest endpoint is programmed to return so just to give you an example of what this problem looks like this is an example request to get all of a specific user's posts and display with their name to obtain this data we actually need to make two separate requests this is because one request to the user slash id endpoint does not get all the data that is required so there are two problems here that can occur when interacting with the rest api first one request to the user slash id endpoint does not get all the data required and thus under fetch is what we need and this also shows the overfetching problem caused by rest apis where we only need the username but we're getting a response that also includes the user address and birthday as you can see below in the response object the second example you can see the second network request to now a different api endpoint that is needed to fulfill the client's main request for all posts from a specific user here we also had to send a request to users slash id slash posts to get the posts and as you can see this also overfetches more data than what the client is asking for by providing comments as well and it's even possible the client only wanted the title of the post but it's getting back the content also so a lot more data than it needs and this is also a second request now so being unable to obtain all the data the client needs in one network request leads to multiple requests and slower response time from the chatter app so what changes could chatter make to reduce the number of network requests and improve the performance of their mobile app well first they could redesign all their api endpoints to meet the current demands of their clients however client demands change and this would require a huge amount of overhaul to their current systems secondly they could continue to create new api endpoints for specific and often requested data but this would eventually lead to an immense api back-end that's constantly growing and trying to keep up with changing client needs so finally the option that we believe would best suit their needs is to combine their back-end services into a single endpoint but how can we do this well graphql could be a solution for chatter's issues with with its mobile app performance so you might be asking well what is graphql well the inflexibility of traditional api technologies led to the to the development of graphql graphql is a strongly typed query language for apis developed by facebook in 2012 to improve the performance of their mobile applications by defining a specification that reduced the need to prepare data on the server and then parse it on the client set with graphql the underlying available data is organized by a schema if you look at the bottom left corner this is an example of a schema the schema creates a hierarchy of type definitions which typically represent objects type definitions have one or more fields which indicates the return type of the data so in our schema example here we have two types we have a book and an author each one has different fields of data that they can return and so this data can be requested by the client specifically so the structure and the strict type system means we can query the schema to see the data that's available for query as well as how return objects may be structured it also allows us to declare which fields from return type are needed and meaning the client will not over fetch the application or from the application and lastly the graphql specification allows us to perform multiple queries in one request even if the return types are independent solving the under fetching issues and preventing the need for multiple round trips if you look at the two examples here the query in the center and the response on the right you can see the query asks for very specific things we're going to get books a particular book um we're just wanting to title the author and then we have another query to get get author we just want the name of the author and the book so we don't need any other information and so the response back from the graphql server provides exactly what's requested in one query right from the client and so this is solving a couple of those problems we talked about another example of a feature of graphql is that it enables nested queries and therefore requires only one client network request to obtain data from various different resources or services on the backend and this prevents both under and over fetching so in our example request here on the left books are related to an author and we are querying for the authors and then sub querying for the author's books and you can see on the right that that's exactly what we're going to get back we are asking for an author with the id2 which is jr tolkien and we're asking for the books the title of his books that he's written and so that's what we get back with one request we get all the data we need and no more so sounds good is graphql being widely used is it something that we might want to use ourselves so graphql actually has quickly become adopted throughout the industry since being released by facebook in 2015. according to the 2021 state of javascript report the percentage of developers using graphql has risen from six percent in 2016 to 47 in 2021 and tops the charts with a 94 developer satisfaction rating and over 84 percent of developers are either interested in learning graphql or definitely use it again so let's review the main benefits for chatter should they choose you grab use graphql so first the client can customize queries to fetch the exact data they need when they need it secondly it reduces over and under fetching of data third it reduces the number of calls made over the network by the client to the api and then finally graphql itself provides a unified optimized public api of services that are reachable through one single endpoint if graphql seems to be the best solution how easy is it for cheddar to switch to a graphql api well one approach would be to change all of cheddar's service endpoints into graphql format however this would require a significant amount of time and money as well as a complete redesign of all the service apis to the graphql format another common problem that would likely have to be addressed is that some of the service apis used by cheddar may in fact be owned by third parties and these would not be available for chatter to change in any way and would have to be accessed in their current api format some examples would be like they may use a payment processor that they don't own they may use an email service these are things they can't change and how they interact with so how then can we take advantage of graphql without actually rebuilding existing apis that we're using well our solution is to use a graphql api server that can be placed in front of our existing api endpoints and here you can see what that might look like from a high perspective with the data source you're using on the right we can put a graphql server in front of that no matter what type of api it's using in this case it shows a rest and then the client can simply query that graphql server to get the data that it needs but before we can do that let's look at what chatter's dilemma is now as we discussed they need a way to integrate existing apis without needing to redesign each one secondly we need to be able to interact with different api types our services might have like rest open open api json schema or even directly with databases like mongodb or sql postgres third we have to be able to integrate third party apis that we cannot change and finally graphql may be a whole new concept for our developers and we need something that doesn't require a complex knowledge of graphql to get started so how can we make the transition to a graphql gateway well in this diagram you can see the configuration without a gateway the client still has to make multiple trips to get different two different graphql servers to access each service's endpoint however when we use a graphql gateway the client can now simply make one request to a single endpoint reducing the need for multiple network requests from the client to the server a graphql server can functionally act as an api gateway for underlying data sources each underlying data source will have a corresponding schema sub-schema that describes what data can be queried from that source non-graphql apis would require what we call resolver functions that expose the sub schema and respond with the data that's requested and we'll talk more about these resolvers a little bit later the graphql gateway itself will aggregate these schemas into a single schema which the client and service can now query as a single endpoint this solves the problem of under fetching because all underlying apis are now accessible via a single client request so in summary the benefits of using a graphql api gateway include having one single endpoint to query and access all data reducing under and over fetching of resources from the client's perspective client can customize queries to only grab exactly the data it needs this is helpful for improving the speed and efficiency of mobile applications while reducing the number of network requests and a final benefit is that the gateway provides a reduced attack surface in terms of security focus can be enforcing security at a gateway level rather than at each services api so i'll turn things over to dylan who will talk about what shatters options are for actually creating an api gateway using graphql thanks justin so yeah i'm just going to share a bit more on essentially what goes into creating a graphql gateway so we have two main approaches for creating a gateway a graphql gateway and that's stitching or federation and these are essentially just two different implementation techniques for creating a single unified graphical endpoint which will represent and interact with your underlying services so let's start with federation federation assumes a company schema should be a distributed responsibility so just as an analogy if you could imagine your underlying services as individual puzzle pieces and each piece would be aware and designed to potentially fit together with the others so to implement federation the underlying services would need to be aware of each other's data as well as contain all the logic for communicating with one another to enable interwoven schemas which essentially just means how to cross or nest data from your multiple services so in this case the gateway will act as a generic agent just responsible for simply combining the requested data and will configure itself by reading the schemas of each underlying service which we call the subschemas so federation's main concern we'll be focusing on this translate and resource layer so how to translate your initial resource to be able to be queried using graphql also how to interact with the other necessary resources for crossed or nested data so what is stitching well stitching assumes a company schema should be a centralized responsibility not distributed so if you can imagine your underlying services as individual pieces of fabric they have no knowledge that they could potentially be stitched together while the gateway acts as the seamstress or seamster responsible for stitching together to collect the pieces of fabric so stitching's implementation unlike federation has your underlying services remaining unaware of each other meaning they can be left unaltered so in stitching's case the gateway will now contain all the logic for loading and combining or stitching the sub schemas and is responsible for merging data from your multiple services stitching's main concern will be focusing on combining the aggregate and translate layer into the gateway so how the gateway can actually stitch together the sub schemas in order to fetch the requested data from the correct resource so stitching uses vanilla graphql while federation expects engineers to be specialists with federation so we wanted to make point out one big distinction between uh stitching and federation and that to implement federation the developer must use apollo's federation specifications which has been compared to or thought of as like learning a new language or syntax stitching on the other hand only requires vanilla graphql and because of this federation's learning curve is quite a bit bigger than stitching so both are effective techniques for creating a graphql gateway but let's compare some of their differences a little bit more so federation's advantages first off we have faster development since new changes don't require full coordination with all other services this allows teams to work on different services in parallel another advantage for federation is that it will simplify your gateway layer so as a reminder the logic for combining sub schemas is moved away from the gateway which will keep your gateway layer thin and less of a critical piece of architecture some of the disadvantages for federation and probably the biggest disadvantage is that it alters your underlying services again since federation requires your underlying services to contain all the logic for communicating with one another another disadvantage would be this big learning curve since it does require applause federation specifications third we have that it can couple your underlying services and again since federation requires the underlying services to interact with each other so stitching some of the advantages for stitching and the biggest advantage probably is that it can leave your underlying surfaces unaltered and again stitching abstracts the logic away from your surface to the gateway layer another benefit is the smaller learning curve obviously you won't have to learn apollo's federation specifications and then some of the disadvantages for stitching first off it'll bloat your gateway layer the increase in logic within the gateway layer will cause it to be more of a critical piece of architecture and second it may require an increase in coordination so even though your services are technically technically not coupled like federation each underlying service still needs to coordinate with the others to make sure any new changes or features won't be a breaking change to the gateway so as a reminder here's chatter's current existing architecture that we shared earlier and in order for chatter to implement a graphql gateway with as little difficulty as possible but still allow their existing services to remain unaltered their best option may be to lean towards a stitching solution so let's take a look at some of the existing graphql gateway solutions for chatter i just want to point out here you may notice the lack of federation solutions this is mainly because of the general complexity to create a managed solution for federation so our first category we have is integration platform as a service with aws appsync so if chatter went with aws appsync they'd have the advantage of using their feature-rich platform but would have to deal with the additional complexity of setting up their gateway onto aws manually and could potentially lose the flexibility of moving platforms in the future dealing with a vendor lock and scenario additionally the cost of operating on aw business platform could be a big deterrent for a small startup like chatter second we have open core solutions with graphql portal so the opencore model essentially means they offer a version with the core features but you'll have to pass through a paywall to access the more feature-rich version so chatter will have the flexibility to host onto any platform easier configuration of their gateway with their provided gui the ability to monitor queries to the gateway but of course in order to access all of portal's features they'd have to pass through the paywall which could increase the cost of their operation and then third here we have the diy or the do-it-yourself solution so chatter's team would have to embrace graphql's big learning curve which could potentially increase engineering time and cost for paying the engineers obviously they'd have the flexibility to implement their gateway with stitching or federation but with their existing microservices they would have to redesign all their services in order to use federation so again even though federation of stitching are both viable solutions our team decided stitching was an appropriate implementation for synapses gateway mainly because we didn't want to alter existing services and we wanted to allow synapses to be more of a beginner-friendly solution and with a few additional features we felt synapse could fit nicely within the graphql gateway scene and maybe a viable solution for a small business like chatter so to summarize the main priority for synapse was to provide a simple way to unify legacy apis into a single graphql endpoint and then three core features we really wanted to focus on and include with synapse was first off to um to provide a simple and intuitive way to configure your gateway so we designed a front end for the gateway which would provide the developer with a gui to aid in adding your existing apis to your gateway and this would eliminate the need to add or modify configuration files manually in the backend directory as well as having to rebuild the gateway after each change so once configured we wanted the developers to have the option for streamlined deployment to have a production ready gateway into the cloud as quickly as possible we also wanted to the deployment pipeline to spin up a low maintenance solution for a small business that may not have the time or resources to manage the fine details of their deployed gateways architecture and lastly we wanted to take advantage of the gateway being a centralized service by giving the developer way to monitor and analyze incoming graphql requests so since a business like chatter will still need to learn and work directly with graphql on the front end of their application mainly for issuing graphql queries to their gateway we felt having a monitoring dashboard would be valuable in helping analyze and potentially debug any situ or issues with these front-end incoming requests as well as helping identify issues with any underlying services as they arise so let's compare synapse to the existing solutions so if chatter went with synapse it would allow them to have the flexibility to host onto any platform just like graphql portal it would be provided a gui to aid in configuring their gateway as well as be able to monitor incoming requests which was we noticed was a common feature that also aws appsync and portal both provided and justified including it into synapse and then optionally shredder would also be able to automatically deploy their gateway onto an aws server and this is really where we felt that synapse could stand out compared to the other existing solutions since this was an uncommon feature we noticed um and then being open source chatter can also extend synapse in the future if they wish and most importantly chatter can leave all their existing services unaltered since synapse is implemented with stitching i'll pass it over to anish to talk a bit more about how synapse actually works yeah thanks dylan uh so now that you've seen why a small business would want to use synapse over some of the other options in the field we can dive deeper into how synapse works and how it was built from a high level using synapse is split into four phases inspired by the open core solution graphql portal first the developer must download and set up synapse on their local machine then they must use the provided gui interface which we refer to as the dashboard to configure their synapse graphql gateway once configured the developer is able to test out their gateway on their local machine lastly if desired the developer will deploy synapse onto aws or the synapse architecture will change slightly and will be ready for high volume traffic as mentioned the synapse architecture changes slightly once it's deployed therefore you can think of synapses having two distinct states one for configuration and one for production in configuration everything is run on the developer's local machine they download synapse and start it up on their local host they then use the synapse gui more formally known as a dashboard to configure the graphql gateway once configured the gateway is able to be tested under local machine after successful testing of the gateway the developer deploys synapse into production where their synapse application will now be hosted and run on aws once in production the developer loses the ability to configure the gateway through the deployed dashboard however this functionality is replaced with management of users additionally when deployed synapse will use a fresh mongodb database wiping the data from the configuration phase only being seeded with credentials of the root user this allows the production gateway data not to be interspersed with testing data that may have been collected during configuration and testing we will go over this process in much more detail through a step-by-step walkthrough of the four phases mentioned before so to start the first phase is downloading and setting up synapse the developer was must first download and configure synapse to run on their local machine they're able to do so by running the command mpx at synapse team start synapse this command will prompt the user for a couple inputs and then will set synapse up on their local machine so that it can be started right away after downloading and configuring the developer is able to run and start using synapse on their local machine by running the command synapse up this will start up and run the entire synapse architecture on the developer's local machine keep in mind the architecture we refer to here is the synapse architecture in the configuration state let's take a look at what this is the configuration state architecture synapse on your local machine looks as so as you can see synapse consists of three components the graphql gateway the dashboard and mongodb synapse utilizes docker files for each part when the command synapse up is run a docker compose file is used to start up all three containers we'll look deeper at each part as we go through our walkthrough so at this point if chatter was using synapse they would have syn ups downloaded set up and running on their local machine as you can see none of their deployed infrastructure would be interacting with synapse at this point and since synapses run on their local machine they can avoid costs from having to configure on a cloud platform so the second phase of synapse is configuring the synapse graphql gateway the synapse up command instantiates a containerized instance of the dashboard on the developer's local machine the dashboard is essentially a gui interface that allows the developer to configure their gateway with a couple additional features the dashboard makes changes to the developer's external file system on their local machine which updates and configures the gateway after changes are made to the file system the gateway container is restarted to reflect those changes therefore the two don't directly talk to each other but are linked via the local file system the dashboard lets the developer configure the gateway by providing a data sources tab where the user can add data sources to their gateway they can easily add as many data sources as they'd like with intuitive forms and tool tips showing them what is required of them all they have to do is add some data sources input the necessary files or urls and click on create synapse and their gateway will be set up and ready to be run and deployed the dashboard is able to do this because of an open source tool known as graphql mesh graphql mesh is an open source tool that takes in a variety of data sources and automatically creates a unified graphql schema and automated resolvers for them the term schema and resolvers may be unfamiliar so let's talk about what these are and why automating them is desirable to explain this we'll show you what happens when a request comes in to a typical graphql server as you can see in the diagram when a request comes in the graphql server will first parse and validate the requests to make sure it is compatible with the graphql schema that the server is using you can think of the schema as a list of rules for how graphql requests need to be formatted with regards to the data being requested this usually is manually written in accordance to the graphql specification and can take a significant amount of time even for someone who may not be new to graphql if the validation passes the server will make an execution plan and pass off the request to the resolvers the resolvers are functions that dictate how to grab information needed from the request in the diagram you see here the resolver functions would have to make requests to the correct rest api endpoints to retrieve the necessary data for the graphql server to send the response back to the client if the data source was a database instead of a rest api the resolvers would be responsible for querying that database usually all of these functions would have to be manually written by the developer and then the information they retrieved would have to be parsed and put in the correct format for the server the server responds back to the client luckily this whole process can be avoided by automating the writing of the schema and resolvers so now that you know why automating the writing of a unified schema and resolvers is desirable and the graphql mesh is able to do so you might be wondering how graphql mesh works graphql mesh works by using either introspection for sources like graphql or by taking in a specification file such as an open api specification for rest apis introspection is essentially making a request to an api or database to inspect the models or types it's composed of as an example let's consider how graphql mesh might introspect the postgresql database first graphqlmesh makes an introspection request which looks at the database to see that it has a books table graphqlmesh is able to take this information and create a graphql schema with a type of book and associated subfields as well as appropriate resolver functions if graphql mesh was introspecting a graphql endpoint instead it would get the graphql schema as a response that it could just use directly this is why the dashboard is able to have the developer only input a url for some data sources like graphql or postgresql graphql mesh also works for certain data sources like rest endpoints by taking in a specification file and a yaml or json format the example here shows an open api specification file that defines a rest api the file describes all the entities supplied by that endpoint as well as outlines all the operations for them graphql mesh is able to use this file to automatically create a graphql schema and resolvers from it this is why the dashboard requires a developer to upload a specification file for some data sources like rest endpoints as you can see here the developer needs to upload an open api specification file in a yaml format to integrate their rest api data source so the dashboard is able to take some inputs from the developer and then use graph graphql mesh to create a unified schema and resolvers for all their data sources but how does setups actually use the schema and resolvers this is where the gateway comes in the server aspect of the gateway is served by apollo server apollo provides a graphql library that includes a graphql server more formally known as apollo server which is able to be instantiated by providing it with the appropriate graphql schema and resolvers synapse uses apollo server by passing it the stitch graphql schema and resolvers created by graphql mesh all that being said you can think of synapse as essentially a wrapper for the functionality of graphql mesh allowing developers with little to no graphql knowledge to easily and seamlessly integrate their existing data sources to form a unified graphql api gateway at this time synapse supports integrating rest graphql mongodb as well as postgresql so now that the gateway has been configured we can look at how chatter setup would change at this point the gateway which has been configured and is running on their developer's local machine is now able to query data from their deployed services their production architecture is untouched or changed and their newly configured gateway can be tested on their developer's local machine without affecting or disrupting their existing infrastructure and workflow so after configuring the gateway the developer is ready to start the third phase testing synapse on their local machine to test the gateway the develop the developer can navigate to their dashboard on their local machine from here they can click on the graphql playground tab to open up a graphql playground sandbox for their newly configured gateway they can test out sample queries and errors to see if the gateway is working to their liking synapse also allows the developer the freedom to add custom logic to the newly created gateway for example custom queries that were not provided by default if the developer manually adds logic to their gateway they only need to run the command synapse restart and the gateway will update with their changes to aid with testing the dashboard also provides monitoring of request latencies and errors made to the graphql gateway for monitoring request latencies it will show you your slowest requests as well as requests within certain time frames these requests can be filtered by individual requests or even by individual field resolvers to find the slowest latencies easily and efficiently additionally the dashboard provides monitoring of errors to aid with testing errors can be filtered by our range and provide lots of valuable information this information includes when they occurred the origin of the request that caused the error the actual list of errors from the request as well as the original query that was sent on the request to cause the errors the monitoring data displayed in the dashboard as well as user data which we will talk about later is source for mongodb mongodb is instantiated by a docker compose file as mentioned previously when the developer runs the command synapse up a question you may be wondering is why did we use mongodb as opposed to a relational database such as postgresql or mysql first off the data we were dealing with is going to have a very high right to read ratio we wanted to store every request that came in envisioning that the user would only check the dashboard for monitoring every so often in order to accommodate an extremely high rate volume mongodb seemed like a good choice as document stores scale very well more so than relational databases secondly our data really was not relational our data was split up into four main collections errors full queries resolvers and users none of the collections needed information from each other and serve their purpose as standalone documents a relational database would have been much more attractive we had to make use of foreign key relations but since our data was not connected at all we opted for a database that would be more efficient at handling a high volume of rights but the trade-off of losing increased performance if we ever wanted to connect our data in the future lastly we did not want to adhere to the strict schemas imposed by relational databases graphql queries by their nature can vary significantly and therefore errors and responses may sometimes differ greatly from predetermined schemas we wanted to go with a database where we would be able to dump information as it came regardless of schema since there is so much variation that could be observed so now if the developer has fully tested their gateway and is happy with it in the current state they can safely move on to the deployment phase to get started the developer must first stop running synapse in their local machine by running the command synapse down after synapse has been torn down on the developer's local machine they can deploy synapse on the aws by running the command synapse deploy this will deploy synapse in the production state which we can examine a little more deeply the production state architecture synapse is essentially the same as it is during configuration all three components are containerized via docker and the graphql api gateway works and looks the same as it is in configuration the biggest distinct difference is that the deployed dashboard is different in production than it is in configuration once in production the functionality of adding data sources and configuring the gateway is no longer present in your dashboard but is instead replaced by the user management tab that is only accessible to admin and root users as you can see admin root users are able to view all authorized users and their role and delete users at their pleasure in addition they are also able to create new users and assign them an admin or non-admin role after the deployment is complete synapse architecture on aws looks as so as you can see each part of synapse the dashboard the gateway and is put on aws ecs for each part aws also provisions an aws fargate instance for them fargate is a technology that you can use with amazon ecs to run containers without having to manage servers or clusters of amazon ec2 instances in addition a load balancer is spun up and put in front of ecs clusters that are composed of the three containerized applications this architecture allows synapse to be able to handle high volume and traffic through aws's automatic scaling and descaling and with that the four phases are complete synapse was downloaded and set up configured tested and then deployed on aws so now that we've deployed synapse let's get back to chatter and see how their synapse gateway looks in production when the gateway is first deployed into production it's able to be queried and is connected to chatter's existing services chatter can then go through the process of changing their client applications to begin clearing the gateway rather than the individual services directly as you've seen synapse enables chatter to easily introduce a graphql api endpoint and begin taking advantage of the client-side benefits of graphql if chatter grows in the future and develops new services they can also easily add them to their gateway through a simple redeployment which we'll touch on a little deeper later [Music] with that i'll hand it off to jay thanks anish so now that we've gone through the walkthrough we can dive a little deeper on some of the details regarding how synapse was able to offer certain features as mentioned before synapse is able to provide monitoring of request latencies and error data to aid in the testing phase as well as to monitor production traffic we mentioned that this data was stored in mongodb but how do we get that data from each request if we look back at the architecture of synapse we can see that the gateway interacts directly with mongodb synapse collects monitoring data from the gateway and dumps it into mongodb as each request comes in this is possible through two plugins that were passed to the apollo server in the gateway let's take a deeper look at how these plugins work and how they were used we can start with the use timing plugin from the open source tool graphql envelop this plugin allows synapse to retrieve and restore request latency data providing several events throughout the lifecycle of a graphql request where data can be extracted the two events highlighted here the on resolver measurement event and the on execution measurement event allowed us to grab the total request latency as well as data on the latency times for the execution of each underlying resolver we created callback functions that would grab the data we needed at these events for direct storage into mongodb in the case of errors we needed an alternative approach as an error may occur before the used timing events we're listening for are able to fire the second plugin we used was an apollo server custom plugin which allowed the gateway to retrieve and store error data since synapse uses apollo server under the hood we were able to take advantage of apollo server events to grab error data apollo server events are different from the events the use timing plugin emits with apollo server emitting a did encounter errors event that we created a specific listener for as mentioned before these plugins are passed alongside the schema and resolvers from graphql mesh to the apollo server this allows any request that hits the gateway to be recorded and then used for monitoring purposes whether whether the response was successful or it emits an error so now that you know how synapse is able to provide monitoring you may still have some lingering questions around deployment and how we were able to deploy synapse onto aws with a single command synapse is able to be deployed through the use of aws copilot aws copilot provides a cli that is able to take docker files and provision the necessary resources to deploy them directly to aws infrastructure under the hood copilot takes the docker files and registers them to aws elastic container registry which is amazon's version of docker hub once the containers are registered aws will generate a cloud formation template based off user inputs to the command line after the cloud formation template is generated copilot will deploy the containers onto their elastic container service and then provision the other resources defined in the generated cloud formation template to make sure that the application is scalable and production ready copilot is able to do this by executing api calls to the aws cli under the hood for that reason the only prerequisite to the points to deploying with synapse is that the developer must have the aws cli configured with their credentials the synapse cli provides commands that simplify the deployment process via copilot once the synap deploy command is run synapse automatically provisions some default settings in the manifest files for your aws deployment as well as provides all the necessary docker images for deployment to the elastic container registry now that we've set the scene a bit we can talk about some of the challenges we encountered during our implementation process so our biggest hurdle was handling some of the limitations the production environment had on reconfiguring an existing synapse gateway that had already been deployed to the cloud it's important to note that there's nothing inherent about the cloud environment that prevents us from modifying our gateway configuration files once it's been deployed in other words the commands that synapse runs under the hood to configure and build an updated gateway are still available to us once deployed however the natural encapsulation provided by docker containers complicates how changes to the gateway are reflected across the other containers our two main challenges with containerization were container encapsulation itself docker containers are naturally encapsulated meaning that enabling communication between containers requires some sort of additional link as a means to send files or commands between containers and we also ran into issues with data persistence even if we could provide a means to directly pass files between containers themselves the fact that docker containers themselves do not persist data on their own means we require an external service to the containers in order to persist the data lastly in the case of our deployment architecture the containers are built and run on aws based on docker images which are snapshots of the local system at the time they were deployed this means that in the event that the gateway container is restarted in deployment any changes would be lost to the gateway and the gateway would revert back to the original snapshot produced at the time of deployment if the containers are not able to communicate with some sort of external storage and the solution to this is pretty trivial in the local environment as we are able to easily create a communication link between containers as well as persist data and we can do this by instructing docker compose which is the orchestrator running our component docker containers locally to share the host's local file system both the dashboard and the gateway containers are able to read and write to the local file system through what are known as bind mounts meaning that changes made to the configuration files by one container are saved locally and instantly accessible for reading by another container so we've talked about how the instance of synapse which gets deployed to aws is effectively a clone of the local docker containers at the time of deployment and we've also talked about the problems and possible solutions to address complications presented by containerization however these these issues become even more challenging in the production environment our local solution replies relies on communication with the host file system obviously your local file system isn't available for deployment to the cloud to mimic the communication pipeline we have locally synapse would need to provision an additional cloud storage service such as either amazon elastic file system or amazon s3 buckets as part of our deployment process to provide an adequate means for communication between containers we wanted to avoid this approach if possible as it extended our cloud architecture footprint which both complicates the deployment process and increases costs because of the added complexity of enabling reconfiguration once synapse has been deployed to aws we decided the best approach was to remove the configuration interface from the deployed application entirely and instead focus on making redeployment as smooth as possible this means since the functionality of adding data sources is no longer present in production updates such as adding a new api to the gateway are completed locally and the updated synapse is redeployed to aws to make changes to the gateway after production the developer must go through a similar workflow to the original configuration workflow described previously this involves the developer making changes to the gateway on their local machine with synapse in the configuration state essentially this means that the developer must go through the previously mentioned configuration and testing phases again locally with the containerized architecture and by removing the need for a connection between files in the dashboard container and the gateway container when in production only the updated container image of the gateway needs to be redeployed to your aws once you are satisfied with the changes you've made locally you could redeploy using synapse redeploy which triggers aws copilot to swap out the running gateway container with a new gateway container built using this updated image so that's a summary of the current state of synapse which begs the question where does synapse go from here as an open source project how can our team and others in the community extend or build upon synapse so one thing we'd like to see implemented is a simplified means for configuring cross api resolvers through the dashboard interface synapse is primarily focused on service unification in order to reduce total request count however the population of fields not explicitly defined in the generated schema or even the creation of new fields that pull data from multiple apis is not currently supported through the dashboard though more graphql savvy developers could add custom resolving scripts to the backend directory in order to enable this currently data can be pulled from multiple apis through a single http request made by the client but this single request would have to make separate graphql queries for the gateway to know which of the api endpoints behind it to pull data from in this example a single request is made to the gateway however the get authors field and the get authors books field are actually queried independently as we can see the response received from the gateway matches the structure of the initial query we get back a json object with nested objects matching the fields of the initial request if we can assume we're working within well-designed apis meaning that our separate services have appropriate foreign keys as seen here with the author's id variable we still get the desired return data however since the fields themselves are not appropriately nested the front end code receiving this response will have to do more heavy lifting in nesting this code for use in client-side components in cases where a field returns an array of responses combining combining these objects may become computationally burdensome if large amounts of unnested data are returned from the server cross api resolvers allow for nesting queries this involved they involve adding an additional resolving logic which extends the schema that was unified through stitching in such a way that apis become aware of the object types available in other apis similar to what is seen in the schema federation solutions discussed earlier as with true federation this cross api awareness is challenging to accommodate via automation for a tool like synapse the simplest approach would be adding a means for the developer to manually write this additional resolving logic directly into the gateway manager with additional resolver logic we would be able to make the get author field aware of the return type of the get author's book field enabling us to nest the queries and the gateway would return an object with similar nesting in this example the books field which is simply the original call to the get authors books service is now embedded as an attribute on the returned author object eliminating the need to manipulate and further combine data in the front-end logic similarly similarly to cross-api resolvers we would like to accommodate another built-in graphql feature which is management support for securing portions of the unified gateway graphql can restrict parts of the schema to only authenticated users with specified roles and we would like to provide a graphical interface to easily accommodate these configurations and lastly but most importantly we would like to implement a way to easily track and update deployed synapses to easily accommodate redeployment users are currently limited to generating and deploying one synapse per aws account through the synapse cli unfortunately this means some common workflows such as having a staging architecture deployed in parallel to the production version aren't currently feasible instead a team would have to would instead have to stage any configuration changes locally and redeploy to update their production graphql container removing this limitation wouldn't be challenging but it would require extending the synapse cli in order to generate the new files and directories required for additional synapses track these directories based on unique names provided by the user and handle edge cases such as situations where local naming does not match what aws copilot sees on aws infrastructure and that's synapse we're the synapse team a remote team of four working across the us and canada thanks to comey thanks for coming out to learn more about synapse and special thanks to our project mentor sienna who was great and to surgeon for organizing the presentation who sometimes is also great um and with that i think we can turn it over to some questions yeah it looks like we have a couple and if you have um any questions feel free to type them into the chat or the q a um one question from cody is about asura and it's he's asking um at my company we use tesura for this use case a graphql gateway we've used hasura and graphql mesh on top of our monolith api as we migrate to microservices i'm basically interested in knowing if that's something we had considered as well hasura yeah i can take this um so for those of the you who don't know what hasura is um sarah is a managed graphql gateway solution without strong emphasis on connecting directly to your database there's a couple other solutions like that as well the reason we didn't mention hasura uh in our existing solutions and things like that is because of how strong of an emphasis it does have on directing or connecting directly to a database the way cody is using hasura at his company is by using asura actions and graphql mesh basically creates a schema that you can connect to an acera action and then integrate your data source in that fashion we just didn't want to include it in our discussion for the presentation because the primary i would say like focus of the company was more so on integrating directly with a database like postgresql or mysql um but he is correct you could also use hasera and combine it with an open source tool like graphql mesh uh the only problem with that would be that the developer would have to use the open source tool themselves and then integrate that with their history managed service um because ira doesn't do that directly for you in their uh interactive gui um anyone wants to add to that no uh and there's another question um if i make a graphql call using synapse that stitches together data from three services but one of those services is down and sends back an error will i still get data from the two successful services yes uh yeah you would still get data from the two successful services each request that goes through gets its own event cycle um and the or the documents that are created in uh are event cycle specific um so even an error won't block other requests from coming through um so you would still get the successful data as well as the error data all right um i don't see any other questions so thank you everyone for coming today and we hope you found synapse to be interesting and of use and we wish you all a good holiday thank you you 