Welcome everyone, we are Apex, an API proxy designed to log and control traffic between microservices. In this video, we will cover six main topics. Firstly, we will explain what microservices are and compare them to the monolithic architecture. Secondly, we will discuss the challenges that come with working with microservices. Thirdly, we will explore existing solutions for these challenges. Next, we will delve into the design and architecture of Apex. Following that, we will discuss the implementation and deployment of Apex. Finally, we will touch upon the challenges we encountered during the process.

Microservices are an architectural choice that provides an alternative to the traditional monolithic structure. Let's start by understanding what a monolith is. In the example of an e-commerce app, all the logic resides within a single app server. This server interacts with a database and communicates with the client. The logic within the app server is divided into subsystems, such as orders, customers, and notifications. These subsystems interact within the same app, running in the same process and memory on a single server. Communication between subsystems is reliable and efficient, as it occurs within the app itself.

In contrast, microservices deploy subsystems independently. Each subsystem, now referred to as a service, runs on its own server. These services communicate with each other over the network. For example, if the orders service needs to create a shipment, it would make an HTTP POST request to the shipping service, which in turn would send a response. This network-based communication brings a range of benefits.

One key advantage of microservices is the freedom to choose different technologies for each service. This allows owners to select the most suitable technology stack for their specific requirements. As long as the services communicate via HTTP, they can implement their logic using any compatible stack. Another benefit is the ability to redeploy services independently. If an owner wants to add a new feature or update a service, they can simply redeploy that specific service without affecting the others. The services maintain stable APIs, ensuring compatibility and seamless operation.

Additionally, microservices enable independent scaling. Suppose one service reaches capacity while others do not. It is possible to scale that specific service by upgrading its server resources, such as memory or processing power, without impacting the remaining services. This targeted scaling optimizes resource allocation and improves overall system efficiency.

While microservices offer numerous advantages, they also introduce new challenges, mainly due to the network-based communication. The first challenge is diagnosing network faults. In complex microservices architectures, workflows involve multiple services, where a single failed request can disrupt the entire flow. However, identifying the source of the failure becomes cumbersome because each service generates and stores its logs separately. Developers must sift through multiple sets of logs, hindering the troubleshooting process.

The second challenge is managing fault handling logic. Network faults can be unpredictable, requiring services to handle retries. However, defining reliable retry logic is crucial to prevent overwhelming or crashing services. Retries need to be implemented with caution and might differ across services. Traditionally, this logic is embedded within HTTP client libraries imported into each service. However, this approach becomes burdensome to manage as the number of services and languages increases.

To address these challenges, existing solutions have emerged: the API Gateway and the Service Mesh. Both solutions rely on proxy servers, but they differ in their approach. The API Gateway acts as a single proxy server with additional features, while the Service Mesh comprises multiple proxy servers working together. Now let's take a closer look at the API Gateway and its functioning.

In the absence of an API Gateway, clients accessing our microservices need to know the specific details, such as host, port, and path, of each service within our architecture. This tightly couples the client with the internal structure and requires updating client configurations when changes occur.

In contrast, an API Gateway serves as a single entry point for clients and abstracts away the complexity of the internal architecture. It receives requests from clients and forwards them to the appropriate services based on predefined rules. Clients only need to interact with the Gateway, simplifying the process for both clients and services. Additionally, the Gateway can handle authentication, rate limiting, and caching, further easing the burden on individual services.

The Service Mesh takes a different approach, employing multiple proxy servers to enhance communication between microservices. Each service is accompanied by a sidecar proxy, enabling direct communication between services. This architecture enables features like load balancing, service discovery, encryption, and fault tolerance. The Service Mesh ensures seamless and reliable communication while decoupling services from network-specific concerns.

The API Gateway and the Service Mesh both leverage proxy servers to overcome the challenges found within microservices architecture. The API Gateway acts as a centralized entry point, abstracting away internal complexity and offering additional features. On the other hand, the Service Mesh utilizes sidecar proxies to enhance communication between services, ensuring reliability and decoupling services from network intricacies. Selecting the appropriate solution depends on the specific requirements and complexities of the system.

Now that we understand the concepts of microservices, the challenges they present, and the existing solutions available, let's delve into the design of Apex. Apex is an API proxy that addresses the challenges faced in microservices architectures. It serves as a centralized entry point, providing logging and traffic control capabilities.

Apex is designed as a standalone service that sits between clients and services. It intercepts incoming requests from clients and forwards them to the appropriate microservice based on predefined rules. Similarly, it receives responses from the microservices and returns them to the clients. Additionally, Apex logs all the traffic flowing through it, providing a centralized location for troubleshooting and diagnosing network faults.

The architecture of Apex consists of two main components: the Gateway Manager and the Gateway Instances. The Gateway Manager serves as the orchestrator, responsible for managing the configuration and the lifecycle of the Gateway Instances. It ensures that the Gateway Instances are deployed and scaled according to demand.

The Gateway Instances are the actual proxy servers that handle the incoming requests and outgoing responses. They are distributed across multiple servers, allowing for scalability, fault tolerance, and load balancing. This distributed architecture ensures high availability and performance for the proxy service.

The implementation of Apex involves developing the Gateway Manager and the Gateway Instances. The Gateway Manager is the control plane responsible for managing the configuration and state of the Gateway Instances. It provides an interface for administrators to manage the proxy service and define the routing rules. The Gateway Instances, on the other hand, are the data plane responsible for handling the actual request forwarding and response handling. They are designed to be lightweight and scalable, allowing for efficient traffic management.

Once implemented, Apex needs to be deployed within the microservices architecture. The Gateway Instances are deployed alongside the microservices, intercepting incoming requests from clients and directing them to the appropriate services. The Gateway Manager is deployed separately to manage the configuration and coordination of the Gateway Instances.

During the implementation and deployment of Apex, several challenges can arise. One challenge is ensuring seamless integration with existing microservices. Apex needs to be compatible with a wide range of microservices implemented in different technologies and languages.

Another challenge is managing the configuration and scalability of the Gateway Instances. As the number of microservices and clients grows, the Gateway Instances need to handle increased traffic while maintaining optimal performance. Ensuring efficient load balancing and fault tolerance is crucial for providing a reliable and scalable proxy service.

Additionally, Apex needs to handle large volumes of incoming and outgoing traffic while maintaining low latency. Designing an architecture that can handle high throughput and response times is essential for the overall performance and user experience.

In conclusion, microservices offer numerous advantages over monolithic architectures, including the choice of technologies, independent deployment and scaling, and efficient resource allocation. However, working with microservices introduces challenges related to network communication, such as diagnosing network faults and managing fault handling logic. To address these challenges, existing solutions like the API Gateway and the Service Mesh have emerged. Apex, our API proxy, aims to overcome these challenges by providing a logging and traffic control mechanism. The design and architecture of Apex, along with its implementation and deployment, are crucial in ensuring effective integration within the microservices architecture. Despite the challenges encountered, Apex aims to provide a reliable, scalable, and high-performance solution for managing communication between microservices. The traditional approach to building microservices presents two main challenges: diagnosing network faults and managing fault handling logic across services. As the number of services grows, diagnosing network issues becomes complex due to distributed logs and workflows spanning multiple services. Additionally, managing fault handling logic becomes difficult as it needs to be duplicated across multiple client libraries in different languages.

To address these challenges, two solutions exist: the API gateway and the service mesh. Both solutions are built on the concept of a proxy server, but they differ in their approach. The API gateway acts as a single proxy server with additional features, while the service mesh consists of multiple proxy servers working together.

A proxy server acts as an intermediary between communicating machines or services. Instead of services directly sending HTTP requests to each other, they send requests to the proxy server. The proxy then forwards the request to the intended service and forwards the response back to the sender.

The API gateway solves the problem of tightly coupling clients to the internal architecture of the system. Without an API gateway, clients need to know the precise details of each service's endpoint. This requires coordination between clients, the front end, and the developers of the internal services. With an API gateway, all incoming requests are intercepted by the gateway, providing a stable API for clients. This means that internal changes to services, such as endpoint updates or even service replication, can be made without requiring updates to the clients.

The API gateway also offers additional features like authentication, caching, and rate limiting, making it useful beyond providing a stable API. However, using existing API gateway solutions for service-to-service communication can introduce unnecessary complexity, as they are primarily designed for client-server traffic.

The service mesh, on the other hand, consists of multiple proxy servers called sidecar proxies. Each service has its own sidecar proxy, and all communication flows between these proxies. Sidecar proxies handle tasks like retries, routing, and caching, allowing the service code to focus on its core business logic. The service mesh also includes a configuration server, which centralizes all the logic for fault handling and other networking concerns. Updates to the configuration server are automatically pushed to each sidecar proxy, ensuring that all services have the most up-to-date configuration data.

The service mesh provides a strong solution to the challenges of diagnosing network faults and managing fault handling logic. It offers scalability, resilience, and fault tolerance, as any failure in a service or sidecar proxy does not disrupt the entire system. However, the service mesh comes with trade-offs, including increased complexity and changes to the deployment process, especially if services are not already containerized.

Considering these trade-offs, we designed Apex, our solution, with a specific use case in mind: a small team migrating from a monolith to a microservices architecture. The team may already be familiar with deploying services using a platform-as-a-service environment like Heroku. Apex's architecture includes a simple proxy that allows services to communicate through it. This proxy relies on additional components to provide comprehensive functionality.

Zooming out, the Apex architecture involves requests coming into the system, where services communicate through the Apex proxy. This architecture is tailored to address the specific challenges faced by a team transitioning to microservices. In this video, we will discuss two solutions to the problem of where to place network fault logic and how to handle network faults in a distributed system. The first solution is an API gateway, which acts as a single entry point for all service-to-service communication. Requests and responses flow through the API gateway, allowing for centralized network fault handling. However, this solution has limitations. It can become a single point of failure and a bottleneck for traffic. Additionally, it may not be suitable for highly scalable and resilient architectures.

The second solution is a service mesh, which consists of a network of sidecar proxies deployed alongside each service. These proxies handle network communication and fault handling. The service mesh provides scalability, resilience, and fault tolerance by distributing network logic across multiple components. Each sidecar proxy acts as an independent entity, which means that if one fails, it does not affect the operation of other services. However, the service mesh introduces complexity and requires changes to how services are deployed.

When considering the trade-offs between the API gateway and the service mesh, the API gateway is simpler to deploy and operate. It is well-suited for service-to-service traffic. On the other hand, the service mesh offers more scalability and resilience but is more complex to deploy and operate. It may also require changes to current service deployment methods.

Now let's discuss Apex, a solution designed for a small team transitioning from a monolithic to a microservices architecture. Apex's architecture includes a proxy that facilitates communication between services. The proxy relies on four additional components to provide a full set of functionality. This architecture allows for centralized network logic updates and provides a simple solution for fault handling and resilience.

The Apex architecture involves requests coming into the system and being processed through the central proxy. All communication between services goes through this proxy, making it a critical component in the architecture. The proxy itself is composed of several files that handle different responsibilities such as authentication, retry logic, and logging.

Logging is an important aspect of Apex. All requests and responses are logged to a centralized logs database. This ensures that information about network faults and request history is easily accessible in one place. Requests and responses belonging to a specific workflow are associated with a correlation ID, allowing for easy tracking of logs related to a particular workflow.

To handle fault logic, Apex relies on a configuration store. This store holds default communication protocols for service pairs, as well as any specific communication protocols required for individual requests and responses. The proxy communicates with the configuration store to determine how to handle each request and response.

Apex's architecture simplifies deployment and operation since it consists of a single proxy and a few additional components. It is not as scalable or fault-tolerant as a service mesh due to the single point of failure introduced by the central proxy. However, for small teams migrating to microservices, Apex offers a good set of trade-offs.

Now, let's dive into how Apex is designed and deployed. Apex comprises five main components: the proxy server, a logs database, a configuration data store, an admin API, and a user interface.

The proxy server is implemented using Node.js, which enables it to handle a high number of concurrent requests and ensures high throughput. The Express.js framework is used for its minimalistic nature and performance advantages.

The logs database is designed with high throughput in mind to prevent bottlenecks. Timescale DB, capable of inserting over 100,000 records per second, is chosen for its rapid ingestion rate.

For the configuration data store, Redis is utilized due to its ability to handle 72,000 requests per second, making it suitable for the high request rate during the request/response cycle.

The admin API, built using Node.js and Express.js, makes use of third-party client libraries to simplify development. The user interface is developed using React.js, offering reusable components and rapid development capabilities.

Apex is deployed using Docker and Docker Compose for simplicity. It allows for easy reproducibility and enables users to deploy Apex locally with a single command.

During the implementation process, several challenges were encountered. One challenge was logging large request bodies in real-time. To address this, the decision was made to keep the body compressed whenever possible. While this adds a step of decompression for log viewers, it ensures efficient logging.

Another challenge was ensuring the persistence of log and configuration data within containers. Container volumes were utilized to store and persist data, allowing for the availability of data even when containers are started or stopped.

Moving forward, the team plans to explore backup solutions using cloud storage for durability and long-term persistence of log and configuration data.

In conclusion, Apex is designed as a streamlined solution for small teams transitioning to microservices. Its architecture facilitates centralized network fault handling and provides simplified deployment and operation. While not as scalable or fault-tolerant as a service mesh, Apex offers a stepping stone for teams looking to migrate to more complex architectures. With its focus on simplicity and compatibility with platform-as-a-service environments, Apex is a viable option for small teams taking their first steps into microservices. The Apex responding service name tells us which service the original service is trying to reach. In this case, Apex is hosted at localhost on port 1989, and the desired path is set to reach CNN. Despite sending the request to localhost, we were able to successfully retrieve the data from CNN, as shown in the response. Logging is another important aspect that needs to be considered. It is crucial to ensure that every request and response is logged in one central location. This allows users to quickly identify and understand any network faults that may occur. The server logs for the proxy provide detailed information, including the incoming request from the original service, as well as any outgoing requests and responses, all of which are logged to our database. Tracing is a key part of the logging process, as it helps to examine the logs for specific workflows. By utilizing a correlation ID, users can easily access and analyze the logs that are relevant to their specific inquiry. A correlation ID is included in the response to the originating service, and this can be used in our admin user interface to retrieve the specific logs of interest. We have ensured that all network fault logic is consolidated in our configuration store. This allows for easy management and updating of configurations. Moreover, the configuration store is an ideal place to store credentials for service authentication, and it also functions as a centralized location to store routing information. With this approach, any changes in the location of a service can be made in the configuration store without requiring coordination with other services. Updating the logic is handled through our admin user interface, making it straightforward and efficient. The admin user interface provides the option to specify custom configurations for specific requesting and responding services. We greatly appreciate your input and are now open to answering any questions you may have. Julius asked whether we considered using InfluxDB as an alternative to Timescale. While we did not extensively analyze the trade-offs between the two options, our decision was primarily based on considering logs as a type of time series data. Since logs are typically written once and not frequently updated, and querying them is often time-based, we felt that Timescale was a suitable choice. The most challenging part of the project was the initial research phase, which involved understanding the problem at a deeper level and exploring different potential solutions. It required us to carefully evaluate trade-offs and select the most appropriate solution for our specific target users. This process was particularly demanding because we had to ensure that our solution addressed the needs of developers while also narrowing down the scope to focus on a specific user base. Derek agreed with this sentiment and added that understanding existing solutions, such as API gateways and service meshes, was particularly challenging due to the technical jargon commonly associated with such solutions. However, once we gained a thorough understanding of the problem, the actual development phase was relatively manageable. When it comes to testing throughout the development process, it was quite difficult. Unlike testing a single application, our solution involved multiple interconnected services, making it challenging to simulate real-world traffic and network faults. We did manage to test the retry logic by setting very short timeout values, but beyond that, comprehensive testing was arduous until we containerized our solution. Once everything was running in Docker, it became easier to test and observe the behavior of the application. Prior to containerization, we relied on tools like Postman to emulate requests and responses between services. Using such tools, we had to meticulously mimic different scenarios while considering the specific characteristics and requirements of our solution. Emil asked whether we were able to test the simultaneous functioning of multiple services with our solution. This metric not only depends on the number of services but also considers the intensity of traffic generated by each service. Unfortunately, we weren't able to conduct extensive testing in this regard. However, we estimated that the bottleneck would likely be the concurrent read capabilities of Node.js and Redis. Both technologies can handle high read rates, and therefore the actual throughput would be determined by the smaller of these two capacities. Nevertheless, testing the theoretical limits is not realistic as requests may fail and require retries, significantly reducing the effective throughput. While we couldn't conduct tests due to time constraints, it's important to note that our solution prioritizes ease of deployment over high scalability and availability. As a result, when teams reach a certain size, it's likely more appropriate for them to migrate to a service mesh solution, as described by NGINX and other vendors. Ron inquired about the trade-offs we considered when deciding which features to include in Apex and how we made those decisions. We anticipated that we couldn't predict every user's specific requirements, and therefore we chose to build Apex using the Express.js framework, which allows for the addition and removal of features through the middleware pattern. With this approach, users can easily customize the functionality by adding or removing middleware as needed. Our core focus was on two key features: logging and customizable communication rules between services. We wanted to ensure that users could easily examine logs in the event of network faults and that they had the flexibility to define communication rules based on their specific requirements. By providing these core features, we aimed to cover the primary use case we had identified. Authentication and routing were additional requirements that we implemented to complement the core functionality. Although we initially planned to add service discovery, we ultimately decided against it as we believed it would be more suitable for larger teams employing a service mesh solution. Users who still wish to implement service discovery can achieve this by utilizing the middleware pattern and forwarding requests to a load balancer instead of specifying the direct network location of a service. In summary, while apex does not inherently support service discovery, users have the flexibility to add custom logic through the middleware pattern or route requests through a load balancer to achieve desired functionalities. In our project, we reached a discovery phase, but we decided to stop at routing. We felt that this was sufficient for our users, considering that high scalability and high availability were not our top priorities. So, in short, it is certainly possible to go further, but we ultimately decided it wasn't what our users needed at the moment.

Ron asked about the trade-offs between using Apex and other solutions, particularly in regard to adding extra features. Deciding which features to include was a challenging task. When we initially conceived Apex, we acknowledged that it would be impossible to anticipate all the use cases that users may have. To address this, we chose the Express.js framework and the middleware pattern. This pattern allows each piece of middleware to process the request and response sequentially. Essentially, users can easily add new functionality or remove existing features, such as logging. They can also add different routes to manage workflows with varying requirements. Our focus was on determining the core features that Apex should have.

Two of the essential features we wanted to develop were the ability for users to examine logs in one place when network faults occur, and the ability to customize communication rules between service pairs. With these two functionalities in place, we incorporated other requirements, such as authentication and routing, to cover the described use case. Unfortunately, due to time constraints, we were unable to implement caching in Apex Proxy. Currently, every request made to the proxy involves multiple queries to Redis for authentication, routing information, and configuration data retry logic. Caching these queries within Apex Proxy could significantly reduce the read rate, but it would create the challenge of updating the cache alongside any updates to the configuration data in Redis. Although we considered caching, it was not implemented in the final product.

Thank you to everyone who attended today. If you have any further questions, please don't hesitate to reach out to us directly. I am Derek Gross, and I worked on this project alongside Kelvin Wang. We would be honored to discuss the project further with you. Once again, thank you all for joining us. Have a great day!