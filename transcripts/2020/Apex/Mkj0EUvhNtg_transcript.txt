so welcome everybody uh we're apex and we're an api proxy for logging and controlling traffic between microservices today we'll be covering six topics first is what are microservices in the first place secondly challenges of working with microservices for the the some of the exist solutions that exist already when working with these challenges then we'll talk about how we designed apex and its architecture how we implemented apex and deployed it and finally some of the challenges we ran into as we did that so first of all what are microservices thing to realize is that microservices are an architectural choice it's it's um an option for how you um how you structure your system and the other choice is a monolith and it often is very useful to understand microservices by first talking about this other choice the monolith so this is what a monolith is we show here an example of an e-commerce app on the right so the logic for all the app is in the app server and the app server is represented by the rectangle with rounded corners there might also be a database so the cylinder at the bottom right that transfers data to and from the app server and finally the cloud on the top left represents a client that's talking to our app server so that client might be a user on a web browser or on a mobile app trying to use our ecommerce app so like in a lot of software the logic itself within the app server is broken up into subsystems so in ruby that might be modules or classes um and there's so there might be an orders class orders uh i've got all this.js here but it could be orders.rb for creating or updating orders there may be a customers class for creating or updating customers and maybe notifications modules for sending push notifications to users so the main takeaway here is that all of this logic runs in the same app in one app in the same app server and for our app to work properly these subsystems will need to talk to each other so suppose orders um needs to talk to customers so on the right you can see an arrow pointing from orders to customers uh maybe the scenario is an order just came in and with a customer id of 85 jfg and ej and now orders needs to find out what the name of that customer is so it might call a method that's been exposed by the customers module or a class module um so remember because all of these subsystems are running in the same app server um in fact they're they're actually running in the same process in memory on the same server and i show this on the diagram the grey blocks represent a block of memory in the app server and our entire app is running in that block and because of this because they're all running all those method calls are happening in process and it's in memory those are really very reliable and fast and basically as long as your code doesn't have any bugs in it you can pretty much guarantee that that that core will succeed um and one more variation of the monolithic architecture is for different systems to have their own database so this might happen for example if the data that's needed by each subsystem has very different formats so now that we understand what a monolith looks like let's explore two main ways in which microservices are different so the first is now um now subsystems are deployed independently so in our e-commerce app on the right you can see that each subsystem orders shipping on all the others are all deployed to a separate server it's as though they're all each each their own app they're no longer on the same server and often they also have their own database in this architecture in the microservices services architecture these subsystems are now known as services secondly services now communicate over the network so for example if orders now needs to create a shipment it would make an http post request to the shipping service and said um it might also send over the relevant information um about the order in the body when the shipment has been created it's probably the case that the shipping service will now notify orders by sending back a 201 http response so even though these two architectures are different so the monolith on the left and microservices on the right are very different the idea is that they are just two different options for delivering the same functionality back to the user so that was what a rundown of what microservices are and at this point we're probably thinking well what's the point why would we choose this architecture over the monolith and the reason why is because using the network to communicate between services gives us several benefits over the monolith the first is a wider choice of technologies so what i mean by that is each services owner is now free to choose any stack that's suited most suited to its logic so maybe it's the case that notifications needs to be fast and so its owner decided to use cpas plus or maybe inventory needs a lot of data analysis capabilities so that the owner of that service chose to use python so they can do that as long as they stick to http when talking to each other the second benefit is that now services can be redeployed independently so if for example the owner of orders decided to add a new feature and update orders to a new version all we need to do to make that change live is to redeploy orders we wouldn't also need to redeploy shipping and inventory and all the other services in fact they those other services don't even need to know as long as the api between these services remains the same um the other services don't really need to change anything about how they currently operate and those so this benefit can make it much faster for teams to ship new features to the overall system thirdly and this is quite a similar benefit to the second benefit is that each service can now be scaled independently so suppose in our e-commerce app orders is the first service to reach capacity so we can scale that service only by upgrading it to a more powerful server maybe more memory or faster processor without having to touch any of the other services or maybe it's the case that shipping instead reaches capacity first we can in that case we can redeploy we can deploy more replicas of shipping again without having to also scale any of the other services and yes again this is only possible as long as the different services maintain the same stable api between themselves so this makes scaling more efficient because now we could we only need to scale what's reaching capacity and all the other services that aren't yet at capacity do not need more resources to be scaled so as we can see there's a lot to like about microservices but using them also comes with a new set of challenges and those challenges are all uh mostly because of the network and in particular with two main fundamental limitations of of the network so the first limitation is that the network has latency um in the monolith if we recall uh when some systems like orders and shipping talk to each other they do so via method calls which are really fast usually within microseconds whereas when services talk to each other over network cops those are much slower typically tens or even hundreds of million seconds to complete so much much much slower second limitation of network is that it's unreliable meaning requests or responses can be dropped at any time so when orders cause shipping that request would fail even if there are no bugs within either ship orders or shipping sometimes these faults are systemic meaning maybe there's equipment failure that's that's causing outage for everybody or maybe sometimes they're just random and so because of these two limitations two main challenges arise when working with when choosing the microservices architecture over the monolith the first is diagnosing network thoughts so often in microservices architecture workflows could span multiple services so placing an order when the client on the left places an order um that might trigger a request a post request to orders which then might have to check stock levels with inventory so orders might then send a second request to inventory and then inventory might also have to create a shipment and so that third request from inventory to shipping represents that and so as you can see this cycle involves three requests and three responses and any one of those hops could fail and when that happens usually as developers our first instinct would be to check the logs but the issue is that now because each service is a standalone app in a way it generates and stores its logs in a different place and so when the developer tries to figure out what's happening they would have to check through multiple sets of logs they would have to first start at request number one check um check the post request in orders see if that request failed if not they'd have to then check request number two from orders to inventory and let's see if that one failed and so on and so forth until they finally find the work the request i failed in this case request number three between inventory and shipping so as you can see this can be really a slow and laborious process the second challenges that comes with using the network in microservices is managing fault handling logic so as we've discussed some network thoughts are simply random and so sometimes instead of diagnosing every single failed request you can just tell a service to try again a few more times and that's that sometimes can work so in this example it's possible that even though the request from orders to shipping has failed twice already and the third attempt it completes successfully and that's all good but defining this kind of logic this retry logic has to be done quite carefully and that's because um let's say if the shipping service is getting overloaded and starting to drop requests but all this retries too soon or for too many times that could overwhelm and crash the shipping service completely other times another consideration would be sometimes you want different retry rules for different services so for example maybe the case for one reason or another that we want orders when it costs shipping to wait three seconds between every attempt and to be tried no more than five times maybe other times we want a more default uh a default rule of waiting five seconds between requests but we try no more than two times um so all this logic has to be done um has to be implement defined very carefully but where so that logic needs to live somewhere in the system and often this logic lives in what's called http client libraries so these are just libraries that are imported into each services code like a rubygem or no package so for example it may be the case that when orders cause shipping that retry logic will live in a shipping client by rb gym that's required into orders this code it's it's uh common for these libraries to not just handle retries but also other networking concerns like rate limiting authentication and caching and because these libraries have to be imported into service code they have to be written in the same language as the service itself so in larger teams what usually happens is that each service has to write a new client for every language so if shipping knows that orders and inventory and notifications all need to call it then the owner of shipping would now might now want to write a client in ruby another one in python and then one more in c plus to handle retry logic and all the other network networking concerns for each of those services and this solution quickly becomes difficult to manage and as the number of services grows each service now has to write a new library for every new language that's added to the architecture and so the number of clients can grow very quickly also now updating global rules becomes much slower suppose the cto now wants to make one global update to traffic rules rate limit all services to 10 000 requests per minute developers will now have to update the same logic in many different libraries across many different services that can be a very slow process that's difficult to coordinate so in summary because microservices rely on the network so heavily they often come with these two challenges diagnosing network thoughts especially when workflows span multiple services and when the logs are distributed in different places and secondly managing fault handling logic that might be similar but not exactly the same across services but lives in multiple client libraries scattered across every service fortunately some solutions exist already to handle these challenges we'll be covering two main solutions the api gateway and the service mesh both of these solutions are built on what's called the proxy server and but slightly in a slightly different way so the api gateway is just one proxy server with many extra features whilst the service mesh uh consists of many different many proxy servers working together so we'll be going into those relationships uh more in this section so first of all let's understand what a proxy server is what it is is just a server that sits between two communicating machines so those machines could be a client or a server or in our case in this example two services orders and shipping so instead of sending http requests directly to each other the order service would now send a audit request to the proxy specifying that it wants to talk to let's say shipping in the host header the proxy uh seeing that request would then forward the request to shipping when shipping sends back a response the proxy simply forwards this response now with this understanding of the proxy let's move on to exploring how an api gateway works so here's a scenario where we have microservices but no such thing as an api gateway we're not using api yet so when the client talks to our ecommerce app it has to know the precise host port and path of every service in our internal system architecture so that tightly couples the client with all our services with our internal architecture and meaning that every time we update our services if we happen to change the endpoint as a result we also have to change update the client so that requires a lot of coordination between um clients the front end and also the developers of our of our internal services the gateway solves that problem so instead of what it what it is it's just a proxy server that intercepts all incoming requests into the system from clients so the benefit of that is that now we can have we can have the api gateway provide a stable api to clients um our services can change internally however we like maybe we can replicate a service or remove a service altogether or upgrade it um whatever we like but as long as the api the clients remain stable in the api gateway the client does not need to update this is useful especially if the internal system the internal architecture gets updated very quickly by teams often it's the case that the api gateway also provides other features other handles other networking concerns like authentication caching rate limiting so it's useful in more than one way it's not just not just for uh for the stable api um and so an analogy i like to use to understand the api gateway is it's like a receptionist it's a large company any visitor coming in doesn't necessarily need to know which employees are present or how employees work together the receptionist is the person who decides whom to notify what access to grant the visitor based on the visitor's identity and the visitors state of purpose so employees come and go they could leave the company all together but visitors don't need to be notified so let's remind ourselves about the challenges with microservices that we set out to solve with these solutions diagnosing network faults and managing thought handling logic so how could an api gateway provide a solution to these challenges well for instance for example for instance we could maybe deploy an api gateway internally between services so notice that this architecture is different to the one i presented a few slides back the api gateway here doesn't sit between clients and services it sits between every pair of service services internally so this isn't exactly the purpose um that the api gateway was designed for but perhaps this could be a solution for our challenges um theoretically yes but existing api gateway solutions aren't quite important for this uh and that's because it has many other features that are specific to handling client server traffic so i've just listed a few here uh examples here so maybe it would have um custom apis for different clients maybe a different api for mobile clients or maybe it would have a feature where it automatically generates stocks for clients or maybe it has monetization features that allow uh api owners to charge for api usage um all of these features are great for client server traffic and they come built in and so that means that when we try to use api existing api gateway solutions for service to service traffic those features add a necessary complexity for us so it's not quite the right fit for us however it is still a useful pattern it is a source of inspiration for our architecture and we'll explain how this is so in a later section the second solution we mentioned is the surface mesh again this uses a proxy server as a building block in that it uses service mesh consists of many different many proxy servers working together the service mesh has two main components what's known as sidecar proxies and a configuration server and we'll be explaining what each of those are so sidecar proxies um this is somewhat complex concept so we approach it uh from within the analogy so let's imagine we have a large team in the company that has to work together that whose members have to work together and they have to communicate with everybody so we could have something like this everybody can talk to everybody directly those arrows look complicated but they're simply just lines of communication between pairs of people so as the team grows some of these scenarios might occur so maybe some team members might be off sick so anybody talking to them would have to retry the next day or maybe it's the case that when the team member needs a specific piece of information he or she doesn't know who has it so they have to try a few different people before they finally obtain that information and maybe it's the case that multiple people want the same piece of information from the same person so that person finds themselves having to provide the same response over and over again so maybe some kind of caching will be useful there and so managing these communication related tasks as this team size grows um can really take a increasing proportion amount of time for each team member and that takes away time and focus from the uh their core responsibilities so one solution to that would be having the team members always talking through their own personal assistant their own pa so pas could intercept all incoming and outgoing messages and manage communication on behalf of their of their team member so you can see now the all the arrows point between pas rather than directly between team members and each all each team member has to do is just talk to them talk through their own pa and so this frees up more time for team members to focus on their core responsibilities so service meshes um this same topology is what a service mesh looks like so instead of team members now we have services so you can see around the perimeter we've got orders shipping inventory the same services instead of talking directly to each other they all talk through side car proxies so these are light pas they're simply proxies which we've already seen what those are that forward requests and responses um but uh each now each service has its own sidecar proxy and all communication flows between them and sidecar proxies can manage things like retries and routing and caching so recall we already discussed that retry logic has to live somewhere and has to be carefully defined and these cycle proxies are what enable this and so the benefit of this architecture in a very similar way to the team members and pas analogy is that the service code can now focus on its core business logic um and all the networking concerns all the retry logic and all the other logic can be outsourced to sidecar the proxies component of the service mesh is configuration server again we approach this from with an analogy um and so in order to do their jobs the pas will need to know who's on leave who's who's in and who's not um who has what information uh who has access to what information maybe financial information is confidential and so not everybody not every team member has access to that and so all this personal personnel info and company policies um need to live somewhere my peers need to know about them so one solution to that is to have one centralized folder that contains all this information um and each pa would then could then also get their own copy of this folder and so whenever and it's possible that um this information changes very very common so people employees could leave the company people could be off sick one day and so whenever this information changes we need a way to notify the pas so if we've already already given each pa a copy of this folder then that update has to be automatically pushed to the pas copies so that is essentially what a configuration server is that centralized folder so back in the service mesh world that that configuration folder becomes a configuration server it contains all the information on routes on retry logic on rate limits and or any updates and and each sidecar proxy gets a copy of that information and if any of that information changes in the configuration server all those updates are automatically pushed to each cycle proxy's test copy and in that way every um every sidecar proxy has always has the most up-to-date configuration data so the combination of those two components cycle proxy using configuration server those are what make up the service mesh so you can see i've highlighted the configuration server and the sidecar proxy within this entire architecture again let's think back to the challenges we set out to solve with easy solutions diagnosing network faults and managing for handling logic the service mesh provides a strong solution to these challenges we've got the configuration server at the top that provides us with one place to define and update logic for fault handling and the sidecar proxies can execute all that logic and also can be responsible for logging all the requests and responses to one place somewhere else what's more because the the service mesh doesn't have any single point of failure or traffic bottleneck it's scalable and resilient defaults so if any one sidecar proxy happens to fail the rest of the only its parent service goes down all the other services and all the other sidecar proxies can continue to operate however this solution also comes with trade-offs it's extremely complex because now there are double the number of components in your architecture that you have to deploy and operate the other the other way in which is complex is that cycle proxies often are containerized and so that changes how each service is deployed so if any service isn't currently containerized yet then we'd have to redeploy that service alongside its cycle proxy in containers so that's not always possible as we'll see so in summary these two solutions embody different sets of trade-offs the api gateway is simpler to deploy and simpler to operate but it's not quite optimized for service to service traffic on the other hand the service mesh provides a skate more scalable and resilient solution more highly available and more highly scalable but then it's more complex to deploy and operate and it also requires changes to potentially requires changes to how services are currently deployed so with that i hand over to derek to talk about apex's design and architecture so after looking at the trade-offs um the pros and cons of working with an api gateway or a service mesh to solve these problems of where to the host network fault logic and and how to actually find a network fault we began to think can we do better but it's important to to ask better for who so as we designed apex we wanted to keep in mind a specific use case a specific user or set of users who would take advantage of apex and thus reap the specific benefits that were best for them uh with acceptable trade-offs so we designed apex for a small team that's migrating to from a monolith to microservices architecture and those microservices might already be deployed perhaps using a platform as a service environment such as heroku the team you know having a work with with monoliths in the past and just getting started on microservices they might have a lot of experience with uh uh deploying to heroku and want to to to stay in those types of environments so we decided that apex's architecture might look something like this where any pair of services communicating with one another would talk through a proxy of relatively simple proxy and that proxy could rely on four additional components to uh achieve a full set of functionality that solves just the set of problems for us zooming out the apex architecture would look something like this requests would come into our system and anytime two services need to communicate with one another they communicate through apex so apex becomes a central point a critical point in in the architecture but every uh communication between services now goes through the central point so we do have one place now where we can um update uh network logic and zooming in a little bit to that proxy we can see that it can be made up of several different files that have different responsibilities so if we need authentication if we need to specify retry logic we want to talk about we want to define how to log our solutions we can have that taken care of in different places within our proxy all requests are logged to one place our apex logs database so the logging section of our of our middleware communicates consistently for every request and response to uh to keep an up-to-date record in exactly one place so that if there is a network fault or or any reason to to look at the history of requests you can see that very easily in one place requests and responses uh there may be several requests and responses that belong to a workflow where again where multiple services perhaps the customer service is communicating with the order service which needs to send a request to the shipping service etc all of those services need to be grouped as a part of one workflow within our lodge database so each of those requests and responses can be given a single correlation id that helps us to understand that those logs belong to one workflow these workflows can get very complicated but as each request and response passes through apex apex can make sure that that correlation id exists for each and every request and response in a workflow we also want to handle the problem of having fault handling logic that may uh have lived in multiple places we want to have one place for for all that fault handling logic so apex will also need a configuration store and that store can hold default data for for how services should communicate with one another but we can also specify different communica communication protocols for pairs of services on any given request and response cycle we can see that the apex proxy may need to communicate with that configuration store multiple times so we need to keep that in mind as we build our request our our solution so apex a single proxy solution uh compares to a service mesh uh very generally in this way where apex is fairly simple to deploy and operate uh since it is one core component that works together with a handful of additional components as opposed to uh to at least one additional component for every service in your architecture as would be the case for the service mesh however there is a trade-off there of course that single point of failure that single proxy in the middle between all uh requests limits our scalability and can present availability problems if it were to go down so we do recognize that apex is not necessarily the perfect solution for every situation but for a small team again that's migrating to microservices and using platform as a service you might not have the ability to containerize their uh their different services apex might make a wonderful set of trade-offs the apex architecture is described by several vendors as a stepping stone to a full service mesh so for a small team just starting off you might decide to just start off with just a single proxy and then eventually uh if it becomes necessary move on to a solution where you have multiple proxies it can be a little bit easier if you move through this stepping stone type of architecture so what did we actually build and how did we deploy apex again there are five main components to apex we have that main proxy server a logs database to store all the log information a data store for our configuration and we also added in an add-in user interface that communicates with the configuration store and the logs database through an api for the apex proxy it was important that we had no unnecessary features we wanted this to be extremely streamlined so we built it in the uh in node.js which gave us the ability to handle a huge number of simultaneous requests and it has high throughput and we took advantage of the express.js framework mainly because it's minimalist but we also appreciated the fact that it's fast and it doesn't add a lot of extra weight to our product for the lodge database high throughput was very important we did not want the logging of of each request and response to become a bottleneck so we chose a solution that has a high ingestion rate timescale db can insert over a hundred thousand records per second even if the database already has a billion or more records in it for our data store again we might be communicating with this data source several times for each request and response so we wanted to make sure that we could read from it very quickly redis would serve this purpose very well because according to redis's benchmark information it can process 72 000 requests per second for our admin api we once again turn to the node.js and expressjs environment specifically because node has a rich ecosystem of third-party client libraries and allowed us to focus on the work we wanted to get done instead of building libraries and we built the admin user face using react js for the convenience it provides the reusable components allow us to develop very quickly we wanted the deployment story for the user to be as simple as possible so we chose to deploy apex using docker and docker compose so deploying apex locally is as simple as cloning our repository changing into the root directory and running the docker compose up command in implementing apex we did come across some interesting challenges so we want to log every request but what happens if we're live logging extremely large request bodies we found as we were testing the apex solution that there were times when we were receiving when the apex proxy was receiving a large response body and taking extra time to to send it over to the lodge database so one solution we chose was to keep the body compressed if a body came to the to uh to the apex proxy in a compressed format we would just send it over to the logs database uh in that compressed form recognizing and accepting the trade-off that if a user needs to go and look at the logs uh the user would might have to uh decompress that uh login from the body to make it human readable we also decided to cue the logs and send them asynchronously at the end of the request response cycle and that allows us to open up the opportunity to optimize the way we use our database connection pool instead of connecting to um the say the lodged database at the very beginning of the request and response cycle we could connect just at the end for a much shorter window and make better use of our connection pool that would help us to process more requests at the same time through the same x proxy we also came across a challenge of how to make sure that log and configuration data is backed up in containers containers are meant to be ephemeral such that if there is a need to spin down a container take it down and then start it up again you can do that but users expect their configuration data and their log data to be persistent over time they do not want to lose that if that container goes down so we eventually decided upon using uh container volumes containers have the uh the option even though they represent uh an isolated environment they do have the opportunity to use volumes to save data to persist data to the uh the underlying host system and we could take advantage of that to make sure that that our revit store and our time scale db lives could be persisted over time in the future however we do plan to look into backing up with a cloud storage solution that is specifically designed to be durable over time take a moment to just show you um what it's like to to use the apex solution again one of our our first order of business is to make sure that we can proxy a request that if we get a request from one service we can pass it on to another service and send the proper response back so apex makes use of a couple of uh custom headers the apex the x apex authorization header allows our system to identify a specific service that is attempting to use it and authenticate that service to make sure that it has access and the second header the apex responding service name tells us which service the the original service is trying to reach so you can see here that apex is hosted at localhost on port 1989 and the path here is the path that we would like to to use to reach our destination which in this case is cnn in the end you can see that indeed although the request was sent to localhost we were able to to get back the data from cnn that we were looking for logging is also an important concern again we want to make sure that every request and response is logged in one place so that when a network fault occurs users can quickly and more easily take a look at the lives to find out what happened inside any request the server logs for the proxy will show us that for a 200 response the incoming request from the original service and any outgoing requests and responses are all logged to our database tracing is a key component of the logging because again when you want to to examine the lodge you're likely interested in one particular workflow at a time which we want to be able to identify with a correlation id so this gif is showing us that the response to the originating service comes back with that correlation id and you can take advantage of our admin user interface to plug in that specific correlation id and pull out the specific logs that you might be interested in looking at also we wanted to keep all of our network fault um logic in one single place again our configuration store so you can see here we're storing default configuration and orders and shipping how when orders communicates with shipping we have a specific set of commute of uh terms for communication the same for shipping and inventory this configuration store is also a great place to to uh to save our credentials to validate our services to authenticate rather and a good place to to host the routing information so that our cnn service or our order service can change its location and we don't have to coordinate that with any other services which if uh if where uh a particular service is hosted happens to change over time you can make the change in the in that single configuration store and every service can still communicate that is also updating that logic is also handled through our admin user interface you can see here that we can specify for a requesting service of orders and a responding service of cnn we can add in some custom configuration thank you so much real we'll open it up to some questions now okay so julius uh asked the question whether uh there were other databases we looked up beside timescale would you briefly consider influx db we i think our main consideration was that um was in deciding that logs were a type of time series data because they're basically a pen only once you've once you've logged once you've stored a piece of logging data you you really want to change that piece of login data and when you query that data um usually the the primary access is time so we decided that logs could be thought of in our case as a type of time series data and so we just went with one or the other so we didn't really um seriously consider like the detailed trade-offs between time scale and inbox db but we just went with a time a common choice one of the common choices for time series data in time sql db so julius asks what was the most difficult part of the project and i'd have to say that the hardest part of the project was probably the initial research of going through the process of seeing a problem and not just recognizing that it represents a a problem that um that developers might have to deal with but painstakingly going through the process of understanding all the trade-offs and the different types of solutions that exist and then you know bringing that that understanding to the process of of crafting a solution that's narrowed down to to a specific user knowing that we can't just build something that's going to work for everyone extremely challenging i probably agree with that um i think especially for me um understanding the the different types of existing solutions and how they work uh the api gateway and the service mesh there were a couple of other solutions as well and often they're clouded in enterprise software speak so it's uh it's hard work to figure out what the actual architecture and what components are included in in those solutions and that for me was probably the most difficult part the actual building uh once we understood the problem wasn't too difficult um so yeah very much agree with derek there yeah i'll put an asterisk on that though and say that we did want to make sure that that this process um that our our solution was very easy for users to deploy and getting it to work properly uh in a containerized environment was was new and challenging as well so mike asked are you able to give us a taste of how you test it throughout development um that's been very difficult for us uh to be quite honest um it's it's not like because what we were building wasn't just say one react app or one rails app where we could just run the test tweets um a lot of it depends on how how it's actually used in production the kind of traffic we're anticipating and the kind of types of faults we're anticipating um those were very hard to simulate that said we did one one way we did what we were able to test the retry logic was um adding using a very small timeout value so that was quite convenient because the timeout in our reddit store in the configuration defines um what we class as a failed request so what do we mean by a fail request right so is it um do we have to wait 30 seconds or maybe does it have to be a 404 that comes back um so we defined that and so the timeout was what defined that it defined how long each requesting service is willing to wait before this or how apex was willing to wait before just calling a request failed so we were able to test the retry logic by simply setting the timeout to a super small value maybe one millisecond with 10 milliseconds um but we we did struggle to go a lot further in in running tests testing was extremely challenging uh before we were able to dockerize our solution so once we we were able to to get everything working all together in docker it was it was relatively easy to to run the command to spin up the the application and touch anything and and see what was happening with it but before that it it really was a lot of using a tool such as postman and trying to think about the specifics of of what makes sense for our request trying to emulate a request coming from one service going to another what that might look like emil sorry by abusing your name uh so you you asked were you guys able to test how many services can simultaneously work for this solution so i think the the the metric that uh really determines um i think like just um the number of services isn't the only metric that matters because it also depends very much on how heavily each service is making requests and responses um so we weren't able to do too many tests but i think what are what we uh estimated was that um would be able to take probably the the bottleneck um so either you know in our in a single request response cycle um every request would have to at least go through node and then also redis so both of those are designed to do to handle a lot of like really a very high read rate um so the way to figure out the answer to how many services can simultaneously work would be to just take the smaller of those numbers and maybe divide that by several uh by a factor of uh maybe by an order of magnitude or so just because the whole point is that some requests might fail might have to be retried and so um so the the the practical the actual um uh throughput would definitely be lower than uh much much lower than those theoretical limits um and so so that that's what we that's what that's the way we could theoretically work out the answer to that question but we weren't able to test test that in the time we had yeah doing thorough load testing would probably require dreaming up several different scenarios for for how different teams might use our system and it's hard to imagine that a scenario where there are perhaps just a few services you know sending requests and responses that may include say audio data like so very large um response buttons like it's hard to see how that might be relevant for um for a team that has very quickly added many many many services that are perhaps just sending a little bit of text data so there are a lot of variables to take in mind to keep in mind but these are the some of the reasons why we wanted to think about the the footprint of our proxy and say we don't want a large system with a lot of extra unnecessary features running um in our environment because we feel it's not going to be able to handle as many concurrent requests so those types of considerations certainly came into play and were at the front of our mind as we were developing the solution there's a final addition to that is um if you think back to our use case the use case that derek described at the beginning of of of section four of our design and architecture we we specifically didn't want to design for the case where a large team uses dozens and dozens of services i think we think by that point they should probably migrate to a service mesh which apex helps them with as nginx and other vendors described um and so so that that is one additional consideration there in terms of um using it trying to use apex for really a very large architecture is that probably apex shouldn't be isn't the right solution when when teams reach that size this so does apex support service discovery so out of the box right now no apex does not support service discovery again we considered our use case for a a small team that's just getting started now they may very rapidly grow to the point where uh a team may very rapidly grow to the point where they want to um implement service discovery so that can be done in two ways um the express uh the the node proxy um using uh expressjs applies what's known as the uh the middleware pattern so users can add their own custom logic to the to the the apex proxy fairly easily they can add and remove the features um that they want um second if you're running your um if you need essentially load balancing uh for for service discovery if you instead of adding uh the direct uh network location of a particular service such as the cnn or the order service if instead you give apex the um the network location of the load balancer apex can forward that request over to the load balancer and the load balancer can take it from there that's that is certainly possible um i think the one of the um beauty of having just having one proxy that mediates or serves the service traffic is as derek mentioned that middleware pattern that allows us to add more features so certainly we did consider adding service discovery but we stopped at routing we stopped it um we felt that was already enough for our users uh given that they um you know one of our trade-offs uh was not to prioritize high scalability and high availability um so so yeah so i guess the short answer is it's certainly possible um and but we in the end decided that that wasn't quite what our users needed yet so ron asks about the trade-offs um for apex versus um other solutions and the fact that that we were thinking about not adding quote-unquote extra features um so how did we decide which features um we would add that's always a tough challenge but when we were first conceiving of apex we said there's no way we're going to be able to think of everything that everybody is going to want um to use a product like this for so we chose um the express js framework and that middleware pattern to allow and uh just to specify the middleware pattern means that when a request comes into the system each piece of middleware in turn has a chance to to to take a look at and and process the request and the response so to to to do something with the information that's passing through one at a time so a user can add in new functionality very easily add test and deploy a user if a user does not want say the the logging functionality the user could could very easily remove that the user could add different routes through the system to um to manage different types of workflows that might have different requirements so it really came down to what were the core features we wanted to have in apex and the two things that we wanted to to do more than anything were make sure that when a network fault does occur and a message gets dropped somewhere in a complex workflow a user has the ability to go look in one place and examine those logs and and perhaps more quickly and more easily understand what happened the other thing we wanted to do was make sure that users were able to customize the the different types of rules they might want to have um for communication between any two pairs of services so having built those two pieces of functionality we rounded out the rest with some requirements such as authentication and routing to to build just the features that cover that use case that we described one thing we did think about um that we ran out of time for uh also is um because now currently we every request on every request node or node proxy apex proxy has to query the configuration stored multiple times first for authentication just to verify the identity of the requesting service secondly the second time for routing information so if someone gives us cnn as the header we then match that to www.cnn.com and finally the third time for like to fetch configuration data retry logic so you can see you can imagine that that if every request queries redis three times or up to three times then that could multiply the read rate very quickly um so we did consider caching what's on redis within apex proxy so that we don't have to clearly request redis every single request multiple times on every single request um but that of course creates the another problem where if the configuration data updates we have to find a way to also update the cache within apex proxy so we so that was something we did consider but eventually didn't get around to doing uh so i think that's uh all we have time for any other questions thank you so much for attending today feel free to reach out to us um directly i am derek gross and um work together on this project with kelvin wang we would be honored to uh to speak with you further about the project thank you so much for attending everybody great thank you very thank you everyone have a good day 