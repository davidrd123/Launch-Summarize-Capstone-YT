Welcome everyone! My name is Melissa, and I'll be leading this presentation today along with my team members Nancy and David. We are excited to share our project called Venice, which is a framework for generating real-time event processing pipelines. Venice is designed to help developers quickly deploy and manage their own event streaming process, particularly for smaller applications and developers with limited event streaming knowledge. It utilizes open source components to set up a pipeline with default settings and simple management tools within minutes. 

In this presentation, we will explore the implementation of stream processing pipelines, the motivation behind real-time event processing, possible implementations of real-time event processing, the architecture of our project, and a couple of the challenges we encountered along the way.

Let's start by understanding the workflow of event processing pipelines. These pipelines consist of interconnected components that include event producers, message brokers, stream processing engines, and event consumers. Event producers generate messages which are transmitted to the message broker. The stream processing engine then transforms the events, and the transformed event data is read by event consumers.

Next, let's discuss the motivation behind real-time event processing. In a single application with a primary database, sharing data can be complicated. Different tools are suited for different situations, such as continuously updating a cache of deals or maintaining a search index. However, maintaining real-time services without sacrificing availability or consistency is challenging. Propagating data across multiple services can lead to inconsistencies and manual recounting. One possible solution is using events, which are immutable objects that describe an application action at a specific time. Events can trigger actions in real-time without sacrificing availability or consistency while retaining the flexibility to use historical data.

Let's now delve into the possible implementations of real-time event processing. One common method is using message brokers, such as message queues or log-based message brokers. Message queues are suitable for asynchronous operations, where order and retention are not crucial. Log-based message brokers, on the other hand, are beneficial when order and retention are important. They can guarantee order and support simultaneous reading from different points. These characteristics make log-based message brokers more suitable for our real-time event processing needs.

Now, let's discuss the architecture of our project, Venice. Our framework utilizes log-based message brokers to propagate events to multiple services in real-time. Producers write events to logs, and consumers read from these logs. The logs provide low latency and high throughput, as well as the ability to split data into partitions and replicate it across machines. This ensures availability and consistency while allowing for the addition of new services and the use of historical data. By utilizing log-based architecture and event-driven principles, Venice enables real-time event propagation without sacrificing performance or scalability.

Throughout the development of Venice, we encountered a few challenges. One of them is the complexity of processing events in a linear fashion when using logs, as a single slow event can cause significant delays. Another challenge is the ability to efficiently query the event data in the desired format, as traditional querying methods can be slow and inefficient. To address these challenges, we introduced two approaches: the naive consumer model and the smart consumer model. The naive consumer model separates the initial writing of event data from the reading process, ensuring fast writes but still relying on queries that may be slow. The smart consumer model, on the other hand, employs stream processors to process events in real-time, enabling incremental updates and faster query responses.

To further understand the smart consumer model, let's explore how it can be implemented. Instead of writing individual consumers for each data sync, we can set up consumers that process the event stream and transform the data in real-time. These consumers act as standing queries and incrementally update their results as the streaming data flows. This approach allows for faster query responses since the data is already in the desired format. Additionally, we can use an event stream processor to abstract common operations and reduce redundancy within the system. This makes the system more scalable and easy to maintain, while still allowing for customization and extensibility.

While events are crucial for real-time changes, it's important to note that databases still play a significant role in storing state. Databases provide fast access to state, while events allow us to respond to changes in real-time. Both events and databases are necessary for a flexible and scalable architecture.

Let's summarize our key learnings so far. In a complex architecture, multiple services often require access to the same data. Events help address this challenge by representing what happened in the application and when. Message brokers facilitate the movement of events between producers and consumers, with log-based brokers being ideal for real-time event propagation. Stream processors reduce redundancy and improve scalability by abstracting common operations. Lastly, a combination of events and databases provides a flexible and scalable architecture.

Now, let's shift our focus to existing solutions for stream processing and where Venice fits into the picture. Managed solutions like cloud platforms or specific tools abstract away complexity but may come with limitations or high costs. On the other hand, the do-it-yourself (DIY) approach provides more control but involves complexity and the need to navigate multiple options for each component of the pipeline. Venice offers a middle ground â€“ it is open-source, provides simplified defaults, is fully customizable, and offers extensibility. It allows users to quickly deploy a streaming pipeline for smaller applications with minimal hassle.

In conclusion, Venice is a framework designed for developers who want to deploy and manage their own event streaming process. It offers a streamlined and customizable solution for real-time event processing pipelines. By utilizing log-based message brokers, stream processors, and a combination of events and databases, Venice enables developers to propagate data to multiple services in real-time without sacrificing availability or consistency. Thank you for your attention, and we are happy to answer any questions you may have. The main objective of this coding Capstone project is to develop an application that can quickly retrieve the location of a person from a database. In order to achieve this, we need to prioritize state changes. State information is derived from events, allowing the application to respond to changes in real-time. In an architecture that incorporates both events and state, databases play a crucial role in providing fast access to state information. 

To summarize the key points discussed so far, in a complex architecture, many services require access to the same data. However, propagating data to multiple services can be challenging. Events can help address this problem by representing what happened in the application and when. Message brokers, such as a log-based message broker, facilitate the movement of events between producers and consumers, ensuring fault tolerance and scalability. Additionally, a stream processor abstracts common operations to reduce redundancy in the system. An architecture that has access to both events and state is more flexible and scalable.

Now, I will pass the discussion to Nancy, who will proceed with an overview of existing solutions for stream processing and explain where Venice fits into the picture. Stream processing can enhance applications by making them more responsive, detecting anomalies, and delivering real-time analytics. These use cases are not limited to large companies; even small companies can benefit from them. The typical workflow of a stream processing pipeline involves event producers creating events and sending them to a message broker, which writes the events to logs. A stream processor processes these events, and the outputs are utilized by consumers such as visualization tools or recommendation services.

There are two categories of existing solutions for implementing stream processing pipelines: managed solutions and the do-it-yourself (DIY) approach. Managed solutions, provided by companies like Confluent Cloud Platform or Landu, abstract much of the complexity but may come with a high cost and the risk of vendor lock-in or limited user control. On the other hand, the DIY approach allows users to have full control over every aspect of their pipeline but can be complex due to multiple options for each component and rapidly evolving stream processing tools. Venice aims to offer a middle ground by providing an open-source solution that abstracts away some complexity while allowing customization and extensibility. It is suitable for developers who want to quickly deploy a streaming pipeline for small applications.

Moving on, let's delve into the specific components included in the Venice architecture. The pipeline's foundation is built upon Kafka, a highly popular open-source log-based message broker. It offers a robust ecosystem, good documentation, and active development. In Venice, there is a three-broker cluster with distributed partitions, ensuring higher throughput and fault tolerance through redundancy. This cluster is managed by Apache ZooKeeper, which handles tasks like leader elections.

For the stream processor component, we have chosen KSQLDB as it appears to be the most approachable option. Stream processing can be complex for developers new to the domain, so Venice aims to simplify the learning curve. While other stream processing engines like Spark, Flink, and Kafka Streams require Java or Scala coding, KSQLDB provides a SQL-like syntax, making it more accessible to developers familiar with SQL. Venice allows developers with different skill sets to leverage the stream processor effectively.

KSQLDB server and its associated command-line interface are the included stream processor components in the Venice pipeline. These components enable developers to write queries in a familiar SQL format and interact directly with Kafka Streams, allowing for greater complexity and customization in stream processing applications.

In addition to the stream processor, Venice incorporates Kafka Connect, which serves the purpose of pushing data into Kafka and pulling data out of it. Kafka Connect provides an API and connectors that enable integration with various data sources and sinks. For example, if you want to automatically write all changes from your database to a Kafka topic, perform stream processing on that data, write the processed outputs to another Kafka topic, and populate a different data store with these outputs, Kafka Connect simplifies the process. Developers also have the flexibility to add connectors for other databases or data sources through Kafka Connect's REST API.

Venice includes Postgres as the data sink, mainly because it is a widely used relational database system, making it familiar to many developers. With Kafka Connect, users can configure it to write topics to Postgres, ensuring that the transformed data is stored in a format that can be easily accessed. This allows applications to query the data in Postgres for further processing or analysis.

Now, let's discuss the Schema Registry component in Venice. The Schema Registry is included to encourage developers to make their applications more resilient and maintainable. Schema registries help track changes in schemas over time and perform validation with the stored schema. This ensures that consumers can confidently interpret the data they receive, while producers can evolve independently. With a schema registry, users can store schemas in one centralized location, eliminating the need for schema duplication and ensuring consistency across the pipeline.

To illustrate the interaction between producers, consumers, and the schema registry, producers register schemas with the registry and send the serialized event data to Kafka. Consumers read event data from Kafka and obtain the associated schemas from the registry for proper deserialization and interpretation. This centralization of schema management provides several benefits, such as easier schema tracking, comprehensive data usage, and the ability to perform schema validations to prevent incorrect data from being written to the log.

Venice encourages developers to use the Apache Avro data format, which offers speed and compactness with a rich schema language defined in JSON. By leveraging Avro, developers can avoid repetitive field names and achieve greater efficiency in handling high volumes of data.

To recap, the key components in the Venice pipeline include Kafka for event storage, KSQLDB for stream processing, Kafka Connect for data integration, Postgres as the data sink, and the Schema Registry for schema management.

Now, I would like to pass the discussion over to David, who will talk about the final components in the architecture and the challenges faced during the project.

David begins by providing a summary of the pipeline structure discussed earlier. The pipeline incorporates producers that create events, which are then sent into the pipeline. Data can be sourced from various origins, including code-controlled events or third-party devices such as databases or sensors. Two methods are utilized to get data into the pipeline: writing it directly to Kafka or using Kafka Connect to read data from external sources and write it to Kafka. The stream processor, KSQLDB, performs transformations on the data, which are then written back to Kafka as new topics. Consumers can either read the original events or access transformed data through Kafka Connect, allowing them to tap into state-focused applications. Postgres is utilized as a data sync, allowing data to be stored and easily queried by applications.

To streamline the pipeline installation process for developers, the Venice project employs Docker containers to package each component. This approach resolves dependency conflicts and simplifies the setup, enabling users to run the pipeline quickly and reliably on their machines. Moreover, developers can easily add additional components to the Docker network without worrying about complex configurations.

Another challenge addressed in this project is the complexity faced by developers in understanding and troubleshooting the stream processing system. To tackle this issue, the team developed a command-line interface (CLI) for Venice. The CLI comprises 14 commands, including management commands for installation, monitoring, and troubleshooting purposes. These commands provide valuable insights into the system, allowing developers to check message consumption, validate schemas, and monitor data flow through the pipeline.

David concludes by highlighting the significance of the Venice architecture in addressing the dual requirements of writing and reading data efficiently within a stream processing pipeline. The Docker container approach simplifies setup and facilitates component integration, while the CLI empowers developers with tools for system inspection and verification.

In summary, the Venice pipeline offers a comprehensive solution for stream processing, incorporating Kafka for event storage, KSQLDB for stream processing, Kafka Connect for integration purposes, Postgres as a data sync, and the Schema Registry for schema management. By packaging these components in Docker containers and providing a CLI for system management, Venice enables developers to efficiently set up, monitor, and troubleshoot their stream processing applications. The purpose of this Capstone project is to simplify the experience for developers. Firstly, we will discuss how developers can use Venice. There are several components that need to work together, and the challenge is how to package them so that they can run reliably on any developer's machine. One problem is that each component may have dependencies that conflict with each other or with what is already installed on the user's machine. For example, Kafka may require version 1 of a certain software, while KSQL requires version 2. To tackle this issue, we have put each component in a Docker container. Containers package the application code along with all its dependencies, isolating them from each other. This allows all the components to run on the same machine. With this setup, the installation process for Venice has been streamlined to just two commands: one to download the pipeline and another to install it. This ensures quick and reliable execution. Another benefit of using containers is that users can easily add components to the Docker network without worrying about configuring them to work together.

Now, let's discuss the complexity faced by developers who are new to event streaming application development. It is challenging to gain insights into the system and ensure that messages are being correctly read and encoded, schemas are registered properly, connectors are working, and data is flowing through the pipeline as expected. To address these difficulties, we have developed a command-line interface (CLI) with 14 commands, including management commands for installation, downloading, and restarting the system. The CLI also includes commands to check if connectors are working, print topics in real-time, and verify data formats. We have found that the CLI greatly simplifies the process of writing an application and provides developers with valuable feedback.

In addition to the CLI, we have also added a user interface (UI) component called Calfdrop to Venice. Calfdrop shares similar functionalities with the CLI but focuses specifically on Kafka. It offers a visual representation of data flowing through the pipeline, allowing users to check if data is partitioned correctly and if the format is as expected. The UI also provides real-time monitoring of consumer performance, giving users a way to assess the health of their pipeline. This UI component complements the CLI and enhances the experience for developers, especially those who are new to this domain.

To give a summary of Venice, it consists of the CLI, which manages the entire event stream processing pipeline, and the UI, which connects to the brokers and schema registry to provide a visual representation. Venice aims to simplify the deployment and development of event streaming applications.

Now, let's delve into two key challenges we encountered during this project. One challenge was managing dependencies between containers. For example, ZooKeeper manages the three brokers, and Docker Compose allowed us to configure all the containers in one file. However, this alone wasn't always sufficient. Sometimes, when we tried to add a connector at startup, the dependencies hadn't started listening yet, leading to communication failures. To address this, we wrote startup scripts that would ping the dependencies and wait until they were ready to receive messages. Only then would the scripts proceed with the necessary connections, schema additions, and topic setups. This solution effectively solved the problem of interdependent containers.

The second challenge we faced was related to data serialization and deserialization within Kafka. Kafka stores events as key-value pairs in raw bytes for efficient processing. However, applications typically work with higher-level data formats like strings, JSON, or Avro. If the serialization and deserialization processes are not handled correctly, errors can occur at multiple points in the pipeline. This can be difficult to troubleshoot, as the errors may not always provide clear indications of their cause. For instance, deserializing a format that doesn't match the serialized format will result in an error. We encountered a specific issue where the key was being transformed into a string, leading to data corruption. To resolve this, we optimized Venice for string processing and now expect string keys for connectors, while still using Avro schemas for the more complex event values.

Throughout this project, we have learned a great deal about stream processing and have successfully simplified the process for newcomers to this field. However, there are still areas that require further work, such as automating more tasks, improving scalability across networks, and enhancing support for external storage. Despite these challenges, Venice offers an easy-to-deploy event stream processing pipeline, making it more accessible for developers.

Now, it's time for questions. Thank you all for attending today's presentation. Feel free to ask your questions in the Zoom group chat. During our Capstone project, we realized that this type of project requires accountability and support, which we received from Chris and the team. While we worked long days, we were guided in the right direction and made aware of potential risks and areas to focus on. This mentorship and guidance were crucial in keeping us on track, especially when we encountered difficulties and got lost in the details. Conversations with Chris, David, and Melissa were instrumental in helping us navigate through challenges. The fact that the three of us were fully invested in the project also motivated us to push through the tough times.

Having a cohesive and well-functioning team was undoubtedly a major advantage in completing our project successfully. As for our Capstone experience in general, it was challenging, but we were able to overcome those challenges by leveraging our teamwork and guidance from Chris and the team.

One particularly challenging aspect was the configuration, especially when dealing with dependency issues. Nancy initially faced several dependency issues, which prompted us to incorporate Docker. This decision greatly streamlined our workflow and made it easier to manage the various components involved in the project.

There were multiple components that needed to work together seamlessly, and any changes made to one component had to be carefully coordinated with others. Ensuring that everything continued to function properly despite such changes proved to be a significant challenge.

Another challenge we encountered was selecting the most suitable stream processing framework. We explored various options, including one called Flink, but faced difficulties with its setup due to its dependence on Java and Maven. Since we were new to this space, we realized that it would be more prudent to choose a less complex option. Ultimately, we decided to go with KSQLDB, taking into account our own struggles and acknowledging that other newcomers to this field would likely prefer an easier option.

For me personally, the most challenging aspect was striking a balance between the big picture and the nitty-gritty details. We had to simultaneously consider the broader project goals and the specific implementation details. Juggling these different levels of abstraction and priorities was a constant challenge, especially when we were working long hours every single day.

Now, let me address some of the questions raised. Craig asked about the rating for Derek. We will provide you with the link to the rating.

Regarding the question about stream processing frameworks, Tyler didn't like the one mentioned. Let me clarify that there is a company called Confluent, which was involved in the development of Kafka. They have a suite of products and services centered around Kafka that we found invaluable in our project.

Combining John and Edmund's questions about testing and scalability, we haven't tested it to the point of breaking yet. The limitations would be machine-dependent, but we also haven't attempted to process a huge volume of events. Accessing real-time data for testing purposes was a challenge, and we mostly relied on our own data producers and recently incorporated Twitter data.

Dylan's question about the system being a better fit for smaller systems has a few reasons behind it. Smaller systems with limited budgets might find it more advantageous to use a managed solution. Additionally, one of the promises we made with Venice is the ability to go back in time and process the entire log from the beginning. This requires setting Kafka's retention to forever, which can lead to storage issues with larger applications generating substantial amounts of data.

As for the question of testing the limits, there is no definite limit on the number of consumers. It depends on the amount of data flowing through the system. If there's a low data rate, then many consumers can easily handle it. However, if there's a high data rate, adding too many consumers could cause issues.

Finally, I would like to thank everyone for attending and participating in this discussion. If you have any further questions or would like to discuss stream processing further, feel free to reach out to us on the Slack channel or visit our case study website (venice-framework.github.io). Thank you all again for your time and interest in our project!