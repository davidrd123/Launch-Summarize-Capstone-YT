all right it looks like we have a good crowd today so i'm gonna go ahead and get started my name is melissa the rest of my team nancy and david will be sharing this presentation with me today our project is called venice it's a framework for generating real-time event processing pipelines there's a lot of information in that tagline but we'll go through it step by step venice is a framework that enables developers to quickly deploy and manage their own event streaming process excuse me event stream processing pipelines venice is built for smaller applications and for develop developers with limited event streaming knowledge it uses open source components to set up a pipeline with reasonable default settings and simple management tools within minutes here's a quick overview of what we'll be delving into the implementation of stream processing pipelines differ but they have several interconnected components that achieve the following workflow event producers generate messages the events are transmitted from the producer to the message broker from there they're accessed by the stream processing engine for transformation and event consumers read the transformed event data now let's see where today's journey will take us we'll start with events and the motivation behind real-time event processing then we'll talk about possible implementations of real-time event processing and how venice fits into that space next we'll discuss the architecture of our project and finish with a couple of the challenges we encountered along the way here we can see that a single application with a primary database makes sharing data complicated there are numerous ways to use data from this application and different tools are better suited for different situations for example an ecommerce application might use orders data to continuously update a cache of deals of the day and remaining inventory to avoid overloading the database with redundant queries as well as a search index to enable full-text search of availa available products and a key value store of personalized product recommendations for customers these outputs are all derived views of the same underlying data typically it's difficult to maintain real-time services such as the stock inventory search index or product recommendation services without sacrificing the availability or the um or the consistency of the data when a product runs out of stock this change should be propagated instantly to all services that depend on the stock data we don't want to display out-of-stock products to our users dual rights are one way to update data across multiple services however partial failures such as a network outage can result in permanent inconsistency imagine writing orders to one database and updating remaining inventory in another if a new order comes in for product x and is written successfully to the orders database but the network crashes before the remaining inventory is updated the two data stores are now inconsistent and will remain that way without a manual recount one possible fixed distributed transactions can provide consistency but may sacrifice availability partial failures can render the application extremely slow or unresponsive so now our question is how might an organization propagate data to multiple services in real time without sacrificing availability or consistency while retaining the flexibility to use historical data we can use events an event is an immutable object that describes an application action at some point in time for example if a new order is placed that order can be modeled as an object like the one shown here an event can trigger one or more actions when a new order comes in the event can start the fulfillment process update the inventory and send an email notification to the customer in streaming terminology a producer generates an event related events are grouped together into a stream also known as a topic that event is then processed by one or many consumers in the example here the application is the producer when a customer creates a new order the application produces an event to the order stream the inventory management fulfillment and notification services are the consumers they receive the event and take appropriate action there are many ways to move data from producers to consumers one common way is to use message brokers message brokers are suited for situations where producers and consumers are asynchronous meaning they operate independently without waiting on one another and where multiple producers may write to a topic and multiple consumers may read from a topic there are two common types of message brokers message queues and log based message brokers message queues like rabbitmq or activemq are preferable in situations where each event may take a long time to process processing order is not important and event retention is not required log based message brokers like apache kafka or azure event hub are preferable in situations where each event can be processed quickly processing order is important and event retention is required let's see them each so to summarize message queues do not retain events and maybe may process events out of order here we see a new message enqueued and an old one removed to be read to contrast log base message brokers retain events and can help guarantee order consumers can read simultaneously from different points in the log without conflict you'll recall our motivating question logs can help achieve these goals logs provide low latency so each write is an append operation meaning producers can write data to the log in a single step and a read is a linear scan so consumers can read data from the log very quickly logs also offer high throughput logs can be split into partitions and replicated across machines this means many reads and writes can proceed concurrently for retaining availability with multiple brokers messages are rerouted to replicas if one crashes with partitioned data and load-balanced consumers if one consumer fails others can pick up the work for consistency events are ordered within partitions key partitions route events with the same key to the same partition ensuring order for example partitioning by product id allows all orders for product x to be processed in the order they are produced logs are also immutable even if the data is red it is preserved which means all services that require the same underlying data can access it this avoids inconsistencies due to race conditions associated with having multiple databases finally logs contain historical information and new information interesting um so new services can be added to the system at any time consume the historical data and follow future events automatically developing new services is also possible with the option to gradually transition or roll back changes now let's have a look at how these pieces are used in a more concrete way producers write events to logs and consumers read from the logs here we can see the producers are a web application sensor and database the events represent user activity sensor data and database rights these activities these events are written to logs and consumed by the cache search index and recommendation service logs don't necessarily fit every problem though events are consumed in a linear fashion so a single slow event could cause major delays the ordering of events is only guaranteed within a single partition this may be problematic when scaling logs also do not support reading self rights for instance if we wanted to immediately show our user a confirmation with data from their submitted form logs would make this tricky however they do still offer a compelling way to meet our specific goal for real-time event propagation to multiple services now we'll take a closer look at two options for working with data from a log we've already talked about how the log based message broker allows us to shift to an architecture that prioritizes events however to achieve real time we still need to be able to consistently write that event data quickly and read it quickly one approach to meet these additional goals is what we're calling the naive consumer model we set up consumers to read from our log these consumers might read specific topics or the entire log they have one job simply dump the data from the log into a data sync this could be a data series database or a data store like hdfs this approach lets us separate the initial right of the event data from where it will be read and like we mentioned in the previous section the throughput on a log tends to be fast so we have our fast rights but now we've hit another another tricky part event data is typically not set up in the exact format that we want to read it instead we need to query the data to retrieve the results we want queries are often queries are often complex they involve aggregation filtering transformation or joining data from many servers many different events these queries to earn new data syncs are still ad hoc they might end up being slow if we need to process all of the data for each one querying in this traditional way can be considered a pull model where we are constantly pulling the databases for changes we could add an index on the data store to speed up reads but then we would slow down writing new information to it counteracting the benefit of using the law we've separated reading from writing but still left our queries constrained if this naive consumer approach doesn't suit our needs what is in the what is another option the alternative we've comes we've called the smart consumer approach which is more of a push model rather than pulling from a static data from static data in a data store we can set up consumers that process the stream from the log these consumers do the aggregation filtering enrichment or transformation in real time before sending the data we can write various scripts that process the data as it is pushed in from the consumer these scripts can be thought of as standing queries that incrementally update their results live as the streaming data flows through then they send already processed data to the data store updating a single value in an existing result is exponentially faster than generating a full report every time the change happens in milliseconds instead of seconds or minutes this is because when the query takes place the data is already in the format we require this incremental updating allows the speed for queries that we're seeking now let's take a look at how we could set up consumers to do this there is a problem where we might want to write individual consumers and it becomes redundant the outputs needed by each data sync might be different but many of the operations to process the events are the same things like aggregations and enrichments that we've mentioned writing individual consumers works well for operations such as filtering or transforming that process one message at a time this approach becomes more challenging when operations are more complex for example aggregating over time or joining two different streams together our other option is using an event stream processor among other things event stream processors abstract these common operations to an engine this makes the system scalable easy to maintain with far less duplicated work adding a stream processor to a log based architecture expands the benefit of the system we can have many services produce events services can write data efficiently by appending it to the log the stream processor can complete operations shared by different services more efficiently so consumers can share the same underlying data and easily transform that data into a format that is ideal for their needs so far i really emphasize the benefits of an event-driven architecture however events don't replace databases completely databases prioritize state for example david lives in australia if he's in your system your app should be able to look up his location quickly a database is great for looking upstate events prioritize state changes state is derived from events if david moves to the u.s your application should be able to respond to that in an application both events and state are useful databases in an architecture provide fast access to state events allow us to respond to changes in real time so let's recap so far we've learned that in a complex architecture many services require access to the same data propagating data to multiple services is difficult with architecture centered on databases events can help address this problem events represent what happened in the application and when message brokers help move events between producers and consumers a log-based message broker can propagate data to services in a fault tolerant and scalable way a stream processor abstracts common operations to reduce redundancy in our system and an architecture that has access to both events and state is more flexible and scalable and nancy's going to take it from here okay thank you melissa let's talk about existing solutions for implementation of stream processing and where venice fits in that picture stream processing can be used to make an application more responsive detect anomalies and deliver real-time analytics these use cases are not unique to large companies small companies may also benefit from them the workflow typically looks something like this event producers create events and send them to a message broker which writes the events to logs a stream processor processes those events the process outputs are used by consumers such as a visualization tool or recommendation service there are two categories of existing solutions for implementing stream processing pipelines managed solutions such as confluence cloud platform or landu abstract away much of the complexity but the cost may be prohibitive to some and there may be the risk of vendor lock-in or limitations on user control the diy approach isn't up as an option for users who do not want to pay for a managed solution or who want control over every aspect of their pipeline however the trade-off is complexity there are many options for each component of the pipeline for example there are multiple options for the stream processing engine also configuration can be a hassle especially since the stream processing space is developing rapidly and tutorials become quickly outdated venice offers a middle ground it is designed for users who want to set up a streaming pipeline with minimal hassle and may be most suitable for developers who want to quickly deploy a streaming pipeline for a small application menace is open source abstracts away some of the complexity comes with reasonable defaults and is fully customizable and extensible you can go in and change any configuration that you want or add more components if you wish to summarize venice offers another alternative to deploying a stream processing pipeline that may be suitable for a particular subset of users venice may be a good fit if you want to start with the basic stream processing pipeline for a small application and most of the default settings will work for your use case venice may not be a good fit if your application has to handle a large volume of events or your use case is complex and would require substantial customization now let's talk about the components that we selected to include in venice here's the generalized pipeline again it looks simple but there are actually more components to it than this to meet venice's goal of helping developers start off with the sound foundation for stream processing pipeline an architecture that their applications can grow with start with kafka we chose kafka as our message broker because it is by far the most popular open source lock based message broker it has a robust ecosystem good documentation and active development venice has a three broker cluster with partitions distributed across the brokers to enable higher throughput and fault tolerance via redundancy this cluster is managed by an instance of apache zookeeper which handles tasks like leader elections our next component is a stream processor we chose kc db because they seem to be the most approachable option stream processing is already complex for developers new to the domain there's a lot to learn venice keeps things as simple as possible many stream processing engines such as spark flink and kafka streams require developers to write stream processing tasks in java or scala this presents a barrier to entry for developers unfamiliar with these languages k-sequel db is a wrapper around kaffa screams that provides a sql-like syntax this means that developers who require greater complexity in their stream processing applications can interact directly with kafka streams venice provides a decent foundation developers with different skill sets can use the pipeline in different ways to maximize their experience k sqldb server and its associated command line interface are the stream processor components we included in our pipeline here are two examples of how sql and k-sql can be used to filter orders from platinum customers the sql query on the left provides a snapshot of a moment in time the k-sql stream is a standing query that will continuously update as new orders arrive we will now introduce kafka connect the kafka connect api and its associated connectors are useful for pushing data into and pulling data out of kafka suppose you wanted to automatically write all changes from your database to a kafka topic do some stream processing on that data write the process outputs to another kafka topic and have the process outputs populate another data store you can certainly do that manually with a one-off script but the kafka connect api provides out-of-the-box features like configuration management offset storage parallelization error handling support for different data types and standard management rest apis confluent hub hosts hundreds of connectors for popular data sources and syncs including for postgres sql we chose postgresql as a data sync because many developers are already familiar with relational databases and can work easily with it you can add other connectors by sending requests to connect's rest api adding kaka connect and postgresql to the default pipeline helps venice support common consumer patterns for a streaming application to summarize kafka stores events in logs k sql db processes streams and kaka connect helps push data into and pull data out of kafka venice includes the schema registry to encourage developers to make their applications more resilient and maintainable schema registries help track changes in schemas over time and can perform validation with the schema stored in one place consumers can be confident in the meaning of the data they are consuming and producers don't have to worry about evolving in lockstep with consumers here's a closer look at the interaction between producers consumers and the schema registry producers register schemas with the schema registry and send their serialized event data to kafka consumers read event data from kafka and get the associated schemas from the registry so they know how to deserialize and interpret the data a schema registry provides three main benefits one schemas are stored in one place any service requiring the data and the topic can request a schema two schema changes are trapped tracking schema changes and when they occur allows users to interpret and use all the data and three schema validations can be performed data that does not conform to the schema can be rejected from the log this prevents typos or incorrect data types from being written to the law venice encourages developers to use the apache avro data format avro is fast and compact with a rich schema language defined in json with avro you will not have to repeat every field name with every single record this makes it more efficient than json at high volumes this slide shows an example of an event and its schema the components we just discussed kafka k sql db kafka connect and the schema registry provide the foundation of the venice pipeline they help make venice approachable extensible and robust hey um so i'm david i'm going to talk about the final components in the architecture and some of the challenges that we faced but i'm going to start with a quick summary just of that there's a lot of components there so i'll start with a quick summary of the entire pipeline so we have producers that create events that get sent into the pipeline this can either be code that you control like an event for an app or it can be from a third-party device like a database or a sensor there's two ways to get data into the pipeline the first is to write it to kafka so this will be from applications you control um and you just write over the network to the kafka brokers that are listening we've got three of these brokers and as nancy said this helps with redundancy and throughput and these brokers are managed by zookeeper which handles things like electing which broker should be the leader if the developers cares about consistency and correctness of their data then they should be using the schema registry which sits across the entire pipeline and as nancy just said helps manage schemas as they evolve over time there's another way if you don't control code where the data is coming from so like a third-party api or a database which is choose kafka connect and they have these workers which constantly read from the data source and will write to kafka and kafka connect can also use the schema registry we've got our string processor which is kc cool db this is where the transformations happen in case sql writes back to kafka so the results get written back as a new topic to kafka and they can register new schemas as well and you write queries in a cli that gets saved on the server and will run anytime there's new data in kafka so we've got transform data in kafka now how do we read that there's kind of two ways that match the discussion that melissa said about state and events it kind of matches up with those things so you can have a consumer that reads the events so responds to the events in real time but there's another way which is more focused on state which is where kafka connects serves another purpose which is um using kafka connect to sync data to store data somewhere else so it could be storing it in a database or it could be storing it in a cache and you basically configure kafka connect to do this and this is a really common pattern obviously because it's accessing state is really common so we've included postgres as nancy said um and with venice you can configure kafka connect to write topics to postgres and i think it's important to stress that it's not just writing events as rows into postgres it's updating the database so it's updating the the database with the results in real time from your transformations so if you jump back to where i live and we're storing that in postgres then if there's a change in event that's a change we can use this architecture to respond to the change with an application but we can also save the new information into postgres and any application that needs that you can use postgres to look it up and so what we've done is we've separated where you originally write data and where you read data from which means that we can answer that original question of how we do both things very quickly which is writing in an ideal format into kafka and reading in an ideal format which is transformed data so that's the basic pipeline that we've set up there's more that we've done to make the experience for a developer a bit simpler so the first is how the developer can use use venice so as you saw there's a lot of components that need to work together and the question is how do we package this up so that will run quickly and reliably for any developer on on their machines and one problem you'll face with this is that each component might have a list of dependencies that might conflict with each other or conflict with what is installed on the user's machines so the examples here is kafka might have some software that it needs which is version one and k sql actually needs version two and so how do you set that up so what we've done we solved this problem with uh each putting each component component in a docker container so all the components you saw on that previous screen are in the docker container and what a container is is it's a package of the application code and all its dependencies so the dependencies are isolated from each other and they can all run on the same machine and then what it means for venice is we can streamline the pipeline installation process down to basically two commands one to download the pipeline or our pipeline and one to install it and it'll run very quickly and the other benefit it provides is that a user can add components they can add components to this docker network that we have and they can do that very simply without having to think about how they configure things to work together so that's how a user uses it but there's also a big source of complexity for someone new to this area which is how you develop an application an event streaming application and what we found was it's really difficult to get insight into the system so for a new developer it's difficult to like discover whether messages are being read correctly and encoded correctly it's difficult to see whether the schemas are being registered and whether they are in the format you expect you have to add connectors and sometimes the connectors will be added but they will fail and it's not really clear that that's happening and obviously you want to see that your data is being written in the formats that you expect and is flowing through all the stages of the pipeline so we built a command line interface that has 14 commands and a lot of them are these management commands so they're installing they're downloading um they're restarting the system they are checking the logs to see what's happening but there's also these commands which solve some of those little tougher problems like did i get it right this time so checking if the connectors are working printing topics in real time to see that the data is coming through in the format that you expect i think what we found is we used it to test applications was it just streamlines its process of writing an application dramatically and the other thing we added the final component for venice is uh ui so we've got this open source component called calf drop and it shares some of the functionality that the cli does but it focuses really on kafka so where your data is stored and what it allows you to do is to have a visual representation of the data flowing through your pipeline so you can check that it's being petitioned correctly you can check that the format is correct you can see if your consumers are consuming slowly or they're falling behind you basically have a way of checking the health of your pipeline in real time we thought that was a nice addition to help someone who's starting out just to return to this final diagram so now we add a cli which is around the outside which manages the whole pipeline and we add the ui which connects to the brokers and the schema registry to give a visual representation and that's venice really that's the easy to deploy event stream processing pipeline that we've created so i'm going to talk about two challenges uh we had lots and lots of configuration challenges but i've picked two that i think are the most interesting so the first one is lots of these containers depend on each other so a good example would be zookeeper manages the three brokers and we used docker compose which is a it lets you configure all your containers in one configuration file and it does this thing where it allows you to say this container depends on these other containers so you have a way of determining the start order for each container but sometimes that isn't enough so we had we'd set up our containers so they would all start in what we thought was the right order so everything that depended on each other would start after that dependency but what we found was for some of the things that we were trying to do to streamline the developer experience for example adding a connector at startup so something that you'd saved you wanted to add immediately again every time you started the system the dependencies would be launched the containers would be launched but they weren't listening yet so the service was trying to communicate to something that wasn't there it wasn't listening yet and it would fail and then you'd have to either manually restart it or you'd have to change the settings so it would restart which caused some other some other issues so our solution to that was pretty straightforward we wrote some scripts that we included on the services that had this issue which would ping the dependency on startup and just wait wait until it was ready to receive messages and only then would it would it run the script that needed to run to do the connections or to add the schemas or to set up the topics so this is the most common issue i think that we run into and the source of my nightmares um this is from troubleshooting this a lot it seems like the most common issue that many developers starting out in this area go through and to understand it you need to understand a little bit about how kafka works so kafka stores events um as a key and a value pair which is written in raw bytes so that's why it can be really fast it's just raw bytes that are written on the log so it's very space efficient but obviously you're not going to work with raw bytes in your application you're not going to write a producer or a consumer that works in raw bytes you might use a string or json or if you follow our convention avro so which means you need to serialize it at the producer end and you need to deserialize it at the consumer end so convert it to raw bytes and convert from raw bytes back into a format you expect and if you mess this up you'll get this error and you can mess it up in a bunch of different ways at multiple places in the pipeline and it's not always clear why it is occurring so an obvious example would be you try and deserialize in a format that doesn't match what you serialized in so just user error and you've used the wrong serializer that's an obvious one but there's also some more subtle ones for example you can serialize to the schema registry using avro but it's not the avro serializer the schema registry expects there's this particular one that it expects and there's no real clear indication other than this error that this is what's happening so we managed to solve all these small problems we had data flowing into kafka and we could get it out of kafka that was all working well and then we had this error when we started to do string processing where our value was correct our value was being deserialized but we had this broken key a key would no longer work and after lots and lots of debugging it turned out deep in the documentation of kc cool db that until very recently message keys are always interpreted as strings so this week they've added some integer support for a couple of types of integers but previously it was always interpreted as a string so you would write in avro you'd have a key and a value in avro but if you did a transformation in what is supposed to be our stream processor it would change the key to a string and so when you start to read that data the string that it's created is this jumbled mess because avro gets converted to binary so this is jumbled mess so obviously that's not great um so us we have an interim solution to i think until kc will start supporting avro and i think we've thought of a better solution as write in the case study but our interim solution is to really optimize for the string processing task so we decided that the stream processor was the whole point of this of this pipeline that you wanting to do these transformations so if we're trying to streamline the experience for a developer then we probably should expect that we'll have transformation data coming through so that means now if you try and add a connector with venice you'll expect a string key and you'll expect an avro value and that avro value can still have a schema which is important because that's the more complex part of the the event but you now have to use a string key in your connections so that's that's venice i think we've we have learnt a lot about stream processing and i think we've made some ground in simplifying it for someone new to this area and there's lots of work i won't go into each of these but there's lots of work i think still to be done that we would have liked to get to mostly around making it simpler to to automate more things and to make it more scalable so making it work across more of the network and say um supporting better external storage and things like that and that is us nancy david and melissa time for questions thank you all for coming it's great to see that we have so many people attending stop sharing um so i think you can ask questions in the zoom group chat yeah there's a chat or a q a window that folks can use there's a q a awesome work that's not a question max thank you max max is also in our cohort any other questions about i know there was a lot that we covered in there and we went pretty quickly i apologize if i was speaking too quickly i knew that we had a lot to get through no questions oh here we go oh the capstone experience in general and how it helped us build such an amazing project uh i mean i can only speak for myself but i think the t like the sort of the instructor team chris and the rest of the folks are including sergeant um sort of helped guide us toward researching different subjects um and so we've read a ton um and and david actually found something about event stream processing and getting data in real time and we just sort of tunneled down this rabbit hole until we landed on oh this seems really complicated and we'd love to make it easier can we do that and sort of went from there but i'll let other the rest of the team speak to that as well um yeah a bunch of different ways for me i think i don't know about the other two but this isn't the type of project that you would ever do without the accountability and support that you get from chris and the team so like it's yeah we're working very very long days but we'll guided and we were doing all the research and all the work but we were guided in kind of the direction that we should be taking and the risks and what we needed to pay attention to so i think for me it was just having that i guess mentorship um and direction setting that otherwise because we got lost a bunch we got lost in the weeds and we got lost down rabbit holes but um what helped us was having conversations with chris and often yeah what david and melissa said and also the three of us were invested in this so even when it got hard i knew that none of us were going to quit so that helped yeah i would definitely say that having a team that worked well together was a benefit to getting this done so um i think that we may maybe didn't speak to our capstone experience in general but i think that answers how we got to this project it's the most the challenging part yeah oh uh i mean configuration and i know nancy started out with a lot of those dependency issues so that's when we incorporated docker um and that really helped to streamline things yeah definitely there's so many components as you saw just making them all work together and work together when you change something at either end so if you like change how you put yourself credit starter or change where you're trying to consume the data just making sure that that still works was really challenging yeah and i would say actually uh determining which stream processing framework to use so that k sql db um we explored a few different options there and i got stuck probably for about a week trying to get one called flink to work it's very popular but it does require knowledge of java and of another framework called maven so i had additional dependencies that i was trying to add that just didn't it didn't work um for me as someone new to the space so that's part of what informed our decision to go with k-sql tv um because we realized we're new and we're having a hard time with this someone else who's new will probably want to reap the benefits of our struggle and just go with the easier option for me the most challenging part was keeping the big picture in my head and also working on the details kind of simultaneously so like you have to think about at the very detail level to implement things and fix configuration issues and debug but at the same time you have to keep in mind like what your goals for the project are but your goals for your career is what your goals for this week this day is it's a lot of different levels of extraction to think at and it's kind of difficult to do that at the same time like every day for 12 hours yeah balancing those priorities is super challenging yeah it looks like craig has a question oh go ahead raid rating for derrick [Music] i will find that link oh yeah uh there's the stream the stream processing one right the tyler oh i didn't like that one oh you didn't okay yeah that's pretty technical so there's like there's a company derek called confluent which um spun off kafka like that i think they helped at linkedin build kafka then they started a company that builds products and services off kafka and they have a bunch of great free resources that we lend on a lot yeah they are they are the company that that actually works on kafka like that is their product and then they that whole suite of things that support it also come from them um i think we can combine john and edmunds into one question about like testing and scalability um i yeah it doesn't we haven't broken it i don't think we tested it until it broke um there will be limits on how much space which is machine dependent and we definitely haven't tried to pump a huge volume of events through it so that's combining with max's question about like how do we test that's is one of the um challenges we faced was getting access to real-time data so we have tested it in a bunch of different ways with our own producers that's simulate data um and recently i kind of tested it with twitter data um so connecting up twitter data to pipe through the pipeline um but i don't think we've really tested the limits of how far it can scale you guys haven't did that uh no i think that answers that i was looking at dylan's question next um and it looks like dylan's asking is there a reason that it's a better fit for smaller systems so one of the reasons is that people who are running larger systems probably have a bigger budget and can go with a managed solution if that better fits their needs also because and david didn't touch on this in our challenges that we hit was that one of the promises that we make with venice is that you should be able to basically go back in time and consume the entire log from the beginning in order to do that we have the retention in kafka set to forever um which will you're going to run up into storage issues pretty quickly with a any kind of sizable application that's generating a ton of data all the time um so for that reason smaller apps are a better fit for the current configuration of of venice obviously you could even with a small application you can generate a lot of data but you can also always add external data and change those configurations on kafka to store it in in a remote storage and that would sort of offset that problem yeah as far as i understand there's no limit to the number of consumers it would be very context dependent on how much data is flowing through so if there's like drip fading data then the consumers can rate it very quickly then you could have many many consumers but if there's a lot of data that has to be read then i think you would probably run into some issues if you added too many consumers i think that answers those questions is there anyone else well thank you all for coming if you do think of any other questions that you have feel free to you know reach out to us we're on we're all on the slack channel um or you can find us through our case study website um which is venice dash framework dot github io so yeah if you have other questions or just want to chat about stream processing it is very technical but i appreciate you all sitting with us through this yeah thank you everyone john just says thanks john thanks thank you oh one more oh max says thank you thank you max so generally back to you 