all right we'll uh get started so today we're going to talk about bento and our experience designing a blazing fast serverless video pipeline we're going to begin by talking about what a video file is and define video transcoding and serverless architecture the two key pieces of bento we'll discuss why serverless architecture is a good fit for a video transcoding pipeline we'll then be ready to dive into bento's architecture along with some of the challenges we encountered during development finally we'll benchmark bento's performance against leading competitors and wrap up with some caveats and opportunities for future work to start off we wanted to explain why video is a compelling field of software engineering video is ubiquitous yet it's often taken for granted we rarely stop to think about how unique and interesting it is for one video is the most abundant type of content on the web this year video is projected to make up around 79 percent of all internet traffic with new video on demand services and video based apps launching seemingly every day video traffic is predicted to grow to over 82 percent of total internet traffic within the next few years video is also distinct from most other web traffic because video files tend to be large usually an order of magnitude larger than images as a result you tend to encounter challenges with storage memory and bandwidth that other file types don't present in addition the size of video files is only increasing due to higher resolution and higher frame rate videos so these challenges will only grow finally video is complex delivering video files over the internet is more complicated than perhaps any other file type the source of this complexity comes from two major challenges what we call the compatibility problem and the bandwidth problem to understand these problems it's helpful to first visualize what a video file is when a video file is first recorded by a phone or camera the raw video data is too large to store locally much less send to most people an hour of 1080p 60 frames per second uncompressed video is around 1.3 terabytes so that raw data is compressed to a much smaller size the software that compresses this data is called a codec it applies an algorithm to compress the data so that it can be easily stored and sent once compressed the data is packaged into a file format called a container containers have extensions you may have seen like dot mp4 or dot mov when you play a video the process is reversed a media player opens the container and the same codec is used to decompress the video data and display it on the device to review a video file consists of two parts compressed video data packaged in a container the first problem that businesses face is that there are dozens of codecs containers and video players each with their own strengths and weaknesses and unfortunately they aren't all compatible with each other using the wrong codec or container could mean that some users can't play your videos this is the compatibility problem so businesses have to decide what codec and container their videos will use usually they make this decision based off the characteristics of the codec and the types of devices they expect their users to have once they've made this decision they need to convert any video files they have into the codec and container they've decided on however now they need to answer a second question how much should they compress their videos generally the more you compress your video data to reduce the file size the greater the decrease in visual fidelity however not all users will have the bandwidth to quickly download larger higher quality video files consider the differences in download speed between a user on fiber internet in their office and the user on a 3g connection going through a subway tunnel attempting to download the same video the person in their office will have a smooth experience whereas the person in the tunnel may have a choppy experience if their video plays at all this is the bandwidth problem for this reason businesses usually create multiple versions of the same video at different rates of compression and thus different file sizes modern media players will detect your bandwidth and deliver the video file most appropriate for your speed to recap businesses that deliver video will use a codec to convert their videos into a single container format at multiple file sizes this process of conversion is called video transcoding the process of ingesting a video file and transcoding to a specific format and file sizes is called a transcoding pipeline every business that delivers video files on the internet will have to consider how they transcode their videos unfortunately there's one last challenge they need to consider transcoding large video files takes a long time we're talking multiple hours to transcode a single 60 minute high definition video in addition transcoding is demanding on memory and cpu transcoding video files will happily eat all the cpu that you throw at it and ask for more if you're an individual who is only transcoding one or two videos a month this may be acceptable to perform on your local machine however for individuals and businesses with regular video demand this can quickly become a pain so how do businesses transcode their videos generally they go with one of two options they build a transcoding pipeline and deploy it in-house or they use a third-party transcoding service so let's talk about each of these options some businesses choose to build a video transcoding farm in-house in this case they need to write custom software and deploy it to their own bare metal servers or cloud service like ec2 once deployed this option is cheaper than a third-party service and it provides businesses with maximum control over how they transcode their videos for example what formats and codecs they support as well as any additional transformations they want to make to their videos however this option requires technical expertise to both set up and scale video transcoding is both slow and error prone even with optimizations and as we still as we'll see later provisioning servers for video transcoding can result in periods where that compute power goes unused now that being said this is a good solution for video platforms like netflix and youtube companies that ingest and stream millions of hours of video each week and can build their own engineering operations around this to minimize the trade-offs the second solution is to use a video transcoding service companies like amazon and zencoder provide powerful cloud-based transcoding these services provide comprehensive solutions for businesses that need to support multiple formats codecs and devices because these services specialize in video they'll be tuned for speed and able to handle the error cases that typically accompany transcoding jobs however the amount of transcoding options many of these services provide may be overkill for smaller businesses that have fewer input and output requirements and as you might expect these services are going to be the most expensive option well that being said the solution is great for media production studios that produce massive amounts of video content that lands on lots of devices and don't want to build their own video engineering companies so let's briefly review the current options along a few dimensions if you go with private servers you'll have the highest level of control over your inputs and outputs your bottleneck is technical expertise to really get the best performance and cost from this model you need a dedicated video engineering team by contrast transcoding services outsource your video engineering team you therefore get the benefits of speed and control but at a higher cost these options are sufficient for some businesses but there seems to be an opportunity here is it possible to build a fast low-cost transcoding pipeline built for businesses that don't have video expertise and won't need a ton of video options we felt that the answer might lie in serverless technology which nathan will tell us more about all right thanks max so bento is serverless as we've been saying uh but i want to talk a little bit about what that actually means as it turns out serverless is kind of an overloaded term these days for instance serverless computing can refer to outsourcing application logic to a third party service and in these cases everything from how the service is implemented to hosting is all going to be taken care of by that third party more often than not however when we talk about serverless we're really talking about function as a service so function as a service or fast refers to application code that is deployed on and executed on third party servers so when you're using fast the developer is still responsible for business logic but that's it in the context of video transcoding solutions the more robust custom transcoding pipelines may be entirely in-house or might be deployed to an infrastructure as a service provider and both will entail a lot of responsibility for the developer where something like a fully featured video transcoding service will of course be software as a service and won't require a development team at all bento as you can see using function as a service falls in the middle and to the left on that scale some key features of fast function as a service are that your code is run in response to some event and the code is executed in stateless containers that are ephemeral or short-lived so they spin up when needed and then they spin down once they've finished executing your code and these containers will scale horizontally automatically in response to higher load that is the number of containers that are executing our code will increase and decrease as needed so some of the benefits that we get when using function as a service the first is really core to serverless computing as a whole and that's delegating the infrastructure management to some third party so using fast developers can write and quickly deploy their application code and then the fast provider worries about provisioning servers the execution environment and so on and also you only pay for what you use so the time that it takes for your function to spin up and execute its code that captures all of the compute time that you'll actually pay for and with most fast providers those times are captured at a really granular level of 100 millisecond increments but the most important benefit for us are the instant scaling of function instances and the concurrent execution of those functions so this amounts to multiple sometimes hundreds of instances of our function executing our code in parallel all at once so these two features are vital to bento in fact given these strengths it turns out that fast is a great fit for video transcoding and there's two primary reasons for that number one is that video transcoding workflows tend to be what we call bursty in nature and that the ability to execute our code concurrently allows us to transcode a video in parallel and that's really the bread and butter of bento and we'll talk more about that in a second but let's talk about the first point here transcoding a video as max said earlier creates a very high demand on cpu and memory and of course larger files will create larger demand and traditionally developers have to provision computing resources high enough to meet a predicted maximum load and you have to strike a delicate balance the available resources should be high enough so that peak capacity is never reached but not so high that you have large excesses of computing capacity that are going just unused given the extreme variation in the size of video files effectively provisioning the right level of computing power for a server that's dedicated to video transcoding is somewhat difficult and this is exacerbated for businesses that have intermittent transcoding needs there may be periods where servers completely idle and then periods where it's struggling at peak capacity and the instant scale of function as a service is well suited for these bursty workflows as you have higher and higher compute demands a fast architecture will scale automatically to meet those demands and then when you have little or even no computing demands a fast architecture will scale down to accommodate that as well so you have no wasted resources when your jobs are small or even non-existent and you can immediately meet sudden large compute demands so function as a service architectures are really flexible and this is the first reason that we think that fast and video transcoding are really suited quite well together the second reason why fast and video transcoding pair so well together is that video transcoding is serial by default when you transcode a video you're essentially working on the video from beginning to end and remember video files are very large and the transcoding process is slow and as i mentioned earlier the parallel computing capability of function as a service is the bread and butter of bento and this is why transcoding on a single server means transcoding a large file one piece at a time from beginning to end so imagine this file as a composite of many portions in this example transcoding the first portion of the video will take roughly 30 minutes okay the next portion will take the same amount of time and so on and so on so the entire transcoding job for a single video handled here in the traditionally linear fashion will take about two hours and so this isn't the kind of process that you start up and grab some coffee and come back and it's finished this is two hours to transcode a single video but what if we could instead extract those same portions the segments of the whole video and transcode all of them at the same time in parallel that would mean in this example four segments 30 minutes for each segment and this time each one is transcoded concurrently with the others transcoding this way would bring our two-hour job down to just 30 minutes so we've already cut the total transcoding time by 75 percent however with bento we don't break our videos down into quarters we break videos down into six second segments this means dozens sometimes hundreds of small portions which will each take a few seconds to transcode rather than 30 minutes as shown here and once you have all of those segments processing in parallel within a system the time to transcoded videos dramatically reduced and this is in fact the system that we built so let's talk about building bento a serverless video transcoder we built bento with the goal of being fast really fast and simple to use additionally vento's deployed to your aws account amazon web services which means your files remain on your own servers the current solutions and in-house transcoding and third-party services they really provide a maximum value for large businesses so companies that deal with an endless stream of huge video demands however many small businesses have video transcoding requirements at a steady but smaller scale so that could mean processing new videos several times a day or week maybe with dead periods in between and it's this kind of bursty video transcoding that's perfectly suited to bento and in addition they may not want to host their videos on a platform like youtube or facebook for a variety of reasons so for our use case we focused primarily on speed and simplicity of use the trade-off being that users will have less control over the transcoding settings and there will be fewer input and output options so we wanted speed and we achieved this by utilizing a fast architecture and given our use case we wanted to keep costs as well as level of technical expertise required at a minimum and also with bento you'll have minimal level of control over the process so video transcoding is very broad and we only wanted to handle basic conversions between formats so having said that let's look at some of the specs and the supported inputs and outputs currently bento can handle up to one and a half hours of 1080p video and up to 15 minutes of 4k video and the inputs we support are any videos that use the h264 codec so that's mp4 mov mkv 3gp and then ts which is a popular streaming format and then we transcode videos to mp4 it's the most common format used today so we wanted to be sure to support that uh so this was the plan we wanted to build a pipeline of functions that pass a video file from beginning to end and it works like this we divide the video into n segments then in instances of the transcode function working in parallel with each other will transcode their own individual segment to the output format and finally the transcoded segments are merged together into a complete video file i'll touch briefly on the tools that we chose to implement this approach bento uses a variety of resources within the amazon web services ecosystem the fast provider that we chose for bento is aws lambda lambda is currently the most widely used function as a service platform and it provides the highest limits on configuration details like timeout and memory aws s3 is used for storing video files later you'll see us talk about s3 buckets to store video files and amazon's dynamodb is used for tracking state within our pipeline as well as for storing information about users videos and then amazon ec2 is used to host bento's admin dashboard that we made and finally within our transcoding lambdas we are using ffmpeg as our transcoding software ffmpeg's a free open source software that performs all kinds of operations on multimedia files and we use ffmpeg for the transcoding stage and we use a few other of its tools elsewhere in the pipeline so let's look at the architecture here we have an overview of the bento pipeline there's essentially four stages an execution a transcoding stage and then an intermediate stage where we determine when to begin the finishing merge process and then the merge stage itself where we concatenate transcoded segments of the video into a completed hole so the executor function obtains the segments of the whole video file and it will fire an instance of the transcoder function for each segment all of the transcoders execute concurrently and after they've finished the merge invoke function will fire the merge function i'll take a focused look at the execution stage now so the goal of this stage is to essentially break the input video into in segments and invoke the transcoder for each one here we have an input video and you'll notice that it's a dot mov file the executor will examine the input video to collect some information about it some of this data will be stored in the database and the information that's stored in the database is really about this particular transcoding job as i mentioned earlier our lambda functions are stateless and so they have no they're unaware of their broader context so this database information will act as state for the pipeline and it will come in handy later when we need to track the overall status of the transcoding job also and this is essential for the next step the executor examines the video to calculate how many segments it will divide this transcoding job into so it writes to the jobs table and to the segments table in our database and then for each of the segments that it calculated from the input file the executor invokes an instance of the transcoder function passing in the time stamps for each of the respective segments and this brings us to the transcoding stage the goal of this stage is of course to transcode the video segments as i mentioned when the executor invokes an instance of the transcoder function it passes in time stamps which essentially act as demarcations for a segment of the original video the transcoder will examine the original input file in memory and using those it received from the executor it will produce a transcoded version of the specified segment so here this transcoder function produced a segment of the input video but in the mp4 format that new segment is then placed into storage s3 and with that this transcoder has finished transcoding its assigned portion of the video it then writes to the database and in effect it says this segment is now completed and this job is now one segment closer to being complete and it's important to note here that though we're currently examining how a single transcoder function works it's actually one of dozens or hundreds of transcoder instances working in parallel so remembering that we have many many transcoders working in parallel to transcode their respective segments of the video let's look at the merge invocation stage the goal with this stage is to invoke the merge function when and only when all of the segments have been transcoded we have we have all of these transcoder instances executing in parallel and we have to make sure that they've all completed before we invoke the merge function so as we just saw the transcoder function will write to the database after it's completed its segment saying this job is one segment closer to being complete and every time the jobs table gets updated in this manner the database will emit an event into what's called an event stream and we've set the merge invoker function to consume these events so the merge invoker will get a database record and it examines the record and if it finds that all of the segments have been successfully transcoded it will invoke the merge function let's finally zoom in to this last stage in the pipeline the merge stage the purpose of the merge stage is to take all of those segments that have been transcoded into the new format and stitch them together back into a complete video when we arrive at the merge stage we'll have all of the transcoded segments sitting in one s3 storage bucket after the merge stage completes we want to end up with a whole video in the last storage bucket so we have all of our transcoded segments in storage the merge function is going to use each one of those segments to actually build up a complete video so we have the first portion in our final bucket now then merge will concatenate the next portion onto the video and then finally the last segment has been concatenated onto the new video file and with that the pipeline is finished and we've now got a brand new mp4 video so this has been kind of a broad overview of the pipeline but now we'll dive a little deeper and talk about some of the more interesting challenges that we encountered while building this pipeline and for that i will turn it over to mike thanks nathan our challenges and solutions well we face two main problem domains one serverless functions work well with event driven architecture but ensuring that functions are triggered at just the right time can be difficult two each of our functions is allocated temporary storage but when working with videos storage can get maxed out pretty quickly pursuing eda to start why we envision our pipeline stages being triggered by events the problems that come about from concurrency it's hard to track the status of a specific process when there are many up and running at the same time and concurrent processes interacting with the same database can lead to race conditions finally our solution capturing changes on a database yda eda applications react to internal and external events without a centralized workflow an orchestrator isn't needed serverless functions are commonly found in this architecture they can respond to events and can also create events that in turn trigger other functions in our pipeline one function stands out from the others merge invoke why is it needed as the name implies it helps coordinate our pipeline's workflow by invoking the merge function at the appropriate time it bridges the gap transitioning from many transcoders running on its left to the merge phase on its right our pipeline is made up of many stages these stages need to be coordinated to ensure the success of a transcoding job timing emerging stage is critical assembling transcoded chunks into a final video should only begin when all the segments of the job are available the merge stage was the most difficult stage to coordinate because on its left hundreds of transcoders can be running at once it's hard to know which transcoder will finish last or when they will have all completed we needed a reliable event to trigger this stage we first considered whether the event could be produced by specific transcoder instance and this leads us to our first problem related to concurrency how to monitor status of these transcoder functions let's zoom in on the transcoding functions that run in parallel here we have four but in reality there may be hundreds we asked ourselves will the last invoke transcoder finish last the first invocations of the function may be more likely to complete before later implications however this is not guaranteed in this case if the fourth transcoder was predicted to finish last the merge function would have been invoked prematurely before all the segments were available since we cannot predict which function would complete last we consider moving away from a strict serverless eda model our goal was now to capture state that functions could read throughout the pipeline with a database on to another problem related to concurrency by introducing a database to our pipeline we discovered firsthand that concurrent processes interacting with a database is a recipe for race conditions a database allowed us to store metadata to track state we created a segments table and a jobs table to capture the state of our pipeline additional logic was added to transcoders after transcoding a segment each function now updates the segment status from pending to complete in the segments table then updates the total number of transcoded segments that are complete for a job in the jobs table our database reliably stores state this is good but the database cannot orchestrate the transition from the transcoding stage to the merging stage on its own we tested a model where any transcoder could orchestrate the merge phase before winding down these functions would read the jobs table check to see if all the segments were transcoded and if so triggered a merge face this approach was not reliable because a race condition was introduced a function writes to the jobs table incrementing its completed segments counter a concurrently running function completes and increments the same counter the previous function performs its read of the counter and sees that all the segments are available the last function to complete performs its read and also sees that all the segments are available the problem is this results in multiple indications of the merge function not what we want even with a database to update and restate the responsibility of orchestrating the merge stage was not suitable for a transcoder our solution capturing changes to the database as events change data capture refers to capturing a time ordered sequence of updates to a table we wanted to ensure that a transcoder instance could read the transcoded segment's counter of the jobs table before another completed transcoder had a chance to update the same counter the creation of an event stream attached to the jobs table provides serialization of updates within the table now every time the counter in a jobs table is increased a record is created in the event stream and this record includes a snapshot of the value of the counter a resource was now required to examine the records of the stream back to the merge invoke function this function was attached to the stream this means that it is invoked when new stream records are detected every time the completed segments counter increases each record provides merge invoke with the counter value of the jobs table if all the segments are ready to merge that phase is triggered while pursuing an eda architecture we encountered common problems related to concurrency it is difficult to track the state of concurrently running processes and race conditions are common a database proved to be a lightweight addition to the pipeline and also gives us the option to track outcomes related to our pipeline's performance let's move on to our second problem domain challenges related to storage now videos are large if our pipeline can only handle small files our use case will be severely restricted let's take a look at how bento went from processing videos around 250 makes to two gigs on aws serverless functions lambdas are containerized containers hold resources that are allocated to each lambda each lambda is allocated 512 megs of temporary storage our starting point was a pipeline restricted to video files of no more than roughly 250 megs this was because of the way we were managing temporary storage the main culprit was the merge stage the merge function retrieves processed video chunks and assembles them into our output video to do all this we first had the merge function download all of the segments to its temporary storage ffmpeg video transcoding software was then able to concatenate these segments the problem is that both the segments and the final video were living in temporary storage before the final video could be shipped off to s3 or permanent storage bento could only handle videos around 250 megs a solution to this problem avoid storing the segments in temporary storage and instead have them processed in memory ffmpeg does not rely on locally stored videos it also accepts another input type http input ffmpay can be provided with a list of urls that each point to a segment's location in s3 storage this means that segments can be downloaded and concatenated in memory now only the output is taking up temporary storage this is helpful because this function has three gigs of memory and only half about half a gig of storage our file size capacity doubled to 500 makes but in the world of video 500 megs still isn't all that much our input the segments could actually total more than 500 makes because the merger function has three gigs of memory to hold and process to segments our storage problem now related to output let's take a closer look at this output bottleneck how the entire final video gets stuck in temporary storage ffmpeg gradually builds up its output and storage once ffmpeg finishes the concatenation process and the entire video was sitting into lambda storage we then moved a file to s3 our pipeline was restricted to files around 500 makes the output of the merger function had to be optimized to make more breakthroughs our goal was to have the final file built up on s3 instead of first having to live in temporary storage we discovered that ffmpeg output can be piped as a byte stream instead of holding the final output locally it could be sent in pieces as a byte stream directly to s3 under receiving n s3 needs to be able to receive files transferred in parts without even knowing how big the file will be s3 storage supports multi-part uploads multi-part uploads an s3 feature allows clients to transfer files in chunks that can be sent in any order our merge video can now be gradually constructed in our permanent storage by passing temporary storage here's a bird's eye view of present-day merger the transcoded segments are processed by ffmpeg in memory thanks to http input and the final video is built directly on s3 via multi-part uploads because our function has three gigs of memory our pipeline can now handle videos upwards of two gigabytes we improved the reliability of our pipeline by injecting state and increased our ability to handle large files now i'll pass things back to max who will go over bento's performance thanks mike now that we've talked about bento's architecture let's dive into bento's performance as a reminder our goal was to build a transcoder that was fast and easy to use we benchmarked bento against two options that individuals and small businesses might choose for video transcoding the first alternative is a free tier ec2 instance with one gigabyte of ram running ffmpeg while this is a bare bones options for professionals it represents a clear baseline to benchmark bento's performance against traditional non-optimized video transcoding on a single machine for the second alternative we chose amazon elemental media convert media convert is a feature-rich transcoding service aimed at professional broadcasters and media companies while bento doesn't provide the same set of transcoding options we wanted to demonstrate the speed bento can achieve against professional grade software as a test we measured the time it took bento and these two alternatives to output a video file to mp4 format at 1280 by 720 resolution we benchmarked against 19 test videos whose size ranged from 4 megabytes to 2 gigabytes and whose duration spanned 7 seconds to 90 minutes in short we achieved our goal in this chart we provide a cross-section of benchmark data that illustrates the results we saw at small medium large and very large file sizes bento consistently performed over 90 faster than an ec2 instance and 50 faster than amazon media convert across a variety of file sizes video duration and format types given these results we feel confident in saying that bento demonstrates that a serverless massively parallel approach to video transcoding results in significant improvements to transcoding speed to wrap up we wanted to touch on a few limitations of our current approach the first is that function as a service providers have execution time limits aws lambda functions for example have a maximum duration of 15 minutes this means that each transcoding function needs to complete its work in under 15 minutes or else it will fail now as you saw even high quality videos move through our pipeline in far less time than that so while we are far from experiencing this in practice transcoding a sufficiently large high quality video segment could cause issues in our pipeline second bento is built to be the fastest video transcoding pipeline therefore optimizations for video quality and file size are not the primary goals there are transcoding optimizations that could be made to further improve the final file size of videos however most of these optimizations would come at the expense of speed we made an explicit trade-off to focus on speed and ease of use finally when you move from the free tier lambda's compute per second is more expensive than ec2 now this is partially mitigated by the vastly reduced time spent transcoding however for businesses that are transcoding 24 7 transcoding on lambda or any function as a service would end up being more expensive again bento is for businesses with frequent but intermittent video demands video platforms and media businesses that have enough content to spend their full time transcoding would not see a cost advantage with bento before we finish up we wanted to highlight some opportunities to enhance bento that we're excited about first we'd like to build support for hls and dash as output formats just as the mp4 format that bento supports is the most widely used video file format hls and dash together represent primary streaming formats used today we'd also like to add more types of video transformations to our pipeline for example many businesses add subtitles and intro outro bumpers to their videos bento's transcoding pipeline is well suited to perform these sorts of transformations in the future so with that we want to say thanks for listening and at this point we're happy to take any questions cool so the first question that we received is from william lovington he asks why did you decide on six second segments so this was something that took a little bit of time to figure out so what our pipeline does is it probes a video to look at the keyframe data uh in the that's part of the videos so key frames occur roughly every two to six seconds in in most videos and we decided to split our video into segments that are aligned along those keyframe durations so in general the maximum length of a video segment would be around six segments um the reason that we decided to do that was because we found that that tended to yield the best uh video and audio quality in our output files uh there's another question are the outputs of a file transcoded in segments technically identical to those transcoded as a single file and are there any output size or quality trade-offs technically identical not exactly there's there's actually a bit of metadata that is used in a video file um and it's kind of metadata about the video in general and we actually take some of that metadata out when transcoding so at a technical level it's not going to be exactly the same but then output size and quality trade-offs we didn't notice any significant variation in output size and then the quality that we're getting is really quite good it looks i think pretty much identical to the original videos as far as quality all right um let's see what else has this project been presented to any of the target customers at a business level uh no our goal is to keep the project for for one open source we will be showcasing a project when we start applying for jobs very soon and that will be for us the true test but of course we're also interested if anyone there are what we didn't cover here is there are open github there are steps to deploy the pipeline and also um a dashboard that makes it all very easy to use so we are also interested to see if we receive any kind of feedback from people testing the pipeline out but none so far there's also some questions in the chat okay one more of those did you explore whether microsoft or google serverless options help with state no i'd be interested to know if you know of any features that would make other serverless providers facilitate state with with their version of lambdas because there are some unique features one is the fact what we talked about today is the temporary storage that lambdas are provided which can't be found elsewhere but if there's other features that would make things like state either to keep track of on other ones yeah i'd be curious to hear about that a couple of other questions can this tool do the reverse can it take a compressed file and play it uh so bento is not a video player um but uh yeah all the video files that it outputs um are playable across uh you know all major media players does bento include audio transcoding options at this point it doesn't but um it would be pretty trivial to extend that option ffmpeg does audio transcoding as well as video transcoding and so uh it would it would be pretty straightforward to extend bento to do audio transcoding options as well that being said audio files are different than video files um in some in some non-trivial ways and so we'd have to examine whether we'd get the same quality trade-offs when doing that how did the output bit rates compare between ec2 aws media convert and bento um the output bit rate so so when we did the transcoding um in bento we aimed to maintain the bit rate of the video file that was uh input um in ec2 we did the same thing in our benchmarks um and amazon media convert there are a lot of options around uh output bit rate so we used um constant bit rate when we did our outputs in amazon media convert but there are lots of different options that you can do in terms of like variable bit rates for videos as well we didn't use those in our benchmarks another question in the q a um how exactly is video metadata dealt with such as titles descriptions authors etc so we did not um explicitly deal with video metadata outside of our um our last step so um when we write to uh s3 in our merge step uh one of the things that we've found is when you're building up a file the metadata tends to be appended to the beginning of the file but not until the file's processing has been completed so if you imagine ffmpeg building up a file from beginning to end it will transcode the video from the beginning to the end and then after it's done it wants to seek back to the beginning of the video to append video metadata and so what we found was because we were streaming the byte stream to uh s3 directly by the time the processing was complete when ffmpeg wanted to seek back to append that metadata it wasn't able to do that because the file was now on s3 it wasn't in local storage and so this was this was a tricky problem to solve the way that we did it was we essentially told ffmpeg to append empty metadata to the beginning of the file in order for it to stream to s3 and have that output file be playable across all media players a couple others here what language did you choose for writing your functions does the choice of language affect what you're able to do with your functions and we we wrote all the functions in javascript in the nodejs runtime and i don't think that i don't think that we got any uh benefits from choosing javascript or node.js that we couldn't get from like say ruby um but node.js does have a lot of useful kind of libraries that we use in our functions like managing a file system within the function so um yeah and and then why did we choose dynamodb as opposed to a relational database yeah so dynamodb is nosql and we didn't really have need for a relational database we're not doing any kind of joins or kind of looking at data together like that we it's kind of just a real light light use case for us to track uh almost like environment variables so we didn't have need of a relational database yeah one additional benefit that we saw and this is more specific to uh just when you're rapidly developing a product is that because dynamodb is nosql it's schema-less which means that we could really quickly add and change the information that we were storing in our tables without having to rebuild a relational database so it made rapidly prototyping very quick and very easy which was a big benefit for us and of course other databases have this feature but it's a little easier when it's right there with on the same platform of your lambdas that's the event stream um so just make it a little easier to get that set up again quickly that it's um yeah a few clicks away or a little more but a few a few configuration options and you have your event stream that's being created by dynamodb and then you can choose an endpoint so that that event stream acts as a trigger for something else what prompted you three to take on this problem with video transcoding or any of you involved in the video industry maybe um i think just kind of the what we were saying earlier in the presentation about video in general it's kind of a fascinating uh i guess thing to work with but uh i i don't have plans or i don't have any uh ideas of going into necessarily yeah uh i think one of the as we mentioned at the beginning of the presentation i think the one of the big appeals was just how ubiquitous video is um it's a pretty strange type of file format uh because it's so large it presents a lot of interesting challenges and we thought that it was an interesting format to work with in terms of uh you know a cloud system um so it just presented a lot of unique challenges that we thought were uh worth tackling um in terms of this being open source and what contributions would we like to see from the community um the there are a ton of ways you could extend uh bento as we said the the transformations that we're performing are very basic right now um essentially we're focusing on a few input types uh one specific output type and we provide resolution options there are a ton of things you can do in the world of video transcoding it's an extremely broad field um and therefore uh the great thing about bento is is it sort of provides sort of a baseline to really extend in whatever direction uh you want to as we said we are most interested in supporting more formats right now hls and dash are i think uh pretty widely used today for um for a lot of video streaming and for our case that's just probably our top priority and in the chat with your final product were there any failed transcoding jobs no i mean in a sense testing is never done but there are there are more optimizations that we were trying to do to make it you know just a a little bit faster and retrieve information from the original video file a little differently but in the end we show we we went with reliability you know 100 so if if any little tweak we're trying to do cause some failed jobs that's not what you're gonna see in the code that's available right now yeah and and like he was saying with testing um we've got our limits as far as the file size that we can handle so anything beyond or approaching three gigs is going to be probably more than we can handle with the current implementation anyway uh but within our sort of parameters i think we've got a basically 100 success rate so i think we'll take this uh last question here um do you see any potential for a product like bento to succeed in the marketplace um the short answer is yes bento is not built to be a commercial product um however uh certainly during the course of our research we found that you know as we said every company and every business deals with video and that's only only going to increase in time and the challenges around quickly transcoding videos uh are only going to grow as file sizes get larger and more formats become so as we mentioned for large businesses and media companies the solutions that are available are pretty good but there are a lot of individuals and small businesses that are just getting into this right now and the marketplace uh still has room to grow in terms of making things uh you know simpler and lower cost uh for a lot of people so um is there potential for product like bento to succeed in the marketplace i think there's absolutely room for that in the future so with that i think we'll wrap up um again thanks again um for watching uh you know building this project was uh extremely uh satisfying and um we're really happy to hear uh that uh that it's being received so well yeah thanks for all the questions too yeah thanks a lot guys 