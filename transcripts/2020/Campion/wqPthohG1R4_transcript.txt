hey everyone welcome to our presentation my name is gabe and today we're going to be talking about kampkin a framework that seeks to help applications fail faster and better in this talk we'll go over what problems we're solving why we're solving them and how it is under the hood that we're solving them we'll also have a q a session at the end of the talk in case any of you have any questions for us thank you to start off in order to understand what camping is it's important for us to first understand what architecture it's best suited for and when it comes to systems architectures the two most popular ones are the monolith and microservices architecture when first building an application engineers usually opt to go the monolith route by building out the entire functionality of the app within one code base this allows for rapid development ease of testing and speed of deployment and in the early stages this makes perfect sense when your systems grow however a popular and logical step is to move to a microservices architecture a microservices architecture divides the code base into a loosely coupled collection of services each with its own well-defined responsibility these services communicate over a network via an agreed-upon communication protocol such as http from the user's perspective it's just as if the entire application was hosted on one server but in the background we have all of these services separated and communicating with each other over the network the benefits of microservices include ease of scaling the ability for teams to release updates or patches independently without the need for company-wide coordination and localization of bugs or outages to only the offending services campion is a framework specialized for a microservices architecture while a microservices architecture offers many benefits it also comes with its own share of drawbacks as can be seen in the diagram all of the services are connected and communicate with one another through the network as signified by the arrows the issue however is that network communication introduces a multitude of potential problems given that the network is inherently unreliable it adds latency to the communication between services and that bandwidth the maximum rate of data transfer through a network path is not infinite through this network what happens then when this link between service a and service b is not working what happens when service b attempts to communicate with service c but service c is not working these are all problems that we have to be cognizant of as we move away from a monolith into a microservices architecture it's also important to highlight the fact that the way in which services communicate with one another varies greatly when services communicate with one another synchronously they wait for a response from the service they're communicating with and will hold back further execution until they receive a response in the diagram service a is communicating with service b it then stops execution and waits for a response from service b once that response arrives service a continues with its execution in other words synchronous communication couples services together a real world example would be if you entered your login information into a site and had to wait a few seconds for the site to verify your information before allowing you to proceed on the other hand when services communicate asynchronously they don't wait for a response back in order to proceed with further execution in a distributed environment as can be seen in this example right here this is often done through the use of a queue with services sending commands and events to the queue and not waiting for that particular task to be completed before proceeding in other words asynchronous communication decouples services an example in the real world would be if you checked out in an e-commerce site and the site told you that they would send you your order status and confirmation to your email at some later date now that we know how services communicate with each other let's talk about failure because it's an inevitable byproduct of microservices and systems in general let's start off by exploring what happens when a service fails in an asynchronous environment as we saw in the previous slide when services communicate asynchronously the calling service continues with its execution and whether or not the task that it's submitted to the queue gets accomplished or not it doesn't really care right so it's business as usual therefore if there is a failure in service b who is waiting for an event to be added to the queue service a won't be impacted by this failure because it doesn't necessarily care when the task gets submitted to the queue gets completed synchronous communication on the other hand is vastly different in terms of its destructive abilities as we can see in the graphic service a has made a synchronous call to service b but it's not receiving a response and the reason for that could be because service b has failed or because there was a network partition somewhere in this case service b has failed but regardless of what occurred all that service a really cares about is a response and it's not receiving one so as we can see a failure in a service that is synchronously called cripples the calling service campaign aims to do something about this problem of how to deal with failures that occur in services that are called synchronously to recap we've learned that both the network and services fail and that failures in a synchronous environment open up a variety of issues what we haven't yet learned about is the exact ramifications brought about when the network or services fail in the real world we don't know what that looks like beyond the theoretical level so let's run through a few scenarios so that we understand why any of this is important in the first place let's go on this journey with mark who's currently trying to access carpartz.com but all he sees is this loading spinner and after a few seconds he begins to lose hope that anything is going to show up what could be the problem here well if your site has a dependency that has failed and your users try to access your site while this failure is occurring the page never loads a terrible experience that can impact a company's bottom line what mark is aware of is that behind the scenes there is a content loading service that is in charge of displaying the entire site the content loading service then makes a synchronous request to another service called the image processing service that is responsible for fetching images and processing them the image processing service however has failed which is causing the content loading service to have to wait for a response that is never coming now the content loading service is receiving multiple concurrent requests from other users who are also attempting to access the site causing it to be inundated with requests soon enough the failure of the image processing service will cause the content loading service to fail as we've seen from the previous slide a failure in one service can lead other services to fail too in this scenario a failure in service d has caused services resources to be over as services requests wait for a response from service d that is not coming so service c has failed uh this failure service d has now led to the failure of service c and the failure of service c has now led to service b being tied up waiting for a response and consuming resources the failure of one service has now set off a chain reaction that's formerly known as cascading failure now every other service that synchronously calls another service that's failed is also going to fail as we can see in the diagram service b is also going to fail and that's going to lead service a's resources to be tied up and as you've probably already guessed service a two will fail so this highlights the fact that in a synchronous environment the impact of a failure in one service will ripple to other upstream services leading them to fail as well so far we've seen mark unsuccessfully attempt to access the car parts content and he's going to be left with an infinite loading spinner we've also seen how the failure of the image processing service has begun cascading into the content loading service but that's not all the image processing services problem has now been fixed and this service is currently attempting to bring itself back up but because it has been and continues to be bombarded by new requests from the content loading service it's uh its attempt to restore normalcy is doomed so a failure in one service can not only cascade into other upstream services but it can also impede the failed service from bringing themselves back up what seemed like an innocent spinner on a website was actually an architecture that was ill-prepared to handle failure this led mark to take his business elsewhere while the previous scenario was hypothetical shopify actually faces this exact problem where one of its services attempted to connect communicate with a redis service that was responsible for storing session data but when the session database service was down it led to the over utilization of the services that depended on it and that resulted in an unacceptable customer experience in this particular case given that shopify receives millions of request hits every day a mere two-second delay in a response led to their services that could originally process five requests per second to now they could only process half a request per second so their resources quickly became over utilized so we now know that making synchronous calls to other services in a microservice architecture is dangerous and that when something fails in such an environment at best things take a little longer than usual and at worst your entire system fails so we need a way to fail better but what does that even look like well to start off we definitely need to incorporate a timeout component into every synchronous request so that a service is never waiting for a response forever we also need some way to track how often a request fails so that we can deem the service to be down allowing us to return an immediate failure instead of making a pointless request we also need to shield the service that has failed from additional requests so that it has some breathing room to bring itself back up and lastly we need a way to track and log failure information so that we as engineers and operators can better understand our system's health so now that we understand the problem and what we need to do to tackle it i will introduce the design pattern that combines all of these strategies the circuit breaker the circuit breaker pattern was popularized in 2007 in michael nygard's book release it it's one of many of what he calls stability patterns which are designed to help lessen and contain the effects of failure in a distributed system preventing failure is a fool's errand the best we can hope to do is to handle that failure gracefully the circuit breaker combines all of all these principles that i just talked about and it provides an effective failure mitigation strategy so what exactly is a circuit breaker well it works the exact same as an electrical circuit breaker a switch is inserted in between a component and the rest of the system in electrical systems it's a circuit being isolated but in an application it's a service if the isolated component fails the switch flips opening the circuit and severing the connection between the failed component and the rest of the system this does two things on the system side it contains the failure and allows the system to continue functioning in some capacity while on the service side it allows the failed service to catch up or heal without the pressure of continual input to see exactly how the circuit breaker pattern works we'll revisit this diagram of a synchronous failure this time with a circuit breaker inserted between the two services now service a's request to service b is passed through the circuit breaker because service b is down the circuit breaker has flipped open and is able to return an immediate response to service a this allows service a to continue execution rather than waiting in vain for a response from service b now i've mentioned a few times now that a circuit breaker flips to protect a service exactly how it flips and how it knows it should flip is really the heart of the logic behind the circuit breaker pattern when the protected service is healthy the circuit breaker is closed and network traffic is allowed to pass in this state the circuit breaker is paying attention to every request and whether it succeeds or fails depending on how long it takes to receive the response and what status codes it gets back if the request is successful nothing changes and everything works as normal if the request fails however the circuit breaker checks to see if the number of recent failures is within a user-defined threshold if it is within the threshold it logs the new failure and continues on in the closed state once the threshold for allowable feature i'm sorry for allowable failures is reached however the circuit breaker flips open severing the connection between the system and the service which is now considered failed this also initializes a cooldown period during which no traffic is allowed to pass during the cooldown period any incoming requests are returned immediately to the system with a failure response and the state of the circuit breaker remains open the cooldown period gives the failed service time to catch up and self-heal without having to address any new input after the cooldown period expires the circuit breaker flips to an in-between state known as half open if there are any electrical engineers in the audience the half open state is very similar to a recloser in electrical systems in this state only a few of the incoming requests are allowed through in order to test the health of the downed service if the request is not allowed to pass if it's not a test request then an immediate failure response is sent back to the system if a number of the test requests are successful the circuit breaker considers the service to be healthy again and it recloses the circuit if those test requests continue to fail however the circuit flips all the way open again resetting the cooldown period cutting off all of the traffic from the failed service and returning an immediate failure response for every request this open to half open to open cycle will continue until the service is operational again earlier i mentioned that the circuit breaker pattern is only one of many stability patterns that help a distributed system address failure so why choose a circuit breaker rather than one of the other patterns first the circuit breaker pattern already leverages two popular patterns timeout and fail fast while both of these patterns have their individual uses they're all the more effective if you combine them second the importance of tracking and logging failure information cannot be overstated when a developer is trying to diagnose a failure in a system any and every scrap of information is valuable a circuit breaker actually gathers this exact information over the course of its normal operation finally in what might be the most attractive to the developer the half open state enables the connection to the service to self-heal this auto recovery feature is unique to the circuit breaker pattern and it prevents the developer from having to manually reset a connection to a service after that service is back up and running existing circuit breaker implementations can be sorted into two major types libraries and service meshes each type of course has its own pros and cons the library implementation injects the circuit breaking logic directly into the code base of the service that's sending the request usually as a class that wraps around the request being sent the fact that the circuit breaking code is a part of the application code base is a major advantage because it means there's no added infrastructure which means there's no latency added to the request also libraries are easy to implement in smaller code bases usually it's just a matter of importing a package reading the documentation and updating your code however this means that the circuit breaker is embedded in the code itself which tightly couples the circuit breaking logic with the rest of the application this can make refactoring and debugging difficult also an imported library doesn't scale easily the developer has to write the code that protects any additional services and this challenge is exacerbated by the fact that libraries are language specific a circuit breaker library built for ruby won't work on a service built in javascript meaning that every new language that needs a circuit breaker also needs its own library to be imported learned and implemented finally libraries are designed for internal services and aren't a great choice for connecting a website to an external api the other way to inject a circuit breaker into your system is through a service mesh briefly a service mesh is an infrastructure layer that facilitates communication between services essentially a service mesh sets up a proxy server in front of every service and while the service itself does the processing the proxies are in charge of the communication between services metric and debugging logs as well as any required config data are kept in a separate layer called the control plane for systems that already incorporate a service mesh implementing a circuit breaker is simply a matter of configuring the circuit breaking functionality that comes standard for most service rest providers adherence to the standard design principle of separation of concerns is one of the advantages of using a service mesh as the circuit breaking logic is removed from the service code base entirely because the circuit breaker is connected to the service mesh's control plane the management config and the logging of multiple circuit breakers is centralized and can be managed in one spot finally service meshes offer myriad features other than circuit breaking ranging from additional stability patterns to load balancing and automatic retry logic those additional features come at a cost of complexity however implementing a service mesh circuit breaker means configuring an entirely new infrastructure layer and can even require the developer to learn new frameworks now if your architecture already includes a service mesh adding circuit breaking functionality to it is the obvious solution but in the absence of an existing service mesh if all you need is a circuit breaker the service mesh is often overkill as opposed to the library uh the service mesh is an additional layer of infrastructure that needs to be set up and maintained but like a library service meshes aren't designed for use with code that's rendered by the browser itself so examining the pros and cons of libraries and service meshes it actually reveals a nice little opening for a third possible implementation of a circuit breaker especially if your primary goals are simplicity reusability and ease of use namely middleware this was our goal we wanted to design campaign to be a black box that could be easily inserted not in the code base not into the global system architecture but directly into the request response cycle between services this would allow it to be inserted anywhere circuit breaker was needed without regard for language compatibility and without having to wade through documentation and additional configuration code so the middleware paradigm offers us this ability but as arthur is going to tell us it comes at a cost that was prohibitive until very recently so we think a middleware circuit breaker is a great way to implement better failing capability especially for externally exposed endpoints and web apps and we saw other projects that had implemented circuit breaking as a middleware but none of them had a lot of traction and we think one reason for this is because of how middleware works see the problem with middleware is latency by definition if a service like campion sits in the middle request needs to be routed through it and then be forwarded on to the destination so for example maybe you're in the midwest you're making a request to the east coast if there's no middleware your request is a relatively short round trip to make but now let's say your request needs to go through the middleware hosted on a server in california this can add quite a bit of distance and thus time to your request we wanted campion to sit in the middle between requests in order to provide circuit breaking functionality but we also wanted it to be as fast as possible to solve the latency problem many people use content distribution networks or cdns a cdn is basically a massive network of very fast connected servers all over the world when you host something on a cdn let's say a video the video gets distributed to every server in the cdn network now somebody in asia wants to download the video their request is routed to the server closest to them so using a cdn make sure that every request has a short round trip often people will cache their static assets like images and videos on a cdn for max speed and then use their server for more complicated requests like database queries because the cdn often sits in front of your server and handles some requests while forwarding others onto your server a cdn is often called an edge so this is how early cdns were designed to just serve up content and not do any processing but in the past few years a few cloud providers have started provide edge compute so the idea here is that instead of just putting static assets on your cdn developers would actually be able to build code that executes on the edge this means no more needing to hit your own server to execute dynamic code and it also means that code can run close to the user so latency will be minimal and everything will be fast the way this works is that developers upload code to the cdn which then gets distributed all all around the world on these cdn servers so we saw this as an opportunity to build a fast middleware that offered circuit breaking by putting our code on a cdn and utilizing edge compute we were able to make sure that our code gets executed close to the user this means no more round trips to california if the user is in the midwest instead the request goes to the nearest cdn server first where camping is deployed and then from there campaign forwards the request onto the destination in our tests this cut down request time dramatically compared to a typical server installed middleware and it made sure that every request was fast no matter where in the world the user was located now there are two major providers in this edge compute space cloudflare and amazon and both came out in 2017. now cloudflare is a cdn company and they give people the ability to deploy code there they call their platform cloudflare workers similarly amazon allows code to be deployed on their cdn which is called cloudfront and they call this product lambda edge and we designed campion to be able to be deployed on either one of them so for campion users who may be wondering which option to choose when deploying their circuit breaker i want to talk briefly about each one now workers is an incredible platform when you deploy campion on workers it works almost instantly in our tests it proved to be faster than the amazon offering on average 28 faster for those who already have an aws account lambda edge is a great option the biggest thing to be aware of here is that aws's cdn takes longer to replicate than cloudflares does so what this means is that deploying champion on aws can take up to 30 minutes before the code rolls out to all the servers on the cdn similarly deleting campaign from lambda edge can take several hours but of course once it's deployed camping works great on lambda at edge and as mentioned earlier requests getting routed through lambda edge are slightly slower than cloudflare but this could be an easy trade-off to make for staying within the amazon ecosystem and i wanted to give you a quick peek of what campion looks like on these two architectures so when cloudflare workers campion is deployed on a worker when a request comes through campion it checks the worker's kv to see what the circuit state is before deciding how to route the request now kv it's like a cdn database it's distributed through uh it's distributed throughout cloudflare's cdn uh so it's very fast and we also store stats in kv like failure and speed of requests on aws campion lives in a lambda that is deployed to the cloudfront cdn so here we store the configuration logs and circuit state in dynamodb amazon's cloud database and here again all stats are kept in a database so the user can access them in order to see the status of their services before we get to take a look at campion let's figure out where campion fits so as ben told us libraries provide a feature specific way to do circuit breaking without adding new infrastructure but they can quickly become difficult to manage when the code base grows and services get written in different languages if you want one place to manage your circuit breaking and you don't want to add anything to your code base you can do that with campion and you can also do that with service meshes but service meshes bundle in a lot of other functionality that not everyone needs or wants and they can be really difficult to install campaign is easy to install and add to your code and we work with any language and because we wanted to be able to use campion in front-end web apps and external apis we deployed it to the edge that means that if you want circuit breaking for internal private services that aren't exposed to the internet we are not the right fit for you but if you want to add circuit breaking to your web app to make sure it's protected from failing dependencies campaign makes that easy to recap we do add more infrastructure to a system than a library but we do so with a great deal less complexity than a service mesh so now let's take a look at campion we ship with a command line tool and an in-browser front-end app with a single command you can easily deploy a circuit breaker and get started in seconds like we mentioned before we offer a central place to manage all your circuit breaker settings and endpoints and we give you great metrics about the state uh the status of your services so let's dive in here uh to set up a circuit breaker all you have to do is type campaign ad or campaign aws ad we walk you through adding a new service you enter the url and the name of your service and we ask you specific questions about how to tune your circuit breaker if you're not sure you can accept our sensible default settings and as you can see here the process completes within seconds we give you a new url at the end this is the url that you will add to all your code wherever you make requests to your service it's that easy any requests you make to the url we gave you will have full circuit breaking protection if your service is overloaded we fail fast if your service is healing we gradually ramp up traffic again as soon as it's healthy let's take a look at what this looks like so here you can see a side-by-side example of the same service but only the one on the right is protected by campion it looks like our account service is down when we try to do a simple get request on the left our terminal is frozen waiting for a response on the right you can see almost instantly a failure response sent back by campion telling us the circuit is open in other words campion realized the service was down and has flipped the circuit to open deflecting all traffic from the endpoint and while we only tested get here campion works with all request methods like post put and delete so let's try something else now i've already added my service that i want to protect here i've named it get restrooms list and this is a public api from refuge restrooms which is a great service that provides location data for safe public restroom access for transgender people the circuit is currently closed just to remind you that means the service is available to all traffic without the circuit breaker interfering now if i want to test it i can flip it to open half open or forced open remember that open will block all traffic for define interval in this case 10 seconds forced open is a developer testing tool that will block all traffic indefinitely until the breakers flip back manually half open will let a percentage of the traffic through so in this example i'll use flip to test out my circuit breaker now i want to switch it to open in order to block all traffic okay great now that the status is open notice how the service immediately returns a failed response telling us the circuit breaker is open after 10 seconds we expect the circuit breaker to automatically flip to half open letting some of the traffic through and it does and now that some of the traffic has gone through successfully the circuit breaker will decide to flip back to closed which it again does automatically and our service is available again we think that uh one of the great things about the way we implemented campion is that the user now has one place to check the status of all their services so as you can see we built a beautiful dashboard using react and redux which quickly shows you the latest information about your services from here you can drill down into the metrics of any individual service view the configuration and see what the status is of the circuit breaker now this is just a great way to display information that in ways that we couldn't do through the command line on the terminal now as you saw earlier when we added a circuit breaker there are many settings that users can fine-tune to make sure their service is running smoothly and the circuit breaker is flipping at the right time depending how many requests you get tweaking these options can make a huge difference in the utilization of your services as gabe talked about earlier with the example of shopify we wanted to make it easy for people to see and configure their settings in one place and along with protecting services another big reason to use a circuit breaker is to find easy way to view the health and status of your services we implemented metrics and campaign to do just that give people a way to see what's happening with their endpoints because all requests flow through campion we're able to time how long a request takes and assess the health of the destination service and that's campion it's an edge deployed middleware circuit breaker with a command line tool and a front end for stacks with more time we'd love to add more features to the front end including all the functionality currently only available on the command line we'd also like to do more work on improving metrics including storing them long term and finally we we'd love to add email notifications so that you know instantly when your service goes down and the circuit breaker flips and if you have any questions uh we'd love to answer them at this time great so it looks like we got a question in by tyrell and the question is are there any other solutions on the market for this and yes so we as ben explained earlier there are quite a few library implementations of circuit breaking but and there are also service mesh implementations but we thought that there was a nice opening for a solution that could that could fit right in the middle not be a library and not be hindered by the lot by a library but also not be overkill like service meshes oftentimes are if you're just trying to implement a circuit breaking functionality there is a uh there is a direct reference to a circuit breaking middleware uh similar to ours uh by a company called i think kraken d um but even that's only available through their gateway through their api gateway it's sort of a function of that uh you i don't know that you can get the circuit breaker similar to the service mesh you have to basically configure and set up kraken d on your uh in your infrastructure in order to get access to their circuit breaker great question by the way so it looks like austin miller has asked a question um do you think cdns will play an increasingly large role in web apps in either way what do you think about the future of cdns uh arthur do you want to handle that yeah i can take this question um yeah we're we are absolutely excited about this space um we mentioned that cloudflare workers and lambda edge are both only about three years old at this point and the the ability to be executing code on the edge is really an amazing opportunity which is one of the reasons why we were drawn to this project um yeah people are starting to host more and more um complex and exciting applications on the edge and i think the next couple years we're really gonna be able to see um some cool stuff coming in that space for sure and catherine asked the question um which part of champion would you say was the most difficult to tackle and i'll just answer my point of view on this really quickly um working trying to make campion uh work for both cloudflare and aws was was a rather interesting task because the apis are vastly different as you can imagine and uh particularly on the cloudflare side where the way that they have their database their key value store implemented it's eventually consistent and by nature a circuit breaker has to be able to tap into um the failure count really like quickly and have the most up-to-date account and with cloudflare that was a bit harder because it was eventually consistent and at times we found it to be to take up to like 10 to 15 seconds to be updated but we just yeah we took it and we because you can't have free lunches out here right so if it took a little longer so be it but it was the faster option anyway so it worked out in the end but that was just my my take on the entire project what do you guys think i will i'll add i'll add to that one of the ways that we got around that uh the eventual consistency issue we didn't really get around it but uh one of the ways that we sort of tried to offset that a little bit is we started for cloudflare in particular you can't query um the values from their key value store directly you have to sort of access them through the keys so what we started doing we started we started storing all of our data in the keys we basically turned our data into a um into a json object and storing that as the key so you only had to sort of grab the set of keys and filter it for the data you wanted as opposed to needing to do an n plus one query to get that data uh rodney asks how does it deal with requests that will naturally take a long time um yeah rodney that's a great question so the configuration in campion which we highlighted at certain points in our presentation we allow you to have really granular control with each service so if you have a service that you know is going to take a long time to come back like maybe you know the requests are two seconds you're going to want to set that setting to be um higher than basically you want to set that setting so that you know it's gonna be three seconds or it's going to be more than a typical request so that is something that you want to configure on a service by service basis um because obviously some services just take a long time to come back um and so that is what you're going to do with that i just want to take some more questions here just knock them out for sure yeah uh terrell asked another question are there options when load balancing would apply in circuit breakers not or vice versa um personally i think it it depends on it's a case by case basis like one of the one of the use cases for campion is to use it with front end rendered code hitting whether it's your api or a third-party api so if it's a third-party api then you don't have control over that but if it's your own api we've encountered that there are some load balancers and and also some api gateways that also have some circuit breaking functionality within them so yeah you could use them for that as well and so it just it depends on how you have your architecture set up but yeah that's my technique yeah ezra asks here um is the response for a circuit breaker status customizable i think you're referring to the response that campaign sends back if the service is down the answer is that it's not customizable at this point we have it hard coded but that's a great uh feature that we could definitely add um yeah good question uh cody wants to know uh if we do i have to use either aws or cloudflare for both my cdn and edge functions or can i use cloudflare workers with aws cloudfront um you have to so in order to get to use campaign you give it an end point and you get back an end point so anywhere you would you can inject an endpoint into your code that's where you can use it mixing cloudflare workers and aws cloud front from sort of a micro service to micro service standpoint is a little bit conflated uh cloudfront's designed to work with lambda at edge so awesome yeah and john asked the question uh how long did it take you uh for you to develop the platform and boys what do you think did it take us uh three three three to four months something like that three months probably oh yeah including all the the research that we did um before we even started coding yeah three to four months i'm sure um there's a question in the chat saying do you know how shopify solved the problem they had with redis yeah that's something we didn't get to in the presentation but it's a great question shopify actually solved that problem with redis uh developing their own circuit breaker so they developed their own circuit breaker called semyon it's a ruby library based circuit breaker um and it's open source it's on github you can check it out uh oh ben you can answer that go ahead i can um jesse and answer your question are there any current limitations to the functionality of camping compared with other similar services um we have the natural limitation that we we discussed where we you are going to add latency to the request we've mitigated that by moving it to the edge but that is still a factor that you need to consider um we looked at and the boys can jump on this if they want but i'm thinking specifically of histrix which is netflix's uh circuit breaker library their um metrics and sort of central config station uh was really something that we were trying to emulate uh and i think we got as close as we were able to but their uh their gui is really really savvy um so that's something that we'd like to to sort of emulate in the future for sure yeah just increasing the the metric tracking capabilities uh would be an awesome feature um janae asked uh what was the testing process like and this was really interesting so we used uh what was the tool that we used arthur artillery yeah we use artillery to make thousands of requests to servers all around the world uh we used uh digital ocean servers we used uh servers that are just exposed on on the internet for testing purposes and we used both cloudflare uh circuit breaker implemented in cloudflare and one implemented in aws and we just made multiple multiple like i said thousands of requests to make sure that our data was was consistent all throughout and yeah so it was interesting um and it was really cool to see how uh more efficient cloudflare was and faster than the aws implementation yeah that's a good question um uh vajid asks what companies would be your target market for this service uh what are some well-known companies that use microservices but haven't invested in either a library or mesh solution are you thinking of monetizing this um we think this is a great solution for small companies especially companies that haven't started really thinking much about failure one of the things once you get into this space of thinking about failure especially synchronous failures you just find that a lot of companies this stuff is not even on their radar they're not thinking about this um and so whether it's a circuit breaker pattern or using some of these other design patterns in your architecture there's a long way to go for a lot of companies to sort of shore up their systems against these sorts of inevitable failures um and we're not thinking about monetizing this we're gonna keep it open source for now thanks for the question uh ben or benjamin uh asked did you all encounter any issues or key learnings from adapting campaign to support many languages um we don't we don't actually i mean technically we work with many languages we don't really i don't know that support is necessarily the right word again it's all it's just inserted in the request response cycle so there's really no um no back and forth in between uh language compatibility since we're you talk to us via endpoints only um we i will say that we currently only oh and this actually uh speaks to a previous question um we only work across the http and https network protocols we have not written campion uh to support any other in particular internal protocols or anything like that that is something that i think we would like to do uh if we find that we're able to although since it has to be routed to an internet provider of aws or cloudflare uh it semi defeats that purpose of going uh internally with that um i will say if you want to ask or if you want to answer to key issues or key learnings from uh adapting campion between cloud services um i will say that if you have to deploy a an application to if you're writing an application that deploys to many different services uh particularly in the aws ecosystem uh i would recommend that you write your um tear down code very soon after your setup code because if you um you know let's say you set something up and then you realize that you have to try that again because something went wrong you then have to click through four or five you have to go find your setup uh your deployment in like four or five different areas and undeploy it and then run it again and get an error message that it was still deployed somewhere you forgot um so it can be a little bit uh complicated to deploy to many services uh which is why the tear down feature would be a really nice it was really nice once we had that we could just easily erase that sort of sped up our development process so true yep uh rodney asked can you tell us about security concerns if any that you had to consider and how you dealt with them so the beauty of using uh cloudflare in aws is that they themselves handle like all of the security concerns pretty much a lot of it and we don't have to necessarily worry about it and the fact that we're acting as a reverse proxy to your architecture so we're sitting between the request response cycle and we're not doing any processing on what information you're requesting um allows us to just keep things pretty simple and not have to go into the weeds about how to secure things and and we're also and if you're worried about security you're probably going to be using https as well so it's going to be encrypted to us and then we're going to be using https to communicate with your externally exposed api so yeah we didn't have to think a whole lot about it yeah good question um john asks what languages and tools did you use to develop the platform uh we we built the core product uh using javascript we we're all pretty big fans of javascript around here and it just makes it really easy to both build a package deploy package there's a lot of other great tools out there that build on javascript too um so the cli tool itself was uh yeah it was all javascript and and the front end um was using react and redux so also javascript but you know library on top of it um yeah we got another question here uh what was the most difficult part to build that's a great question i i don't i don't know i mean it was all interesting there were there was a lot of hiccups along the way uh working with the different documentation was interesting but i wouldn't say that it was difficult um implementing the circuit breaking functionality and the logic was not incredibly difficult once we understood the the fundamentals of it and how it's supposed to work um so i don't know if there was anything in particular that was was really difficult to in my opinion and i'll i mean i'll just add on to what gabe's saying which is i don't know if this is what you're asking but if we talk about the most difficult part of the project i think with any project maybe sometimes the most difficult part is figuring out um beforehand exactly what you're gonna do so limit limiting in scope deciding what features you're going to build i think once we sort of finalize in our minds what campaign was going to be i think the build process actually came pretty quickly we just threw a lot of man hours into it until it was built but the the initial process um was definitely complex 100 yeah there's this natural tendency to want to over complicate things and add more features than you necessarily need and so yeah for sure i will i will say that both of those guys are being very modest because um we had it was it was very easy to enable the get function the get method uh was pretty easy to implement uh but the post method the actual passing of data from our circuit breaker to the service uh and then and passing back any altered data was surprisingly complicated uh arthur and gabe spent a lot of time and really like really shined when they figured out how to formulate the headers and how to adjust the headers and the body and uh of each request and response for the put and the post methods in particular yeah and then ezra wants to know if campion is open source we are open source in fact um we are [Music] uh i think we'll probably work on getting that the open source licensing um so that anybody that wants to use campaign can all you need is an aws or a cloudflare account yep and we handle all the all the work in the background for you that is until we decide to monetize it [Laughter] if there's nothing else yeah this was awesome thank you all so much for coming everybody really appreciate it you 