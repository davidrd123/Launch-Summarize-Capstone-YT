Welcome to our presentation! My name is Gabe, and today we will be discussing Kampkin, a framework designed to help applications fail faster and better. Throughout this talk, we will explain the problems we aim to solve, why we are solving them, and how Kampkin addresses these issues. There will be a Q&A session at the end for any questions you may have. Let's begin.

To understand what Kampkin is, it is crucial to first grasp the architectural context it is best suited for. In the world of systems architectures, the two most popular choices are the monolith and microservices architecture. Initially, when developing an application, engineers often choose the monolith approach, building the entire app's functionality within a single codebase. This facilitates fast development, easy testing, and quick deployment. However, as the system grows, transitioning to a microservices architecture becomes a logical step.

In a microservices architecture, the codebase is divided into loosely coupled services, each with its own defined responsibility. These services communicate with each other over a network, typically using a communication protocol like HTTP. From a user's perspective, the application appears to be hosted on a single server. However, in reality, these services are separate entities communicating with each other behind the scenes. Microservices offer benefits such as scalability, independent release updates, and the localization of bugs or outages to only the affected services.

While microservices architecture has its advantages, it also introduces its own set of challenges. In the diagram, you can see that all services are interconnected and communicate via the network. However, network communication brings many potential problems. The network is unreliable, introducing latency and limited bandwidth. What happens when the link between Service A and Service B fails? What if Service B attempts to communicate with Service C, but Service C is not working? These are the critical issues we must address when transitioning from a monolith to a microservices architecture.

It is worth noting that services can communicate with each other synchronously or asynchronously. Synchronous communication means one service waits for a response before proceeding with further execution. In this case, Service A waits for a response from Service B before continuing its execution. On the other hand, asynchronous communication allows services to continue execution without waiting for a response. This is typically achieved through queues, where services send commands and events without waiting for completion.

Now that we have explored the topic of failure, let's understand its impact. Failure is an inevitable byproduct of microservices and systems in general. Before we delve into the details, let's consider a hypothetical scenario. Mark wants to access a website called carpartz.com, but all he sees is a loading spinner. After a few seconds, he starts losing hope. What could be the problem? Behind the scenes, there is a content loading service responsible for displaying the entire site. This service makes a synchronous request to the image processing service, which fetches and processes images. However, the image processing service has failed, causing the content loading service to wait indefinitely for a response. Concurrent requests from other users further overwhelm the content loading service, causing it to fail. Thus, a failure in one service can lead to a cascade of failures in other services.

To better understand the consequences of failure, let's examine another scenario. In this case, Service D fails, causing Service C to be tied up waiting for a response. This failure in Service C then leads to Service B also waiting for a response and consuming resources. Ultimately, Service A's resources become tied up as a result of the failure. As we can see, failure in a service that is synchronously called can cripple the calling service.

These scenarios highlight the impact of failure in a synchronous environment. It can lead to cascading failures, where one failure sets off a chain reaction, ultimately resulting in the failure of multiple services. These examples demonstrate why making synchronous calls in a microservice architecture can be perilous.

To overcome these challenges, we need a way to handle failures more effectively. We need to incorporate a timeout component into every synchronous request so that services are not left waiting indefinitely for a response. Additionally, we need a method to track the frequency of failures, allowing us to identify when a service is down. This would enable us to return an immediate failure response instead of making redundant requests. Moreover, it is crucial to isolate services that have failed from receiving additional requests, giving them the opportunity to recover. Finally, logging failure information is essential for engineers and operators to gain insights into the overall system's health.

Now that we understand the problem and the necessary steps to address it, let's introduce the design pattern that combines all of these strategies: the circuit breaker. The circuit breaker pattern, popularized in Michael Nygard's book "Release It" in 2007, is one of many stability patterns designed to mitigate the effects of failure in a distributed system. The circuit breaker is a switch inserted between a component and the rest of the system, serving as a protection mechanism.

Analogous to an electrical circuit breaker, when the isolated component fails, the circuit breaker switches off, severing the connection between the failed component and the rest of the system. This serves two purposes: it contains the failure, allowing the system to continue functioning in some capacity, and it gives the failed service an opportunity to recover without the burden of constant requests. The circuit breaker pattern combines all the principles we discussed earlier, providing an effective strategy for mitigating failures.

To understand the workings of the circuit breaker pattern, let's revisit the diagram showing synchronous failure, this time with a circuit breaker inserted between the services. When Service A makes a request to Service B, the request goes through the circuit breaker. If Service B is down, the circuit breaker immediately returns a failure response to Service A. This allows Service A to continue execution rather than waiting indefinitely for a response from Service B.

The heart of the circuit breaker pattern lies in how it flips open or closes to protect a service. The circuit breaker operates in a closed state when the protected service is healthy. During this state, it monitors every request, recording the response time and status codes. If a request fails, the circuit breaker checks if the number of recent failures exceeds a user-defined threshold. If it does, the circuit breaker flips open, severing the connection between the service and the system. This initiates a cooldown period during which no traffic is allowed. Incoming requests are immediately returned with a failure response, and the circuit breaker remains open.

The cooldown period provides time for the failed service to recover without the pressure of new requests. After the cooldown period, the circuit breaker enters a half-open state, similar to a recloser in electrical systems. In this state, a limited number of test requests are allowed through to evaluate the health of the downed service. If a non-test request is received, an immediate failure response is sent back to the system. If the test requests are successful, indicating the service's recovery, the circuit breaker recloses. However, if the test requests continue to fail, the circuit breaker flips open again, resetting the cooldown period.

In conclusion, the circuit breaker pattern provides a robust solution to handle failures in synchronous communication between services. By incorporating timeouts, tracking failures, isolating failed services, and logging failure information, the circuit breaker effectively mitigates the impact of failures. It allows systems to handle failures gracefully, preventing cascading failures and ensuring system resilience. This pattern has proven to be effective in various real-world scenarios, as demonstrated by examples from companies like Shopify. By implementing the circuit breaker pattern, applications can better handle failures and provide a more reliable user experience.

Thank you for joining us in this presentation on Kampkin and the circuit breaker pattern. We hope you now have a clearer understanding of the problems we aim to solve, as well as how Kampkin addresses those challenges. We are now open to any questions you may have. The circuit breaker pattern is used to handle failures in a distributed system. It acts as a shield between the failed component and the rest of the system, ensuring that the system can continue functioning in some capacity while allowing the failed component to heal without the pressure of continuous input.

To understand how the circuit breaker pattern works, let's revisit the diagram of a synchronous failure. This time, we will insert a circuit breaker between two services. In this scenario, service A's request to service B is passed through the circuit breaker. Since service B is down, the circuit breaker flips open and immediately returns a response to service A. This allows service A to continue execution rather than waiting indefinitely for a response from service B.

The circuit breaker flips open to protect a service, and how it flips and determines when to flip is the crux of the circuit breaker pattern's logic. When the protected service is healthy, the circuit breaker is closed, allowing network traffic to pass through. In this state, the circuit breaker monitors each request, noting whether it succeeds or fails. If a request is successful, everything continues working as normal. However, if a request fails and the number of recent failures exceeds a user-defined threshold, the circuit breaker flips open, severing the connection between the system and the failed service. This also initiates a cooldown period during which no traffic is allowed to pass. Incoming requests during this period receive an immediate failure response, and the circuit breaker remains open. This cooldown period gives the failed service time to catch up and self-heal without having to handle new input.

Once the cooldown period expires, the circuit breaker transitions to a state known as "half open". In this state, only a few incoming requests are allowed through to test the health of the failed service. If a test request is successful, the circuit breaker considers the service to be healthy again and recloses the circuit. However, if the test requests continue to fail, the circuit breaker flips open again, resetting the cooldown period and returning an immediate failure response for every request. This open-to-half-open-to-open cycle continues until the service is operational again.

The circuit breaker pattern is just one of many stability patterns that help distributed systems handle failures. So, why choose the circuit breaker pattern over others? Firstly, it leverages two popular patterns: timeout and fail-fast. Combining these patterns with the circuit breaker pattern amplifies their effectiveness. Secondly, tracking and logging failure information is crucial for diagnosing system failures. The circuit breaker pattern collects this essential information during its normal operation. Lastly, the circuit breaker pattern offers an auto-recovery feature unique to this pattern called the half-open state. This state enables the connection to the service to self-heal, eliminating the need for manual connection resets when the service is back up and running.

There are two major types of existing circuit breaker implementations: libraries and service meshes. Libraries are language-specific and directly integrate the circuit breaking logic into the service's codebase. This offers the advantage of no added infrastructure latency and is relatively easy to implement in small codebases. However, libraries tightly couple the circuit breaker logic with the rest of the application, making refactoring and debugging challenging. Additionally, libraries do not scale easily across multiple services and are not suitable for connecting websites to external APIs.

Service meshes, on the other hand, are an infrastructure layer that facilitates communication between services. They use a proxy server in front of each service, which handles communication while the service itself focuses on processing. The service mesh's control plane manages metric, debugging logs, and configuration data. While service meshes provide separation of concerns and centralized management of circuit breakers, they introduce complexity and often require learning new frameworks. They are also not designed for use with code rendered by the browser.

In the absence of an existing service mesh, another option that emphasizes simplicity, reusability, and ease of use is implementing the circuit breaker pattern as middleware. Middleware acts as a black box that can be easily inserted into the request-response cycle between services without modifying the code base or system architecture. This allows the circuit breaker to be inserted anywhere it is needed, regardless of language compatibility, without the need for extensive documentation or configuration.

Previously, the latency associated with middleware was a drawback to its adoption. However, recent advancements in edge computing have provided an opportunity to build fast middleware. By deploying the circuit breaker code on a content distribution network (CDN) using edge compute, it can execute close to the user, minimizing latency. CDNs distribute code globally, allowing requests to be routed to the nearest CDN server before being forwarded to the destination. This approach significantly reduces request time and ensures fast responses regardless of the user's location.

Two major providers in this edge compute space are Cloudflare and Amazon. Cloudflare offers workers, a platform where code can be deployed instantaneously, making it faster than Amazon's Lambda Edge on average. However, Lambda Edge is a suitable option for those already using an AWS account. It is important to note that AWS's CDN replication process takes longer than Cloudflare's, so deployment and deletion of the circuit breaker code on AWS may require more time. Requests routed through Lambda Edge are slightly slower than those through Cloudflare, but this trade-off may be acceptable for users who prefer to stay within the Amazon ecosystem.

Both Cloudflare Workers and Lambda Edge provide suitable platforms for deploying Campion, a circuit breaker middleware. In Cloudflare Workers, Campion is deployed on a worker. When a request comes through, Campion checks the worker's KV (a CDN database) to determine the circuit's state before routing the request. In Lambda Edge, Campion lives in a Lambda function deployed to the CloudFront CDN. Configuration, logs, and circuit state are stored in Amazon's DynamoDB, while all statistics are stored in a database for easy access.

In conclusion, the circuit breaker pattern is crucial for handling failures in distributed systems. It provides a mechanism to contain failures on the system side while allowing services to catch up and self-heal on the service side. Implementing the circuit breaker pattern can be achieved through libraries, service meshes, or middleware. Libraries offer simplicity and easy integration but lack scalability and compatibility across languages. Service meshes bring centralized management but introduce complexity and are not suitable for browser-rendered code. Middleware, implemented as edge compute on a CDN, offers simplicity, reusability, and ease of deployment with minimized latency. Both Cloudflare and Amazon offer suitable platforms for deploying circuit breaker middleware, each with their own advantages and considerations. Campion, a circuit breaker middleware, can be deployed on either platform, providing efficient failure handling for externally exposed endpoints and web apps. Campion is designed to be deployed on either Cloudflare's CDN, known as CloudFront, or AWS's CDN, called Lambda Edge. If you are a Campion user wondering which option to choose for deploying your circuit breaker, let's briefly discuss each one.

Cloudflare Workers is an incredible platform for deploying Campion. In our tests, it proved to be faster than the Amazon offering, with an average of 28% faster response times. If you already have an AWS account, Lambda Edge is also a great option. However, it's important to note that AWS's CDN takes longer to replicate than Cloudflare's. This means deploying Campion on AWS can take up to 30 minutes before the code rolls out to all the CDN servers. Similarly, deleting Campion from Lambda Edge can take several hours. Despite these limitations, Campion works great on Lambda Edge, and requests routed through it are slightly slower than those routed through Cloudflare. For some users, this might be an acceptable trade-off to stay within the Amazon ecosystem.

Now let's take a closer look at the two architectures. When Campion is deployed on Cloudflare Workers, it operates as a worker and checks the worker's key-value (KV) store to determine the circuit state before routing the request. The KV store is a fast and distributed CDN database provided by Cloudflare. Stats such as failure and speed of requests are also stored in the KV store. On the other hand, when Campion is deployed on AWS Lambda Edge, it resides in a Lambda function deployed to the CloudFront CDN. In this case, the configuration logs and circuit state are stored in DynamoDB, Amazon's cloud database. Similarly, all stats are kept in a database for easy access by the user.

Before diving into the details of Campion, let's understand where it fits into the circuit breaking landscape. Libraries provide a feature-specific way to implement circuit breaking without adding new infrastructure. However, they can become difficult to manage as the code base grows and services are written in different languages. Service meshes, on the other hand, bundle in a lot of additional functionality that not everyone needs or wants, and can be challenging to install. Campion fills the gap between libraries and service meshes. It offers a centralized place to manage circuit breaking settings and endpoints without adding complexity. It works with any language and is suitable for front-end web apps and external APIs. However, it may not be the right fit for protecting internal private services.

Now, let's take a closer look at Campion. It comes with a command-line tool and an in-browser front-end app. With a single command, you can easily deploy a circuit breaker and get started within seconds. Campion provides a central place to manage all your circuit breaker settings and endpoints, and offers insightful metrics about the status of your services. To set up a circuit breaker, simply use the "campion add" or "campion aws add" command. This will guide you through the process of adding a new service, where you enter the URL and name of the service, and answer specific questions to fine-tune your circuit breaker. If you're unsure, accepting the default settings is also an option. The process completes within seconds, and you are given a new URL to add to your code for circuit breaking protection.

Campion's circuit breaker works by blocking traffic to a service that is overloaded or not functioning properly. Let's see this in action. In a side-by-side example, we have two instances of the same service, but only the one protected by Campion is functional. When a request is made to the unprotected service, the terminal freezes while waiting for a response. However, the protected service instantly returns a failure response, indicating that the circuit is open and traffic is being deflected. It's important to note that Campion works with all request methods, including GET, POST, PUT, and DELETE.

To further demonstrate Campion's functionality, let's protect a public API. In this case, we'll use the "get restrooms list" service from Refuge Restrooms, which provides location data for safe public restroom access for transgender people. Initially, the circuit breaker is closed, meaning the service is available without interference. However, we can test the circuit breaker by manually flipping it to open, half-open, or forced open. When the circuit is opened, all traffic is blocked for a defined interval, such as 10 seconds. Afterward, the circuit breaker automatically flips to half-open, allowing a percentage of traffic through. Finally, it flips back to closed when the service is deemed healthy. The dashboard provides a convenient way to monitor the status of all services and their respective circuit breakers.

Campion offers extensive configuration options to fine-tune the behavior of the circuit breaker for each service. These options can have a significant impact on the utilization of your services based on the number of requests received. To simplify the configuration process, Campion provides an intuitive dashboard where users can easily view and adjust settings. Additionally, Campion captures metrics that help in assessing the health and status of endpoints. By timing the duration of requests and monitoring the health of destination services, it provides valuable insights to ensure the stability and reliability of your services.

In conclusion, Campion is a middleware circuit breaker deployed at the edge, with a command-line tool and a front-end app. It offers a centralized solution for managing circuit breaker settings, endpoints, and metrics. While it adds some infrastructure to the system, it is much less complex than service meshes and more comprehensive than libraries. The option to deploy on either Cloudflare Workers or AWS Lambda Edge provides flexibility to fit different use cases. Given more time, further improvements and features could be added to the front-end app, including email notifications and enhanced metrics storage. We hope this gives you a comprehensive understanding of Campion, and we are happy to answer any additional questions. The project took a bit longer than expected, but it was the fastest option and it worked out in the end. That was my perspective on the project, but I would like to hear what you guys think. One way we tried to address the eventual consistency issue was by storing all our data in keys using Cloudflare. Instead of querying the values directly, we stored the data as a JSON object and accessed it through the keys. This way, we only had to grab the set of keys and filter it for the desired data, eliminating multiple queries.

Rodney asked a great question about handling requests that naturally take a long time. In Campion, we provide granular control for each service. If a service is known to take a longer time, you can set the timeout setting accordingly, giving it more time to respond. This configuration can be done on a service-by-service basis, as some services inherently take longer than others.

Terrell asked if there are options for applying load balancing and circuit breakers together or separately. It really depends on the specific use case. For example, if you're using Campion with front-end rendered code that interacts with an API, you may find that certain load balancers or API gateways already offer circuit-breaking functionality. So, you can use them together. However, it ultimately depends on the architecture of your system and how you have everything set up.

Ezra inquired about customizing the response for a circuit breaker status. At the moment, the response sent by Campion if a service is down is not customizable. It is hardcoded. However, it would be a great feature to add customization options in the future.

Cody wanted to know if they are required to use either AWS or Cloudflare for both their CDN and edge functions or if they could use Cloudflare Workers with AWS CloudFront. In order to use Campion, you need to provide an endpoint and receive back an endpoint. You can inject this endpoint anywhere in your code. Mixing Cloudflare Workers and AWS CloudFront might be a bit complicated because CloudFront is mainly designed to work with Lambda at Edge.

John was curious about the development timeline of the platform. We spent around three to four months developing the platform, including the research phase before coding began.

A question came up about how Shopify solved the problem they had with Redis. Although we didn't cover it in the presentation, Shopify actually developed their own circuit breaker called Semyon. It's a Ruby-based circuit breaker that they open-sourced on GitHub. It would be worth checking out if you're interested.

Ben asked about the limitations of Campaign compared to other similar services. One natural limitation is that Campaign adds latency to requests. However, by moving it to the edge, we have mitigated that to some extent. We aimed to emulate the metrics and central configuration features of Hystrix, Netflix's circuit breaker library. Although we got close, their GUI is more advanced, and we would like to incorporate similar capabilities in the future.

Janae asked about the testing process for the platform. We used a tool called Artillery to make thousands of requests to servers globally. We tested both Cloudflare's circuit breaker implementation and AWS's implementation. By making these extensive requests, we ensured that the data remained consistent throughout.

Vajid inquired about the target market for the service and whether we planned to monetize it. We believe this is an ideal solution for small companies, especially those that haven't focused on failure prevention. Many companies are unaware of these issues, and we aim to provide them with a simple solution. As of now, we plan to keep Campion open-source.

Benjamin asked about any issues or key learnings from adapting Campion to support different languages. We don't necessarily support different languages since Campion is inserted into the request-response cycle. There is no compatibility issue between languages. However, Campion currently only works with HTTP and HTTPS protocols, and we haven't developed support for other internal protocols.

Rodney asked about security concerns and how we addressed them. Using Cloudflare and AWS handles the majority of security concerns. As a reverse proxy, Campion sits between the request and response cycle without processing the requested information. Additionally, using HTTPS ensures encrypted communication, providing an added layer of security.

John asked about the languages and tools we used to develop the platform. We built the core product using JavaScript, and the CLI tool and front-end utilized React and Redux. JavaScript was a great choice for us due to its versatility and the availability of helpful tools and libraries.

A question was raised about the most difficult part of building the platform. It was an interesting journey overall, with some challenges along the way. Implementing the circuit-breaking functionality and the logic behind it wasn't overly difficult once we understood the fundamentals. However, the post method, particularly passing data between the circuit breaker and the service, proved to be more complex. Our team spent a considerable amount of time working on formulating headers and adjusting the body of requests and responses.

Ezra asked if Campion is open source. Yes, Campion is open source, and all you need is an AWS or Cloudflare account to use it.

We appreciate everyone who joined us for the presentation, and we hope you found it informative. If you have any more questions, feel free to ask. Thank you all for your time and interest in Campion. Once we settled on the campaign concept, the build process progressed quickly. We dedicated a substantial number of hours to bring it to life, although the initial process was complex. There is an inclination to complicate things by adding unnecessary features. Both Arthur and Gabe are being modest because implementing the GET function was relatively simple. However, the POST method, which involved transferring data from our circuit breaker to the service and then receiving and modifying the data, proved to be surprisingly intricate. Arthur and Gabe invested a significant amount of time in figuring out how to formulate and adjust the headers and body of each request and response for the PUT and POST methods.

Ezra asked if Campaign is open source, and the answer is yes. As a matter of fact, we are open source. We intend to obtain open source licensing so that anyone with an AWS or Cloudflare account can use Campaign. We handle all the background work, making it effortless for users. However, it's worth noting that we may decide to monetize it in the future.

Before we conclude, I would like to express my gratitude to everyone who attended and made this event a success. Thank you all for coming; your presence has been truly appreciated.