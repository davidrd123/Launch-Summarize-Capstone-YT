In this video, we will be discussing Backpack, which is a portable backend for web applications. We will cover the concept of a backend as a service (BaaS), the architecture of the Backpack core application and API, how we packaged it into a single instance using containers, the transition to a multi-instance architecture, and finally, how we built an admin panel for managing Backpacks.

Let's start by understanding what a backend as a service is. In an application, there are two main parts: the frontend, which is what users interact with, and the backend, which handles business logic and data storage. The backend stores the information needed for the frontend and handles any changes to that information based on user interactions. This can include tasks such as user authentication, database management, and file storage.

Developing a backend involves dealing with various complexities, such as hosting, SSL certificates, data security, server configuration, and real-time connections. A backend developer needs to ensure that all these features are seamlessly integrated with each other and with the frontend code. This can be quite overwhelming and time-consuming.

To simplify this process, a backend as a service (BaaS) abstracts away much of the complexity involved. It encapsulates all the backend functionality and provides a simplified API interface for frontend developers to interact with. Instead of building out the backend functionality themselves, developers can interact with it through the API provided by the BaaS.

Now let's delve deeper into the concept of a BaaS and how it works. The typical architecture of a BaaS-backed application consists of a frontend that handles user interface concerns and a backend that provides basic functionality. These two parts interact through API endpoints exposed by the backend. Frontend applications usually communicate with the BaaS using an SDK (software development kit) specific to their platform. For example, a web application would use a JavaScript library as the SDK. The SDK provides convenient methods for interacting with the backend API and accessing services such as database storage and authentication.

Using a BaaS has several advantages. It simplifies frontend development, reduces development time, and minimizes complexity for frontend developers. BaaS is particularly useful in scenarios where developers want a development sandbox for experimenting with new ideas, when building a minimum viable product, or for frontend developers who don't have the necessary backend skills.

However, using a BaaS also has its trade-offs. Although it simplifies development, it comes at the cost of reduced control over the server and backend functionality. Different types of BaaS providers offer varying levels of control. Infrastructure as a service gives developers complete control over the server and operating system. Platform as a service removes the need to manage the operating system but still requires developers to handle the application code. Back end as a service takes abstraction a step further, with developers only responsible for the frontend code and all backend services being managed by the provider.

Popular BaaS providers include Google's Firebase, Amazon's Amplify, and several smaller providers. However, existing BaaS solutions often suffer from proprietary limitations, lack of customization, and potential vendor lock-in if the application outgrows the service. To overcome these limitations, we developed Backpack as a portable backend as a service. It simplifies backend functionality, is easy to configure and use like proprietary solutions, but also provides ownership and control over data by being open source and allowing hosting on any desired platform.

We specifically designed Backpack for development teams that need a sandbox environment for frontend developers. It enables a single system administrator to set up and monitor the system with minimal effort. Frontend developers can then log on to the Backpack system, create their own Backpack backend, and start working on their applications.

Now let's move on to an overview of the Backpack core app and how we built a generic backend. The Backpack core app consists of several components encapsulated within the green area in the diagram. The frontend application uses the Backpack SDK to interact with the backend through HTTP or websockets. The backend exposes API endpoints that provide access to typical backend core services.

User authentication is a fundamental feature in any application, allowing users to register, log in, and log out securely. Implementing authentication systems is complex, but the Backpack SDK simplifies this process by providing methods to easily incorporate user authentication into frontend apps. Each Backpack instance comes with a preconfigured user's collection, and frontend developers can utilize the SDK to handle user authentication seamlessly.

File storage is another essential feature for many applications, but existing solutions often rely on cloud storage services like Amazon S3. To provide users with more control over their data, we developed our own self-contained storage system. We store files in the local file system and metadata about each file in a database collection. This approach offers faster file access while maintaining data integrity.

Data persistence is also a crucial aspect of a backend. To address this, we needed to create a generic data management system that is highly flexible since we do not know how developers will want to structure and model their data. We chose to use a NoSQL database, specifically MongoDB, to provide the required flexibility. This allows frontend developers to create schema-less data collections without any constraints. However, for certain collections like user information or storage metadata, we utilize the JavaScript Mongoose module to enforce predefined schemas.

Next, let's discuss routes, which determine how resources are accessed in an application. Normally, routes are hard-coded based on predefined collection names. However, since in Backpack, users can dynamically create collections at runtime, we needed a way to handle newly created collections. To achieve this, when the Backpack core app starts, it dynamically creates routes based on the existing collections in the database. If a new collection is created while the application is running, the necessary routes for that collection are generated on-the-fly.

Real-time functionality is increasingly important for many applications. It enables instant communication between a server and connected devices, allowing for applications such as chat apps, multiplayer games, social feeds, and collaborative editing apps. Backpack leverages websockets to provide real-time communication. Websockets, unlike traditional HTTP requests, facilitate bidirectional communication, persistent connections, and near real-time data transmission.

Working with websockets can be challenging, as it requires dealing with various considerations, such as setting up a websocket server, handling authentication, broadcasting messages, and managing user channels. On the frontend, developers also need to handle websocket connection establishment and potential failures, as well as defining the desired websocket communication behavior. To simplify this process, we provide backend developers with a comprehensive, generic SDK that handles most of the complexities involved in websocket communication.

Now that we have covered the architecture and core features of Backpack, let's address the challenge of packaging the application and its dependencies for easy deployment. Setting up a backend instance with all the necessary components can be cumbersome and time-consuming. We needed a way to simplify this process, and the answer came in the form of containers, specifically Docker containers.

Containers enable us to package the Backpack express API, database, and their dependencies in a self-contained and portable manner. With containers, there is no need to worry about installing specific versions of Node.js or managing environment variables. All the necessary components and configurations can be bundled together, making it incredibly easy to deploy Backpack instances anywhere.

By leveraging containers, we ensure that Backpack can be deployed consistently across different environments, regardless of the underlying infrastructure. This portability, combined with the simplicity of setup and configuration, makes Backpack an ideal choice for frontend developers who need a sandbox environment without the hassle of complex backend setup.

In summary, Backpack is a portable backend as a service solution that simplifies backend functionality while providing ownership and control over data. It abstracts away the complexities of backend development, making it easier for frontend developers to focus on building their applications. The Backpack core app encompasses user authentication, file storage, data persistence, dynamic route generation, and generic real-time functionality. Leveraging containers, Backpack can be easily packaged and deployed in any desired environment, enabling rapid application development with minimal backend effort. The provided diagram demonstrates the HTTP request-response cycle, which is always initiated on the client side in a web browser. The client sends a request to the server, which processes the request and responds with a response that is sent back to the client. On the right-hand side of the diagram, there is a websocket connection shown. To establish a websocket connection, the client sends an HTTP GET request to upgrade the connection to websocket. The server approves the request, and the websocket connection is established. 

One key difference between websockets and node.js is that with HTTP, the server cannot send information to the client unless the client requests it. In contrast, with websockets, both the client and the server can freely send information. 

Front-end developers typically work with websockets in various ways on the back end. They need to consider setting up a websocket server, handling authentication via websockets, broadcasting messages or HTTP responses, creating and managing user channels, and interacting with the database. On the front end, there are tasks such as creating the initial websocket connection, handling connection failures, and coding specific functionalities for the websocket connection. All of these tasks require significant effort and coding work. 

However, when using websockets with Backpack, a backend framework, it becomes much easier for front-end developers. Backpack takes care of most of the backend tasks, including websocket server setup, authentication, channel management, and message broadcasting. On the front end, front-end developers are provided with a comprehensive set of generic SDK (Software Development Kit) methods that allow them to perform various tasks with websockets. 

The video then explains how they designed the backend API for Backpack. However, they realized that setting up the backend architecture for each deployment would be cumbersome and time-consuming. It would involve installing Redis for session caching, ensuring the correct version of Node.js is installed for running the API, and setting up environment variables for API configurations. To make it easier for front-end developers, they needed a way to package the Backpack instance and simplify its deployment. 

The solution they found was to use containers, which are tools for packaging applications along with their dependencies. Containers provide an isolated and consistent environment, independent of the host system. While they make use of the host operating system for kernel tasks, they are lighter weight than virtual machines. They offer a way to bundle the required dependencies, including Redis and the API code, into a single package. 

They provide a comparison between virtual machines and containerized applications. Virtual machines require a hypervisor and a full guest operating system, leading to higher resource consumption and slower startup times. In contrast, containerized applications only require the host operating system and leverage its resources, resulting in faster startup times and lower resource consumption. 

They explain how they containerized Backpack by separating its components into different containers. Each container runs a single process: the Express API, the server for database interactions, the Redis database for session caching, and the Nginx web server for front-end hosting. They use Docker networking capabilities to connect these containers and map their ports to the host machine. 

The video then discusses the need to support multiple Backpack instances within a single system. They explore two options: multi-tenant architecture and multi-instance architecture. 

In a multi-tenant architecture, multiple applications use a single Backpack instance. The applications send requests to the shared Express API, and the server handles data interactions for all applications. The Redis session cache and Nginx web server are also shared. Routing becomes challenging as requests need to be identified and handled based on the application they come from. Scaling is complex and requires adding specific endpoints for administrative tasks, as well as managing the shared database.

In a multi-instance architecture, each application has its own separate Backpack stack, including an independent Express API, server, Redis cache, and Nginx web server. Routing requests to the appropriate Backpack instance requires a reverse proxy. An admin panel is needed to manage the separate instances. Scaling is simpler as each new application can have its own Backpack instance, but managing containers across multiple servers becomes necessary.

After comparing the two architectures, the team decides to pursue a multi-instance architecture due to its benefits, even though it introduces additional challenges. They acknowledge that container orchestration is required to manage multiple containers across servers. 

The video concludes by highlighting the need to package the existing Backpack system into a single logical unit for easy deployment. It also mentions that routing and scaling will be further addressed in more detail in subsequent discussion. In order to scale our backpack application, we need to address a few challenges. Currently, our single-instance deployment works well, but scaling it becomes more complex. To overcome this, we want to package our system into a single unit that can be easily spun up or torn down on demand. However, this introduces new problems, such as how to handle routing in a system with multiple backpacks running different web applications. Additionally, scaling vertically by upgrading the server becomes expensive, so we aim to scale horizontally by running backpacks on multiple servers. Ultimately, we want to distribute individual components of each backpack across different servers for better resource management. Achieving this requires coordination between containers on different machines and poses network communication challenges. So, our to-do list includes spinning up and tearing down individual backpack instances, deploying instances across multiple servers, managing communication between the servers, implementing routing for requests, simplifying deployment, and providing front-end developers with administrative access to individual backpacks.

To address the first item on our to-do list, we decided to use a container orchestrator like Docker Swarm. A container orchestrator is a tool that manages, scales, and maintains containerized applications and their communication networks. We chose Docker Swarm because it is built into Docker by default and simplifies our system by eliminating dependencies. Docker Swarm allows us to define a containerized application as a stack using a stack file, which is similar to the compose file. The stack file specifies the services, their deployment details, environment variables, and network configurations. By combining the stack file with Docker's command-line interface, we can package our backpack stacks as containerized applications and use Docker Swarm to spin them up or tear them down as needed.

In order to deploy our system across multiple servers, we rely on Docker Swarm's concept of nodes. Nodes can be either manager nodes or worker nodes. Manager nodes handle deployment and management tasks, while worker nodes run the services. With Docker Swarm, the manager nodes determine where to deploy each service defined in the stack file, and overlay networks are used to enable communication between services running on different nodes. This allows us to treat each containerized application as a cohesive logical unit, regardless of their physical location. By using Docker Swarm and its worker and manager nodes, we have now addressed the second item on our to-do list.

Routing requests from the internet to the correct backpack presents a challenge. In a single-instance version of backpack, we exposed ports to the host machine, enabling requests to reach the appropriate container. In Docker Swarm, we need a way to abstract the port exposure across multiple servers. Docker Swarm offers a solution for this called the ingress routing mesh. It is a virtual network that spans the entire Swarm system and allows services to be published to a single network instead of exposing ports on individual machines. This allows Swarm to route requests to the correct container, regardless of which server is hit by the request.

However, when we have multiple backpack instances, we encounter a port conflict issue. Multiple applications cannot listen to the same port. To overcome this, we introduce a reverse proxy. Instead of exposing services from each backpack through the ingress routing mesh, we put a reverse proxy in front of all backpacks. The reverse proxy listens for requests through the routing mesh and routes them to the appropriate backpack. To facilitate communication between the reverse proxy and the backpack stacks, we introduce another overlay network.

Handling service discovery for new backpack instances is another challenge. Traditional reverse proxies require configuration files with pre-specified rules. To address this, we need a mechanism to inform the reverse proxy about new backpack instances. For example, if a developer spins up a new backpack, the reverse proxy needs to know that it should handle requests for that backpack. One possible solution is using subdomains in the routing. When the Swarm receives a request for a specific subdomain, it can redirect the request to the appropriate backpack.

To summarize, we have tackled the challenge of spinning up and tearing down backpack instances using Docker Swarm as our container orchestrator. With the help of workers and manager nodes, we can now deploy our system across multiple servers. The overlay network allows us to manage communication between services running on different nodes. By implementing a reverse proxy and an additional overlay network, we can handle routing requests from the internet to the correct backpack. We have also addressed the issue of service discovery by using subdomains. With these solutions, we are one step closer to building an effective system that can scale horizontally across multiple servers. The project aims to address the challenge of exposing the port to a specific server, as the services running on that server are unknown. Instead of individually exposing each service, a single network, known as the ingress routing mesh, is created on top of the entire swarm. When a port is published, it is done within this network, allowing for routing through the swarm. However, this approach does not support multiple backpack instances, as conflicts arise when multiple processes listen to requests from the same port. To overcome this problem, a reverse proxy is introduced. This reverse proxy sits in front of all backpacks and routes requests through the ingress routing mesh to the appropriate backpack. Another overlay network is introduced to establish communication between the reverse proxy and the backpack stacks.

Service discovery becomes a challenge when a new backpack instance is created. The reverse proxy needs to be aware of the new backpack and route requests appropriately. Typically, reverse proxies use configuration files that require restarting the proxy to pick up changes. However, this is not an ideal solution as it leads to downtime. To address this, a reverse proxy called Traffic is introduced. Traffic is designed specifically for containerized applications and can obtain updates from Docker Swarm about running stacks dynamically. This allows for routing requests to the correct stack without restarting the proxy or manually managing configuration files.

SSL encryption is also important to ensure secure communication, especially when handling sensitive information like user passwords. Traffic simplifies the process by automatically requesting SSL certificates based on its configuration, regardless of the specific hosting details or domain names used.

The next step involves making it easy to create and tear down backpack instances, as well as providing front-end developers with a user-friendly way to manage their individual backpacks. To achieve this, an admin panel is developed. The panel allows developers to register and sign in as administrators, spin up backpack stacks with user-friendly names, and easily remove backpack stacks when no longer needed. The admin panel communicates with Docker through its Command Line Interface (CLI) and provides real-time feedback by piping the output from CLI commands back to the front end.

The admin panel is built as a separate stack, comprising an NGINX-hosted front end, an Express API serving as the back end for the admin panel, a Postgres database to store admin user information, and a Redis instance for session storage. Traffic sits at the front of the swarm, handling requests from the internet. It is joined to the admin panel and routes requests to the appropriate backpack stack. The admin panel is connected to each backpack stack via an overlay network, allowing for interaction with the application data and user management functionalities.

To facilitate user management tasks, the Express API in each backpack stack is configured to listen to requests from two ports. One port is connected to Traffic for everyday backpack usage, while the other port is only exposed to the admin backend. When an admin wants to view the data in their backpack, the backend establishes a WebSocket connection with the specific backpack and proxies the connection back to the admin panel. This persistent connection enables additional user management tasks to be handled by administrators.

In conclusion, the project successfully creates a portable backend, known as Backpack, capable of supporting multiple instances from a single deployment. With the integration of Traffic as a reverse proxy and the development of an admin panel, the system provides secure and user-friendly management of backpack instances. The project is licensed under the MIT license, meaning it is open source but not subject to the GPL. The chosen architecture for securing the database and ensuring a single interaction with it is through the application code. To maintain this, an alternative method was needed for the admin panel to communicate with the Express API. Ultimately, a new architecture was created where the admin panel is connected to an overlay network, which in turn connects to each Express API running on each backpack.

To ensure security, each backpack now listens to requests on two ports. The first port is connected to normal backpack usage and allows traffic to route requests. The second port is only exposed to the admin backend. When an admin wants to review the data in their backpack, the backend sends that request to the specific backpack. A WebSocket connection is then created, which gets proxied back to the admin. This persistent connection allows the admin to access additional user management tasks. With this approach, the last architectural to-do on the system was completed, allowing each backpack to be ready for deployment.

This architecture enables Backpack to function as a portable backend that can support multiple backends from one deployment.

Thank you for joining us for this overview. We are now open for questions.

One question raised was if there was consideration given to using a multi-tenant architecture by creating instances of an admin panel to separate concerns. This was indeed considered, but concern over isolation issues led to exploring alternative architectures from the beginning. The multi-tenant architecture, with one Express API and one server with a database for each backend, raised enough isolation concerns to warrant exploring a different approach initially.

Regarding licensing, all capstone projects are licensed under the MIT license, making them open-source but not covered by the GPL.

Another question asked whether the Backpack API allows developers to configure the resources for each container in an instance. This ability to allocate more memory or CPU to specific MongoDB or Redis containers is part of the future work for the project. Given that Docker Swarm is being used and each service runs in its own container, it wouldn't be too complex to scale out specific instances across multiple services based on demand. However, this feature has not been implemented in the initial version of the project.

Thank you for your time and we appreciate your participation. If you have any further questions, please feel free to reach out on Slack. Enjoy your Friday!