okay i think we'll get started uh we're here to talk to you today about backpack backpack is a portable back end for your web applications so we're going to start today by talking about what a back end as a service is then we'll move on to talking about the backpack core application and api and then from there we'll talk about how we packaged up that single backpack into a single instance architecture with containers then we'll move on to talking about how we built out to a multi-instance architecture and from there how we build an admin panel to work with our backpacks there are two parts to an application there's the front end part which is the part of the website that you can see and you can interact with there's a back end that handles business logic and data the back end stores the information that you need to display on the front end and handles changes in that information in response to user interactions from the front end so what kind of data and business logic are we talking about well a front end for example might prompt a user to enter a username and password that information would be passed to the backend or it's checked against stored data to determine whether or not a user is authorized to access the particular web page another example would be database management for example in a chat app a frontend might request the last 20 messages for display the backend would look into the database and return those 20 messages along with any necessary metadata about which user added the message or when it was added for example the frontend would use the metadata to display it in the correct order a final example would be for file storage so in a social media website for example you might want to allow users to upload a photo so the front end could take that photo send it to the back end with a request to save it and the back end could store it in the file storage or whatever storage mechanism they're using and send a message back with a link so that the photo could be retrieved for future use so those examples don't cover all of the complexity associated with building out a back end in addition a back-end developer is going to need to handle a whole bunch of other issues like hosting ssl certificates data security server configuration real-time connections and they're going to need to make sure that these features are all seamlessly integrated with one another and with the front-end code that's a lot to deal with it'd be nice to simplify things a bit so that the front end developer has less to worry about and that's what a backend is a service or a baz is intended to do a baz is an application that provides an abstraction over all of the complexity of the back end it encapsulates all of that functionality and provides a much simpler api interface for a front-end developer to interact with with a baz instead of building out back-end functionality you interact with back-end functionality via that api so let's dig a little deeper into the concept of a baz and how it works so here you see a diagram of the typical architecture of a bazbacked application you have a front-end that handles ui concerns it could be on any platform ios android or web application on the back end you have all the basic functionality that you need these two parts interact via the api endpoints that the back end exposes to interact with the services it provides typically frontend applications communicate with the baz using an sdk that's a software development kit this can be written in the language of the front end platform so in the case of a web application it would be a javascript library um the sdk sdk provides convenience methods for interacting with the backend api and it exposes that functionality provided by the database storage authentication or other services so why would we use a baz it facilitates front end development it reduces development time and it reduces the com complexity at least from the perspective of the front-end developer um so it would be good to use in a case where you want a development sandbox for trying out new ideas where you're developing a minimum viable product or for front-end developers who just don't want to worry about back-end concerns or maybe don't have the skill to build out the functionality they need so what are the trade-offs in using a badge because it's not for every situation it does simplify things but that comes at a cost so generally with these kinds of services they provide an abstraction over the server in the back end and the greater that abstraction the less control you have over the server and backing functionality but the faster you can build your app so for example with infrastructure as a service um the developer is responsible from for everything from the operating system on up that makes infrastructure as a service very low in abstraction very high on control with platform as a service the service handles the operating system and some other basic things as well as deployment but the is responsible for all of the application code for both the front end and the back end so this moves us up in terms of um abstraction but down in terms of the level of control compared to infrastructure as a service um finally with back end as a service we take that abstraction one step further so now the developer is only responsible for providing front-end code so all of the the services um that the pas provides are also made available so there's much less for the developer to be responsible for so one of the prime examples of a baz is google's firebase um it's maybe the most popular baz right now there's also amazon's amplify and there's a bunch of smaller providers as well so why would we not use an existing baz well firebase amplify and all of the smaller providers all have the same sort of problem of being proprietary so they're not open source they're not customizable um and you have the problem of vendor lock-in if your application outgrows the bath service so we built backpack to address those concerns backpack is a portable backend as a service it simplifies back-end functionality it's easy to configure and use so just like the proprietary versions but in addition it provides ownership and control over your data it's open source and you can host it wherever you like we built backpack for use on a development team that needs a sandbox environment for front-end developers so um backpack would be used in a situation where a single system administrator could set up and monitor the system with relatively little effort and then that system would be available provide easy access for one or more front-end developers who could then log on to the backpack system create their own backpack back end and then they would be able to work on whatever application they're they're trying out thanks leela hi my name is holden hinkle and now we're going to talk about the backpack core app and how we built a generic back end the backpack core app consists of several parts which are all encapsulated in the green area shown here the front end application uses the backpack sdk to interact with the back end either through http or websockets and the back end exposes api endpoints that provide access to typical backend core services which we'll discuss in the next few slides the backpack sdk is a javascript library with convenience methods for interacting with the back end api this gives a front-end developer instant access to backpacks core back-end features which include user authentication file storage data persistence and real-time functionality every application needs a robust user authentication system so users can safely register log in and log out of an application authentication systems are complex and difficult to build the backpack sdk provides methods to easily add user authentication to a front-end app each backpack comes with a user's collection in the database ready to go after a user of a front-end app logs in and authenticates the user receives a cookie with a session id that is tracked in a redis instance storage is another feature feature that is common to applications usually applications use a cloud storage system like amazon's s3 one of the reasons we built backpack is because we want users to own their own data and not have to use cloud storage so we knew we had to build our own self-contained storage system there are two common approaches to this the first is to store files in a database this provides data integrity however accessing files can be slow the second approach is to use a file system storing and retrieving files in a file system is faster than using a database but maintaining data integrity is more difficult because metadata can't be stored with the file we considered these tradeoffs and decided to go with the latter approach we store files in the local file system and metadata for each file in a database collection the next component we'll talk about is data persistence this was tricky because we needed to create a generic data management system that is super flexible because we don't know what kinds of data a front-end developer will want to store or how they'll want to model it backpack uses a nosql database because it provides that required flexibility for our use case using mongodb front-end developers can create schema-less data collections so they are not constrained in any way however sometimes a schema is necessary the javascript mongoose module allows us to enforce a predefined schema for the user's collection which stores user names and passwords and the storage metadata collection that brings us to our next topic routes how do we create routes for accessing resources that don't exist yet and are unknown we needed to create a way for front-end developers to access the data in the collections they create normally these routes would be hard-coded you'd have a name for a collection of data like photos for example and define your routes based on that collection name so what happens when a front-end developer creates a new collection like cars we generate all of our routes dynamically when back when the backpack core app spins up it iterates over all of the collections in the database and dynamically creates the routes what happens if the front-end developer creates a new collection while the application is running though that's not a problem the routes for the new collection are created at run time the next challenge was to create a generic was to create generic real time functionality but first what's real time real time refers to the ability to send information to a server or from a server to other connected devices the moment something happens real-time communication is ultra fast you can send and receive data at the same time and you could receive data that wasn't explicitly requested why why would you need real-time communication though real-time communication is used in all kinds of applications from chat apps to multi-player games to social feeds and collaborative editing apps backpack uses web sockets for real-time communication websockets are like http in that they're both the communication protocol over tcp ip however unlike http websockets provide full bidirectional communication between a server and a client the connection is persistent and stateful and communication can happen in near real time because the connection doesn't have to be re-established for each request this this diagram illustrates the differences between http and web sockets the left hand side of the diagram illustrates what's known as the http request response cycle the cycle is always initiated on the client side in a web browser the client makes a request which is sent to the server and the server processes the request and answers with the response that is sent back to the client the right hand side of the diagram illustrates a websocket connection to create a websocket connection the client first sends an http get request to upgrade to a websocket connection then the server approves the request and the websocket connection is established the key difference to node is that with http information can't be sent from the server to the client unless the client first requests it with websockets on the other hand both the client and the server can freely send information so how do front-end developers normally work with websockets on the back end there are all kinds of things that need to be considered for example how do you set up a websocket server how do you deal with authentication via websockets how do you broadcast messages or http responses and how can you create and manage user channels and lastly how can you interact with the database on the front end there are other things that need to be set up for example how do you create the initial websocket connection how do you deal with connection failures and then for each thing that you need to have happen over a websocket connection you need to code those things up yourself and all of this is a lot of work so how do front-end developers use web sockets with backpack it's much easier on the back end just about everything is taken care of for you the websocket server setup authentication just works channels are available and messages are broadcast to the connections that should receive them on the front end you're given a set of comprehensive generic sdk methods so you can do just about anything you need to do for websockets okay so we've explained how we put together our backend api but this architecture would be a pain to set up every time you needed a back end you'd have to install you'd have to install redis for session caches you'd have to make sure you installed the right version of node to run the api you'd have to set up environment variables to tell the api how to interact with and redis so um this isn't very helpful um if we're trying to make it easy on the front developer so we needed a way to package our backpack instance and make it easy to use so our challenge is how do we package our express api database and dependencies for easy deployment wherever and the answer to this is containers so why would we use containers um well containers are a tool to package an application with its dependencies and so that means the and redis instances that we need get packaged up with the api code and can all be installed in one bundle um this provides containers provide an isolated consistent environment so consistent means that every time you load the container or the containerized environment you get all the dependencies and the correct versions and it's isolated in that it doesn't really interact with your host system so whatever version of node you're running in your host machine it doesn't matter you can run a different version of node inside the container environment it does make use of the host operating system for kernel tasks but otherwise it's independent from the host os so in this way it's similar to a virtual machine but it's lighter weight so here's a comparison between virtual machines and containerized applications so on the right you have the virtual machine so living on the host machine which is sort of represented by the yellow and the host machine's infrastructure is available there's a hypervisor that acts as an intermediary between the host machine and the virtual machines and then you see the virtual machines in the sort of beige color on top and inside each virtual machine you've got application code but you also have to install a full guest operating system on the left we've got containerized applications again they're living on the host machine they have the host machines infrastructure they also can make use of the host machine's operating system and we have the docker daemon installed to manage the containers then we have the containerized applications again in the beige boxes but you notice in this example we have only the application code running each container because each container can make use of the host operating system that means that containers use fewer system resources and they're faster to spin up which is great because part of what we want to be able to do is spin up a backpack on the fly so how do we go about containerizing backpack well containers are meant to run a single process per container so we're going to zoom in here on just the container operating in its host machine so again we have the infrastructure of the operating system and docker and then on top we have a single backpack instance so backpack has four components it has the express api that we explained earlier that handles the application code it has the server which handles database interactions which is the database for storing data and it has the redis database for session caching um so each of those is a separate process and then in addition we have an nginx web server that's kind of off on its own that handles front end hosting so each of these is a process each of these needs to run in its own container which means a single backpack instance requires at minimum or containers so here you can see the single backpack instance is sort of noted by the dotted lines which is a way we're going to i think we're going to use throughout the rest of the presentation so if we have all these containers living on the host machine that's great but they can't just live independent of one another they need to communicate so docker provides networking capabilities so we have a bridge network that we use to connect the express api to the bongo server and the redis session cache um notice the nginx server isn't connected over that bridge network because it doesn't actually need to interact with any of the other components also docker allows us to map container ports to host machine ports so we have a port exposed from the express api and from the nginx web server to listen to requests from the internet so how do we do all this how do we coordinate these services docker provides what's called a compose file it's a basically a configuration file where you provide service specifications for application components so um we have four components in our system that we've outlined um and those four together make up a single application so the service specifications are how those services are going to interact together so the docker compose file allows us to name the services and then explain uh explicitly how we're going to expose ports what volumes we need for storage environment variables that are necessary and it also allows us to define the interactions over the network that these different components are going to use so that's great um we have a single working containerized backpack instance um but our actual goal is to support multiple backpacks so we're going to talk about how we went from the single instance to a multi-instance architecture so the question at this point becomes how do we support easy deployment of multiple backpack instances within a single backpack system we saw two options for building out our system this way one is a multi-tenant architecture where a single backpack instance provides back-end functionality for multiple applications um and we also thought maybe we could use a multi-instance architecture where we have a different backpack stack or different backpack instance for each application using the system so we're going to talk about both of these in a little more depth this diagram represents the multi-tenant architecture that we considered so in this diagram you can see on the top there are three applications using our backpack system and those three applications are all interacting with a single backpack instance again denoted by the dotted line so all three applications are sending requests to the same express api and then we're using a single server to handle data for those applications we also have a single regis redis cache redis session cache and we have a single nginx web server for hosting so in this setup routing is a little complicated because we're going to have to hit we're going to have to hit that api with requests from all three applications and then have a way of determining which application the request is coming from and how to handle the request accordingly so this would involve either handling it based on the url or with unique api keys but it would have to be written into the api code um we also have an issue where if we need to have administrative tasks for example the front-end developers who are working on these apps might need to inspect their data or create a new application in our system um we're going to have to add specific api endpoints to handle those applications those administrative tasks these new api endpoints are going to require their own authentication system and maybe even a new administrative database and when it comes to scaling because the express api is stateless we can scale horizontally if we need to grow  also supports horizontal scaling either by replication or by sharding so that was the multi-tenant option um but now let's consider the multi-instance architecture so in this setup again we've got three applications using our system but now we've got three independent backpack instances handling requests so one for each application that's using the system um so like app one is going to hit the system and all of its requests are going to be sent to the app one backpack instance which has its own api express api server its own server its own redis cache and its own nginx front end hosting um this is going to require though a reverse proxy to route request to the appropriate backpack stack because we can't handle that in the application code anymore we're also going to need an admin panel for managing the system since individual backpack stacks will need to be spun up and managed and that's going to require a network for handling communication between the new components in the system as far as scaling we can spin up a new backpack for each new application but we're going to run out of resources on our server at some point so we're going to need to a way to manage container containers across multiple servers so how do we decide let's look at these again so on the left we have the multi-tenant architecture where we have a single backpack instance handling requests from multiple applications um in this case we have a simpler architecture because we just have the single backpack instance but the downsides of that are that we have overloaded application code trying to handle routing and administrative tasks as well as handling back-end functionality we also don't have isolation between the applications using the system they're all being handled by the same database scaling is a little complicated in this architecture as well on the right we have the multi-instance architecture where each application using the system has its own backpack stack its own api express api and its own database this version offers full isolation so each the code for each application is completely independent of the other applications it's a little easier to scale because we just have to spin up a new backpack instance for each new application and it has uh it respects the separation of concerns so app code only handles back-end functionality it doesn't try to handle routing or administrative tasks but the the downside of that is that we need additional components so we have a reverse proxy for handling routing and the admin panel for administrative tasks the other downside of this model is that now we're going to need a container orchestrator to manage our containers okay thank you leela hi everyone my name is ito moskovic and i'm the third member of the backpack team so we felt that the costs involved with building a multi-instance system were well worth the benefits that lila just covered especially given the availability of existing networking and container orchestration tools so our challenge becomes how do we take our system from here and a quick review this is our single instance backpack deployment that we have so far there's the express api running on its own container we have our database and also the redis session store and those three different containers are all communicating with one another over a back end network and then we have this fourth container running nginx as a web server that can host static files for like a front-end website the api and nginx are both exposing ports so that they can handle requests that come from outside of the docker system so how do we go from here to here and what this is is one server that is now somehow running multiple backpacks on it and in order to get here we have to figure out how to basically package up the existing system into a single logical unit that we can then spin up on demand or tear down but once we figure that out that's going to introduce some additional problems that we'll have to solve in order to make this work for instance how would we deal with routing in a system like this this is a slightly more complicated problem that we'll deal with in a lot more detail but if we have multiple backpacks all exposing the same ports all running different web applications how do we make sure that the system routes requests to a specific application to the right backpack the other challenge that this introduces is how do we scale this out so eventually if we keep spinning up backpacks on one server it's going to run out of resources so there's two options typically for scaling and one is to just keep pouring money into the server and continue to upgrade it in a process called vertical scaling but that gets very expensive pretty quickly or the other alternative is to scale out horizontally instead and figure out some kind of way to run backpack across multiple servers and that's what we wanted to try to do here so this diagram is illustrating three different servers each of which is able to run multiple backpacks so if we want to get here this adds even more complexity to the routing issues that were introduced with the previous architecture but even this isn't exactly where we want to end up if we want to build an effectively an effective system to scale horizontally we'd like to try to get here where instead of deploying full backpack instances to the various servers that are a part of our system we'd like to be able to run individual services that are a part of each backpack across these different systems because they'll let us do some nice things such as maybe have one server that's a little bit beefier than the others maybe we can put all of our  services running for each of these different backpack instances on there because they might need more resources versus our nginx container which probably doesn't need quite as much power to be able to run effectively and we can just toast that somewhere else instead now getting to here where we now have multiple multiple servers hosting individual components of each backpack is only going to make our networking challenges even harder because now we somehow have to coordinate these containers across different machines so getting here introduces this kind of to-do list that we'll have to walk through in order to effectively build a system that looks like that last slide so we started off with figuring out how do we spin up and tear down individual backpack instances what we're calling stacks here once we figure that out can we then get to spinning up these instances across multiple servers and if we do that that's going to involve this other to-do of can we manage communication across these different servers once we handle that we have to go back to that routing issue and say how do we take a request that's coming from the internet from outside of the system and send it to the correct backpack within the system after we get all that done how can we make it easy to do all this so we don't want to reintroduce all of the complexity that we were hoping to solve with the system by making the deployment really difficult and then finally if we're going to split up these backpacks across multiple servers if there isn't going to be a centralized place where all the data for all of these different backpacks is going to live how do we give front-end developers access to administer their backpacks so to handle this first to-do spinning up and tearing down backpacks we relied on something called a container orchestrator and here's the definition of a container orchestrator from docker it's a tool to manage scale and maintain containerized applications and the networks they use to communicate so unfortunately this is one of those definitions that is kind of requires definitions for each word inside of it so we'll unpack this over the next few slides but before doing that um we chose to use docker swarm as our container orchestrator there are two popular ones dockers forum and kubernetes we chose swarm since it's built into docker by default so that helps us eliminate the dependency of our system it's a bit simpler to use than kubernetes but still handles everything that we would need from the previous slide so we thought it would be a good choice for our use case so jumping back into this definition the first part of this that we'll cover is what is a containerized application and in swarm terminology a containerized application is called a stack and the stack would represent each of the different components that need to be working together in order to have kind of a single fully running application like in this case it would be our api plus plus radius plus engine x and the back end network that they use to communicate we can define a stack using something called a stack file and this is really similar to the compose file that lila introduced but it is structured a little bit differently for swarm instead and the stack file again defines what are the different components so which services are going to be a part of this application where each service kind of represents one of these containers how should that service be deployed so do we want multiple instances of each service running what environment variables should we pass into these which networks should each service be joined to you all of these tricky installation and deployment details can be defined using the stack file we can take the stack file and pass that in as an argument to the different command line commands that docker exposes for interacting with swarm so by combining stack files with docker's cli it's command line interface we're able to check off our first to do we have some way to package our backpack stacks as containerized applications and we can use swarm in order to spin them up or tear them down as we need to get to this next step we'll also rely on docker swarm to be able to spin up our system across multiple servers so docker swarm has this concept of nodes and a node is nothing more than a server that is joined to the swarm there's two types of nodes there are manager nodes that handle all of the deployment and management issues that come when you want to spin up a stack so they're kind of responsible for figuring out where different services should be deployed and how to spin them up and the second type of node is the worker node and worker nodes are responsible for actually running the services themselves so when we spin up a backpack on a swarm and let's use this as an example where we have one manager node and four different worker nodes that are all part of the same storm docker swan will figure out okay where should i deploy each service that is defined in this stack when we first add a stack maybe the manager node is going to kind of load balance each of the services across the available worker nodes so here we can see that it chose to put each service on a different node so now this introduces the question of well if all of these services are a part of the same stack they're supposed to be working together as part of a single containerized application how do they do that if they're running on totally different machines and to solve that problem docker swarm introduces something called an overlay network and it's a virtual network that kind of exists on top of all of the different worker nodes and manager nodes that are joined to the swarm and which lots of services communicate with one another based on the names that they're given in the stack file so here despite the fact that each of the services in this single stack are running across different worker notes using overlay networks we're still able to manage communications between them so as we spin up additional stacks we can see here that swarm and the manager nodes will just keep figuring out how to deploy the services for each stack based on the available resources across these different nodes so we can see here that it doesn't matter how different these configurations are whether we have all the services across all the nodes or for one stack if it has all of its services just running on one for the purposes of the swarm we can continue to treat each of these containerized applications as a coherent logical unit so that takes us through our next two to deuce using swarm and its concept of worker nodes and manager nodes we can now deploy our system across multiple servers and with the help of overlay networks that we can also define using swarm we can still manage communication within our stacks so now we can get to that tricky issue that i've been putting off which is routing requests from the internet to the right backpack so earlier when we were talking about the single instance version of backpack lila said that we could expose ports from the services that are running within docker to the host machine and this would allow requests that were coming from the host machine from outside of docker to make their way to the right container within the host machine this works really similarly in swarm but because swarm is built to run across multiple computers and have i'm sorry multiple servers each of which can have multiple services running on them it needs a way to provide some kind of abstraction that says we're not going to expose the port to a particular server because you don't know which services are running on that server instead we'll expose just one big network that's kind of on top of the entire swarm and when you publish a port when you say that this service should listen to requests from port 3000 that publishing is going to happen into this network and not individual machines so that additional network that lives on top of this whole system is called the ingress rerouting mesh and here's just a quick diagram of how this would look if we had one backpack running in the swarm across all of its different servers it still kind of looks like a logical unit we could have it exposed a port still port 3000 through the ingress routing mesh another port 3001 for nginx and regardless of which server gets hit by a request swarm will figure out how to route it correctly through its routing mesh the problem with this is that we want to be able to support multiple backpack instances and each of these instances each of these stacks is defined by a stack file that says that the express api is listening for port 3000 requests and the nginx web server is listening for requests from port 3001. unfortunately you can't have multiple applications or multiple processes listening to requests from the scene this conflict basically means that we can't do what we were doing before by exposing individual backpacks through the routing mesh a common solution to this kind of problem is to use a reverse proxy instead so rather than exposing services from each backpack through the ingress routing mesh we'll instead put a reverse proxy that's sitting in front of all the backpacks it's going to listen for requests that come through the routing mesh and it will figure out then how do i route this request to the appropriate backpack and the one additional component that we'll have to introduce when we create this reverse proxy is another overlay network because this reverse proxy will need some way to communicate with each backpack stack so by introducing reverse proxy by joining it to an overlay network that is then joined as well to the relevant services on each backpack stack we've found some way to solve our port conflict problem but now we have a new problem which is related to service discovery so if a developer spins up a new backpack on a running system how do we tell the reverse proxy about that new backpack it's gonna need some way to say hey i know that i should field requests for this new backpack and pass it along but the problem is that typically reverse proxies work with these configuration files that you have to specify before you spin up that proxy so in this case we might have a rule using subdomains let's say if the swarm receives a request for app1.domain.com then the reverse proxy will look at that sub-domain app one and pass it to the backpack stack that also has that same name app1 so in this instance if somebody spins up a new backpack we would need some way to either dynamically update the configuration file being used by the reverse proxy so that it can update its rules and know that it should field requests to it but that would then require usually restarting the reverse proxy so that it can pick up the new configuration which would lead to downtime and just sounds really complicated so we want to try to avoid that kind of scenario and thankfully we can by using a specific type of reverse proxy and this one's called traffic with this little gopher guy is there uh their uh logo guy um so traffic is a reverse proxy that's built specifically for containerized applications and what that means is that it can ping docker swarm for updates about which stacks are running within swarm so as a developer spins up a new stack we're able to dynamically update the routing rules within traffic to make it aware that it should route requests to that specific stack without restarting and without requiring us to manually manage configuration files the way that this would work is that when we ask a developer to spin up a new stack it'll be as simple as giving that stack a name with a form such as the one you see on the left when they do that that name will get assigned to the stack itself within swarm and it'll also get used within the routing rules that we can pass to traffic so that it knows to handle requests for that specific name and with that we've checked off our next to do by using traffic as a reverse proxy joining it to each backpack stack with an overlay network we are now able to route requests from the internet to the appropriate stack within our swarm there's just one more benefit that we get from traffic that's important to cover and that's ssl so we talked about how we want to offer user authentication to front-end developers through the core api and that comes with a lot of security implications and one of the things that we really don't want to do is to pass sensitive information like user passwords over unencrypted communication protocols like http and the common solution for that is to encrypt that traffic using the https protocol to do that you need to get something called an ssl certificate and that needs to be issued by a certificate authority so this comes with a whole host of challenges especially for us when we don't know where developers will be hosting our system and which domain names they'll be using to host the system because these are all details that you need when you request an ssl certificate but fortunately by using traffic we're able to configure it to automatically request us to sell certificates based on its configuration when it spins up kind of regardless of how it's initially set up by the containers so just a quick aside there our last two steps are to make it easy to actually spin up or tear down our backpacks and finally to give our front end developers who are using the system a way to actually manage their individual backpacks to see the user data that's associated with backpacks and also any of the actual application data that's been generated for their apps so building the admin panel we talked about how we have currently a way to manage our stacks using these command line commands plus these stack files and that gives us a way to spin up or tear down backpack stacks unfortunately this isn't a very easy system to deal with so it's going to require some security problems such as giving front-end developers root access to the server's running backpack so that they can run these command line commands it would require typing which everyone hates memorizing commands arguments other environment variables managing the stack file and all of that is just not going to lead to a very good or secure user experience so what we wanted to do is to abstract all this behind an easy to use graphical user interface in our admin panel the admin panel would let front end developers register and sign in as administrators it would let them spin up the backpack stacks with that user-friendly name we were looking at before with traffic and it would also let them remove backup backpack stacks so if they're done working with a specific application if they were just testing something and want to tear it down we wanted to give them a way to do that very easily and here's what we came up with so we've got our admin panel on the front end we would build some kind of back end that would handle requests from the admin front end that would then be able to communicate with docker through its cli and we would have the stack file available to that process and so as the administrator took different actions from the front end it would be able to handle these different stack management tasks and in order to provide some visibility to the front end user about what was going on with the stack management we would pipe the pipe the standard output and standard error from the docker cli back to the front end so the administrator could confirm that things were spinning up or tearing down correctly and so here's kind of what this looked like in an early prototype we have our name we're filling out our backpack and then we can see that the cli is piping these commands back to the front to actually show that yes we were able to spin up this lull service and then here we're deleting the backpack one so to build this out we have one more stack that we build one more containerized application we have our front end that we're hosting with nginx over on the left the bottom left we have another express api that we're hosting as the back end for the admin panel that is then communicating with a postgres database which we're using to keep track of the front-end developers who are signed in as admins and then finally one more redis instance that we're using to track session storage so when admins sign in or sign out of the admin panel so we have the stack and then we put it together with our existing system like this so we've got traffic our reverse proxy sitting at the very kind of front of the swarm it's handling requests from the internet basically it is also joined to the admin panel which can handle requests that go to admin.thedomain.com and then anything else any of these other sub domains traffic will route to the appropriate backpack that takes us down to our very last to do which is to somehow connect this admin panel to an individual backpack to let the front-end developer interact with the data inside of it right now our express api for the core backpack app has the ability to deal with collection management so the application data that's been stored in a backpack we have different endpoints that we can use to interact with that we have a way to register users and log them in which is great if you're a user of the backpack and want to start using that but we don't have a way for let's say an administrative type of user the front-end developer to do any kind of user management so to see okay who are my users to necessarily delete users or just do some of the other tasks that an administrator might do with the users in their application so that's a problem because we want our admin panel to provide those kind of administrative tasks so let's say that we wanted to bypass the express api because it didn't have these user management capabilities we didn't want to build in those user management capabilities into the express api because those didn't seem like very secure functions to expose kind of as apis to the internet at least not without some even more complicated authorization systems so one option that we had was to maybe connect the admin stack directly to the instance of a backpack so that it could interact with the user collection kind of regardless of what the express api for that backpack was allowing the problem with that is that the database isn't actually exposed to anything outside of its individual backpack stack the only thing that can interact with it is this express api that's also a part of that backpack and we felt like this was a good a good architecture a good way to secure the data within the database and also confirm that there is only one way to interact with it which is through the application code so we didn't really want to do away with that necessarily so what we have to do is to create some other way for the admin panel to communicate with the express api to build the user management functionality into the express api and only expose it to the admin panel and to do that we ended up with this architecture where basically the admin panel is joined to one more overlay network and that overlay network connects it to each of the express apis running on each backpack so the way that we secured this is that each backpack is now going to listen to requests from two ports the first port is going to be connected to traffic so that traffic can keep routing requests to it from normal everyday backpack usage the second port is only going to be exposed to the admin backend and so when an admin wants to review the data in their backpack the backend will send that request to the specific backpack that's going to create a websocket connection that gets proxied back to the admin and that persistent connection will then expose some of these additional user management tasks that we wanted administrators to be able to handle and so through doing that we're able to check off the last to do on our system here our last architectural to do and have each of these kind of ready to go and that's how we built backpack as a portable back-end that can support multiple back-ends from one deployment this is a little snapshot of what we look at thanks so much for for joining us and we're happy to take any questions you might have okay um so we have one question was there consideration to using the multi-tenant architecture but creating instances of an admin panel to create separation of concerns um i think that we did give that some thought for sure uh we were a little bit more concerned about the isolation issues with the multi-tenant architecture so this again was the system where we would have one express api and then one  server each with a database for each of the different backends that we're running from it and we felt like the isolation issues there were compelling enough to simply explore a different architecture to start with so wouldn't go too far with seeing how we might be able to make the multi-tenant architecture a little bit easier to work with from the administration perspective um great leela holden you want to take the next one um yeah as far as the licensing i think all capstone projects are licensed under the mit license um so it's open source but it's not gpl great okay um we got another question does the backpack api allow devs to configure the resources for each container in an instance like if i want to give my mongodb or redis container more memory or cpu so that is something that we listed as part of our future work because we're using docker swarm because we have each of these services running in their own containers it wouldn't be too complicated in order to say that hey this specific backpack is getting let's say a lot of requests to hit the database let's try to scale out its specific  instance across multiple services that would be relatively straightforward to do but it's not something that we've built into this first version quite yet okay well i think that that's it thank you guys so much for your time we really appreciate it um if you have any other questions we're around on slack all the time but thanks again hope you have a great friday 