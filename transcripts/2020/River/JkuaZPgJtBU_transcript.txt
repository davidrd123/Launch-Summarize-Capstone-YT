okay so i see that we have a good number of participants and we'll get started with the presentation so hi everyone and thank you for coming today i'm katrina and i'm here with my teammate mark and we're here to present river so river is a drop-in real-time service for web applications it's cloud-native easy to deploy and ready to scale with your needs we had lots of fun while building this project and we hope to share a bit of that enthusiasm with you today now i want to say a quick word about cloud native as you might be wondering what that means cloud native means that river was designed and built with cloud services in mind there are many buzzwords associated with cloud computing but also a lot of very interesting ideas we will explore some of these more in depth during the presentation so who would use river and how would you use it we target application developers river is meant to be used alongside an existing application it will allow you to easily publish events from your backend apis and these events will become available to web clients in real time i started a quick outline of this presentation so you know what you're getting into so real time is such a wide space that at first i want to clarify what real time means in the context of web applications and which use cases we had in mind when designing river i will then talk about why a separate real-time service might be a good idea and mark will walk you through an example of adding real-time functionality to an existing app we will then look at some of the existing solutions before diving into how we built river and automated its deployment finally we will say a word about how you can use it so if we're talking about a real-time service the first question we should address is what is real time real time is a very wide space and we want to narrow the focus a bit and clarify what we're about real time is about a fast exchange of messages a change will happen somewhere in our system and other parts of the system should be notified as fast as possible what as fast as possible means though there is quite a lot depending on the context are we talking about microseconds latency is a missed deadline considered a system failure for us the answer to these questions is no keep in mind that we're in the context of web apps this is not about an airplane control system even within the realm of web apps some systems are much more sensitive to delays than others gaming or financial stock trading could be examples here in this case a higher latency can have serious consequences these are sometimes called firm real real time soft real time is much more flexible in that case a higher latency will result in a worse user experience not a system failure collaborative apps are a good example you can think of a collaborative text editor such as google docs a discussion forum like slack or a shared calendar the idea here is that the real-time application should update to the correct state without user intervention these are the type of applications we had in mind while building river now if you think of the http request and response model without user intervention would mean without the client sending a request and that's not an option what we need is an open channel of communication through which messages can be sent between the server and the client with that idea in mind there are a few options in terms of how the messages are exchanged the first thing to notice is that the connection will always start with a normal http request from the client these are the blue arrows in this diagram it would be a disaster from a security standpoint if servers could initiate a connection with the client xhr pulling as it sounds like means that the client will pull the server this means it will make a request on a periodic interval to see if new data is available with server sent events after the initial request and response it's a one-way server to client streaming of text-based data these are the green arrows finally with web sockets we have a bi-directional message-oriented streaming of text and binary data between client and server the red arrows to the right in our case we needed a bi-directional communication channel and we chose web sockets for that reason so these are some interesting numbers when we think about the advantages of using websocket over http this is about metadata overhead so with the websocket protocol with each message sent there are 2 to 14 bytes of metadata overhead with server sent events 5 bytes and with http with each request and response there are 500 to 800 bytes of metadata without counting the cookies so this is a significant difference so to recap a little in the past few decades we've seen an evolution from hypertext documents and web pages to dynamic and interactive web applications http was not designed for web apps http was designed with simplicity in mind and the metadata overhead i just mentioned makes it a less an ideal choice for real-time communication so to improve the performance of our application and the user experience we want another model of communication where the server can pass data to a client without prior client requests this is made possible by websockets websockets provide a very interesting alternative but they do have some downsides two major pain points when working with web sockets are compatibility with existing infrastructure as websocket is a different protocol distinct from http and scale and security many of the existing components of the infrastructure for example servers and load balancers are built configured and maintained with http in mind the final question i want to address in this section is is real time hard well it depends real time can be simple or hard and it depends on the requirements of your application does it require data persistence does it need to scale how secure should it be building a simple chat app is actually very easy to do there are many libraries out there that make working with web circuits easier if you want to add real time to an existing application that could be another survey you might need to make a lot of modifications to your existing code base and it can be worth considering putting that logic in a separate place same goes for building a resilient and scalable application this is a new set of problems that could be difficult to handle if all the logic sits in one place so as i mentioned in the beginning river is a drop in real-time service that means it's a separate component anytime you add an extra piece to your system it brings in some additional complexity and as a general rule of thumb you should only add complexity when you really need it so the need for a separate real-time service has to be justified in this section i want to look more carefully at the idea of a separate real-time service we've already talked about the idea of persistent connections and how different it is from the usual request and response cycle what we haven't mentioned yet is that your server a process is now responsible for maintaining the open websocket connections having a separate server dedicated to maintaining these connections will allow you to upload the management of persistent connections from your main application server this is a known issue as illustrated by this port so http is optimized for short and bursty transfers as a result many of these servers proxies and other intermediaries are often configured to aggressively time out idle http connections which of course is exactly what we don't want to see for long lived web socket sessions so in short a stateless request and response cycle like http and persistent connections like websocket have different technical requirements these in turn can influence the configuration of your server and cause issues that will be amplified as a load on your server increases this is the same idea a bit more forcefully said by the creator of the flask framework keep your websockets miles away from the rest of your application code keep them in a separate server if you can they have completely different performance characteristics than your regular http requests so this is basically the idea of separation of concerns you have two different pieces of functionality they do something different keep them in a separate place more generally this idea plays well in a microservices architecture where each component can scale independently according to its needs which brings me to the next point what if you need to work not only with one server but with multiple backend services again this is commonly referred to as a microservices architecture with a microservices architecture instead of working with one gigantic code base your application logic is split into multiple smaller services now if you want to add real-time functionality does it mean that each back-end service must manage both http and web socket connections this will look something like this or probably more like this since each back-end service needs to maintain an open connection with each client what we really want is something like this a separate real-time component responsible for managing the websocket connections and providing the real-time functionality the backend services can now send normal http requests to the real-time service in a sense they are now decoupled from the clients as is the real-time service that is responsible for sending messages to the clients so where does that leave us at a very high level what are the pieces of a real-time application well real-time applications consist of clients a real-time communication layer and back-end servers so before handing it over to mark i want to take a moment to go over a few communication patterns and see where that real time layer fits in you may have heard the terms publish and subscribe in the context of messaging this is often referred to as the pub sub messaging pattern the exchange of messages can happen between backend services or between the backend and the front end as in this case so keep in mind that here we're talking about a browser pub sub system the back-end services publish events to the real-time service which sends them to the subscribers it's a many-to-many relationship some obvious advantages of this pattern are that it decouples the publishers from the subscribers they do not need to be aware of each other publishers simply publish events to a specific channel while subscribers can subscribe to a channel in order to receive events with this decoupling it also allows multiple receivers for a single event and finally it provides application developers with a simple interface for sending messages in a consistent format this is basically a variation of the same pattern so generally events can be almost anything a change in a database or a message received from an external api with this triangular pattern messages sent by the client to the server over a normal http request and response cycle are treated as events to be published as you can see the real-time service is added to an existing application without the need to modify the ex the existing http communication i won't go into too much detail since mark will now take over and walk you through a concrete example thank you catherine let's take a look at what is needed to add real-time functionality to an existing application we'll use a discussion forum as an example a traditional discussion forum does not usually have real-time features and in this case to know if the new message is in the forum a user would need to refresh her browser what are the actual steps needed to add this real-time functionality to an existing discussion forum application such that a user doesn't have to refresh your browser to see new messages let's examine how the request response cycle changes if at all and how events can be sent to alert our users of new messages in real time here we have an existing discussion forum application application server is on the left and our client is on the right we're leaving about the database for clarity but a persistent data layer does exist here of course there are many clients but we will look at just one at a time here for simplicity and clarity just like any other credit application to add a new message to the form our client sends an http post request to the server the server then validates the request saves it to its database and sends back a response with a new web page with a client's new post telling everyone how much she loves dogs this is as expected but what happens when a second client makes a post here we see a second client making a new post to the discussion forum but of course our first client can't know about it without refreshing her browser there's no open channel of communication between the server and our first client what would it look like if we did have an open channel of communication to be able to add real-time functionality to our application we'll open a websocket connection between the client and server we'll use this websocket connection to send update to the client when a new message comes in now when dog fan 55 our second client sends her message declaring hello for dogs our server is able to update our first client without any need for her to refresh her browser this update happens via a websocket message sent over our open connection between the server and our first client so far we have talked about how to add real-time functionality to our application without adding any external pieces to our infrastructure and as catherine talked about before this can be a valid approach for some use cases a river is a separate service how does a separate service actually work with an existing application like our discussion forum here now we see river in the picture our application still has its existing http communication paths we want to add real-time functionality as simply as possible so we're not going to alter these paths at all but what is the first step that needs to happen here first our client needs to open a websocket connection with river it is through this connection that the client will receive updates without having to refresh her browser second our client needs to tell which messages she wants to receive river may be sending updates concerning many forums or subforms and she doesn't want to receive every new forum post only those new posts for the forum she is currently viewing she is currently on the dogs forum so she subscribes to the dog's channel our client makes a new forum post in the usual manner via an http post to the server and the server saves the new forum post to its database now our server needs to somehow notify river of this new form post in the dogs forum so that all clients that are subscribed to the dog's channel can be given the update of the new forum post to do this our server publishes to a specific channel here the dog's channel via an http post request to river we can think of this publishing as an event about which all subscribed clients will be notified finally river takes this publishing of an event turns it into a websocket message and sends it to all the clients that have subscribed to the dog's channel here again we are showing just one client for clarity but you can imagine many clients all looking at the dog's form and receiving this message in real time now we can see the overall pattern of the browser facing publish subscribe model take shape our existing application does not modify its http channels of communication the application publishes events to river and the clients open a persistent websocket connection to subscribe to and receive events from river now we can see the bi-directional nature of the communication between river and its clients the need for bi-directional communication is the reason we chose to use web sockets over server sent events if you recall server sent events only permit one-way communication from server to client our clients need not only to receive messages but also to send messages to unsubscribe to and subscribe or to subscribe to and unsubscribe from channels of course we can have many clients not just one client and many back-end services river is unconcerned with which clients are subscribing and also unconcerned with which back-end services are publishing it simply accepts publish events on a specific channel and broadcasts that event to subscribed clients but we've introduced one problem now how does river know who these publishers and subscribers are how does a separate service how does a separate real-time service authenticate if it is decoupled from the backend services which it supports we will get to our solution to this problem later in our presentation catherine is now going to talk about existing solutions in this problem space thank you mark and so before going into how we actually built river i want to say a word about some of the existing solutions as we said before and this is a well-known problem and once you have decided that a separate real-time service would work well for the needs of your application you have quite a few options broadly speaking the existing solutions can be divided into two categories on one side there are many open source libraries to serve a very wide variety of needs we've looked at quite a few of them and they were definitely an inspiration to us centrifugal for example is a scalable real-time messaging server written in go it runs as a separate service and maintains persistent websocket connections with clients it follows a very similar pattern to what mark does despite and presents itself as a user-facing pub sub server its use case is very similar to ours it has been tested at scale and is under active development on the downside while it has been used at scaled it's not out of the box ready to scale the deployment scenario is left to you and while there are some good instructions on how to deploy and configure your server that complexity is still yours to manage on the other side there are many profitable companies built around this problem space these are hosted paid services we've built some demo applications with pusher and it's very easy to use it provides a simple web interface for creating real-time apis libraries for multiple languages and strong guarantees on the downside it will cost you some money the free plan will give you up to 800 concurrent connections to support 10 000 connections it will cost you 500 per month so looking at these options we thought there was a space in between these two categories the open source solutions were great but difficult to deploy the commercial solutions were easy to use but costly so we decided to build a simplified open source version of pusher our main goals were to be easy to deploy and ready to scale exactly what the open source solutions don't give you the trade-off we made was to keep it simple we don't offer many features beyond the core publish and subscribe functionality there is another non-open source solution we've looked at and that i want to mention the zone is a sports streaming company we were interested in a specific problem they solved with their own in-house solution the problem wasn't the streaming itself of live events but publishing live updates to millions of users eagerly waiting for this critical information they looked at amazon web services for a good solution and didn't find something that matched their needs out of the box so they came up with their own solution and wrote a few good articles about it their requirements were pretty similar to ours so this picked our interest they needed to publish sports updates to users allow users to subscribe to new messages and perform with high traffic on a global scale in the next slide i will show you a diagram of the infrastructure for this specific solution a system they call bobby there are many pieces there so don't get overwhelmed i just want to point out a few interesting aspects so this was an inspiration for us while building river as we knew it had been tested in real life under a high load and proven robust it was also a validation of the use case we had in mind what i want to point out is simply the overall shape of the system at the right users subscribe and receive updates through the application load balancer to the left backend services can publish events the whole centerpiece is the real-time layer and the websocket server lives as a containerized application deployed through the aws elastic container service now there are many other pieces in there but it's enough said about existing solutions mark will present river and the specific choices we made while building it now let's talk a little bit about how we built and designed river when designing river we had a few goals in mind we wanted it to be easy to use easy to deploy and ready to scale to achieve these goals we had to make a few trade-offs first river has a simple interface and not many features or options and when river is deployed there is no configuration finally it deploys on amazon web services only so far we have looked at how river interacts with an existing application and have explored this triangular pattern now let's zoom in on river and look at the components and resources that make up its internal infrastructure at its heart river is a node.js process running a websocket server that's it while a single process on a local machine can provide all the functionality and features of river we had something else in mind we built river to be resilient and ready to scale here we see our same familiar triangular pattern but turned on its side we still have our three pieces the back end services the clients and river but here river is represented as a single node.js process which is where we started in our development of river notice that our node.js process is not only accepting and holding websocket connections but is also accepting the publishing of events from the backend services via http post requests since we decided to work with aws we needed a place to put our node.js process to start with we put it in an ec2 instance aws's standard virtual private server and this worked at this point river has all the functionality that we have discussed so far and can actually be the separate real-time service that we have been describing but we knew we would want to be able to horizontally scale so we added another ec2 instance which brings a couple other pieces that i will introduce we knew we wanted to be able to have more than one instance of the river server running one for resiliency such that if one server goes down another is available while the first one gets back on its feet and two to be ready to scale horizontally to be able to handle more traffic than one ec2 could reasonably handle but this second instance introduces new problems how do the nodes communicate with each other when i say nodes here i mean an ec2 instance if an event gets published to one node how does the other node know about it to solve this first problem we added the elasticash for redis service which is amazon's drop-in replacement for redis redis is usually thought of as a key value cache something that it does very well but here we're actually not using it as a cache redis also has a very powerful built-in publish subscribe mechanism and here the nodes are the actual publishers and subscribers and redis allows the nodes to communicate with each other just to reiterate the point here this is an example of an internal or back end service pub sub and not the same pattern that river fits into that we've been talking about overall which is that of a browser facing pub sub service we still have a problem on the other side of our nodes now we don't want to have two different urls for accessing river so we added a loan balancer in this case an application load balancer which is a subservice of aws's elastic load balancing service the load balancer serves to distribute traffic between our nodes and provides a single point of entry for our clients and services at this point we felt pretty happy about our system but we noticed one issue and that is the fact that our node processes are handling both websocket connections and http requests for publishing events we're mixing websockets in http and as catherine alluded to earlier this is not the best idea is there a way we can have a separation of concerns since redis is acting as our published subscribe mechanism for our nodes is there some way we can publish to redis directly so what we did was add this back channel as we like to call it for publishing events via http now river has two entry points one for browser-based websocket connections and one for services to publish events to river via http the websocket connections are now completely decoupled from the http publish requests the api gateway service exposes a public url and when a published event comes in via an http post request the lambda function validates this request and sends a publish event to various redis then publishes the event to the node's subscribers and then on to our connected clients now we have a pretty robust system with a nice separation of concerns but thinking about two of our goals being easy to use and easy to deploy we have an issue here ec2 is a great service and gives developers a lot of power and customization but also comes with some burdens upon deployment we need to apply security updates we need to make sure the correct version of node.js is installed on an ongoing basis we need to apply further security updates operating system updates manage the logs and just in general all of the work that comes with managing servers we wanted to abstract all this away for users of river so we decided to containerize our node.js process and move to a different aws service now we have the final version of our infrastructure and as you can see we have moved from ec2 instances to amazon's elastic container service we have taken our websocket server and put it in a docker container which allows us to focus solely on the business logic of river and not concern ourselves with server maintenance at all we no longer have to worry about node version conflicts or logs filling up and can simply deploy our docker container without thinking about underlying dependencies elastic container service is a fully managed container orchestration service ecs has two launch types the ec2 launch type and the fargate launch type we chose to use the fargate launch type which amazon describes as quote serverless compute for containers we also put our internal infrastructure inside a virtual private cloud this allows us to provision a logically isolated section of the aws cloud and creates the virtual network over which we have complete control as i mentioned ecs has two launch types with the ec2 launch type which we did not choose the developer has to manage the instances on which ecs runs the developer must provision and scale clusters patch and update servers and other tasks associated with server maintenance in other words the same problems we were trying to move away from when choosing a containerized approach with the fargate launch type aws abstracts all of this server infrastructure abstracts away all of this server infrastructure we only need to define our tasks and services which tell ecs how our container should be run and fargate takes care of everything else this allowed us to meet our goal for river of not only being easy to deploy pretty easy to use as well now catherine is going to talk about how we automated the deployment process and the associated challenges we faced as you can see while building river we face significant challenges that was only half the battle though as we didn't want to only build river for ourselves we wanted to automate the deployment so that other people could easily use it now i want to take a second to come back to this idea of cloud native we didn't want to throw a lot of buzzwords at you right at the beginning but now that you've heard mark describe our infrastructure we're in a much better position to talk about this idea containers container orchestration and serverless functions are all part of what being cloud native means these are big advantages that are worth mentioning they help us build systems that are resilient and easy to manage when you combine these with automation you allow engineers to make high impact changes easily another common element of cloud native systems is the use of declarative code to describe the infrastructure and this is what i want to talk about now this diagram represents different ways to interact with aws from manual towards automation these are alternatives that you can choose depending on your needs many people will be familiar with the web-based management management console and it's a it's a good entry point in the world of aws it allows you to create your resources easily by clicking through the web interface it's not an option though when you want to automate the process as it's not easily repeatable for that we have to look at sdks software development kits and blueprints sdks allow you allow you to write scripts in a familiar programming language to build your infrastructure this is great but anyone who has worked with these scripts can attest that it's still a lot of work defining your resources in a specific order and having them interact one with another is not always an easy task in response to that a new category of tools has emerged often called blueprints the difference between sdks and blueprints is the difference between how and what between an imperative language and a declarative language with an imperative language you give specific instructions on how to achieve an outcome with a declarative language you simply describe what outcome you want now i want to backtrack a little and walk you through a few different scenarios of creating a lambda function i chose the lambda function because it's a relatively simple resource and i wanted the code to fit on the slides you can imagine that for a more complex resource such as an ecs cluster the overall complexity would be much greater this is how you create a lambda function using the command line interface you need to first define the function itself and manually zip it then run a long command from your terminal with many flags i want to attract your attention to the last line where the lambda function is assigned a role this is a major pain point when deploying aws resources you have many different resources and they need to communicate in a secure way for that you use roles and permissions in this specific example the role was created by another long comment that i haven't included on the slide of course the command line interface was not an option for us as it doesn't allow for automation so we look next into these blueprints i mentioned with aws cloudformation templates being one example so these are json or yaml templates where you simply describe what resources you want and what they should look like you will still need to describe rules and permissions but it makes the deployment consistent and repeatable this was definitely a step forward for us but on the downside these templates are not very user friendly they can run over many hundred line of code and most of these are simply default settings so you end up copy pasting boilerplate templates provided by aws and looking through those hundreds of lines for these specific parameters you need to change while researching better options we came across aws cloud development kit it's a very interesting concept as it allows you to write code in a familiar language such as javascript or python and model your infrastructure using object oriented code under the hood it will generate cloud formation templates for these json or yaml files pre-configured with sensible defaults it makes infrastructure components easy to customize and easy to share in accordance with the object oriented code reduce idea and it makes your whole infrastructure easy to deploy and easy to tear down on the downside it's very new as it came out in 2018 and while the ducks are good there are no books yet on the topic and not a lot of good examples so for more specific configuration requirements you're on your own overall though i must say that it worked really well for us and this is an example of what it looks like um the lambda function is simply a javascript object so i know there is a lot of code but basically that's it and another advantage of how is how easy it becomes for your resources to interact one with another as they are defined as javascript objects they can easily communicate within the code itself the complexity of defining roles and permissions that i mentioned before is abstracted away from you this line is all that's needed to include the lambda in a virtual private cloud that allows it to interact with other resources belonging to the same private network the vpc itself was created as an object in the same file and it's easy for me to refer to it from within the code itself so to give you an idea our main deployment file is about 100 150 lines of code and that file generates more than a thousand lines of cloud formation templates these long yaml piles so for us it was a clear win now we've talked a lot about how we built river and automated this deployment what i want to emphasize is that our goal was to take that complexity away and provide a tool that's easy to use so this is what we're going to talk about in this concluding session hopefully it will be a bit lighter on the technical side deploying river is one comment after cloning the repository you just have to type cdk deploy that's all the end result is that all your resources will be created on your aws account as you can see the process takes a while but it's meant to be done once know also that it's one comment to roll back all these changes so it makes it easy for you to test if river is a good fit for your needs as we mentioned in the beginning river is meant to be used alongside an existing application so besides deploying it you also need to interact with it this is what these outputs are for we provide libraries with a clean and simple interface to make it easy to interact with river this is an example express app on the server side so you can simply include the river library as a module you initialize it with your own api gateway endpoint and a key whenever you want to publish a message you simply need to add one line of code in this case every time a post request is received for the add task path that event will be published similarly on the client side you can include river as a module you provide your load balancer endpoint and a valid json web token you can subscribe to any channel you want and instruct river on how it should react to specific events with an event name and a callback now this is a very simple demo but as the end result of these few lines of code your existing application is now a real-time application these are two different browsers browser tabs and they represent two users that could live in different cities like mark in new mexico and myself in montreal when one user updates the state of the application the ui gets immediately updated for both users without the need to issue an http request so i will now let mark say a word about authentication and about our load testing results before closing the presentation thank you catherine previously we had mentioned the problem of how does this separate service know who's connecting to it how can we make sure that only certain clients can establish a connection to river the previous slides gave a little hint jason webb tokens when river is deployed a 256-bit secret key is generated which is used to sign the json web tokens these tokens are what allows river to be decoupled from the application server but also able to authenticate clients when a client connects to river a valid token is expected let's look at an example of how these tokens might be generated and passed around among the three pieces of our triangular pattern after river generates the secret it's shared with the existing application before attempting to connect to river the client requests a token from the application server and sends a lock sends along its cookies and since this client is presumably logged in the server will have some notion of who this client is from its cookies and will verify that indeed this client should receive a token the server then generates and returns a valid token signed with the river secret the client then establishes a connection with river note that river allows a connection to be established from any client but it expects a valid token within the first 15 seconds of connection otherwise the connection is terminated the client then sends a websocket message containing the valid token whoever uses the secret to verify the token and sends back a message notifying the client of successful authentication finally river adds this client to its pool of authenticated clients which allows it to subscribe to channels and receive messages note that this is just an example workflow and river doesn't make any demands on how tokens are issued passed around or how their expiration time is set this is left up to the application developer using river the only thing that river expects is a valid json web token within that first 15 seconds of connection for the final section of our presentation let's talk about load testing if you remember one of our goals for river was to be ready to scale to know if we had actually met that goal we needed to test river under some sort of load and attempt to simulate the real world conditions of a small to medium web application to know if river could stand up to such conditions we set some goal numbers we wanted to have thousands of stable concurrent open websocket connections thousands of messages per second being sent by the server and the ability to handle 100 new connections per second to synthetically generate clients we use the artillery i o testing library which gives native support to socket i o river's underlying websocket engine we set up two ec2 instances to run artillery we used two instead of one because if we asked artillery to generate clients too quickly it gave us high cpu warnings which could have led to erroneous test results so we split the load onto two ec2 instances two containers was just an arbitrary number we picked so we could extrapolate how much load the system can handle for a given number of containers to have our server generate lots of websocket messages we use the concept of a presence channel this is a special kind of channel where when a client subscribes to the channel this subscription event prompts the server to send a message to all the existing subscribers on the channel here we see a fourth subscriber on the left causing the server to send a message to each of the three existing subscribers on the right you can imagine as the number of subscribers grows the number of messages being sent each second grows very quickly for example the one thousandths one thousandth subscriber to this presence channel will prompt the server to send 999 messages and if we're generating many many clients every second you can see that there will be a lot of messages flying around once we have many clients connected and subscribed so in the end how did river do to start with we were able to support 20 000 concurrent connections without any errors at the beginning of the test river was able to add 100 new connections per second but by the time we got to the end of the phase of the test where we were adding clients this rate decreased to about 15 new connections per second probably due to the large number of presence channel messages flying around at that point during the peak of the presence channel messages we estimate that these were being sent at the rate of 300 000 messages per second this may sound like a lot of messages but keep in mind that these are very very lightweight text-based messages around 37 bytes each and note that these are not http request response cycles but messages being sent over an existing connection despite not reaching the goal of a steady addition of 100 users per second we felt that we met our overall goal for river to be useful as a real-time service within the context of small to medium web applications we're still working on river we have a few ideas for future work aws gives us a secure tls encrypted web endpoint for the api gateway service where back-end services publish events but it's application load balancer service the endpoint where clients connect via websockets does not come with tls out of the box to enable secure web sockets a developer using river would need to add his custom domain and encryption keys to the application load balancer this is a straightforward task but we would like to add an option during deployment to automate this right now we have an api key and secret for authentication but we could better leverage some amazon services to provide stronger security for both the api gateway and the secret that validates json web tokens we could leverage the cognito service as an identity provider for json web token authorization for the api gateway aws also has a service called secrets manager that would allow a rotating and more robust secret for our websocket server finally a feature we would like to add would be this idea of a message cast for dropped connections imagine if a user lost their websocket connection for a short period of time like going between cell towers or a train going through a tunnel then when she reconnects she picks up with river right where she left off not by having to access the applications database but by accessing rivers cache of recent messages so we've reached the end of our presentation about river thank you so much for coming and attending our our talk um we have time for some questions now here we got one question so melissa asked about the cost we had during testing [Music] and the average expense we had with aws billing for all that usage um i don't know if there's a particular way for river users to anticipate their costs um like we got our aws bills at the end of last month um just and it involved a lot of other costs for just like developing and and setting up things so you know if catherine you have anything to say about that i i think it's there will always be a cost because it's a very robust infrastructure so it cannot be free we try to lower the cost finding ourselves like river free and as mark said on the short term the cost is low uh usage over a long term is something we could not um assess but i like your idea melissa of like maybe um being more clear about extrapolating i said i think you said like if this is what it costs for a week how much would it cost you for a year and then being better able to compare with existing solutions it also depends on like how you provision um the how you set everything up like right now river is set up to default kind of choose like the smallest sizes of aws resources like for example the redis piece that we picked is i think it's like one cent an hour or something like that but there's other redis services um that are you know a few dollars an hour so it depends also we we also have two other questions that are listed but i think they're interesting um one is from derek and derek asks which unique challenges did you face or benefits uh you experience working as a team of two as opposed to three and i i can start maybe and mark you you will jump in i think it's kind of obvious in the sense that when you're two you fight less because we're only two percent we get along very well we're both very respectful of each other uh ideas and so i think in that sense for specific parts of the project where you actually need to agree and it's it's not a given that we will have the same opinion so we just have to let the best idea win i think there is an advantage of being two and it depends on the personality of the the team member on the downside of course it's it's more work so the workload falls on the two of us and when there are lots of tasks that need to be done it's a lot for sure sometimes i think we we would have wished to upload some of the work to someone else but overall i think um it was a very fun experience to both of us i was sad that i think um working with catherine was very easy and smooth and enjoyable so um that wasn't a challenge at all from my perspective yeah i think we're very lucky because um we get along and we have complementary stress uh which i think is important but i i think i want to write a whole capstone article on collaboration because it's a it's a very important topic and after kind of a few intense months i have a lot of reflections on the topic so it might be coming soon and i'm reading danielle's question that was also mentioned on slack recently so he's asking catherine you mentioned that you add a lot of inner resistance to aws what were those how were you convinced otherwise as well as how laborious is it to hypothetically okay that's another question so two questions first uh first question would be like where was my inner resistance coming from and the second one is like how could river work with a different cloud services maybe the second one i i would like to mark i want to say a few things about the first question without getting too deep in in the reads and i think my inner resistance i'm not sure it was completely justified i was definitely the one it took a long time to convince me to go with uh aws i think that part of it was like my inner teenager was just resisting the idea of going with the main the mainstream player and we did spend a lot of time exploring other ideas right and i personally spent a few weeks exploring very fringe ideas and reading a lot on different topics until i realized the following so i was kind of getting too far away from from the mental model i know so these french topics were very interesting but um and i think it was mentioned in zlatan so the capstone project is a balance right between different things so you have to be able to achieve something in a given time frame it has to resonate well with your teammates and it has to resonate well with the job market so at some point i understood that my research were going too far away from these goals and mark convinced me to try to aws and actually when i started using it my inner resistance that was not kind of a valid reason melted away because it's it's a very powerful model and again as well as was mentioned on slack it's not necessarily aws related right it's cloud services so again suddenly my inner resistance transform into a fascination because it's very powerful it gives individual developers a lot of power and i think suddenly we felt like we could build an empire from our own little home office which which is really fun so i think it's definitely worth exploring and again keeping in mind the different goals of that you want to achieve with your project yeah so this was a long-winded answer to this question mark do you want to address the second question of um hypothetically how could we connect river to a different cloud service um yeah that's a good question um there there are definitely a lot of pieces that could be reused since since our websocket server lives inside of a docker container the course any cloud service that you could use docker on you could directly use that and and the libraries that interact with that docker container as far as like replicating something like we did with aws i i'm not exactly sure mainly because i've never worked with different cloud services besides aws but my guess be it was a lot of work for us to set up this and automate the deployment of this particular aws infrastructure so my guess to set up something comparable on a different cloud service it would be equally time consuming and challenging i think there's a couple chat there is a question in the chat thank you ronnie saying excellent and so how did you yeah how did you guys come to a decision on specifically 15 seconds for a connection to be made and what is the logic magnet in terms of security and mark correct me if i'm wrong but i think there we base ourselves on the existing standards so maybe i hope it was clear from the presentation but we looked at a lot of existing solutions and we we looked at the way they did it and of course examine if it made sense and i believe this this is kind of an industry standard the 15 second yeah i think um i think it's just like an industry yeah we didn't just like pull it out of a hat if if that's what you're wondering um it's it's kind of like an industry standard if you look at different libraries that work with jwts and websockets is kind of standard it gives the client a chance to to send that message with the jwt um kind of in the same way that you know any http endpoint can be accessed by any any client in the world like anyone can connect to river but it'll only last for 15 seconds if you don't have that jwt of course i mean that could lead to if like if somebody was like trying to ddos river like in doing a thousand new connections per second for 15 seconds so there could definitely be a problem but um it was kind of a trade-off that has to be made at some point i i see that there are no more questions coming so i just wanted to a few concluding words right and thank you all for coming i want to give a shout out also to the um other members of this capstone cohort i think they did amazing work so team maestro and team jade i'm very impressed by their project uh if there is one thing i've learned in these past few months it's the power of collaboration it's it's very very powerful so it was like a pleasure to work with these talented engineers and especially with mark of course that was a great teammate and so thank you all thank you sir john for hosting if you have any questions we love to talk about this project so it's um difficult to convey everything in a presentation so feel very free to reach out to us for questions generally your technical questions so thank you everyone thank you so much for coming 