thank you for joining us on our presentation jade framework and open source deployment environment for jam stack applications today i'm going to cover what the jam stack is what infrastructure for a jam stack web app is what jade is at its core our framework and the evolution of jade and the experiences and lessons we learned building this framework so before i mention what jam stack is i want to talk about a couple typical web app architectures so here on the right you see a web app architecture for static websites it's a very simple architecture and it's good at doing what it was designed to do serving static content to the client on request however the limited the small architecture simple architecture brings with it limited functionality if you want dynamic functionality you have to actually expand your architecture and on the right here you see an architecture for standard web applications now with improvements to the browser capability and the introduction of sql databases we're now able to do more on websites basically you can grab dynamic content not just static content however there are some challenges related to security scalability and performance regardless of which architecture you use whether for static websites or static web apps what they have in common is the way that web pages are built so initially the client will make an initial request to the web server for the html file or any static file once the client receives the html file it will see references to other static content such as css images or js so now the client will make a second request for those supporting static assets once the client has all the asset it needs it will now assemble the page on the browser an example would be if you're visiting your friend's blog you go to their blog site you'll get back an html from the web server now your browser will look at the html and see that there's references to styling and images so now you make second quest for those images it'll be part of that blog site and how about for dynamic content well when it comes to dynamic content this is where the app server plays a role so any dynamic functionality comes from the app server applying business logic and databases providing data to the app server continuing with the example of the blog site imagine you want to exceed the comments your request will go to the app server the app server will source raw data from the database and then the app server will apply some business logic to that data maybe filter it or filter it by date or filter by person and then return those comments to the browser so that you can see those comments this is how websites web pages are built using typical web app architecture now you can see that the client has to make multiple requests for all the assets before it can build the page what if we can pre-build the page ahead of time instead of having the client make multiple requests for all the static assets and then build the page on the browser on client side what if you pre-built the pages ahead of time and when the client request comes in we can serve that directly to the client as is and the client doesn't have to make more than one round trip request it allows pages to load faster and it reduces client-side rendering well there is a tool for that and it's called the static site generator which helps pre-build your files this static site generators such as hugo jekyll and cat speed to name a few take source code and they take data from a content management system and apply these four processes to it compile and minify transfile and bundle i'll explain compile in more detail in the next slide but minifi will take your source code and compress it tense file will make your code compatible with all modern web browsers and bundle will combine your source code into a single file when it comes to compiling this is where static site generators are most useful what happens is data needs to be sourced from a content management system and that data along with the source code is sourced to the stack site generator and the static site generator compiles both the data and the source code together and spits out the build files how does this work the data from the content management system is applied to templates and the static html file is output so with rebuild files can we make any changes to our architecture well on the left you see that before we had the web server serving the static assets to the client on request but now we can have a cdn do the same too the cdn is also capable of serving static content if you see the diagram on the right we can actually substitute out the web server with a cdn what this does is this reduces costs and improves the response time because cdns are located closer to the client than web servers can we make any changes to the backend well before the app server and the database were responsible for dynamic functionality we can actually abstract that away into apis and serverless functions before we had the client cdn app server and database as architecture now we have the client interacting with cdn api and service functions this we refer to as a serverless model actually this type of architecture is actually in existence right now many websites exist that use this architecture and there are many companies that exist solely to provide developers of such websites support and products and that is called the jam stack architecture well the jam stack actually stands for javascript apis and markup you have the browser using running javascript at runtime your apis these could be either third-party apis or custom-made apis will now be responsible for the back-end logic or the dynamic functionality and markup which i explained before is pre-built at build time often with a static site generator now when it comes to markup and pre-building pages it's important to note that not all web pages can be pre-built ahead of time pre-building a web page is most is only useful if you can source data ahead of time now what is the paradigm shift from the standard web app architecture to the champs tech architecture well now the front end is the orchestrator for dynamic functionality before we have the app server handle dynamic functionality that job now is handled by the browser running javascript at runtime the browser will now be responsible for invoking api for making api calls and invoking serverless functions for those business logic and here's a quote from matt billman he's the ceo of netflix which is one of those companies that serve us support and product to jam stack developers the modern browser is an operating system in its own right no longer merely displaying documents downloaded from the web but capable of running full-fledged web applications this is important because in the jam stack the front end is able to be the orchestrator of dynamic functionality because the browser is now capable and stronger than ever before so now that we've had an overview of the jam stack we'll dive deeper into the attributes which is serving static content so with jam stack your whole site has to be served from a cdn any request from the client goes to the cdn and the cdn is responsible for supplying all static content and this has many advantages one is faster response time cdns have edge locations closer to the client allowing latency to be decreased by hundreds of milliseconds also it's much easier to scale cdns because of the ease at which you can add and remove cdns from a system also because you're using cdns in place of web servers that reduces the surface attack surface area for attacks web servers are often the target for malpractice such as ddos attacks and hacking attempts and finally there's less maintenance with cdns you're most likely going to be using a third-party cdn so that third party will be handling the responsibility of maintaining and securing cdns now how about dynamic functionality and the jam stack dynamic functionality is now abstracted away into api's reusable apis before the app server was responsible for managing the data and handling business logic we can now have that abstracted into apis data management can also be abstracted into apis allowing the client to be a coordinator of such data and there's also service functions which help with dynamic functionality not all apis and front-end logic can replace the back-end functionality sometimes for example you need some tasks to be run from a secure location and not a client side like the browser in that case service functions are useful and some examples for when you use service functions are authentication forms and events and now all this talk about dynamic content static content apis dynamic functionality you might be wondering what you can create the jamstack website well that's actually quite a variety of websites you can create on the on the screen are three examples of real gemstock websites on the left is this static nike ad campaign in the middle you have fba catalog where you search for a product and returns that product most likely using a database api and on the right you see rye and beyond cottages search view and pay for rentals in england because you have to because you're paying for rentals that means it's using a payment api because you have to log in and be authorized to view your rentals this also includes authorization and authentication and because you're searching for uh rentals then there's most likely a database involved this is some of the functionality dynamic functionality that champs text sites can have now in summary the jam set workflow is very simple there's only three steps develop your code pre-build your files and distribute to the cdn however this simple three steps are only for sites gemstock sites that you don't plan to update often or at all however if you do plan to update often you need to automate the system so that's easier for the developer and for that reason the jamstack community has come up with this following for back best practices version control automated builds atomic deploys and instant cdn invalidation i'll explain this in more detail in the upcoming slides so here's a typical overview of a jam stack workflow one you want to use version control version control such as github bitbucket and code commit allow the developer to coordinate with other developers and also most importantly they have they implement webhooks and the webhook is an event that gets triggered every time something happens such as code commits merging with the master branch what not and so let's say you're updating your blog site and you make a change you make the update in your source code and you push the commit well github will send out a webhook and that webhook goes to your build server and this is where automated build comes into play anytime a web hook is sent to a build server the build server gets notified that there's an update to your source code we'll then pull data and we'll pull the source code from the git repo and it will start the process for pre-building the pages and atom automated builds once the build is done it will upload to storage but what's important to note is that this should always be an atomic deploy meaning all or nothing you don't want your files to be half built and then there's an error and yet still upload the files if any user is streaming or surfing your website then they'll be inconsistent state and there can be downtime because of that if there is an error you want to roll back the build you want to roll back to upload and try again later now once you've uploaded once the build has been done and it's been uploaded to storage the cdn can now deploy your pre-built files and serve them to end users directly the important thing to note here is that the entire site should live on the cdn and one final step is instant cdn invalidation what happens is you don't want your users to be looking at steel or old content every time you make an update you want your old content to be invalidated on each build and this is usually done with either a service function or a build server but for our presentation we will assume that the service function is responsible for invalidating cdn cache so that the server always gets the latest content each time a new page is built now now that i'm done explaining to jamstack i want to make it clear that the jam stack is not the perfect solution for all websites jam stack is most useful for web pages in which you can source data at build time and pre-build your pages with that out of the way i'd like to pass on the presentation to jeremy thanks canal so as granola stated the jam sac is a web application architecture that shifts as much as possible the building of pages from runtime to pre-run time so it's especially advantageous when content can be sourced pre-run time in such a case the entire site can be pre-built and served from a cdn which gives you the advantages in performance security and scalability that chronolog already spoke about now that we understand what the jam stack is and the advantages it provides let's take a look at what is required to launch a jamsec application first if you were to do so manually so this is a high level breakdown of the services that you would require and the steps involved in provisioning those services if you were to do so manually you'd require storage to host the site a cdn to distribute the site serverless functions to at a minimum invalidate old content on the cdn a server to listen for web hooks from git repositories build the site from source files and other data and said send the built files to a hosting environment you'd also need to set up roles and permissions on each of these services to define how they can interact with each other as well as roles and permissions for each user that has access to these services we've laid out 16 high-level steps here to provision these services but in reality each of these steps has sub steps each service requires configuration to work with the jamsec architecture and there's a lot that goes into setting up ec2 to handle building the site which we'll get into in more detail later in the presentation in addition this needs to be completed for every application a developer creates it's a very time consuming process that requires fairly in-depth knowledge of the cloud provider and its services then subsequent to provisioning the underlying infrastructure for your application a jamsec application requires certain procedures to deploy updates in addition to committing your changes to a git repository you need to initiate the build process through a static site generator as a reminder as chronolog discussed a static site generator essentially combines the developer's source code with data and generates static assets in html css and javascript source code is hosted in a git repository and data can come from a variety of sources such as markdown files in a git repository or content hosted with a content management system now once the site is built the developer needs to upload these files to a hosting environment then ensure that the previous build is invalidated on the cdn so that end users are always seeing the most recent version of the site the developer would have to follow these steps every time they make a change to their code base which may be okay if they're making very infrequent updates but in most cases this is a workflow that you would want to automate to save yourself time and eliminate potential for errors so let's look at what type of systems you need to implement in order to automate this workflow essentially you need a system to detect changes and pull code on every update you need a system to build the site a system to deploy the build site to hosting and distribute to a cdn and a system to invalidate old content you could build each of these systems yourself and like for some organizations that might be a realistic thing to do but for most front-end developers and teams that just want to focus on building their application it doesn't make sense to provision all these resources and build and implement these systems manually so wouldn't it be great if there was a service that does this all for you well there is and here we're referring to it as jam stack as a service or jazz so a jazz provider manages the build and deployment processes for developers so all the developer needs to worry about is writing their code and committing it to a git repository everything else is handled by the js and the application is distributed to the end user by a cdn so essentially a js is providing the jam stack workflow which we already talked about to developers and this is what the build and deployment processes look like from a high level so a commit pushed to a git repository initiates a web hook that notifies the build server that a change has been made the build server pulls the git repository parses the source code and determines whether it needs to reach out to any other data sources so say in the example of a blog where we host blog posts in an external content management system the source code in the git repository will be configured to let the static site generator know that it needs to reach out to that content management system api and grab the relevant data once the server completes the build we move on to the deploy stage where the assets are sent to hosting distributed to a cdn and the previous build is invalidated on the cdn this is just a high-level overview we're going to dive into each of these steps in more detail in the upcoming slides now there's already many existing providers that handle build and deployment of gemstack applications here we're highlighting a few net netlify gatsby cloud aws amp amplify and versal which are some of the most prominent providers in the industry now they all provision the underlying architecture and handle the build and deployment processes for developers just as we've discussed and also have several other features and in most cases are a perfect solution for deploying a jam stack application but with the exception of versailles they are all proprietary they're not open source they all have set fee structures above and beyond their free tier and they all limit the developer to the provider's set architecture this is essentially why we built jade shade stands for jam stack deployment environment first of all jade is open source so developers can take the source code change it make it their own customize it as they see fit next fees are based solely on usage of underlying aws resources jade provisions aws resources for the developer which means they're not tied into any set fee structure but rather charged based on usage of those underlying resources and allows customization of all individual underlying resources on aws so the point being that jade is built for developers that want full control over their infrastructure without having to provision that infrastructure themselves okay so we talked about what the jam stick is when the gemstock architecture is most advantageous and we've discussed why the jam stack architecture is advantageous especially in circumstances where content can be mostly sourced before runtime and we discussed the typical underlying infrastructure of a jamstack web app so let's dive into what jade is at its core when you initialize jade it provisions these six underlying aws resources it configures these services and sets permissions and roles for how the services may interact with each other and which users may interact with those services it then connects these services with a github repository to be clear aws and github are by no means the only providers of cloud and repository hosting services that exist we simply chose them because of their prominence so jade connects public github repositories and these aws services to automate the build and deployment process for developers so again as we discussed before the goal being that the developer makes a commit to their git repository and both the build and deployment processes are handled by jade to deliver the application to the end user this is essentially how the underlying aws services are broken out into the build and deployment processes ec2 is responsible for handling the build and the rest of the services are involved in deploying the application we'll get into more details of each of these processes coming up so perhaps the most important piece to understand and where the jam stack differs most from a standard web application architecture is the build step here's a high level overview of what jade's easy 2 instance looks like where we handle the build jade stores configuration related to the user and the provisioned aws services it has an nginx web server set up as a reverse proxy to handle incoming requests it has a node application that we built which contains the logic used to handle the build process and we maintain a copy of the most recent source code from the project's git repository as well as the most recent build of the application zooming in on our node application it's essentially comprised of four distinct components server to handle routing of incoming requests from nginx build together the required resources and build the application logger to log the results of each build and update to send the built application onto the deploy process so let's take a look at how jade handles the build and deploy processes from start to finish starts when the developer makes a commit to github during initialization jade provides a url to the developer to add to their github repository web hook settings so that anytime a commit is pushed to that repository a web hook is sent to jade's ec2 instance and its first stop is the nginx web server nginx ensures that the request is coming in on the correct port if not it sends back an http error response code 502 indicating a bad gateway if the port is correct the request is proxied to jade's node server the node server is set up to accept requests from the slash web hook route if the request is sent to a different route the server sends back a 404 not found error if the request is to the correct route the server passes the request to build.js build.js pulls the master branch of the repository from github and checks to see if it's changed since the last build if it hasn't changed jade sends back at 202 indicating the request has been accepted for processing but not completed but if the master branch has changed jade sends back at 200 indicating the request succeeded and initiates the build process this is where the process of building the application is actually initiated so jade utilizes the user config stored on ec2 which holds information such as the build command for the static site generator utilizes the source code already pulled from github and it reaches out to any external data sources such as a content management system these are all the resources required to build the application taking a deeper dive into the build process first of all jade is configured to use gatsby as a static site generator and contentful as a content management system there's many options out there for both static site generators and content management systems we chose gatsby and contentful just just because of their prominence other static site generators and content management systems could be utilized with further configuration by the developer but jade is built to work out of the box with these two tools when the build process is started gatsby compiles the source code received from github with data received from contentful minifies the source minifies the code to save space transpiles it to code that can be interpreted by all browsers and bundles the assets into a static web application once the site is built jade utilizes the aws configuration which holds the s3 bucket information to send the files off to the deployment process now moving on to deployment the key components involved are an aws s3 bucket that stores the built application an aws lambda function that invalidates the old build on the cdn and a cloud front distribution as a cdn which distributes the application to the end users let's walk through the process of what happens when a new build is received by s3 so the s3 bucket that stores the live build of the application has an event associated with it such that whenever a new build is uploaded to the bucket the event fires and triggers an aws lambda function this function is responsible for invalidating the previous build on the cdn a quick aside here we wanted to highlight that cdn invalidation can be difficult first of all assets distributed by aws cloudfront expire in 24 hours by default what this means is that even though we connect cloudfront to the s3 bucket that hosts the application when a new build is uploaded to the bucket cloudfront will not pull those files until 24 hours from when the last build was uploaded you have to let cloudfront know by invalidating the older files or pointing to newly version files the most updated files are versioned by the static site generator with the exception of those which aren't meant to be cached ever such as index.html which essentially points to all the other assets but individual files such as index.html can be invalidated we took these characteristics of how the static site generator versions most files and that we can invalidate individual files on the cdn into consideration when we designed our process for invalidating the previous build aws recommends that you minimize the number of invalidations because only a thousand invalidations are provided each month for free now if we were to invalidate every file on every deployment these invalidations would add up very quickly so to minimize the number of invalidations while also ensuring that users always see the current version of the application jade invalidates only index.html since that file references the newest version of all other files this forces the cdn to pull the most up-to-date files from s3 and the user will always see the most up-to-date version of the application and that's how the new files are distributed to the cdn which completes the deployment process and with that i'd like to pass it on to edmund to talk about jade's features and the evolution of jade's architecture thanks jeremy okay so we've talked about what gemstack is we've discussed the different types of infrastructure that's necessary for jams that app and the gemstackers service and we've also talked about jade as a core at its core now i'd like to discuss how we thought about jade and how we decided to move forward with new features that we thought would be relevant to teams and developers with instance interested in the jam stack and so at first like to start with the slide on multi-user control so let's say that you have a team of very enthusiastic developers who've just learned about the jam stack and listen bob are both very interested in starting a javascript web application but based on the current configuration of our of our framework only one user can control j at a time as you can see alice's machine has both the aws permission and jade configuration file necessary to update and play with her infrastructure as necessary that means that bob who doesn't have these two pieces of information will not be able to access the aws site what what could be a possible solution is that the analyst could share her files with bob however there will be instances where the config file doesn't get passed across or some files are not up to date and as a result even though bob is able to access aws and permission is granted his command still fails ultimately so we decided to go and solve this by allowing the use of cloud-based permissions and configuration files stored only on aws now when alice decides to update and initialize a new ada a new jade framework she runs the command jade in it on her computer it spins up a im api it starts to create a jade group and now alice and bob can both be manually registered as part of the users that have permission to edit the jade framework subsequent to this a new jade file is created and uploaded to dynamodb which is basically a nosql database that is acting as jade single source of truth that means that if other services require some information they can retrieve it from dynamodb and if the user wants to see the latest configuration of their files say what the build command is or when it was deployed they will be able to go to dynamodb and see that information on right away now with this configuration set up alice and bob are both able to run jade commands that permissions will be checked based off the ion configuration then they would update the configuration files on dynamodb automatically and finally the command will be successful so because jaden because alice and bob are both very enthusiastic developers they also want to consider running multiple jam stack applications so this is what happens in a zoomed out view as a zoomed in view of a jade server running 1j app currently what you can see on the left is the core structure that we had and on the right is the ec2 memory usage of the application based on our experience of developing jade we realized that the kind of supporting files that are used by gatsby and other and all of its dependencies can take up a lot of space and because of this high amount of space if they wanted to deploy another app they may actually face a problem where they don't have enough memory to support both apps at the same time this means that not only is ec2 running out of memory it also means that ec2 is now very cluttered because it has two different jade servers running at the same time it's sharing user configuration and it has a too many concerns at the same time now one one thing that we could do to solve the memory issue is to scale vertically and this is the power of giving developers a framework to work with we can allow them to upgrade their ec2 server from the three tier which we are currently using to a more powerful server which would have even more memory but vertical scaling has its own drawbacks and at some point there will be a number of apps which would restrain the which will cause the memory issue to rise again so instead we allow kind of horizontal scaling but each j app now uses its own dedicated server so on the left if you run a new j application j at opel it spins up one ec2 server for you and if you run jade at topaz you and add another ec2 server that's running parallel to the one that you have previously for opel this actually introduces a new realm of exciting opportunities for us to allow the users to add more control to their aws infrastructure so from the command line we implemented a command which allows you to freeze your ec2 asset your ec2 server so if you run jade freeze opel the ec2 server will stop running which means that it's not waiting and well it cannot receive a new build red book but it also means that it's not incurring cost let's say you're not developing on opel you're only focusing on topaz for a few days or just that day itself and when you want to go back to developing on opal you run jade unfreeze opal and you're ready to develop again within a few seconds another interesting problem we faced when we were experimenting with the different types of builds that we could do is that we realized that the node version inside ec2 that we were running was always going to be the latest version of node and that latest version we were using was currently node version 14. however we realized that there could be users um that were dependent on earlier versions of node that we weren't using in the ec2 server so as you can see in the top diagram get bv2 runs perfectly fine with node version 14 but on the bottom diagram get b version 1 does not run with node version 14 and so that causes a dependency error but we realized we could do is to um use docker containers to contain build dependencies and allow the user to have specific control over which build environments they were using for their for their build tools so on the top we allow you to have a docker container running node version 12 and gatsby version 2 and on the bottom diagram no version 8 running gatsby version 1. as a result both build environments are now containerized and no longer depend on the node version that we use to run our node application which means that there's a nice separation between the built the build itself and the ec2 servers configuration the next thing that we wanted to implement was the was back to pronounce point about atomic deploys being a key part of the jamstack workflow so users may want to view previous builds and let's say you've uploaded a new build and you can now see it on the on your s3 bucket this version 1 lives there but as soon as you update your code a new build gets pushed and it overwrites the previous version which means that actually version one has now been removed from storage and version two is the one that's being used by the cdn next so we decided that we would have separate buckets for live and historical builds so that every time a new build is made we will update the live build and allow it to delete the old version as before but we'll also zip the build files and store them in a separate build called historical builds and this zip file will be accessible for you so that if you wanted to go back to the previous version and see how it ran you'll be able to do so very easily another interesting thing that we thought about was that during the development process if you're a developer and you wanted to see what your site was going to look like there wasn't actually a way for you to build those files see the site before it actually gets pushed onto the live site so let's say you run the command git push origin master from your terminal what happens is as expected the command gets sent to github and so does their source code the files are built because of the github web hook this build file gets uploaded to the s3 bucket and then the distributed site gets pushed onto the cdn but if you did the same process again and you had an error or you accidentally changed the colors you can see the color has now been changed from green to brown then you only detect the error as soon as it's been deployed to the cdn and by that point it's too late because the end users can already see the brown side when what you wanted was a green site so what do we do to fix this problem we allowed you to use a staging branch which lets you see your site before you actually deploy it to the cdn so as you can see on the top part of the diagram if you type git push origin staging your push gets built your push goes to github it gets built and then it gets deployed to a staging bucket instead of the live bucket and so then the developer can go into aws look at the s3 files that have been having pushed and they can preview the site and check that everything works as intended and only after that they can push to master and then the distributed site gets pushed to the cdn at this stage however we realize that managing versions on aws has become extremely complex not only do you have to be aware of the historical builds you have to be aware of the staging branch as well the branches that you're using for master and and staging and that's just a lot to keep track of beyond what you can see on sg so if you went to the aws console you'd have to go to s3 check the three different buckets go to dynamodb to update the configuration and then finally go to cloudfront and then manually invalidate the cache which we thought is a lot of complexity that is not desirable so what we decided to do was to allow you to have an admin panel that abstracts this complexity away for you first we see the we use the command line again to spin up the admin panel this allows you to see the preview of version one and version two you get to see other relevant information associated with it and then you choose you get to view the site before you deploy it and all of your other information will be available for you to set up webhooks and stuff like that so this is what our admin panel looks like when you run djade admin command it spins up a local server running express.js that local server communicates to aws on your behalf and then it returns information and builds a react dashboard for you so if you look at the react dashboard you can see that we're looking at test project it has the link to the production site a link to the staging site a link to the actual github repository and the workbook address for you to connect your github with the ec2 instance finally it stores all the build history that you had before and you're allowed to build download the build files to see which version you like and then make that commit to the latest version of your master branch in github so that was a long journey from where we were from the understanding the elements of a jazz provider so just to give a review um this is these are the key steps that we identified that jazz providers deliver the git repo is connected to a build server the build server pulls information from the cms that build file gets sent to the storage storage then updates the cdn and the function as a service invalidates the cdn accordingly so the end users can see download files directly from the cdn and this is the final architecture of the jade framework where we use github connected to an ec2 server ec2 server contains the docker build command sorry docker built environment pulls from the cms and then sends those build files to the s3 buckets live staging and history and some updates are made to the dynamodb configuration file before the update to the cdn is made and then it reaches the end user so some future work that we would like to implement so one of the first things we will do is to allow users to select the live site from the admin panel as explained before the versioning allows you to see and download the previous builds but it doesn't allow you to choose so that would be one of the first things i think that we would be interested in developing further the second thing is a very common feature amongst uh jess providers which is to allow you to control your own lambdas your own serverless functions directly from a console so if you look at netify or if you look at for sale they have features that allow you to see all the relevant servers functions that you have and we think that that would be a really nice feature to work on next we also want to allow sourcing from other git hosting providers such as gitlab and bitbucket and to allow the automatic setup of webhooks for each j app so that you don't have to do it manually you just need to give permissions to these git hosting providers and the final thing that we'll be interested in doing although developers who are interested can't do this manually themselves is to support other languages in that build step with the docker container so allowing the use of ruby and jekyll or go and hugo so this will allow for more types of ssgs and more build files to be to be supported and ultimately that that piece would be really interesting to see as more people use new types of ssgs so that's the end thank you very much for listening and thanks for coming today 