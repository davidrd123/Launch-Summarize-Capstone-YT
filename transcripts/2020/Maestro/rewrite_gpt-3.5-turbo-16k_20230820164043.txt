Today, we are excited to present Maestro, a framework that offers rapid iteration for serverless orchestration. In this presentation, we will cover the concept of serverless and different patterns related to working with serverless architecture. We will also dive into orchestration patterns and how they can be used in real-world examples. Additionally, we will discuss AWS Step Functions, an existing solution in this space, and highlight some other solutions built on top of it. Finally, we will introduce Maestro and discuss what we aimed to achieve and what we have accomplished so far.

When we talk about serverless, it is important to clarify that it refers to offloading application code to the cloud. This has several benefits, such as eliminating the need for developers to manage local servers, resulting in significant time and cost savings. In this presentation, we will specifically focus on the subset of serverless called Function as a Service (FaaS). FaaS is characterized by small applications with a single responsibility, running on individual containers that can be quickly spun up and down in response to events. This scalability allows the cloud provider to charge only for the time when the functions are executing.

Currently, AWS Lambda is the most popular cloud provider offering in the serverless space. It has certain limitations, such as an execution time limit of 15 minutes by default. However, Lambda allows developers to write code in seven different languages and provides up to 250 megabytes of deployment size and 3000 megabytes of RAM. Despite its advantages, there are some challenges associated with working with Lambda. These include state management, cold starts, and limited visibility into the system during execution.

To address these challenges, several patterns and practices have emerged for developing serverless applications. These patterns cover various aspects of serverless development, including availability, failure handling, event management, and communication between services. One common area where patterns are applied is orchestration, which involves managing the workflow between multiple Lambda functions and other services.

To illustrate the concept of orchestration, let's consider an example scenario. Imagine a sole developer in a startup who is tasked with building an application that allows employees to request the provisioning of a resource, which then needs to be approved by a manager. If approved, the resource is provisioned, and the employee is notified. To tackle this scenario, the developer can break down the process into components represented by different Lambda functions.

There are several challenges the developer needs to address along the way. These include chaining the execution of different components, handling input and output processing, managing state, dealing with asynchronicity, and handling branching logic. Additionally, communication between services presents its own set of challenges, including where to put orchestration logic and how to ensure reliability and resilience in case of failure. Moreover, the limited visibility provided by AWS makes it difficult to monitor and debug the execution of Lambda functions.

To tackle these challenges, different orchestration patterns can be applied. For example, the proxy lambda pattern allows a proxy lambda to sit in front of another component and conditionally invoke it. This pattern can be useful for validation purposes. Another pattern, the orchestrator lambda, serves as a central point of communication between different services and orchestrates the workflow. This pattern allows for executing components synchronously or asynchronously, depending on the needs of the application. However, with these patterns, there is a risk of overloading the orchestrator lambda and mixing business logic with orchestration logic, leading to poor separation of concerns.

To address this issue, Maestro introduces a dedicated orchestration layer. This layer allows for separating the business logic from the orchestration logic, making it easier to iterate on workflows without modifying the orchestration logic. With Maestro, developers can focus on refining and improving the business logic while maintaining a clear separation between different concerns. Additionally, Maestro provides predefined primitives such as retry, branching, and timeout, making it easier to implement complex workflows.

To implement this orchestration layer, Maestro leverages AWS Step Functions, which is Amazon's implementation of a state machine. Step Functions allow developers to define workflows using the Amazon States Language (ASL), a declarative language written in JSON. With ASL, developers can describe their application as a mixture of inputs, states, and transitions between those states.

Step Functions provide different types of states, including lambdas, choice states for branching logic, and weight states for pausing execution for a specified time. By leveraging Step Functions and ASL, developers can easily define complex workflows and orchestrate the execution of multiple Lambdas and other services. The visual representation of the state machine in the AWS console provides developers with a clear view of the workflow and makes it easier to understand and modify the orchestration logic.

In conclusion, Maestro aims to simplify serverless orchestration by providing an efficient way to separate business logic from orchestration logic. By leveraging AWS Step Functions and the Amazon States Language, developers can define complex workflows and manage the execution of multiple services. This separation of concerns enables faster iteration and easier maintenance of serverless applications. With Maestro, developers can focus on improving the business logic while relying on the orchestration layer to handle complex workflows and ensure reliable execution. The state machine focuses on the transitions between each step, as described by Rowan Udell, an AWS Premier Consulting Partner. State machines allow you to define your application based on inputs, states, and the transitions between those states. AWS offers a state machine solution called AWS Step Functions, which serves as an orchestration layer for serverless resources like Lambdas. To define a state machine using Step Functions, you utilize Amazon States Language (ASL), a declarative language written in JSON. The ASL code consists of a start state and a list of states, each representing a different step in the workflow. These states can include Lambdas, as well as other types of states like choice states for branching logic and weight states for pausing execution. Each state in the state machine specifies the next state to be executed, and the end state signifies the completion of the state machine.

Let's take a look at a simple example of a state machine written in ASL. In this case, there is only one state called "Hello World," which references a Lambda function that returns the string "Hello World." After the Lambda completes its execution, there are no further states defined, indicating the end of the state machine.

Now, let's explore a more complex example: the manager's decision-making process. Suppose the manager needs to decide whether to approve a resource provisioning request. This workflow can be implemented using Step Functions. The state machine incorporates several states, including choice states to handle different outcomes based on the manager's decision. The provisioner state demonstrates how retry logic can be implemented using the built-in retry feature in ASL.

After executing a state machine using the AWS console, you can access an interactive execution event history that provides insights into the inputs and outputs of each state transition. Additionally, the visual workflow in the console displays color-coded representations of the outcomes of each state transition, enabling easy analysis.

In summary, Step Functions offer orchestration capabilities, simplified through built-in features like choice states, retry logic, unified logs, and visual workflows. However, working with ASL can sometimes be challenging, particularly due to its lack of intuitiveness. Additionally, managing infrastructure deployment via the AWS console can be tedious and error-prone. To address these challenges, several frameworks have been developed.

Firstly, the Serverless Framework is a widely used tool that leverages AWS CloudFormation, allowing the developer to create a configuration file encompassing all serverless resources. While it provides support for Step Functions through a plugin, the templates it offers require manual configuration, and converting ASL from JSON to YAML might be necessary.

Then, there's the Serverless Application Model (SAM), an AWS tool similar to the Serverless Framework. SAM also utilizes CloudFormation, automatically generating permissions for resources. It offers a single template for deployment, but the deployment process can be time-consuming, requiring manual responses to prompts.

Regarding purpose-built frameworks supporting Step Functions, there aren't many available options. One such framework is Step, developed by Coinbase. This framework allows developers to define state machines using a higher-level language like Go. However, Step has limited adoption and is currently in beta. Additionally, Step can only modify existing state machines, making it less suitable for quick iterations during workflow development.

Now let's delve into some automation tools provided by AWS that can aid in deploying infrastructure. First is AWS CloudFormation, which enables you to create a configuration file encompassing all serverless resources, known as infrastructure as code. Frameworks follow either the CloudFormation approach, where configuration files are compiled down to CloudFormation, or the second option: utilizing AWS Software Development Kits (SDKs) to interact directly with individual resources. The choice between these approaches often depends on the framework's design philosophy and purpose.

To address the pain points of working with ASL, we developed a specialized framework called Maestro. By illustrating a walkthrough of working with the AWS console to create a state machine, we can highlight the challenges faced by developers. The process involves navigating to the Step Functions console, selecting the state machine service, and providing the ASL definition. Creating the ASL definition can be time-consuming, especially for developers unfamiliar with the language. After defining the state machine, the developer needs to provide valid ARNs (Amazon Resource Names) for each Lambda in the state machine.

Creating a Lambda in the AWS console requires several steps, including configuring the Lambda, defining its runtime, and setting up the necessary permissions. Once the Lambda is defined, the ARN can be copied and pasted into the state machine definition. This process is repeated for each Lambda in the workflow. As a whole, working with the AWS console to deploy infrastructure alongside a state machine can be laborious and error-prone.

In conclusion, while AWS Step Functions provide orchestration capabilities, there are challenges when working with ASL and deploying infrastructure manually through the AWS console. Several frameworks, including the Serverless Framework and SAM, offer support for Step Functions, but they were not specifically designed for this purpose. Purpose-built frameworks like Step provide higher-level language support but may have limited adoption. Introducing Maestro, our specialized framework, addresses the pain points of working with ASL and simplifies the deployment process. With Maestro, developers can easily orchestrate their serverless workflows, saving time and effort. In this coding Capstone project video, we will discuss the process of setting up a working application using the AWS console. We have a diagram that shows four lambdas: the requester, the manager, the provisioner, and the access granter. Additionally, there is an orchestration layer represented by an AWS Step Functions state machine.

To begin, we need to navigate to the Step Functions web console. This can be done by opening a new browser tab and going to console.aws.amazon.com. Once there, we select the Step Functions service. This takes us to the state machine-specific page where we can find the "Create State Machine" button. Here, we can provide the Amazon States Language (ASL) definition for our application. This step can take several minutes or even hours for developers who are not familiar with ASL.

After providing the ASL definition, we need to make some final adjustments, such as changing the retry back off rate. Once everything is set, we click the "Save Definition" button to save and execute the state machine. However, we encounter a problem. The AWS console requires valid ARNs (Amazon Resource Names) or resource names for each lambda in the state machine. This means that before we can save the state machine definition, we must first create all the lambdas.

To create a lambda in the AWS console, we follow a series of steps. We navigate to the Lambda console and configure the lambda by giving it a name, configuring the runtime, and setting permissions. After defining the lambda, it can be saved. Once saved, we retrieve the lambda's ARN, copy it, and paste it into the state machine definition tab. This process needs to be repeated for each lambda in the workflow.

Once all the lambdas have been created, we can go back to the state machine tab and provide the necessary configuration details, such as names, tags, logs, and permissions. After saving the definition, the state machine is ready to be executed. This manual setup process is not only tedious but also time-consuming. To tear down the resources, we need to navigate to the specific service in the AWS console, select the resource, such as a lambda or state machine, and follow a series of steps to delete it.

Now, let's explore a solution to this cumbersome process: Maestro. Maestro is a framework for building, deploying, and iterating with Step Functions locally on a development machine. It is open source, runs on Node.js, and is available as an npm package. With Maestro, the deployment process becomes much simpler and faster.

When deploying with Maestro, we start by creating a new Maestro project with a single command. We can optionally modify the project template provided. Once we finalize the tweaks, we deploy the project to AWS using the Maestro deploy command. The deployment process takes about three seconds, a stark contrast to the lengthy and cumbersome manual deployment using the AWS console.

Let's take a closer look at the deployment process using Maestro. We start in the terminal by creating a new Maestro project. This command prompts us to choose a template for our project, making it easier to bootstrap our workflow. After choosing a template, we can see the project structure in the terminal. Once we make our final tweaks, we deploy the project with a single command. Within three seconds, all the resources are deployed to AWS, as seen in the AWS console. To tear down the project, we use the Maestro teardown command, which takes about two seconds to remove all the resources.

Maestro uses templates to simplify project creation and reduce the overhead of manually writing ASL. It provides eight official templates, and developers can create and share their own templates. The Maestro new command creates a new project based on a chosen template. This allows developers to work locally, reducing the need to navigate the AWS console. The deploy command rapidly deploys the entire project in seconds, far faster than other frameworks like AWS SAM. The teardown command removes the project and its resources quickly, enabling rapid iteration.

While Maestro offers significant advantages, it also has some limitations. Currently, Maestro supports only the Node.js 12.x runtime for deployed lambdas. It also lacks native support for other AWS services, such as DynamoDB, SQS, EC2, and ECS. Developers must manage the deployment and teardown of these services separately. Additionally, Maestro's configurability on a per-project basis is limited.

During the development of Maestro, the team encountered several challenges. One challenge was working with AWS IAM (Identity and Access Management). We had to wait for IAM roles to propagate to all AWS data centers when using a newly created role for a lambda. Determining the roles, permissions, and policies required for the state machine and lambdas also presented challenges. We faced difficulties reconciling asynchronicity and concurrency to create a stable and fast deployer. Furthermore, Maestro operates in a niche ecosystem with few frameworks specifically designed for Step Functions. As a result, we had to innovate and overcome unique challenges.

The Maestro team continues to improve and develop the framework. In the future, we aim to provide official support for other AWS services, enabling native deployment and teardown. We are committed to enhancing Maestro's capabilities and expanding its versatility.

In conclusion, Maestro offers a powerful solution for deploying and iterating with Step Functions. It simplifies the setup process, reduces the need for manual configuration, and significantly speeds up deployment and teardown times. With its template-based approach and fast deployment, Maestro empowers developers to focus on their business logic and iterate rapidly. Thank you for joining us, and we welcome any questions you may have. In our coding Capstone project, we encountered some challenges in reconciling asynchronicity and concurrency. Our goal was to have a rapidly deployable and stable system. It's important to note that we are working in a niche ecosystem with limited frameworks for handling step functions. As a result, we had to pave our own path and continuously develop our project.

One of our future goals is to officially support other AWS services like DynamoDB, SQS, EC2, ECS, and more. This way, our deployer and undeployer in Maestro will be able to handle these services natively. Thank you for joining us today. If you have any questions, we are here to answer them.

One question we received is about how we dealt with resource dependencies during deployment with AWS. This is a broad question, but let me explain it in the context of our example workflow. To deploy our workflow, we had to consider three services: Step Functions, Lambda, and IAM. Both Step Functions and Lambda depend on IAM. Therefore, in order to deploy to AWS, we needed to ensure that we could create the Step Functions and Lambda services by having the appropriate IAM roles.

However, we encountered a challenge. Even though the SDK allows us to check if the IAM role exists, it does not guarantee that the role has propagated to the other services yet. We had to implement a retry mechanism to wait for the IAM role to propagate to the other services. Through trial and error, we determined the backoff rate for these retries. Thus, our main dependencies were understanding the order of role creation and ensuring the roles had propagated to the different services.

If you have more specific questions about resource dependencies or any other related topic, please feel free to ask. We appreciate all the thoughtful questions and discussions.

We want to highlight a comment made by Cornell, who rightly pointed out that Step Functions and Lambdas work together and are not mutually exclusive. The goal of Step Functions is to abstract the logic orchestration, which otherwise would require choreographing the Lambdas. With Step Functions, you can keep your business logic contained in separate Lambdas while coordinating their execution using workflows.

We received another question about the terminology of orchestration versus choreography. In this new field, there is a lot of terminological overlap and confusion. In our research, we relied on resources that use the term orchestration to refer to the coordination of services in a workflow. We found academic papers that discuss various orchestration patterns, especially in relation to Lambda functions. While there may not be well-established best practices, these papers were valuable in providing insights into orchestration patterns.

If you are interested, we recommend checking out the Serverless Chats podcast by Jeremy Daly, an AWS Serverless Hero. It covers various topics related to serverless development and can provide a wealth of information.

Now, onto the question of whether we considered any cloud providers other than AWS. We did explore other providers, but ultimately chose AWS because of their strong position in the field of orchestrating serverless functions. AWS Lambda is the leading service in this area, making it the most suitable choice for our project. While other cloud providers, like Microsoft's Azure, offer similar services, AWS stands out as the prominent provider in this domain.

At this time, we do not support any other cloud providers. However, it's worth mentioning that Netflix has developed an internal tool called Conductor, which shares some similarities with Step Functions. If you find our discussion about Maestro and its capabilities intriguing, you may want to explore Conductor and other similar services.

We appreciate everyone who has been following our journey and supporting us throughout this Capstone project. It has been a rewarding experience, and we are grateful for your interest. Thank you for being here and take care.