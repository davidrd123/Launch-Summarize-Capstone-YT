foreign thank you for coming to our presentation on Nexus an instant graphql framework my name is Felicia and I'm joined by my teammates Kim Ben and Matt together we built Nexus an open source graphql framework here is Nexus in a nutshell we take a user's existing data sources generate a graphql API and deploy that API to the cloud so that all the user's clients can make requests to a single endpoint Nexus helps with the middle part of this diagram building and deploying the server quickly before we dive deeper into Nexus we'll talk about rest apis and some of the problems that have led to the use of graphql next we'll discuss the challenges developers face when they Implement a graphql API and some of the existing Solutions finally we'll share why and how we built Nexus and the decisions we made along the way now let's jump into apis when we decouple parts of a system to separate concerns we need a way for those parts to communicate in a standardized way an API or application programming interface serves as that link the most common API pattern today is rest a restful API provides endpoints corresponding to an application's significant resources one reason for its popularity is that rest is simple for developers to build and understand Russ particularly shines in credit applications where client Pages correspond directly with rest endpoints however issues can arise when a new client use case with different data access patterns needs to integrate with an existing rest API each client type has different data needs so a fixed endpoint may not serve both at the same time over fetching is when one client receives too much data that it won't use due to a mobile device's limited screen size fewer details can be displayed on this example catalog page compared to the desktop application this means less data is needed from the API to load this page fetching too much data can lead to a user experiencing higher response times resulting in a slower page load fetching too little data or under fetching can also be a problem when an endpoint doesn't contain all the required data the client must make additional requests this can cause a waterfall effect resulting in multiple round trips between the client and server round trip latency from additional requests can significantly increase page load time this is especially problematic on mobile devices where a typical page takes 88 longer to load than on a desktop Beyond application performance overfetching and under fetching affect how front-end developers spend their time when an API is not a good match for the front-end client's data needs developers must spend additional time figuring out how to fetch data filtering over unnecessary data and orchestrating many API calls in some cases dealing with data can take the majority of front-end development time leaving little time to spend on building critical UI fortunately development teams face with supporting multiple clients are not limited to using fixed endpoints that cause under fetching and overfetching they can adopt the rest apis to make their endpoints fit each front-end use case one option is to add a new endpoint that will only return the data necessary for a particular use case another is verging the API for each use case providing separate sets of endpoints for each type of client with data returned based on the needs of that client a third option is allowing clients to customize a request with query parameters while customization can make a rest API more flexible it can also make it more difficult to work with in 2015 graphql emerged as an option to create apis that are highly customizable and flexible to handle many different clients several key factors can lead a team to adopt graphql for their API when they need to support multiple front-end clients that have different data access patterns and different bandwidth needs foreign access is nested interrelated recursive or when it varies frequently or when data is distributed across many different sources so how is graphql different from rest a rest API exposes many endpoints that correspond to resources in contrast a graphql API exposes a single endpoint this difference contributes to many of the benefits graphql can offer each front-end client uses the single endpoint to request all the required data front-end developers no longer need to keep track of many different endpoints and their standards back-end developers no longer need to add new endpoints adapt their endpoints or version them for different client types with a single endpoint the source of data is simplified but how does graphql support fetching multiple resources for different client types using a single endpoint by introducing a query language that clients use to request the data they need queries are customizable and the same query can fetch multiple resources at once the response will contain precisely the information requested no more no less in fact query language is the ql in graphql here is a simple query this query requests a book resource by its ID in addition to the type of resource we also get to decide which Fields related to this resource we would like returned here we're requesting the title page count and author specifically the author's name when we send this query in an HTTP request the response from the graphql API matches the structure of our query this means our mobile client can query for one set of data about each book while our desktop client can query for a different set of data about each book this helps front-end developers save time and reduces the amount of data sent over the wire now let's recap what we've discussed so far apis help to standardize communication between parts of a system rest is the most common API pattern today but problems like Under and Over fetching can degrade performance and impact developer productivity in some cases like adding a new mobile application graphql was introduced to address these problems a graphql API exposes a single endpoint allowing different clients to request the data they need via customized queries now I'll hand it off to my teammate Kim who will just who will discuss graphql implementation challenges thank you Felicia so while the front-end developers enjoy the benefits of a single endpoint the responsibility for supporting a graphql API falls to the back end team building a server that integrates multiple data sources into a single endpoint can be a challenge for small teams who have limited resources or are brand new to graphql first developers must get up to speed on the graphql specification and learn about the unique components that make up a graphql server the learning curve with this can be steep because graphql differs significantly from rest they'll also need to decide on approach to writing their schema each approach has its own trade-offs which can complicate the decision and slow down the setup process secondly they must deploy their new server so that the front-end clients can query the API they need to decide which services to utilize among the many options provision and connect their chosen services and deploy and manage that infrastructure there are many steps that go into building and deploying an initial graphql service and this process can be time consuming tedious and error-prone Nexus was built to address these issues for small teams who want to get the benefits of graphql with less of the initial overhead of setting up their graphql server and deploying it to the cloud to understand the challenge that developers face while building and deploying let's first look at the components that comprise the graphql API on the server the main components of a graphql server are schema and resolvers a graphql schema defines the data types that can be requested and the relationships between types resolvers are functions that retrieve the requested data specified in the schema a type definition describes a queryable type within the graphql schema each type includes fields which define the properties of that type and each field can be a base type like string or integer fields are also used to represent association between types for example a type of book may have a field author which corresponds to an author type thus associating these two types resolvers are functions that fetch the data for each field in the query they specify how the data fields in the type definitions are retrieved before developers can create a schema they may choose between two approaches for schema writing schema first or code first the schema first approach utilizes the graphql schema definition language or sdl for short using sdl allows the developer to Define their schema and a programming language agnostic way since the sdl is not tied to a specific programming language the resulting schema is an outline of the data model that lacks implementation details in a schema first approach the resolvers are written separately and the developers chosen programming language however this approach involves a new language and the sdl defined schema is not complete on its own the separation of resolvers can lead to duplication requiring more maintenance over time for a code first approach the process begins with defining types along with the resolvers and a chosen programming language and this approach the code serves as the schema the code first approach is self-contained which means the entire schema is defined in one place instead of divided regardless in code form the overall structure of the data model is more difficult to quickly understand ably the complexity of a schema grows as multiple data sources are added many teams will opt to modularize their schema separating each data source into its own schema they will then need a strategy to combine them into a unified schema if they want to maintain the benefit of a single endpoint so we've talked about a few challenges facing developers as they begin to learn how to build a server in this new paradigm but building a server is only half the battle the developer still needs to deploy it in order for the front-end team to make queries to it this process can be complicated and involves many steps when deploying to the cloud there are a variety of platforms a developer might choose we focus on the most popular Amazon web services also known as AWS here is a list of resources that need to be provisioned in order to deploy a basic server on AWS each resource is provided from a different AWS service deploying on their own a developer will need to understand these AWS services and the resources they offer as well as choose the right ones to support their architecture these resources must also work together and coordinating communication between them can introduce bugs that are difficult to troubleshoot this can be tedious for the developer and increases the time spent to get a server running for clients to query currently AWS offers 200 plus services with more added all of the time the sheer volume of services plus their confusing names and acronyms has confused so many developers that they started writing AWS in plain English articles these articles strive to explain what each service is used for and what it should have been called instead so to assist the developer and provisioning resources AWS provides a few different tools first is the graphical user interface that is used for provisioning Resources by hand next is a command line interface that can be used to provision Resources by interacting with their API both of these tools strive to make provisioning convenient for the developer however they both involve dozens of sequential steps to provision and then connect all the different resources required in the slides shown you can see an example of the commands that need to be run in the AWS CLI to provision a single ec2 instance this is a simplified example the AWS CLI documentation for ec2 alone offers 575 commands this volume is what developers struggle with although having many options can be good it can also be hard to navigate them especially for someone new to infrastructure to recap developers brand new to graphql will need to get up to speed on the graphql spec this includes the schema definition language type definitions and resolvers they'll need to research and weigh the pros and cons of each schema writing approach and as they add multiple data sources they'll look at how to modularize the schema and then recombine them into a unified schema additionally they need to understand the many different cloud services needed to deploy their server the process of combing through hundreds of confusingly named services with thousands of commands and configuration options to provision Cloud infrastructure compounds the API setup process now large organization organizations can often afford to dedicate teams of developers to learn the graphql specification and Implement their API in-house on the other hand smaller organizations may decide early in their initial research that the complexity which graphql brings outweighs the benefits besides the performance benefits the secondary goal of switching to graphql is to save developers time so if the time spent getting the API running is more than the time saved on the front end switching to graphql will not appear to be worth it small teams can benefit from an alternative to building their own gracule API as they explore the graphql ecosystem they will find several existing solutions that can help make the process faster and easier now that we've discussed graphql implementation challenges I will turn it over to my teammate Ben to talk about existing Solutions and where Nexus fits in with them thanks Kim the use case we had in mind while building Nexus would have a few General requirements and an existing solution would need to allow the developer to set up a server in just a few minutes as well as combine multiple data sources into a single endpoint since that is frequently distributed across several services assisting with deployment would speed up the process further finally it should be extendable to accommodate future needs at the highest level of abstraction are managed to cloud services like hasura and stepson these are graphql API generators that automatically create manage and deploy the server for the user setup can be done in just a few minutes previous knowledge of graphql isn't required to get an API running most managed Services support a wide variety of data sources and offer the ability to combine data sources into a unified graphql layer they may not support every source so developers must ensure that those that they use are currently supported deployment is handled entirely by the service this is a primary benefit Outsourcing deployment results in a lack of control over the infrastructure that's the that supports the API and no opportunity to optimize the cost of that infrastructure the abstraction of both server generation and deployment makes it challenging to extend a managed service for example adding a data source that isn't directly supported may not be feasible managed Services typically offer a robust set of additional API optimization features like caching monitoring and observability a managed service abstracts much of the complexity away for the user leading to speed and ease of setup however they trade off extendability and control developers who want additional control and extendability can use several open source graphql API generator libraries like Wonder graph or graphql mesh generator libraries help to get developers started more quickly than if they were writing graphql code from scratch they still require manual setup and configuration which often involves some basic graphql knowledge and Library specific syntax generator libraries support a wide variety of data sources and they are also highly extendable because the developer has access to the code that is generated for sources that aren't supported they offer the ability to extend the schema manually or with plugins plugins are also used to add Advanced features for those that aren't offered directly the code can again be extended manually while Library documentation might include some basic deployment guides for different Cloud providers and architectures they don't typically provide direct assistance with deploying a server without deployment assistance developers have full control of their infrastructure decisions and costs but this comes at the expense of the developers time and the deployment complexity that Kim was just talking about we built Nexus to serve as a framework for quickly generating a graphql API from multiple data sources and deploying that API to a user's AWS account we wrap several open source tools for API generation and deployment speeding up the configuration and use of those tools Nexus uses graphql mesh a powerful graphql API generator to build the user server we reduce the configuration of this open source library to a few simple commands so no experience with graphql or the mesh library is needed to get started Nexus currently supports three data sources users can add their existing postgres database directly to their generated graphql server with only their postgres connection string they can add internal and third-party rest endpoints using a Json file that follows the open API specification they can even add other graphql apis to their server for data sources we don't currently support the user can extend their API using mesh's collection of additional sources or by manually extending the schema Nexus deploys the generated graphql server with a single command we make deployments simpler by abstracting infrastructure setup and making a few decisions for the user the deployed infrastructure is provisioned entirely in the user's AWS account and is fully modifiable by them Nexus doesn't offer Advanced features for fine-tuning the API like managed services do however users are free to add features as needed because the generated code lives on their local machine Nexus offers users some of the benefits of extendability and ownership that come with an open source Library along with some of the deployment abstractions of a managed service at the cost of advanced API features we built Nexus for teams who would like to deploy build and deploy a simple graphql API in just a few minutes with the option to extend it over time by simplifying the setup process we save developer time and make open source generator and deployment tools simpler to use we abstract the deployment process while using a user's AWS account the code that Nexus generates is user extendable to add new data sources and other API features as needed now that you've learned a little bit about Nexus we'll demonstrate a few of the features we've built to show the process of going from xero to deployed graphql API first we'll take a look at setup and building the server Nexus is available as an npm package Global installation gives a user access to all the commands and a local admin dashboard after installation the user can initialize a new project with the init command they'll run this in an empty project folder where we'll generate their server files we collect an initial postgres database as the primary data source to avoid confusing bugs later on when the server is generated the postgres connection string is validated for user when they input it and the user now has a basic graphql server built from their postgres database from a single Nexus command let's take a look at adding a second data source a few endpoints from a rest API adding the rest endpoints is similar to adding the postgres database the user provides the local or hosted location of a Json file that defines their endpoints following the open API specification adding a graphql data source is similar now that the user has multiple data sources connected it's time to start the API locally and test it the dev command opens a graphical interface for exploring the unified schema and making test queries this test server is identical to the API that will be deployed so it can be used to accurately pre-plan client queries while the server while the server generated and tested it's time to deploy Nexus simplifies the deployment process down to a single command Nexus deploy all the complexity of packaging the server code and provisioning the infrastructure is handled in the background by this command after a few minutes the server is deployed to the user's AWS account they'll get back a URL in the command line that they can then use immediately to query the deployed graphql server from any of their front-end clients now that the server is deployed the user can run the dashboard command to start a local dashboard that displays information about their server and advanced configuration options this includes the server status the endpoint an authorization key and data sources that they've added users can also add edit or delete data sources a local development server is also provided here for testing after changes are made to the data sources a helpful message will alert the user to redeploy so that the deployed server is up to date redeployment can be accomplished here in the dashboard with a single click or with the Nexus redeploy command if the user wants to remove the provision infrastructure at any time they can use the destroy command to de-provision all AWS services that have been set up since the infrastructure is deployed to their account they can make changes directly within the AWS console the destroy command ensures all services are removed at once so they're not charged by AWS for services that could be missed with manual deprovisioning now that you've seen how Nexus can help save users time we will go behind the scenes and explore how we built it my teammate Mac will cover Nexus architecture as well as some of the development challenges we encountered along the way thanks Ben go ahead yeah go for it sorry about that to start here's a diagram of the Nexus architecture that we'll be discussing in this in this section we can illustrate the Nexus workflow in two parts creating the graphql API server and deploying it first let's look at how we create the graphql API server for the user Nexus wraps the open source tool Gap kill mesh to generate schemas and combine them into a unified graphql API the main point of interaction between Nexus and graphql mesh is through a configuration file where yaml is used to specify a variety of options when a user interacts with the Nexus command line tool they start by entering the init command behind the scenes this command first installs necessary graphql mesh packages and creates a configuration file in the user's current working directory Nexus prompts the user for a postgres connection string which is then validated by using the PG promise library to make a connection to the database the validated database connection information is then passed to a template where we format it for the user into valid yaml and is applied to the configuration file additional data sources are added to the configuration file through the add command the user provides data source information to Nexus which is then formatted into yaml and supplied to the configuration file once the data sources are added graphql schemas can be created for every data source supplied to it each data source is consumed by a source Handler which introspects or gathers information about the data source the result of introspecting each data source is an individual graphql schema defined using the sdl these schemas can be viewed in the user's project directory the generated schemas are then combined into a unified schema schema customization in the form of Transformations can be implemented at both the individual and unified schema level for example we add an authorization middleware function to the unified schema for the user to prevent unauthorized access once the schemas are generated and combined and any modifications applied the local graphql server is ready to deploy next we will look at how we deploy the server for the user since we generate the graphql server on the user's local machine we need a reliable way to to deploy their application code and ensure that it runs on their AWS infrastructure foreign we chose to deploy the user server using containerization let's briefly discuss the process and clarify a few terms Nexus writes a graphql server code on the developer machine this application code has dependencies that also live on their machine if the application is transferred to a new machine errors may occur due to application dependencies not being present containerization encapsulates the application code libraries and dependencies that are required for the software to run doing this allows the application to be run on many different systems while reducing dependency issues Docker is an open source implementation for containerization some some key Docker terms include images image repositories and containers a Docker image is like a blueprint of the application which includes all the dependencies we store Docker images in a repository similar to how we store source code on a remote repository like GitHub foreign ERS are created through images and run the actual application code we can pull the docker image from our repository onto a cloud machine to run the application for Nexus creating and pushing a Docker image to a private Repository pulling the image down onto Cloud infrastructure and running a container from the image provided us a code deployment workflow that helped eliminate application dependency issues between the user's local machine and the cloud hosted machine foreign command we provision the AWS services to get their server up and running we start by locally building a Docker image from the generated graphql API server next we use terraform and infrastructure s code tool to create a private image repository on Amazon's elastic container registry we considered using aws's Services cloud formation as an alternative but decided against the proprietary tool in order to offer multi-cloud deployment in the future we then use the AWS CLI to push the docker image to the newly created private Repository we also use terraform to provision AWS elastic container service and fargate elastic container service makes managing and scaling containers easier fargate Works alongside to provision and manage all the infrastructure that is required to run the container meaning the user will not have to directly manage the underlying ec2 instances together they allow the developer to spend less time managing resources but still allow them to make changes or scale in the future additionally Nexus handles the setup of related infrastructure components such as virtual private Cloud subnets internet gateway Security Group router and routing table here here is a user's high-level AWS architecture after deployment is complete we now like to highlight a few of the technical challenges we encountered as we built Nexus part of the reason we developed the dashboard was to facilitate some Advanced server configuration that would require many steps if done at the command line we'll walk through one configuration optionally implemented connecting data across sources as the user makes some of these changes in the dashboard they'll want to test them quickly we needed a way to ensure the local test server was always up to date with the current configuration finally when changes have been made and tested we want to ensure the user will will redeploy their changes to their deployed architecture let's jump into why a user might want to connect data across sources and how we help them accomplish this here's the basic setup after a user has initialized their Nexus project they now have a graphql API that each client can query for the data they need while each data source has been combined in the unified schema there's still a problem data stored across multiple sources is often related an example could be product data such as books stored in a database with cart information stored in Shopify when separate data sources contain related data are combined in a generated server the relationship across sources isn't automatically recognized we can still fetch information from both in a single query however the relationship that exists between the two sources isn't represented in our graphql API this slide shows a query with the cart field from one data source and the books field from another data source when the relationship isn't defined in a graphql API over fetching can occur one of the main problems we're trying to avoid by using graphql in the first place since we can't shape a query that shows a relationship between the two data sources we treat we retrieve a response that has all the data for one cart and data for all books the front-end developer would then need to filter only the books that are required for this particular card our challenge was to find a way to help the user to find these inherent relationships between data sources within their graphql server if we can connect these data sources they can be queried together as shown here fetching a specific cart would yield only the books for that cart even when books come from another source in order to associate two data sources we needed to capture a few pieces of information We Gather this from the user with the form on the dashboard this form is built dynamically and pre-filled with the available options based on the user's generated schemas we need to know which types from two data sources that the user would like to associate to display these we need to load all the individual schemas and filter only the relevant types in each one this ensures the user only sees valid options for types that can connect on the form here we'll use a cart and books to demonstrate once we know the two types we can display the available fields for each of those types for the user to select these fields should represent the data Association typically an ID field stored in both data sources in our example the car's book ID field is directly related to the ID of a book the user knows this but our graphql server doesn't know this once we know how the data is associated from the user selections we can build the necessary connection with the graphql schema here's what the process of building the connection to the schema looks like first we add a new field to the user's cart a book in the background we extend the cart schema by adding a new field to the cart type with a custom type definition in the user's configuration file this defines the association within the graphql server and will allow the user to request books related to a particular card next we need to Define how we resolve this new book field we accomplish this by adding an additional resolver within the user's configuration file now when the user queries a cart and selects the book field the correct book data is returned when the user makes a change to the server on the dashboard like connecting two data sources as we just seen locally testing these changes is a logical Next Step to assist with testing we run a local development server that is accessible within the dashboard however graphql mesh offer no option for hot reloading this development server we needed a way to keep the server up to date without forcing the user to shut down their dashboard every time they make a change so we implemented our own hot reloading function anytime the user makes a change to their server such as adding editing or deleting a data source the changes reflected in their configuration file we use the watch function from node's file system library to keep track of changes within the configuration file when there is a change we kill the running development server and load a new one by implementing this simple hot reloading function we avoid outdated information in the user's test server and prevent them from having to shut down the entire dashboard anytime they make a change to their server in addition to reloading the dashboard's local development server on user changes we needed to clear redeployment workflow as the user shifts their focus from configuring to testing they may forget to redeploy the changes they made to help remind them that their local servers out of sync with their deployed server we implemented a basic change notification system we created the helper function within the dashboards backend that is invoked on every user change this function updates an environment variable on the project folder that tracks changes we included logic in our react code for the dashboard that displays the alert if changes have been made and there and if there's an active deployment if no server is deployed or no changes have been made there's no need for redeployment so the alert is not visible when there are changes in a deployed server the alert is visible and the user can click the corresponding button to initiate redeployment after they initiate redeployment we lock the dashboard's deploy and redeploy functionality until it exists until it's successful in the background the docker image is rebuilt locally once the image is ready we use the AWS command line to push the new image to the user private image registry and then update fargate to utilize the newest version of the image finally we wait for a successful deployment and update the user's dashboard to reflect the status I'm now going to hand it off to Felicia to close us out thanks Matt we've built Nexus to serve as a framework for generating a graphql API from multiple data sources and deploying that API to users AWS account Maxis offers users some of the benefits of extendability and ownership that come with an open source Library along with some of the deployment abstractions of a managed service at the cost of advanced API features by simplifying the setup process we help developers save time and make open source generator and deployment tools simpler to use and that concludes our presentation thank you everyone for joining us we really appreciate your support so we will now open up to your q a for any questions you might have and luckily we already had a couple questions come in in the middle uh Justin from the synapse team which is a graphql project from last year's Capstone has a couple of questions in here the first one is what are some of the tools that you use to create the new graphql unified schema or the individual schemas from the different sources or did you code that yourself Matt you want to talk a little bit about how we got to graphql mesh and some of the steps we looked at along the way it was pretty interesting Journey sure yeah so I think naively we tried to initially code that up ourselves like a a graphql schema generator based on some database information uh but we decided to move on from that just because it would have take taken a lot more time um so then we we researched more and then we found another open source tool post graph file which was a graphql schema generator based on postgres um and that was that was a good step in the right direction but uh it didn't offer us the schema generation from other data sources so we put that to the side and then more research we found mesh graphql mesh which uh generates schemas from multiple different data sources um so that was just the right fit so we we stuck with that open source tool and Justin SEC second question is did you create a way for the user to enter the data Association and then Auto create resolvers I think we touched on a little this a little bit about this but maybe we could dig in a little bit more um we did we did um we do auto generate resolvers we started with just postgres uh connections and um it was pretty interesting we we actually we didn't get to a chance to talk about this because it got a little bit more in depth but we we ended up basically when we connect data sources we have the tendency to uh create a new problem an additional problem once those data once those two uh fields are connected and we're starting to request data from multiple sources within the same nested query um and so there's like there's an N plus one problem that gets created where we're fetching data from a cart that returns say three books and then we go fetch each one of those books individually from the second data source so we ended up having to implement some filtering options and additional uh some additional configuration for the transformation along the way from from Individual schema to unified schema that would allow us to add an additional resolver just to get the data we needed from the source to ensure that it was there and we didn't create that n plus one problem for the user but then ultimately yeah we are Auto creating uh resolvers that that don't exist from the originally generated schema is there anything you want to add to that anybody no I think you you captured that really well yes I think Ashish uh how's it going great work you mentioned extendability as a concern When comparing Solutions and creating Nexus uh what kind of use cases can afford to ignore extendability is it only for schemas that aren't likely to change or are there other factors anybody that's a really great question yeah um I think you I think you've pretty much encapsulated it so if the team knows what they're gonna going to need in the future then they might not need extendability right um so if they go for like a fully managed service and let's say um let's say they support postgres and that's all they have and they're like an established team and they know they want me to postgres in the future uh that could be an option let's say you have a smaller team that's just starting out and maybe an example would be something like they um they're slowly getting bigger right like they're slowly getting bigger and they don't know if maybe they'll need to incorporate another graphql API or a mongodb database for example in the future then extendability would be really valuable to them in that in that regard does anybody have anything to add to that I think that was it we looked at a couple of managed services that had uh you know coming soon on their website for data sources like mongodb like like you're saying Kim and then we looked at articles from them from two years ago that said coming soon mongodb so it's instances like that where you just you really give up your control your control completely to the service and hope that they'll support it um of course you could you could of course like generate a separate API from another service that supported but then you're either having to Port over all the settings and the setup that you've done in one service to that second service or then operate two Services separately and have two different endpoints so we kind of kept that that in mind when we were thinking about this idea of extendability thank you everybody for coming we really really appreciate you being here and so glad to get to share our project with you all yes yeah thanks everybody thank you have a great day have a great day bye 