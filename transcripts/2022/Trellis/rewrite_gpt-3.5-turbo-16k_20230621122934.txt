Hello, everyone. My name is Cody Williams, and I'm here today with Martin Graham, Marcos Avila, and Mammad Al Shanti. We are excited to share our experience building Trellis with you. In this presentation, I will provide a brief introduction to Trellis, set the foundation for its objectives, and discuss some technical background and problems that inspired the project. Then, Marcos will explain how Trellis solves these problems, followed by Martin, who will walk us through its architecture. Mammad will then dive into our engineering decisions, and finally, Martin will wrap up with a description of our future work.

So, what is Trellis? Trellis is an open-source, low-configuration deployment pipeline for teams developing serverless applications. It aims to minimize the number of steps required for teams to start deploying their code to the cloud. Its dashboard provides a centralized space for teams to monitor and manage multiple deployment environments. Today, we will discuss the challenges, trade-offs, and choices we faced while building this functionality.

To understand the utility of Trellis and the problems it addresses for its users, let's first examine its technical background and the deployment-related problems that inspired us to build it. Deployment is at the core of Trellis. Traditionally, software developers and operations teams had separate roles, with developers writing code and operations teams handling the deployment. This division often resulted in friction and slowed down the release cycle.

To enable faster application iterations and quicker delivery of features to users, many organizations adopted the DevOps philosophy, combining development and operations roles into a single team. With DevOps, less friction means shorter development cycles. Continuous integration became a crucial step in achieving this, where all code changes are committed to a shared repository in a version control system.

Continuous delivery is another DevOps practice that ensures code changes are continuously deployed and tested for validation. Deployment environments play a key role in continuous delivery, with development environments used for preliminary code examination, staging environments used for comprehensive testing, and production environments serving real users.

However, manually deploying and promoting code through these environments can be time-consuming and error-prone. Automation is a more effective approach, allowing teams to focus on development tasks. Automated deployment pipelines typically involve a build server handling steps such as obtaining code from version control, running tests, and deploying the code. A centralized dashboard provides transparency and collaboration for the pipeline.

Now that we have a general understanding of automated deployment pipelines, let's explore how they can be implemented in different application infrastructures. On-prem infrastructure refers to servers fully managed and controlled by the organization. For on-prem deployment pipelines, code is deployed to staging and production environments on the organization's own hardware. Development environments, on the other hand, often reside on the team's local machines to mimic production.

Cloud hosting provides an alternative to on-prem infrastructure, allowing teams to deploy applications on third-party-managed machines. Cloud-hosted deployment pipelines function similarly to on-prem deployments, with staging and production environments running on the same hardware as on-prem. Local development environments can still be used before deploying to production.

Serverless applications, running on cloud platforms, offer a different deployment pipeline implementation. Serverless resources abstract the underlying servers, and deployment environments must maintain logical separation of resources while sharing the same cloud infrastructure. Development environments for serverless applications can benefit from the staging or production-like environment to get a real-world feel, without the same financial burden.

Different options are available for implementing automated deployment pipelines, including in-house solutions, open-source tools, and third-party/SaaS deployment pipelines. In-house solutions can be developed using open-source tools, but they can be more difficult to implement, requiring additional personnel and resources. Third-party/SaaS deployment pipelines, like Travis CI or Circle CI, provide feature-rich platforms for deployment in various scenarios.

However, some smaller teams may need a low-config, inexpensive, opinionated solution that still offers control over the pipeline's infrastructure and data. Trellis was built to address this need. It is a low-config continuous delivery deployment pipeline for teams developing serverless applications on AWS. Trellis simplifies the setup of automated deployment pipelines, allowing teams to focus on feature development.

In Trellis, users can connect their GitHub repositories and select which branch to link to each deployment environment. When code is committed to a repository, Trellis automatically deploys the new code to the configured environments. Unit testing can be enabled or disabled for each deployment environment, providing confidence in code functionality. Users can promote code to the next environment once they are satisfied with its performance.

Trellis prevents automatic deployments to production, as manual promotion is necessary to ensure code quality. Rollback options are available for all deployment environments, allowing users to revert to a previous application version if needed. Trellis also provides a tear-down option to uninstall all cloud resources associated with a deployed environment.

With Trellis, users have control over the deployment pipeline while enjoying a simplified, low-configuration experience. It allows teams to focus on development tasks without compromising on operational efficiency. Trellis fills the gap for smaller teams seeking an opinionated, inexpensive solution with full control over their deployment pipeline.

That concludes our presentation on Trellis. Thank you for your attention. We hope this has provided valuable insight into our project and the challenges we addressed. Martin will now provide further details on our future plans for Trellis. The project does not involve any deployment scripts and instead provides a simple web-based dashboard for deployment management and monitoring. Some teams require a solution like "seed" that is opinionated, requires minimal configuration, and is cost-effective, but still allows users to have full control over the pipeline's infrastructure and data. To address this need, the project team built "trellis". 

Trellis is a low configuration continuous delivery deployment pipeline designed for teams that develop serverless applications on AWS. It simplifies the setup of an automated deployment pipeline, allowing teams to focus more on feature development rather than operations. Trellis users get a low configuration deployment pipeline and control over their data as an open-source, self-hosted application.

When users log into trellis, they can create new applications and choose a GitHub repository that contains the source code for each application. By default, the development environment is linked to the main branch of the selected repository, making it easy to deploy changes to a cloud environment. Users can set up other git branches to customize the source code for each deployment environment. 

Whenever code is committed to an application's GitHub repository, trellis automatically starts deploying the new code to any environment that is configured to deploy that branch. If users want to deploy a commit that occurred before trellis was configured, they can manually deploy the most recent commit for any branch. 

Trellis allows users to enable or disable unit testing for each deployment environment. This lets them see the output of the `npm run test` command every time a deployment happens. They can also set up a different command to run tests if needed. 

Promoting code to the next environment can be done with the press of a button. Trellis prevents automatic deployments in production to avoid impacting end users with bugs, errors, or infrastructure issues. Manual promotion to production is the only way to deploy an application to end users. 

If a deployment environment runs into issues due to problematic code, users can roll back to a trusted application version. Trellis provides a rollback button for all deployment environments, allowing users to redeploy or roll back to a previous application version. 

In instances where users deploy their code but decide that it requires further development and should not advance along the pipeline, trellis provides an option to tear down all cloud resources connected with the deployed environment. 

Trellis validates code modifications, prepares them for testing and promotion, and performs these tasks through a build server, a dashboard, and a back-end. The build server deploys to target environments from an isolated machine and allocates unique cloud resources for each environment. Users can request teardowns and rollbacks for all deployment environments from this build server. These jobs are executed on isolated machines to ensure reliability. 

The trellis dashboard interface allows customers to control deployment pipelines. Users can trigger manual steps in the deployment pipeline, customize deployment environments, manage pipeline features, and monitor the deployment status. 

The back-end connects to the front-end and build server. It maintains user logins, environment-specific settings, GitHub repository data, deployment environment status, and build server output. Its API initiates the build server, updates the database, and sends data to the dashboard and build server. 

Now I'll pass the floor to Martin, who will discuss the specific architecture of trellis components. 

Trellis was designed as a serverless application hosted on AWS to better serve teams that deploy serverless applications to AWS. This allows users to adopt trellis into their existing workflows and leverage their existing serverless expertise. Trellis's usage patterns align well with a serverless application, as builds are intermittent and may occur concurrently, making auto-scaling desirable. Trellis is also often dormant for extended periods, making a pay-as-you-go model suitable. 

Given the serverless nature of trellis, the architecture choices for its components were guided by this design approach. Before discussing trellis's build server, it's important to understand how the resources that make up a serverless application are defined and processed. The build process of trellis is built on the "serverless stack framework" (SST), which is an infrastructure-as-code (IAC) tool that wraps Amazon's cloud development kit (CDK). Using IAC tools enables developers to specify resources using a programming language instead of deploying resources manually. This adds consistency to the deployment process and provides commands for deploying the application and the cloud resources it relies on. 

Trellis executes a build command provided by SST to deploy users' applications to AWS as defined in the application source code. These IAC deployment commands need to be executed by a computer, which is where the build server comes into play. Traditionally, deployment pipelines use a dedicated build server to run the build process and deploy applications. While trellis, as a serverless application, does not have a dedicated long-standing build server, for the purposes of this discussion, we will continue to refer to this component as the build server. 

Trellis's build server is implemented as an AWS ECS Fargate task, which provides on-demand computing via containers. Containers are standalone executable software packages that include an application and all its dependencies. Each time a Fargate task is invoked, a new container is spun up based on an image stored by trellis in the AWS Elastic Container Registry. This container is provided with the necessary information to complete the build process, such as the commit to deploy and information about the target deployment environment. Each container has access to a sandbox file system for source code cloning, dependency installation, and build command execution. Build server containers are scaled in and out according to incoming requests, with each deployment to an environment invoking a new Fargate task for a fresh and isolated environment. Once their work is completed or if they are trusted to deploy AWS resources on behalf of users, the containers are de-provisioned. 

To deploy to AWS resources on behalf of users, the build server must have credentials with create and delete permissions for all AWS services used within the application. Trellis stores these credentials using AWS Secrets Manager, which ensures that only users with affirmative permission can access the secrets. During the build process, build server containers use the AWS software development kit (SDK) to programmatically access trellis secrets stored in Secrets Manager. Logs of the build process are captured by AWS CloudWatch, Amazon's log collection service.

Now let's discuss the trellis back-end. The back-end of trellis primarily consists of multiple Lambda functions. AWS Lambda is Amazon's function-as-a-service offering, where each Lambda function is an individually defined unit of executable logic. When invoked, the cloud provider runs just that function on their servers, and the infrastructure that runs each function is provisioned and deprovisioned according to incoming demand. AWS API Gateway serves as the front door for trellis's back-end. It accepts incoming HTTP requests and triggers the associated Lambda functions for each URL endpoint. The back-end also interacts with AWS CloudWatch to retrieve logs produced by the build server.

In addition, trellis uses AWS DynamoDB, a serverless key-value NoSQL database, to store data about the state of the deployment pipelines it manages. This includes information about users, the status of applications and deployment environments, and other relevant metadata. 

The trellis dashboard is built as a single-page application in React. It populates the dashboard by making calls to trellis's backend. The dashboard code is stored in an AWS S3 bucket, which is Amazon's object storage service. Static files, such as the dashboard application, can be directly retrieved from the S3 bucket. The dashboard is served using AWS CloudFront, Amazon's content delivery network, which allows trellis to avoid using a long-standing web server to serve the dashboard.

Now, let's move on to discussing some of the engineering decisions and trade-offs made during the development of trellis. 

The first decision we'll discuss is the implementation of the build server. Since trellis aims to conform to the serverless model, where compute resources are spun up on-demand, we considered two AWS services: AWS Lambda and AWS Fargate. Both Lambda functions and Fargate tasks support pay-as-you-go pricing, which aligns with trellis's intermittent build execution. Additionally, both services offer auto-scaling capabilities, accommodating the use case of multiple applications with multiple deployment environments and the potential for concurrent builds. However, there are some differences to consider. 

One factor is the startup time. Lambda functions have a shorter startup time, around five seconds, while Fargate tasks can take up to one minute to start. Considering the intermittent nature of the builds in trellis, we favored Lambda functions to minimize the impact of startup time on the overall deployment process.

Another consideration is the time limits of each service. While deploying a modest application like trellis takes about 20 minutes, Lambda functions have a timeout of 15 minutes. In contrast, Fargate tasks have no timeout and remain active until the task is complete. To ensure that deployments are not interrupted due to timeouts, we chose to use Fargate tasks for the build server.

The second engineering decision revolves around sending real-time deployment data to the dashboard. This data is crucial for informing users about the state of their application deployments within trellis. The deployment data includes the overall deployment state, the date of deployment, and the deployment logs, which can be used for debugging purposes. However, due to the architecture of trellis, with Lambda functions shutting down as soon as the build server starts, there is no direct communication between the build server and the dashboard.

To tackle this challenge, we divided the problem into two parts. The first part involved retrieving the deployment state from the build server, while the second part focused on sending the retrieved information to the dashboard. We explored two options for retrieving the data from the build server, and the first option was to configure the build server to trigger a Lambda function after each command. This Lambda function would then store the data in the database. This approach would result in sending deployment state updates to the database in patches, dispatching updates after executing each command. However, considering that the deployment command may take over 20 minutes to complete, we realized that users could be waiting for an extended period without any deployment state updates. Hence, we decided against this option.

Instead, we devised a solution that involved capturing logs of the build process. The build server frequently fetches the deployment state of various resources from AWS CloudFormation and logs this information. By capturing these logs, we can gain a comprehensive picture of the deployment status. This solution allows us to retrieve the deployment state from the build server and send it to the dashboard in real-time. 

In conclusion, the trellis project provides a low-configuration continuous delivery deployment pipeline for teams developing serverless applications on AWS. It simplifies the setup of an automated deployment pipeline, allowing teams to focus on feature development. Trellis users get control over their data as an open-source, self-hosted application. The architecture of trellis is designed as a serverless application hosted on AWS, leveraging various AWS services. The build server, implemented as an AWS ECS Fargate task, handles deployments and scales based on demand. The back-end consists of multiple AWS Lambda functions, providing various functionalities for trellis. The trellis dashboard, built as a single-page application in React, allows users to control deployment pipelines and monitor deployment status. In the video, we discussed the different deployment states: deployed, deploying, getting removed, removed, or encountering an error. The user receives data about the overall deployment state, deployment logs, which can be used for debugging purposes. Let's now review the flow of a deployment request in Trellis. The dashboard sends a request to an API Gateway, triggering a Lambda function. This Lambda function initiates the build process. It responds to the dashboard as soon as the build server starts building and then shuts down. The Lambda function cannot send the deployment information to the dashboard since it is not active for the entire deployment duration.

For automatic builds, GitHub sends a request to the API Gateway, triggering the Lambda function, which then triggers an ECS Fargate container. In this case, there is no direct communication between the build server and the dashboard. To address this, we divided the problem into two parts. First, we needed to retrieve the deployment state from the build server. Second, we had to send the retrieved information to the dashboard.

To retrieve the deployment state from the build server, we explored two options. The first option was to configure the build server to trigger a Lambda function after each command. This Lambda function would store the data in a database. However, this approach would result in delayed updates, as the deployment command could take over 20 minutes to complete. Real-time updates were deemed more beneficial for the user, so we decided to use AWS CloudWatch instead.

CloudWatch is an AWS tool that connects specific AWS resources and publishes events related to them. We connected the build server's log group to CloudWatch to collect logs from its containers. When an event is triggered in the log group, a Lambda function is invoked and writes the logs to the database. This enables Trellis to send the logs to the dashboard in real-time.

Now, let's discuss how we send the data from the build server to the dashboard. We considered two options: polling and WebSocket. Polling involves the dashboard sending HTTP requests at regular intervals to retrieve data. However, this method can generate unnecessary requests and introduce a delay. To enable bi-directional communication between the client (dashboard) and the server (build server), we decided to use WebSockets.

We implemented WebSockets in Trellis using the AWS WebSocket API, which relies on an API Gateway to maintain open connections between the clients and the backend AWS service. The build server sends logs to a CloudWatch log group, which triggers a Lambda function. This Lambda function then uses the WebSocket connection to send updates to the dashboard in real-time. Users can view the deployment logs as they happen on the build server from the dashboard.

Moving on to future improvements for Trellis, we identified several features we'd like to add. One is the introduction of user roles and permissions per application. This would limit production environment control to certain developers while allowing other team members to view the deployment pipeline's status. We also want to optimize deployment times by caching dependencies and storing build artifacts.

Caching dependencies in a shared file system, like AWS Elastic File System or an S3 bucket, can help avoid re-downloading them for each deployment. Storing build artifacts would speed up rollbacks by eliminating the need to rebuild previous versions. Additionally, we aim to improve the developer experience by creating a command-line interface (CLI) for managing and monitoring deployments from a local console.

Furthermore, we want to allow users to connect Trellis to existing CI/CD pipelines when their serverless application is part of a larger application. This way, they can integrate Trellis seamlessly into their existing workflows.

In conclusion, Trellis provides real-time deployment updates by retrieving deployment state and logs from the build server and sending them to the dashboard using AWS CloudWatch and WebSockets. Future improvements include user roles and permissions, optimization of deployment times through caching and artifact storage, a CLI for developers, and integration with existing CI/CD pipelines.

We appreciate everyone who joined the presentation of Trellis. If you have any questions, please feel free to use the question and answer feature, and we'll be happy to answer them. Visit trellis-deployment.github.io for more information and contact details. Thank you all for attending! During our analysis to find the best fit for our use case, we realized that prior to working with AWS, we were somewhat clueless. However, we understood that the best way to learn is by getting hands-on experience, even if that means encountering failures along the way. One of the great advantages we had was the ability to rely on our teammates. Whenever one of us didn't understand something, we had the freedom to ask someone else for help. Each team member contributed something valuable, and working with AWS turned out to be the best way for us to learn.

There's one additional thing I want to mention about my personal reflection after building Trellis. In previous years, I had been working on projects mostly on my own, which meant that it was up to me to implement them. Working as part of a team to create something was a completely different experience. It presented a whole new set of challenges, such as maintaining synchronization between team members and ensuring that we all agreed on the various directions we wanted to take. However, it was an incredibly valuable experience.

If there are no further questions at the moment, I'd like to share some additional information. If you want to read more about Trellis or find links related to the project, you can visit trellis-deployment.github.io. There, you will find a more comprehensive version of the case study, which delves into the details of Trellis, as well as information on how to get in touch with any of the team members.

It seems that we don't have any more questions, so we will go ahead and conclude for today. On behalf of Cody, Marcos, Mammad, and myself, I want to thank everyone for attending, and I hope you all have a great day. Thank you!