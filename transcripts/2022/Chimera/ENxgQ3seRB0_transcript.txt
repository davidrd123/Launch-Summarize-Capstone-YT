[Music] thanks for coming everyone uh we are chimera chimera is an open source tool for performing automated canary deployments of containerized microservices that communicate via a service mesh in this presentation i'm going to start off by giving just a quick breakdown of microservice architectures and then i'll take a look at what deployments look like in these type of systems before going deeper into canary deployments specifically i'll then pass it along to wes who's going to talk about some potential solutions that containerized micro services can use to perform canary deployments and then to talk about what makes chimera unique amongst those options we'll then pass it to will who's going to talk about what it's like to use chimera and give a quick demo and then josh will explain some of the challenges we faced and the trade-offs we made while building chimera before going into the directions that chimera may go in the future so first i'm going to talk about microservice architectures this is a type of design where the major parts of your system are broken out into isolated applications those applications communicate internally by making network requests to one another managing these lines of communication is one of the most difficult parts of having a microservice architecture however there are some tools that help to simplify this for a developer one such tool is a service mesh this abstracts away all of the complex logic of traffic routing and inter-service communication and by setting up your service like this it allows each of your micro services to be managed to scale and to update without having to worry about how it's going to affect any other part of the system and that's really important when we start talking about deployment by having a micro service architecture it allows you to follow this concept that's known as cicd this stands for continuous integration continuous delivery this is a process where your code is in a constant state of being updated and as frequently as possible you are pushing those changes to production what's important to note about this process is that updating the code is only actually half of the process the other half is deployment and because these deployments are happening frequently it's a process that has to be seamless and it has to be able to pick up on and handle any errors that pop up during the process no matter how thorough a development testing suite is there's always a chance that there's going to be unforeseen errors that show up when that code is pushed to production your deployment process has to be able to handle that quickly a traditional approach to deployment would be known as an all at once deployment this is where at a designated time you take your application offline you update it to the new version of the code and then once that's running you put it back online while this is simple it does introduce some potential issues the first issue is downtime availability is one of the primary concerns of a modern application if you're doing an all at once deployment you're sacrificing availability to get your new code online and if you're talking about doing continuous delivery this means potentially taking your app offline several times a day the second issue is errors or more importantly how many of your users are affected by errors it all at once deployment means that all of your code has been switched over to the new version which means 100 of your users are going to be affected by any errors and that leads to our third issue which is rollback if there's an error that's too big to be fixed in production it means you're going to have to roll back to the last stable version of your application and within all it wants deployment this is essentially another full deployment process with more downtime there's a few popular solutions that address some or all of these issues for companies that want to do continuous delivery first i'm going to look at blue green deployments this is a process that involves first spinning up a whole new environment to host the new version of your application traffic continues to flow to the old version while that new version is starting up once it is started up you're able to shift 100 of your traffic over to the new version the old version remains active while you monitor for errors if anything goes wrong you can quickly switch 100 of your traffic back to the old version this is a good solution but there are some downsides first you have two full environments running so you have to deal with the costs associated with that and second since all of the traffic is since all of the traffic is going to that new version that means 100 of your users are affected if there's any errors next we'll look at rolling deployments this is a this can be used if you have a horizontally scaled application where you have multiple instances of your app running to handle large amounts of traffic a rolling deployment involves updating each of those instances one at a time so first you spin up a new instance with the new code and then once that's up and running you take down an old version of the code if there's any errors only the users who end up directed to that new instance will be affected by it plus you're saving on costs because you're not duplicating your entire environment the downside here is with rollback if you're late in the deployment process most of your old stable versions are no longer running so a rollback means that you're going to have to wait for all of those to start back up before your applications back to full capacity and this may mean you end up facing availability issues so now let's look at canary deployments first you spin up a new version of your application and then once that's running you put a load balancer in front of the new and the stable versions or you reach into whatever routing tool your infrastructure is currently designed to work with you then shift a small portion of the traffic over to the canary version and once it's receiving traffic you monitor it for errors if there is an error you can switch 100 of the traffic back to the stable version of your application or if everything's satisfactory you can slowly shift more and more traffic over to the canary and monitor at each step once 100 of the traffic is going to the canary version you can take down the old version with confidence because you know that the new version can handle the load if there's any errors only a small number of your users are affected early in the deployment and no matter where you are in the deployment process the rollbacks are always fast the downsides of canaries again you have two full environments running but more importantly when talking about continuous delivery this is a complex process with a lot of moving parts that will need to be automated if it's happening frequently to see how this process looks for containerized micro services i'm going to break it down into a couple of different components first at the core of this is where your applications are living for containerized microservices this is going to mean interacting with some sort of container orchestrator like kubernetes docker swarm or amazon's elastic container service or ecs these are tools that help a developer to deploy their application manage the compute resources that the application is running on and configure how those resources should scale in a canary deployment this is where you're going to control the deployment of the canary version and the tearing down of a version at the end of the process next we'll look at a traffic routing tool this is going to be a load balancer or some sort of infrastructure tool that handles your router for chimera this is where that service mesh comes into play remember that's the tool that's used for managing communication between microservices most service meshes include what's called a virtual router this is a tool that dynamically directs traffic based on your instructions in a canary deployment process this is what will be adjusted to slowly shift the traffic over to the canary version the third component here is a monitoring tool this is a service that tracks the health of the canary it's usually it's usually going to be pulling in some sort of time series data to monitor things like latency compute resources and any errors that your canary is returning the final piece here is some sort of canary management tool this is a service or if this process is being done manually this is a developer that has to use the container orchestration tool to deploy the new version of the application manipulate your traffic routing tool and then monitor and make decisions based off of whatever the metrics tool is returning i'm now going to pass it over to wes who's going to break down what options are out there for containerized micro services that want to automate their canary deployments and to see how chimera fits into that picture thanks trevor so to start this part of the talk we will look at some of the current automated canary deployment tools for containerized microservices generally speaking there are a few distinct categories and we'll briefly look at each of them one by one the first kind of automated canary deployment tool is kubernetes centric kubernetes is still the most popular container orchestration tool so it's no wonder that many automated canary deployment tools work exclusively in the kubernetes ecosystem even though these tools are limited to a kubernetes type container management system they are typically quite flexible regarding the traffic riding tools and monitoring tools with which they integrate kubernetes centric canary deployment tools are open source which means you can usually tell how they work or in other words why they would promote the canary service or not during the canary analysis stage of deployment moreover as long as you already work within the kubernetes ecosystem these tools are easy to set up and use however if you're using a container orchestration system like aws's elastic container service or docker swarm then these tools are not available to you the second category is the ultra flexible open source solution these solutions work with just about any chosen combination of container orchestration traffic routing and monitoring tools this is often because these solutions are themselves large collections of freely configurable microservices however the trade-off for maximal flexibility is that these solutions often come with the daunting challenge of configuring their microservices and using them alongside your current microservice infrastructure the third category is the cd as a service platform these products offer continuous delivery and deployment as a service they offer a streamlined experience that abstracts away much of the setup and operational complexity of the flexible open source options that we just discussed although cd as a service solutions are easy to get up and running out of the box these tools are typically quite feature rich and flexible and that generally implies that competent usage requires some expertise and experience finally cd as a service platforms are not as transparent as open source solutions it is often difficult to interpret how they work or why they would continue rolling the canary forward or roll it back this is because their internal infrastructure is inaccessible to the user and they sometimes use unsupervised machine learning tools that lack transparency by design so let's summarize the landscape of automated canary deployment tools for containerized microservices up to this point first we have the open source kubernetes centric tools where the simplicity of their usage depends on whether you're already working within the kubernetes ecosystem second we have ultra flexible open source solutions there are tremendous flexibility often comes with the cost of additional complexity finally we have the cd as a service platform these are often quite flexible solutions but their inner workings are inaccessible moreover the simplicity of their usage depends on whether you're using default settings or making fine grain configurations at every opportunity generally speaking what is missing is an open source easy to easy to use solution that simply works for those outside the kubernetes ecosystem and that is why we built chimera for the rest of this part i will introduce chimera's central features before passing it on to will who will give you a better indication of what it's like to be a chimera user to introduce chimera it is helpful to recall that canary deployments for containerized microservices involve a container orchestration tool a traffic routing tool and a monitoring tool so where's chimera's place in all this well these components are neither individually nor jointly sufficient for automating canary deployments as trevor made clear a specialized tool needs to integrate with these other tools to automate a canary deployment every automated canary deployment tool shares that in common and in that sense chimera is no different but to see how chimera in particular works let's take a look at how chimera integrates with each of these components taken one at a time to start let's look at which container orchestration tool chimera supports and will slowly build out from there so rather than kubernetes you can currently use chimera for containerized microservices managed by aws's elastic container service on the right you can see a stable version of a task surrounding a stable version of your containerized app the basic idea here is that the currently stable container is part of a group of one or more containers that share compute resources that the task manages furthermore ecs is responsible for deploying the task scaling it out and registering it under a subdomain because chimera automates canary deployments it automatically deploys canary versions of your service by controlling some of these managerial features of ecs in particular when given an updated container image of your service chimera instructs ecs to deploy your canary task which in turn runs your containerized canary app importantly ecs can deploy containerized services in one of two ways elastic cloud compute or fargate fargate is a compute resource where aws manages the compute instances upon which your tasks run so that you don't have to and currently chimera only supports ecs for container management with fargate deployment next you can use chimera for automatic traffic routing for infrastructures currently using aws's managed service mesh which is called app mesh like other service meshes appmesh consists of virtual components that abstract away the fine-grained details of inter-service networking load balancing and service discovery virtual nodes for example are best thought of as pointers to ecs tasks aptmesh uses the configurations of virtual nodes to configure an envoy sidecar proxy on ecs tasks these site card proxies intercept all traffic to and from the application container and apply the appropriate network communication rules another abstraction of service meshes and app mesh in particular is a virtual router appmesh uses a virtual router's configuration to split traffic through weights in this way it's like an abstraction over a traditional load balancer because virtual nodes correspond to ecs tasks chimera also automatically deploys a canary virtual node and a containerized envoy proxy during the first stage of the canary deployment this is necessary for chimera's control over traffic routing to propagate down to the actual containerized services during the traffic routing stages of a canary deployment chimera reaches out to atnesh's control plane and automatically modifies the weights of virtual routers to shift the traffic from the stable service to the canary for monitoring metrics chimera currently uses aws's cloud watch chimera deploys a prometheus-configured cloudwatch task into the user's ecs cluster the containerized cloudwatch agent of that task uses prometheus to scrape error response metrics from the canary's envoy proxy the cloudwatch agent then stores that time series data on cloudwatch checks the canary's health based on these metrics every minute in particular chimera automates canary analysis by checking whether the metrics are above or below a user-defined threshold value and decides whether to promote the canary based on that threshold analysis finally by controlling atmesh and ecs chimera also automates tearing down relevant infrastructure so suppose the canary service succeeds and becomes the new stable version in that case chimera automatically removes the old virtual node from the virtual router's routes removes that virtual node and removes the relevant ucs infrastructure on the other hand if the canary service fails chimera automatically ensures that all the traffic is routed back to the stable version and tears down all the canary-related infrastructure now before getting a better idea of how a developer uses chimera let's see where chimera fits within the landscape of automated canary deployment tools currently chimera is limited in certain respects if your containerized microservices do not use either ecs or appmesh then chimera is not the right fit for you however chimera is an excellent tool for those whose containerized microservices are orchestrated by ecs and communicate via appmesh moreover it's open source and provides easy to understand canary analysis and finally as will will demonstrate getting chimera up and running could not be simpler all right thanks wes um so now i'm going to show you what using chimera is like from the perspective of a developer so chimera was designed to simplify the deployment process and its introduction into a developer's workflow should accelerate the deployment process while also reducing the cognitive overhead required to do so and therefore it was critical that the installation and usage of chimera be a familiar and streamlined process for any developer so to accommodate this uh requirement our team chose to dockerize chimera and that way the developer needs to only have docker and docker compose installed on their local system docker compose is then used to build the images for an nginx or front-end react app and a node.js express backend and this simple installation process allows comer to be deployed locally or in the cloud based on whatever the developer's needs may be once installed performing carrier deployment with chimera is just a streamlined process presented step by step through a simple web app that's accessible on the local host so to best demonstrate this i'm going to walk you through the process using our dev teams logo app as an example so here you can see that the logo app is a simple micro service that returns a black and white logo whenever the page is refreshed and our developers have been hard at work on a new color version of this microservice and now it's time to deploy it but this app is just one of many microservices on their infrastructure and as such it's an ideal candidate for canary deployment but manually performing a canary deployment in a complex environment such as this is both time consuming and prone to error and then although our dads are great at making new logo apps they're not experts on service meshes container orchestration and metric analysis and therefore they're going to use chimera for deployment and to do so they'll provide their aws credentials and some basic infrastructure information khmer is then able to query aws for extensive details on their existing infrastructure and configuration settings and it's going to be able to leverage this to provide the developers with the minimum number of decisions and options that they'll need to make in order to successfully perform a canary deployment uh there is a one-time setup process that's going to prepare the existing infrastructure to work with chimera the developer just needs to define a namespace that's going to be used for metric collection and provide an aws virtual private cloud id and this is going to place an agent on their infrastructure that's used to scrape the metrics required by chimera to perform canary deployments but once this is complete their infrastructure is now ready uh for chimera to perform a canary deployment to any of their services uh so to start they're gonna provide um they'll be provided with a selection of services on their infrastructure and the developer simply chooses the candidate service for deployment in this case it's going to be logo one and they're going to provide a name for the new service being deployed uh here it's going to be logo 2. america is then going to query aws and provide a tailored set of options for configuration and because every infrastructure is different camaro attempts to minimize the number of assumptions it makes while still providing clear and simple steps for configuration so for new infrastructure the developer needs to provide custom details starting with a new task definition family uh the url for the docker image where the new application can be found and then a new virtual node configuration settings based on existing infrastructure are selected from predefined lists created through queries to aws and finally the canary deployment itself can then be customized by the developer through setting up the interval length over which traffic is shifted the amount of traffic shifted uh by each interval and the metrics which are tolerable for deployment to continue uh and now we get to the fun part where we actually get to deploy our canary service so the deployment phase is going to begin with preparing infrastructure for the new canary service the control plan for the service mesh is updated with a new virtual node that acts as an endpoint for intramesh communication to the canary and the canary is defined with a new task definition including docker images for both the application and the on voice one or more instances of the tasks are going to be spun up with containers for both the applications and the envoys and then at this point chimera is going to have to wait for these new changes to propagate and the canary services to be fully discoverable by the service mesh so here you can see our devs clicks begin deployment and chimera gets to work they can see as the virtual node task definition and ecs services are created and then chimera pauses to wait for the service discovery changes to propagate and at this point 100 of the users are going to be directed to the uh version one of the logo app and refreshing the page is always going to return back this black and white logo so when chimera does receive confirmation from aws that the changes have fully propagated traffic can start to be shifted to the veneer camaro is going to shift traffic by adjusting weights used by virtual routers in the service mesh and based on the previously configured input from the developer a percentage of the traffic is directed by the virtual router to the new virtual node representing the canary chimera status updates once the ecs service is ready and a percentage of the traffic is shifted from the stable the black and white app to the canary or color app so you can see here and now when the page gets refreshed uh roughly 50 of the time a user is going to be directed to the original black and white logo and 50 of the time a user is going to be directed to the new color logo and every minute chimera is going to be comparing the metrics that were generated by the canary against that previously defined tolerable threshold and at the end of an interval the metrics are satisfactory the next interval begins and a greater percentage of the traffic is going to be directed to the canary and this is going to continue until 100 of the traffic is directed to the canary for an entire interval and at the end of this finer interval if the metrics are still satisfactory deployment is a success and cleanup begins but if at any time the metrics are unsatisfactory chimera is going to recognize this deployment as a failure and so here you can see there appears to be a bug with our new color logo app and users are occasionally experiencing 500 errors where no logo is being returned so what happens when there's an error well chimera detects a failed deployment and it instantly begins rolling back to the original service and infrastructure and all traffic that's currently directed to the canary is immediately shifted to the original service the virtual node task instances and definitions of the canary are torn down and removed so here you can see our dev watches there's a spike in 500 errors appearing in the canary health chart so chinemera recognizes the failures and automatically begins rolling back the infrastructure to the stable black and white logo app and our devs can rest easy knowing users can continue to access the original black and white stable version of the logo app so when deployment does succeed chimera begins cleaning up the old service in a process that's very similar to the rollback of a failed deployment so it's going to tear down and remove the virtual node task instances and definitions of the original service the black and white logo wrapped so here our devs watched as the carrying help metrics indicate a successful deployment and all traffic is now directed to the canary the color logo app and the stable node services and task definitions are removed now all users are directed to the color version of our logo app and the black and white logo app has been safely removed from the infrastructure our devs can now get back to work and focus on their new blockchain version of the logo app so by amalgamating all the required options and predefining as many selections as possible chimera vastly simplifies the process and greatly reduces the potential for developer introduced errors during deployment and i can speak to this personally having tried to de-register a task definition after a manual canary deployment and as my team watched on somehow managed to delete my entire micro service architecture so this is just one challenge we faced while designing chimera and to talk to you further about the design decisions and challenges that went into creating chimera i'm going to turn things over to my teammate josh all right thanks will uh so i'm going to talk about some of the challenges and design decisions that we face while building chimera and then i'll wrap up by talking about some additional features that we would like to add to come here in the future then we'll open it up to any questions you might have about chimera uh so before i talk about the technical challenges i'd like to walk through some of the design principles that we kept in mind while designing chimera so the first of these principles was ease of use for the end user it was important for us to minimize the amount of configuration that we required from the user and we wanted the process of using chimera to be transparent and for it to provide clear and understandable results we also wanted to design chimera so that it would have a minimal impact on the user's existing infrastructure aws resource configuration can be very complex so we wanted chimera to modify these resources as little as possible to reuse existing configuration options enter into and to introduce as few new resources as possible into their infrastructure so with those principles in mind one of the early design decisions we faced was how chimera would actually interact with aws we evaluated several options and we decided against using third party tools like terraform for the sake of simplicity we wanted to avoid introducing too many separate tools or other solutions into the project so that left us with the options provided by aws specifically cloud formation the cloud development kit and the software development kit cloud formation and the cloud development kit are both infrastructure as code options uh cloud formation allows you to deploy aws resources by providing template files that describe your intended infrastructure and the cdk is closely closely related to this it's a library that is available for several programming languages that allow you to actually design the infrastructure using a familiar programming language the cdk can then be used to synthesize and deploy cloud formation templates the software development kit on the other hand is a bit more conventional it's a library provided by aws that allows you to interact with aws resources directly in your code using a familiar programming language we chose to use the software development kit and there were three main reasons for this the first is that it allows us to create and configure aws resources whenever we need to as the canary deployment is being performed second it allowed us to pull metrics directly from aws cloudwatch to determine whether the canary is healthy or not as we go through each stage of the deployment and third it allows chimera to determine when each phase of the canary deployment can start and end based on the results of queries to the aws resources for example when we need to wait for the services the new services to be discoverable so that we can actually route traffic to it uh in addition to these three things the sdk also gave us the ability to hold true to our overall design goals a good example is that using the sdk gave us an easy way to reuse existing configurations anywhere we could with the sdk we can pull down the exact configuration of an existing service and then modify only the options that chimera needs and then deploy an almost exact replica of that service running a new version of the code this in addition helped us reduce the configuration input that we required required from the user usually given just a few names describing those resources we could use the sdk to find most of the information that chimera needs to do its job so once we understood how chimera would communicate with aws the next challenge was determining how chimera would collect the metrics for the new service it deploys we knew from the beginning and i've mentioned before that we wanted to take advantage of cloudwatch because it's an observability and monitoring service provided directly by aws but getting the information that we needed into cloudwatch was the hurdle that we had to actually address because our users infrastructure is running on aws fargate we don't have access to an underlying machine and therefore we can't use any operating system level tools to collect metrics or networking information instead we have to rely on what is made available by ecs and by the data plane of our service mesh which in this case is the envoy proxies that are attached to each service so fortunately aws provides a containerized application called the cloudwatch agent which can be used to collect metrics from the envoy proxies and then export them to the cloudwatch platform in order to do this the task definition of each ecs service must be updated to include the cloudwatch container this solves the problem of collecting metrics from the proxies but it wasn't in line with one of our design goals because it requires reconfiguring multiple services and adding several new components into the user's infrastructure so to address that problem we decided to instead use prometheus to collect metrics from the envoy proxies prometheus is a monitoring service that can be configured to scrape the proxies at regular intervals and collect specific metrics a single cloud watch agent that's been configured to use prometheus can be deployed directly onto the user's ecs cluster and then prometheus will handle the task of scraping the proxies and then the cloudwatch agent running in that same service will handle exporting the collected metrics to cloudwatch so doing it this way greatly reduced the complexity of the infrastructure that chimera needs to create and it has very little impact on the user's existing infrastructure so now that we knew how we would collect the metrics the next choice was which metrics would be considered when deeming a service healthy or unhealthy so we began this decision by considering what's commonly referred to as the four golden signals which are saturation traffic latency and error rate so we evaluated each of these and we considered how their inclusion in chimera's health checks would impact our guiding principles so first with saturation which is a measure of the amount of available system resources that are actually being used this is an important metric because the performance of a system can degrade even before it reaches maximum capacity but it can also be a complicated metric because there are several kinds of resources to consider like cpu memory and storage and on top of that which resource resources are relevant is also dependent on the nature of the application so for that reason we decided that chimera would not consider saturation for health checks because including it would require either more complex user configuration or chimera would have to be more opinionated and potentially less transparent about the deployment process and the results of that process so the next signal is traffic which is another way of considering the system's use of available resources this is instead commonly measured as the number of http requests sent to the application per second and this metric comes with complications that are similar to those of saturation the nature of the application will dictate what constitutes a useful measure of traffic for example certain streaming applications may be more interested in network i o rate rather than a simple number of uh connections to that service so for that similar similarly to saturation we decided that camaro would not consider traffic for health checks so that left us with latency and error rate and latency is the measure of the time required to handle a request and error rate is a signal that measures the amount of requests to the service that fail and failure can mean a lot of things depending on the task the service is supposed to perform but in many cases simply capturing all http 500 requests is sufficient in other cases this might involve end-to-end testing to actually determine whether the content of the response is what was expected so we decided for chimera that error rate would be the primary health check metric a key benefit of using canary deployments is to minimize the amount of users that can be negatively impacted by errors in new code and we felt that tracking error rate was the signal that's mostly most closely aligned with that goal and in keeping with the design goals of simplicity and ease of use we decided that chimera would just track the number of http 500 status codes returned by the service this keeps chimera configuration simple because the user can define a basic threshold and if the number of failures exceeds that threshold chimera can initiate an automatic rollback so another big decision that we have faced was which strategy would chimera actually use to perform canary deployments a common strategy involves comparing the performance of the new version with a copy of the production environment which we call the baseline the baseline will have the same version and the same configuration as the production deployment but it'll be deployed at the same time and in the same way as the canary that's being tested and the reason this is useful is because you can minimize factors that don't involve new code from skewing the results of the comparison for example if you compare a brand new version of a service to a version that's been running for a while issues like cash warm-up can make it seem like the canary is underperforming when it's really not so we decided that it was not worth the added complexity of using a baseline specifically because of the fact that chimera works with systems running on fargate even if chimera does deploy a baseline it doesn't actually have control over when that service starts or when it stops in fact it could be starting and stopping in in between requests without us even knowing because the hardware is managed by aws we can't remove the factors that could skew the results of a comparison further because of that we decided that chimera would not use comparison based analysis and would instead use a threshold analysis so chimera just determines the health of a service by simply comparing the metrics to those user-defined thresholds so now that we've talked about some of the design decisions um we're pretty happy with how chimera has turned out but there are some features that we would like to add in the future for example in its current state chimera can be used as a part of a typical ci cd pipeline but when the build phase is complete chimera has to be manually run to actually deploy the result of that build phase so we would like to extend chimera so that the user can actually set up automated canary deployments that occur in response to some event for example pushing a docker image to some kind of image repo another feature we would like to add is to add web hook features to chimera so that the user could provide a web hook url and that would allow chimera to send deployment status notifications to that url and this would allow chimera notifications to be sent to a slack channel for example or some other tool that could keep the team updated on the status of the deployment so that is chimera thank you for attending the presentation and we'd be happy to take any questions you have yeah i'm not seeing any ques oh there we go there's a question okay so what unique challenges are there for deploying a canary into a microservice service mesh context as opposed to for a standalone app so i suppose so i guess what we're asking is um is there what what particular challenges are there when we're talking about these containerized micro services versus a standalone app that doesn't have to be containerized and run on ecs or something like that anybody want to take that one uh yes uh yeah okay okay call for it nice uh yeah i think uh one of the biggest additional challenges with having the larger infrastructure is the inter-service communication which in chimera's case is the aws app mesh um it just adds a a couple more layers of moving parts that have to be automated and a few more places that things could break that we'd have to watch for yeah i would say that's and i would add you know it's with a single you know if you have like a monolithic application you don't have to worry about for example say you have in a micro service architecture say you have one team that's updating one service and another team's microservice depends on that one um that is a challenging part of doing these kinds of deployments because one team service may be getting failures that don't stem from the service they're actually responsible for and so doing canary deployments like this could allow you know you take advantage of the service mesh to deal with that um so unless anybody anybody else has anything else to add we have another question it's uh why is envoy necessary and does the user of chimera have to add it to their ecs task definition okay i'll i'll answer this one um so the the envoy is necessary because that's how app mesh works um the envoy proxies are actually what handle the inter-service communication for app mesh so that they're known as the data plane and um that's just the envoy is just an open source tool that's just what app mesh uses and the second part of your question is um the the user does have when you set up a service mesh on pcs you do have to update your services task definitions manually to include that envoy proxy but it's just a configuration property on that task definition but i'll i'll add because there's a few ways to interpret this question is the user doesn't have to do anything during the canary deployment chimera will automatically do that uh well i had the envoy okay it looks like we don't have any other questions again thank you all for attending i hope it was at least interesting and that you enjoyed it yeah thank you thanks everyone thank you guys 