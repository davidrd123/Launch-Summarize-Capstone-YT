[Music] 

Thank you for joining us today. We are Chimera, an open-source tool for automated canary deployments of containerized microservices that communicate via a service mesh. In this presentation, I will give a brief overview of microservice architectures and deployment methods. Then, Wes will discuss potential solutions for canary deployments in containerized microservices, with a focus on what makes Chimera unique. Will, in turn, will demonstrate how to use Chimera and share his experience. Lastly, Josh will discuss the challenges faced during Chimera's development and future directions.

Microservice architectures involve breaking down system components into isolated applications that communicate internally through network requests, managed by a service mesh. This simplifies the management of microservices, allowing for scalability and updates without affecting other parts of the system. Continuous Integration Continuous Delivery (CI/CD) is essential in such architectures. It involves frequently updating code and seamlessly deploying changes to production.

Traditional deployment methods, like "all at once" deployment, can be problematic. It often results in downtime, affects all users in case of errors, and rollback is complex. To address these issues, various solutions have emerged for continuous delivery, such as blue-green deployments, rolling deployments, and canary deployments.

Blue-green deployments involve spinning up a new environment for the new version of an application while traffic continues to flow to the old version. It allows for easy rollback if errors occur, but it can be costly to maintain two full environments, and errors can affect all users.

Rolling deployments are suitable for horizontally scaled applications. Instances are updated one at a time, minimizing the impact of errors on users. However, rollbacks can be challenging if many old stable versions are not running, potentially leading to availability issues.

Canary deployments involve spinning up a new version, putting a load balancer in front of both the new and stable versions, and gradually shifting a portion of the traffic to the canary version. Monitoring for errors enables quick rollback if necessary. Early in the deployment, only a small number of users are affected, and rollbacks are fast at any stage. However, managing canary deployments can be complex and requires automation for frequent use.

For containerized microservices, canary deployments involve controlling three main components: the container orchestrator, the traffic routing tool, and the monitoring tool. Container orchestrators like Kubernetes, Docker Swarm, or AWS Elastic Container Service (ECS) manage the deployment and scaling of applications.

Traffic routing tools, such as load balancers or virtual routers within service meshes, handle routing requests. Chimera utilizes service meshes, like AWS App Mesh, for seamless traffic shifting in canary deployments.

Monitoring tools track the health of canary versions by collecting time-series data on latency, compute resources, and errors returned by the canary. Finally, canary management tools, like Chimera, automate the deployment, traffic routing, and monitoring processes.

Now, let's explore the existing options for automating canary deployments in containerized microservices. There are three main categories. First, Kubernetes-centric tools are designed for container management in a Kubernetes ecosystem. They offer flexibility in integrating with various traffic routing and monitoring tools but are limited to Kubernetes environments.

The second category consists of ultra-flexible open-source solutions. These solutions can work with any combination of container orchestrators, traffic routing tools, and monitoring tools. However, their configuration can be complex and integration with existing microservice infrastructures may pose challenges.

The third category is CD-as-a-Service platforms. These platforms offer a streamlined experience by abstracting away setup and operational complexity. They are easy to use out of the box but often lack transparency in their inner workings, making it difficult to understand their decision-making process.

To address the need for an open-source and user-friendly solution that works outside the Kubernetes ecosystem, we developed Chimera. Chimera integrates with AWS Elastic Container Service (ECS) and can be used with AWS's managed service mesh, App Mesh.

With Chimera, ECS manages the deployment of stable and canary versions of containerized microservices. App Mesh handles traffic routing, and the monitoring tool collects data on canary health. This integration simplifies the automation of canary deployments and is suitable for those not using Kubernetes.

We will now pass the floor to Will, who will provide a hands-on demonstration of Chimera and share his experience as a Chimera user. Following Will, Josh will discuss the challenges encountered during Chimera's development and the trade-offs made. We will conclude by discussing potential future directions for Chimera.

Thank you for your attention thus far. Stay tuned for the rest of the presentation. [Music] Chimera is an open source tool that aims to simplify canary deployments for containerized microservices. It addresses the gaps in existing tools by providing an easy-to-use solution for those outside the Kubernetes ecosystem. In this transcript, we will introduce the central features of Chimera and provide insights on how it functions from a user's perspective.

Canary deployments for containerized microservices typically involve three components: a container orchestration tool, a traffic routing tool, and a monitoring tool. Chimera integrates with these components to automate canary deployments. It supports containerized microservices managed by AWS's Elastic Container Service (ECS), making it a versatile tool for users within the AWS ecosystem.

When using Chimera with ECS, a stable version of the containerized app is deployed within a task that manages compute resources. ECS handles tasks such as deploying, scaling, and registering the app. Chimera takes control of some of these management features to automate the deployment of canary versions of the service. It instructs ECS to deploy the canary task with an updated container image, which runs the containerized canary app.

Chimera also works with AWS's managed service mesh called App Mesh for automatic traffic routing. App Mesh consists of virtual components that handle inter-service networking, load balancing, and service discovery. Chimera deploys a canary virtual node and a containerized envoy proxy during the canary deployment process. This allows Chimera to control the traffic routing, modifying weights of virtual routers to shift the traffic from the stable service to the canary.

For monitoring metrics, Chimera makes use of AWS's CloudWatch. It deploys a Prometheus-configured CloudWatch task into the user's ECS cluster. The CloudWatch agent, running as a container, uses Prometheus to scrape error response metrics from the canary's envoy proxy. These metrics are stored in CloudWatch and used by Chimera to analyze the canary's health. Chimera checks the metrics every minute and decides whether to promote the canary based on a user-defined threshold value.

In terms of infrastructure management, Chimera automates the tearing down of relevant infrastructure depending on the outcome of the canary deployment. If the canary service succeeds and becomes the new stable version, Chimera removes the old virtual node, routes the traffic back to the stable version, and removes the relevant ECS infrastructure. On the other hand, if the canary service fails, Chimera directs all traffic back to the stable version and tears down all the canary-related infrastructure.

Chimera is designed to be an easy-to-use tool that simplifies the canary deployment process. Installation and usage have been streamlined to ensure a familiar experience for developers. Chimera is dockerized, requiring developers only to have Docker and Docker Compose installed on their local system. The images for the frontend and backend of the app are built using Docker Compose. This allows Chimera to be deployed locally or in the cloud based on the developer's preference.

To demonstrate the usage of Chimera, we will walk you through the process using our team's logo app as an example. The logo app is a microservice that returns a black and white logo upon page refresh. The developers have worked on a new color version of the app and want to deploy it using canary deployment. Using Chimera simplifies this process and reduces the cognitive overhead required.

To start, developers provide their AWS credentials and some basic infrastructure information to Chimera. This allows Chimera to query AWS for existing infrastructure details and configure the deployment process accordingly. A one-time setup process prepares the infrastructure to work with Chimera. This involves defining a namespace for metric collection and providing an AWS Virtual Private Cloud ID. Chimera places an agent on the infrastructure to collect metrics required for canary deployments.

Once the setup is complete, developers are presented with a selection of services on their infrastructure. They choose the service they want to deploy, provide a name for the new service, and proceed with the configuration process. Chimera provides tailored options for configuration based on the existing infrastructure. Some options require custom details, such as a new task definition family and the URL for the Docker image containing the new application. Other options can be selected from predefined lists generated through queries to AWS.

During the deployment phase, Chimera prepares the infrastructure for the canary service by updating the control plane for the service mesh. A new virtual node is added as an endpoint for communication with the canary, and the canary task definition is created with the necessary Docker images. Multiple instances of the canary tasks are spun up, including containers for the application and the envoy proxy. Chimera then waits for the changes to propagate and the canary services to be fully discoverable.

Once the changes have fully propagated, Chimera begins shifting traffic from the stable service to the canary. It adjusts weights used by virtual routers in the service mesh to direct a percentage of the traffic to the canary. This shifting of traffic occurs in intervals, based on the configuration set by the developer. At each interval, Chimera compares the metrics generated by the canary against a user-defined threshold. If the metrics are satisfactory, the next interval begins and a greater percentage of the traffic is directed to the canary.

This process continues until 100% of the traffic is directed to the canary for an entire interval. At the end of the final interval, if the metrics remain satisfactory, the deployment is considered a success. Chimera then proceeds with cleaning up the old service, tearing down virtual nodes, and removing related infrastructure. If, at any point, the metrics indicate failure, Chimera recognizes the deployment as unsuccessful, instantly rolls back to the stable service, and removes the canary-related infrastructure.

Chimera aims to simplify canary deployments by minimizing the number of decisions and options developers have to make. It provides a step-by-step process through a simple web app that can be accessed on the local host. By amalgamating the required options and providing predefined selections based on existing infrastructure, Chimera reduces the potential for errors introduced by developers during the deployment process.

Building Chimera came with its own set of challenges. One challenge was ensuring that the installation and usage process was familiar and streamlined for developers. Dockerizing Chimera and using Docker Compose simplified the deployment process and allowed for local or cloud-based deployment. Another challenge was integrating Chimera with ECS and App Mesh to automate canary deployments. By leveraging these existing tools and their APIs, Chimera effectively integrates and automates the deployment process.

In terms of future development, there are several additional features that we would like to add to Chimera. Some of these include expanding support for other container orchestration tools beyond ECS, integrating with other traffic routing tools, and providing support for monitoring metrics from sources other than CloudWatch. We also aim to improve the user interface and user experience of Chimera to further simplify the deployment process.

In conclusion, Chimera is an open source tool that simplifies canary deployments for containerized microservices. By integrating with ECS and App Mesh, it provides an easy-to-use solution that automates the deployment process while minimizing the potential for errors. The installation and usage of Chimera are streamlined, and its deployment process can be controlled through a simple web app. Chimera aims to bridge the gap for those outside the Kubernetes ecosystem and make canary deployments accessible to a wider audience. The purpose of this project is to direct traffic to the canary until 100% of the traffic is directed there for a full interval. If the metrics are satisfactory at the end of this interval, the deployment is considered successful and cleanup begins. However, if the metrics are unsatisfactory at any point, the deployment is recognized as a failure and rollback to the original service is initiated.

One issue we encountered was a bug in our new color logo app, where users occasionally experienced 500 errors and no logo was being returned. When an error occurs, our system, Chimera, detects the failed deployment and instantly rolls back to the original service and infrastructure. All traffic that was directed to the canary is immediately shifted back to the original service. The virtual node task instances and definitions of the canary are then torn down and removed.

On the other hand, when a deployment is successful, Chimera starts cleaning up the old service in a similar manner to a rollback. It tears down and removes the virtual node task instances and definitions of the original service. Our development team can then focus on their new blockchain version of the logo app, knowing that users are being directed to the stable color logo app.

Chimera simplifies the deployment process and minimizes the potential for developer errors by combining all the required options and predefining many selections. This helps maintain simplicity and transparency in the deployment process. 

One challenge we faced while designing Chimera was the risk of making errors during manual steps, such as deregistering a task definition. I once accidentally deleted my entire microservice architecture in front of my team. This incident highlighted the need for a more streamlined and automated deployment solution.

Now, I'll hand it over to my teammate Josh, who will discuss the design decisions and challenges we encountered while building Chimera. He will also touch on some additional features we plan to add in the future.

Our primary design principle while designing Chimera was ease of use for the end user. We aimed to minimize the need for configuration and keep the deployment process transparent and understandable. We also wanted to minimize the impact on the user's existing infrastructure and avoid adding new resources whenever possible.

To achieve these goals, we explored various options for Chimera's interaction with AWS. While third-party tools like Terraform were available, we decided to stick with options provided by AWS, specifically CloudFormation, Cloud Development Kit (CDK), and the Software Development Kit (SDK). We chose the SDK because it allowed us to create and configure AWS resources, pull metrics from CloudWatch, and determine the start and end of each deployment phase based on queries to AWS resources.

Next, we needed to collect metrics for the new service deployed by Chimera. Since we didn't have access to an underlying machine in our AWS Fargate setup, we couldn't rely on operating system-level tools. Instead, we used the CloudWatch agent, containerized within each ECS service, to collect metrics from Envoy proxies and export them to CloudWatch. However, to reduce complexity and minimize impact on the user's infrastructure, we opted to use Prometheus to collect metrics from Envoy proxies instead. This involved deploying a single CloudWatch agent configured to use Prometheus onto the user's ECS cluster.

Determining which metrics to consider when evaluating the health of a service was another crucial decision. We considered the "four golden signals": saturation, traffic, latency, and error rate. After evaluating each, we decided to exclude saturation and traffic from Chimera's health checks. These metrics would have required more complex configuration and made the deployment process less transparent. Instead, Chimera focuses on latency and error rate. Error rate, measured by the number of HTTP 500 status codes returned by the service, became the primary health check metric due to its alignment with the goal of minimizing user impact in canary deployments.

We also had to decide on the strategy for canary deployments. While a common approach involves comparing performance with a baseline, we found it challenging to implement due to the lack of control over when services start and stop in our Fargate setup. Consequently, Chimera utilizes threshold-based analysis rather than comparison-based analysis. It compares metrics against user-defined thresholds to determine the health of a service.

Looking ahead, there are a few additional features we plan to add to Chimera. Currently, it requires manual execution after the build phase of a CI/CD pipeline. We aim to automate canary deployments triggered by specific events like pushing a Docker image to a repository. Additionally, we plan to incorporate webhook functionality into Chimera to send deployment status notifications to external tools like Slack.

In conclusion, Chimera simplifies and automates the deployment process while minimizing the potential for errors. It employs a threshold-based analysis approach, focusing on error rate and latency as primary health check metrics. With its ease of use and minimal impact on existing infrastructure, Chimera provides a robust solution for canary deployments in microservices and service mesh contexts. The purpose of using Chimera, a canary testing tool, is to minimize factors that could skew the results of a comparison between a new version of a service and an existing one. For example, factors like cache warm-up can make it appear that the canary version is underperforming when it is not. Therefore, using a baseline for comparison was deemed unnecessary due to the added complexity and lack of control over when the baseline service starts or stops. Chimera works with systems running on Fargate, where hardware management is handled by AWS, making it impossible to remove factors that could affect comparison results. As a result, Chimera utilizes threshold analysis instead, by comparing metrics to user-defined thresholds to determine the health of a service.

Now that we have discussed some of the design decisions, we are pleased with how Chimera has turned out. However, there are some features we would like to add in the future. Currently, Chimera can be used as part of a CI/CD pipeline, but it requires manual execution after the build phase to deploy the result. We aim to extend Chimera to allow automated canary deployments triggered by events such as pushing a docker image to an image repository.

Another feature we plan to add is web hook integration in Chimera. This would enable users to provide a web hook URL where Chimera can send deployment status notifications. This integration would facilitate sending notifications to tools like Slack, ensuring the team stays updated on the deployment status.

In conclusion, Chimera is a canary testing tool that compares metrics against user-defined thresholds. It eliminates the need for baselines due to the complexities and lack of control over service start and stop times in Fargate-based systems. We have discussed future enhancements, including the ability to automate canary deployments in response to events and the integration of web hooks for deployment status notifications.

Thank you for attending the presentation. We are now open to any questions.

(Question 1) What unique challenges are there for deploying a canary in a microservice service mesh context compared to a standalone app?

The deployment of a canary in a microservice service mesh context introduces additional challenges due to the nature of containerized microservices and their dependencies within a mesh. With a microservice architecture, inter-service communication becomes crucial, requiring automation and careful monitoring to prevent failures. The use of a service mesh, such as AWS App Mesh, adds extra layers and complexity to the infrastructure, presenting more potential points of failure. Canary deployments help address issues where one team's update to a service impacts another team's microservice that depends on it, allowing for better management of failures within the service mesh.

(Question 2) Why is Envoy necessary and does the user have to add it to their ECS task definition when using Chimera?

Envoy is necessary because App Mesh uses it to handle inter-service communication. The Envoy proxies serve as the data plane in the App Mesh architecture. When using Chimera with ECS, the user does need to update their task definitions manually to include the Envoy proxy. This update is a configuration property in the task definition. However, during the canary deployment process, Chimera automatically handles the inclusion of the Envoy proxy, so the user does not need to take any additional action.

Thank you all for attending the presentation. We hope you found it interesting and enjoyable.