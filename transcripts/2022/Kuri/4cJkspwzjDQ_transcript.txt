hello everyone thank you for joining us today my name is chris and my colleagues are anna arjun and tony we have been working together as a fully remote international team to build query and open source dead letter queue as a service for distributed applications before we explain in more detail what query does we'll introduce some important concepts through a use case we'll look at what a dead letter q is used for and what are some problematic areas when using one we'll introduce some of the existing solutions for these problems and compare them to our solution we'll explain how query can be installed and used and finally we will cover how query was built some of the challenges we face during the building process and some future directions we'll start with a use case and introduction let's look at the fictional company by me by me is a young but growing e-commerce business recently the engineering team advised the higher-ups that the business application would benefit greatly in terms of organization reliability and speed by adopting a microservice architecture instead of the current monolith the popularity of microservice architecture can be attributed to the benefits it has over a monolith the code base is split into several functional units of code where each unit represents a service a service is independent of the other services in that it can be deployed and scaled separately the microservice architecture also gives each service the flexibility to use the programming language and data storage system that are most suitable for its function using a microservice architecture however does increase the complexity of communication within the application while a monolith can use method calls for communication between different parts of the application the distributed nature of microservices calls for a different approach in order to maximize the benefits of a microservice architecture asynchronous communication can be used the communication is structured into messages which are held in a message queue that sits between the services the service that adds messages to the queue is called the producer and the service that retrieves the messages is called a consumer one of the main benefits of the placement of a message queue is that the services can be decoupled that is the processing of a message on the consumer side becomes independent from the sending of the message from the producer side this adds more resilience to the entire system so if the consumer fails for some reason the producer is not affected the producer can continue to add messages to the queue which will get processed when the consumer recovers from the failure for the buy me company to work well and processes to run smoothly is crucial that the service that receives the orders is able to forward the details of these to the stock inventory service the communication between the two services should be able to accommodate a growing company allowing for high throughput the by me team knows that there are multiple options available for message queue systems one option that they consider is the amazon simple queue service sqs is a fully managed messaging service that provides high availability and unlimited storage of messages an sqsq is a distributed messaging system meaning that the queue is distributed over several aws servers to allow for redundant storage of the messages within the queue due to this distributed nature aws standard sqsqs support at least once delivery best effort ordering of messages and nearly unlimited number of api calls per second a standard queue would be a good option for an application that doesn't require strict ordering on messages and can handle deduplication of messages within the application code for these reasons the team decided that an aws standard sqsq would be a good choice to handle messaging between the ordering and inventory services after some weeks of testing and development the transition was completed and the company confidently moved their new aws architecture into production in the beginning everything appeared to be working smoothly soon after though the team notices that the application has started slowing down and customer service tells them that they are receiving complaints when the team checks the queue between the ordering and inventory services there seemed to be many more messages than they would expect to see they discovered that a considerable number of messages still in the queue should have been processed by the inventory service already to debug this issue the buy me team starts to think about the different ways a message could fail to be processed by a consumer there are a number of reasons why a message could fail for one since the consumer must communicate with the queue over a network any network errors could lead to message failure other consumer related failures might be due to a consumer dependency for example if the data store the consumer depends on has an outage these types of failures are temporary in nature and once the failure has been repaired the message should be able to be processed by the consumer other failures are due to more permanent errors such as an invalid message format or a bug in the consumer application code that leads to the inability to process a valid message these errors lead to what has been called a poison message after every failed attempt to process the message it becomes visible again in the queue and the consumer is bound to retry processing it while the poison message stays in the queue it wastes resources on retries that will never be successful so let's see how poison messages are created and how they can affect the workings of the queue and the communication between producer and consumer we'll assume that the producer is attempting to send three messages to the consumer the consumer successfully processed the first one removes it from the queue processes the second one and removes it from the queue but fails to process the third message causing it to become a poison message this means that the message will stay in the queue while new messages arrive from the producer the consumer will re-attempt to process the message unsuccessfully causing delays to the processing of the new messages they have arrived and that continues to occur repeatedly the consumer fails to process the poison message while the processing of the new messages is delayed until the poison message is deleted by the queue when the message retention period expires but imagine you have three ten hundreds or even thousands of these poison messages in the queue there will be more and more delays at the risk of the cube becoming clogged up and the application coming to a complete standstill the team at by me concludes that they are dealing with poison messages in their queue they could of course just delete the unprocessed messages but that would mean orders would go unfulfilled causing customer dissatisfaction and profit loss to try to alleviate the issue and find out what could be causing it they decide it is time to add a dead letter q now i'm going to hand over to anna who is going to tell us more about their letter queues and the solutions that our team at biome came across during their research thank you chris according to aws a dead letter q or a dlq as we will refer to it from here onwards is a queue to which a source queue can send messages if the source queue's consumer application is unable to consume the messages successfully in other words we can think of a dlq as a backup queue that captures all messages that fail to be processed from the main queue by the consumer application configuring a dl2 ensures that unprocessed messages will not remain in the queue and no messages will be lost during communication between services when configuring in dlq it is imperative to specify the conditions under which the source queue moves messages to the dlq how many times will the consumer attempt to process the message before stopping in other words what is the cues received count setting the number to a low value such as 1 would mean that a single failure to process the message will cause the source queue to move it to the deal tune this won't allow for any processing retries by the consumer and might prematurely send the message to the dlq there is a chance it could have been successfully consumed with a further re-attempt setting the number to a high value such as 20 enables the consumer to retype processing it multiple times although that this might seem initially like a good idea imagine there be hundreds of messages due to be retried by the consumer at a rate of 20 attempts each such a large number of failed attempts to process and delete the message from the source queue might increase costs and place extra load on the hardware it is best to move failing messages to a dlq after just a few processing attempts let's have a look at how a source queue configured with a dlq and a received count of two actually works as previously we'll assume that the producer is attempting to send three messages to the consumer the consumer successfully processes the first one which is removed from the queue processes the second one which is also removed but fails to process the third message the receive count is now one and as the consumer retries and fails a second time increasing the receipt count to two when the message is polled a third time it is deemed failed instead of becoming a poison message as chris described earlier since the queue has been configured with a dlq the message will be sent there for further analysis and processing undoubtedly the importance of a dl2 lies in a number of factors first it helps manage the life cycle of unconsumed messages which is fundamentally a workflow of possible paths that a failed message can take in addition it provides debugging capabilities by offering a peek into message contents this can be analyzed to diagnose application issues that might need to be addressed centrally with the provision of a dlq failed messages always have a place to be stored minimizing data loss and supporting application reliability ultimately a dlq is a source of error handling strategies ranging from simple reprocessing of messages by the source queue to advanced analytics of message data for real time insights now that our team and by me have the dlq in place they are ready to start debugging the problem to begin with they would like to examine the failed messages in the dlq they head over to the aws sqs console to take a look in the console they are met with some obstacles one issue has to do with what is referred to as the message retention period the message retention period dictates the lifetime for every message that first enters a queue this number will not get reset when a message gets sent to a dlq but will continue to count down from the time it was sent to the source queue when this period expires the message will get permanently deleted from the dlq removing any record of it this is a red flag for the buying team as they obviously do not want any orders to get deleted until they are properly fulfilled another issue is that the entire list of messages is not immediately available for viewing instead the buy me team must manually pull for it while the team can change the polling settings for the dlq depending on the settings they still might not be able to see all the messages with each bowling attempt as only a random subset of them is fetched once a message has been fetched they are able however to inspect the message body and attributes a first look makes them suspect that the messages are failing due to character encoding errors to debug this issue they would ideally like to manually modify each problematic message and resend it to the source queue to see if the changes would allow the message to be successfully processed by the inventory service although the console provides redrive functionality it does not offer the option to modify the messages before resending even if they were able to modify the messages the aws sqs console only allows for battery drive and not individual resending the team decides that the building functionality of the aws dlq does not meet the requirements and they start looking for a better solution while researching solutions the team comes across dl2 implementations by other message q services there are a number of open source and proprietary message systems and invariably they all provide options for setting up a dlq it turns out that much like aws the dlq implementation of other services also comes with a number of drawbacks most of the services require the user to first manually create a queue and then configure it for use as a dlq at best users are provided the opportunity to only manually call for messages view them and batch resend them to the source queue interacting with or operating an individual message is also not a common functionality finally the message retention period of the source queue dictates that if a user requires more time to analyze the message they would have to manually move it to a third location before the message gets deleted it becomes apparent to the team at by me that they are not the only ones frustrated by these limitations in fact there are a number of tools available for making it handy to monitor dead letter messages in testing development and production environments the leading platform in this area is serverless 360 a service reaching features that fulfill a large number of requirements easy viewing of messages search options for filtering them accessing the reason for message failure operating on both individual messages and batches modifying deleting and re-driving messages and setting a timer for particular operations while service bus explorer a competing open source solution does not have the full range of features that serverless360 offers it does check some of the boxes that our team and buying is interested in however neither one of them is compatible with aws the team find themselves in a bit of a bind currently they are restricted by the offerings of the aws dlq but can't make use of any of the available dlq monitoring tools we discussed missing out on the rich functionality and this is where our project comes in as they continue their research for a suitable solution the byb team comes across curry curry is an open source dead ladder queue as a service for small distributed applications that use aws infrastructure when we compare curry with the functionality of the aws dlq and the capabilities of the existing monitoring tools kuri meets the majority of the requirements is open source and compatible with aws queues me team is encouraged by the spine and decides to look into it in more detail aside from being open source and suitable to use with aws queues there are a number of reasons why curry would be the perfect tool for the buy me team firstly by providing just a handful of responses to command line questions posed by curry they will have at their disposal a ready-made pipeline incorporating a source cue and a deal cube they even have the option of setting up both from scratch or providing the details of an existing source queue and letting curie set up a dlq on their behalf when the core infrastructure is ready the user is required to complete three steps only within a couple of minutes query is configured and ready to use finally the easy to use query dashboard will provide them with the ability to access the messages without the need for polling sort messages based on their timestamp or body text modify the body and attribute some individual messages delete and re-drive into mutual messages or perform batch delete and redrive operations this is all very promising news for our team at binay but let's see in more detail what is actually involved in the process of installing and using curry there are three steps to installing and using curry first we'll download the npm package globally from our console it was our goal from the beginning of the project to make the installation of curry as seamless as possible the user will not need to manually update configuration files or make any adjustments once the setup is complete once installation is finished we can check which commands are available in the query package and we can see the user has the options of deploy and view when deploying the application our team at by me will choose fakuri to create a dlq only as they already have source queue they'll provide its url their aws region and instruct query to send notifications to their slack channel finally they'll confirm the configuration options and then watch curry come together in the provision of the various aws resources and that's it curry is deployed leaving us with a final step of viewing and interacting with a curry dashboard let's see what that looks like when opening the dashboard the user can first see a summary of the current state of the dlq at the moment there are 25 dead letter messages whose id timestamp and body can be seen in a table format using the navigation at the bottom we can also take a quick look back and forth through the complete list there is also functionality available for for sorting the messages and we can do that based on the timestamp here we can see the messages sorted from latest to our list and we can also sort according to the message body for example sorting in ascending order will bring the bulk discount orders to the top of the list as we've discussed already these messages have been sent to the dlq because the inventory service failed to process them the user might want as a first step to give the inventory service one more opportunity to try to process them again in this case they can redrive all the messages back to the source queue for one more attempt one by one they get deleted from the dlq on their way back to the source queue the inventory service will pull the source queue for these messages as they arrive and retry processing them for a set number of times each as we've previously seen it appears in our case that none of the messages were processed which means that the source queue will have sent all 25 of them back to the dlq in terms of individual message processing the user has a number of options through the action drop-down they can view the message details which include the id timestamp body and a number of optional user-provided properties thinking back to our team at by me let's say our user has communicated with customer services they have been informed that for the customer with id 147 the order has been attended to and there is no need to reprocess it in this case the user can simply go back one step and permanently delete the message from the queue as an example of another action the user can take let's go back to the action dropdown and view the details of another message in this case the user from by me continues to suspect that the error occurs because of an unexpected character in the squ number however they're not 100 certain so after they remove it they simply store the updated message a quick check reassures them that the updated value has persisted and they can now seek further advice after conversations with a customer and inventory services it's been confirmed that removing the non-autonomic character should resolve the issue the user can go back into the message details update the message body to indicate the modification and then re-drive it to the source queue the message is processed by the inventory service and doesn't return to the deal queue if the user doesn't want to perform any further operations on the remaining messages they can always delete them all and empty the dlq and now i'm going to hand over to arjun who is going to take us through kuri's infrastructure and the steps we took to implement it thank you anna here we have a high-level look at the curry architecture we have broken the infrastructure down into six high-level components which consists of the inter-service communication failed message handling fan out notification persistence and user interaction throughout the next few slides we will break the architecture design down into smaller digestible components this will allow us to explain what resources the components consist of and their relationship with one another these resources have been connected together by using the aws software development kit allowing us to provide developers with a desirable dlq pipeline to improve the developing and maintenance experience in their projects the first component that we are going to discuss today is the inter-service communication this component consists of three resources the user's producer consumer and the aws simple queue service queue the producer can be any service or services within a distributed system or microservices architecture that needs to communicate with other services for asynchronous tasks the purpose of this is to decrease user response time and complete time-consuming and non-urgent tasks in the background on the other hand the consumer can be any service or group of services that needs to access messages from the producer it's important to note that the consumer can only access messages that it has been granted permission for here we can see an example of a typical message that would be sent to sql serialize the json format we have the message id to identify the message the message body for important text to be provided with the message usually for descriptions and finally some attributes that provide additional metadata about the message content or purpose the producers and consumers are connected by aws's simple queue service which is a fully managed messaging queue service that enables the user to decouple and scale microservices distributed systems and serverless applications in terms of trade-offs and design decisions for the queue we had to decide which sqsq option to use aws offers two types of queues the first option which seems which was mentioned earlier is a standard cube which provides maximum throughput best effort ordering and at least once message delivery this means that messages can be provided to the queue at a faster rate the order is not a priority and delivery of duplicate messages may occur the second option is the first in first out cube which is designed to guarantee that messages are processed exactly once in the exact order that they are sent in we decided to opt for the standard queue since we are more likely to need a queue that can handle a high level of volume provide consistency and a redrive option would affect the message order anyways at this point in our infrastructure messages will be sent from the producer to the sqs and the consumer will pull for messages and try to process them but what if messages are failing to be consumed this brings us to the next component in our infrastructure the failed message handling component this component consists of the dead letter queue that is directly attached to the previously seen source queue as mentioned earlier the aws provided simple queue service dlq is designed to capture messages that cannot be processed or consumed successfully by the consumer service when curry is configured it provides the dlq that will automatically be configured to the user's source queue this is done by creating a sqsq setting it as a deal cube and providing it with the aws the resource name of the source queue curry has two options available for developers the primary option is the main queue and dlq option where the developer configures curry to provide a brand new aws q and dlq this is suitable for developers who are starting a new project or have an existing project that needs a queue the second option that developers can configure is the dlq only option this is preferable for developers who already have a project with an existing queue and would like to add the currydlq to the infrastructure in terms of trade-offs and design decisions for the dlq we have decided to use the sqs dlq that does a great job at collecting failed messages and opens the message pipeline for the curry infrastructure this allows curry to provide additional dlq functionality rather than the alternative of consuming failed messages directly from the sqs itself which would be far more challenging and not make much sense since the provided aws dlq does a great job at collecting these failed messages at this point the failed messages are being collected in the dlq but what do we do with these build messages we need some way of alerting the developer of these messages and providing them a way to interact with them and possibly resubmit them from to the source queue the next component will answer this question the final component consists of a lambda that sends a message to a simple notification service topic and two lambdas that subscribe to that topic we will explore each of these resources and their responsibilities in the next few slides to begin with the simple notification service is a fully managed messaging service for both application to application and application to person communication curry makes use of both of these avenues the main design decision that needed to be considered for the sms is whether we want to use a standard or first and first style sns topic we decided to go with the standard sns topic for similar reasons to working with a standard sqsq which is best effort messaging ordering at least once message delivery and highest throughput in publishes per second however there is one more main reason that differs with the sns and that is the subscription protocols available for a standard sms topic we need to subscribe lambdas to our sms topic by using the lambda protocol and fifo sms topics only offer the sqs protocol the sns takes the message from the publish to sns lambda and passes a copy of it out to his subscribers during deployment curry creates an sns topic and adds two subscribers the post to slack and the right to dynamod lambdas between the dlq and the sns topic we have the publish to sns lambda however before going into the functionality of this resource let's take a look at what a rat lambda really is a lambda is a serverless compute service meaning the user can execute code without provisioning servers functions can be invoked in response to certain triggers making it event driven and it can scale with the workload by concurrently invoking more instances of the function when needed this ability to continuously scale is perfect for curry because dead letter messages can come in at unexpectedly varying rates this would depend on the amount of provider services connected to the source queue and the amount of traffic coming into the application the publish to sns lambda is responsible for pulling the dlq messages the code with the lambda will reformat the meta structure message structure from sqs formatting to sms formatting which varies slightly at that point it will pass the message to the sns topic for distribution to multiple other resources both the post to slack and write to dynamo lambda subscribe to the sms topic they also add a timestamp to the message ensure that the message attributes are formatted correctly and pass the message to either slack or dynamo having two lambdas that subscribes to the sns topic help decouple concerns which is good coding practice and provides the options for additional functionality in the future by adding another lambda to subscribe to the sms topic the next component notification is one of two branches from the fano component this component consists of slack which allows dead letter message notifications to be directly sent to the user right away of us are familiar with slack but for those who are not slack is a messaging program for teams and organizations to communicate on curry offers the optional feature of dlp notifications being sent to the user's preferred slack channel this is set up by the developer coding a slack app within the slack api interface on api.slack.com once this is created the developer can supply the provided slack webhook path to curry during the initializing and configuring process before deployment once this is set up any messages that arrive in the developer's dlq will trigger a notification within their slack channel of choice the notification will identify itself as a career notification and provide data about the dlq message such as the source queue name message body timestamp and message attributes this is especially convenient for developers who would like to closely monitor failed messages in development or production environments without having to constantly keep an eye on the curry dashboard so now the developer has been quickly notified about the dlq message where can they go to take action this brings us to the second component that branches from the fanout component persistence the persistence component consists of dynamodb that stores all the dead letter messages to provide the ability for developers to view edit delete and or redrive their dead letter messages back to the sqsq the messages need to be stored in a persistent data store for this we decided to use dynamodb a fully managed serverless key value nosql database designed to run high performance applications at any skill dynamodb allows curry to store dlq messages that can be queried by the application servers api in terms of design decisions for the database there were two main options that were considered the first is aws dynamodb and the second is aws relational database service we decided to go for the nosql over a sql database due to the fact that our messages are already in json format and would need to remain in json format to be redriven back to the main queue nosql databases are far more suitable for storing and working with json data the final component to discuss within the curry infrastructure is the user interaction component this component consists of the node.js application server which provides an api to the react dashboard to consume this explains the two-way relationship that the user interaction component has with the persistence component the user interaction component also has a one-way relationship with the inter-service communication component as it provides an api route for the message to be redriven back to the main queue the application server is built primarily with node.js on the express framework this combined with aws dynamodb sdk allows curry to provide restful api routes that are consumed with the react build dashboard in terms of design decisions for the application server we decided to go with rest apis as we knew we would need to interact with the database to provide the developer with the dashboard that makes it easy to interact with their dlq messages this brings us to the end of our high level components and here is a diagram of the query architecture with all the resources we've gone through to recap a message which a message would originate from the producer in the top left corner it would then go to the sqs queue the message could either be successfully consumed or the consumer service by the consumer service or the message could fill and be sent to the dlq this is where the publisher sns lambda will pull the dlq for the message and pass it to the sns topic the sns topic will then send a copy of the message to the post to slack and write to dynamo lambdas each lambda would then pass a copy of the message to either slack or dynamodb at this point the developer would be able to use the curry dashboard to interact with the application application server which uses its api to interact with the messages stored in dynamodb when the developer has made all the modifications needed for the message in the dashboard the redrive functionality can be used to move the message back to the source queue so how does curry make life easier for the developer from an implementation perspective curry eliminates the need for the developer to set up and configure aws resources that would provide these features for a dlq by looking at the table shown we can see that the aws i am role created during deployment provisions seven policy permissions these are policies relating to the sqsqs s3 bucket dynamodb the sns topic and lambdas we can also see that there are three cues and topics created three lambdas one dynamo table one s3 bucket and two topic subscriptions this creates a grand total of 17 resources that curry provisions for the developer as a direct result of this the user will also need not need to learn how to use and integrate the aws sdk this can vary case by case depending on the application that the user is developing it may be an application that is heavily reliant on aws or the developer may just be using an sqsq from aws either way curry saves a lot of development and learning time for the developer in addition developers can use query no matter which software development stack they are using as long as they are using an aws queue within their infrastructure this provides tons of flexibility as as this is the only user infrastructure dependency career requires finally once deployed the curry dashboard can be accessed from anywhere and on any device as long as it has the aws cli and node.js installed next tony will be speaking about the implementation challenges and room for possible future work thank you very much so whilst designing and implementing our project we came across a number of different challenges and design decisions in which we had to weigh the benefits and trade-offs of the available options i'm going to share with you three of the challenges we faced when designing curie and explain how we overcame them we will be looking at which aws development kit we chose to deploy our infrastructure how we monitored our dlq to provide real-time monitoring for our users and how we overcame an unusual design choice by aws in relation to sns topics aws offers two different development kits that allow developers to create or interact with their resources the software development kit or cd kit sdk and the cloud development kit or cdk the sdk allows developers to develop and deploy applications from a variety of languages by calling the aws services through their provided apis it is an imperative framework in that you must programmatically create and configure each component as well as being able to create resources its primary focus is on interaction with existing components of infrastructure it can be used to make small changes to configurations and provides asynchronous functions the cdk on the other hand only allows the creation of resources and bundles them together into a stack it is a declarative framework which does a lot of the heavy lifting under the hood such as creating permissions and roles it is aws's offering of infrastructure as code once the stack is created interaction with the provisioned components is limited and any changes that can be made are much slower than the sdk let's take a closer look at the pros and cons of each developer kit the primary benefit of the cdk is that once you have created the code to deploy the stack it can be reused an unlimited amount of times which initially seemed perfect for what we were trying to create mercury especially as it is available in javascript and would also join our infrastructure for us drawbacks encountered during our prototyping included its slow speed compared to the sdk and the fact that it doesn't allow for asynchronous operations which we'll explain in more detail in a few slides time also available in javascript the sdk's main benefits where it's speed in relation to the cdk and the wide range of asynchronous functions it makes available its drawbacks without each component of our infrastructure would need to be created one by one thus creating more potential areas for failure we would also have to join the infrastructure with lambdas and subscriptions manually given the pros and cons of both development kits it seems like the cdk was better suited for our needs however there remained some concerns in the team about its relative speed which was something we deemed important to our cli talking of our cli we had two major goals in mind for its design we wanted it to be as simple as possible for the user and our ultimate goal was that they would be able to deploy the application using one simple command coupled with that we wanted the whole process to be as fast as possible and we wanted to be able to keep the user informed of the progress during setup given that these goals match with some of our concerns about the cdk we decided to do some additional research and testing before deciding which developer kit to use in our final version of curing the cdk is primarily designed for devops engineers at large companies where provisioning the same piece of infrastructure over and over again is an everyday task whereas the sdk is divine designed for an on-the-fly manipulation of infrastructure and changes to single components of the overall infrastructure curie seemed to sit in the middle of these two use cases and so before deciding on which developer kit to use we decided to do a spike test to get an idea of how each choice would work in production we used some boilerplate code from aws to create a stack with each development kit and tested it with a prototype of our cli tool during testing the cdk's lack of asynchronicity presented us with difficulties it forced us to split our initialization process into two commands one to get user's information and the other to deploy the infrastructure its speed limitations were also exposed when working with the cli as the five plus minute wait times felt very inconvenient coupled with this was the fact that we either had to show nothing in the terminal during deployment or a term of aws code unrelated to qa despite posing a few more problems from a development point of view the sdk was the clear winner with regards to user experience as a team we were confident that we could replicate all of the cdk's functionality to go along with all the benefits the sdk offered and we therefore decided to move forward with the sdk to deploy our infrastructure using the sdk still posed a number of problems we had to figure out the most efficient way to create each resource manually zip the landers add permissions and subscriptions to each component and then deploy them in the correct order on top of this we also had to add the components that the cdk provisioned under the hood such as the s3 buckets for holding the ladders the overall process of this was tough and challenging with many hours spent debugging but we hoped it would help us create the product we enrolled upon completion of the sdk version of our infrastructure we were delighted to see that curie now deploys in under one minute and with a single command the user was also able to see real-time progress of the deployment into terminal the second challenge i will talk about today relates to how we can alert the users that a message has entered the dlc our initial instinct was to use aws cloud watch as we could easily provision it to continuously monitor the dlp during prototyping it quickly became apparent that cloudwatch would not be suitable for curie's needs although we always received an alert when a message entered the lq time delay was sometimes over one hour our goal for curie was to have give the user real-time updates and therefore we had to search for a faster and more consistent solution we already knew that lambdas could call an sqsq for messages we also knew that they were highly reliable and instantaneous allowing us to process dlq messages without delay we now needed to find a way to alert users using a lambda slice ethos of being user-friendly and highly available seemed to go hand-in-hand with curie using slice webhook service we were able to immediately send a post request to a user-provided web hook path which would provide instantaneous notifications of dlp messages right to the user's phone or laptop this meant our new pipeline now begins at the dlq messages are then pulled by a lambda and immediately sent to slack the integration of slack however created another problem for us unfortunately once a lambda successfully processes a message that message is permanently deleted from the sqs queue this meant there was no way to pull the queue with a second lambda which would send the message to dynamodb our solution to this was to provision a lander whose sole responsibility was to send the message from the dlq to an sms topic sms topics can have multiple subscriptions by sending the initial message to an sns topic we allowed ourselves to have multiple lambda subscriptions each with a single responsibility our current setup has one lambda to post to slack and another to send the message to dynamo but with this architecture it would be simple to add more subscriptions in the future not only had we solved the problem of how to send messages to multiple places we had created a scalable and resilient architecture for curie our final challenge directly relates to our introduction of the sns into our architecture when sending a message to an sqsq a user can add up to 10 attributes to the message the attributes come in the form of objects and are converted to strings before being sent across services as there is no way to distinguish if an attribute of 1500 was sent as a number or a string each attribute has a type property that allows the stringified object to be passed back into the original object correctly unfortunately the number data type in sns topics isn't supported for aws lambda subscriptions any data type of number is changed to string by aws this presented a huge problem for us as maintaining the integrity of user messages is vital to query our solution was to read the type of the attribute before the message is sent we then append the data type to the attribute before we send it to the sms once our sns topic publishes the receiving lambda is responsible for reading the appended data type removing it and making any required updates to the type value on the attribute let's take a quick look at a demonstration of that so we want to send an attribute called quantity which has a type of number before sending it we append the data type to the value of the property after being sent to the sms and received by the lander you can see that its type has been changed to string but we have managed to maintain reference to its original data type in the value before processing the message any further the lambda removes the appended data type and converts the type property back to number with that we were able to maintain the data types of all message attributes across our entire infrastructure so with our challenges out of the way and our project completed we can now turn our attention to some of the future work the team would like to add to curry we are all extremely pleased with the final version of curie and greatly enjoyed the experience of taking something from an initial concept to a fully working npm package available for public download as is the case in software development there will always be room for additional features that could be included in qe version 2 and beyond given more time to work on curie we would be excited to release the following features the ability to access consumer logs that would let us provide a reason a message was dead lads queued creating slight notifications that allow users to delete or re-drive the message from directly from slack and a teardown command that will destroy the infrastructure for the user with a single cli command and with that i would just like to say on behalf of the team thank you very much for listening to us today we hope you've enjoyed our presentation we hope you have a great understanding of what qe does and we would like to open the floor now to any questions that you may have so we have our first question here and which says what did you enjoy the most while working on queueing and so i think i'll begin with my thoughts and then we'll let the team answer with some of those um personally um my favorite part my most enjoyable part was working with the sdk for aws and the reason for this is because during core um i have no experience whatsoever using aws and then in the prep work we did some basic um work regarding it but it wasn't until we really started exploring and getting into details with the sdk that i began to really understand what aws is how powerful it is all of the different options that you have available to you and it was really nice to see that helped create the project that we are also proud of that was my most enjoyable thing um pass it on to other members of the team i can kind of chime in on that as well i i felt quite similar to you tony um i think walking into this none of us really had that much knowledge about the specifics of the different resources aws offers but just going through it and learning about the different resources and how they work together and the different sdks and cdk options um it was a challenging process but it was very very rewarding once we got something working and built together yeah i can i can jump in as well i i think the part that i enjoyed the most was when the four of us had decided to do kind of joint programming so not even pair programming so i i remember it was the first kind of week of the project and we were thinking about where to begin and kind of the first pieces of code to write and we we decided to just all of us to sit around and create the uh the express application kind of the back end for our curry project i thought that was really great i think it worked really really well for us and i think it's going to set us up really well for the future as well working in teams and having to to program with with other people yeah similar for me so i really enjoyed the team working experience um learning to both do the group programming and programming you know we really learned to know each other strings and take advantage of each other's strengths and we really complemented each other a lot and also just being able to manage time because we're so internationally distributed it was a little bit of a learning curve to learn like how to work together both asynchronously and synchronously but it came together so well in the end and it was just a wonderful experience okay so our second question is from pauline and she asks how did you decide on dlq as a topic so basically during the ideation phase the way our team worked together that we would all the first few days we went off and research our own areas and then each day we would have a meet meet-up and we would just kind of discuss which topics we'd looked into and answer any questions from other members of the team and as we began to answer those questions and have those discussions what appeared to be a very doable and like worthwhile project like the shape of that begin to begin to come together um the dlq we were looking into queuing services basically and when we looked at the topic of a dlq with regards to aws and it was that point that we realized the functionality was very very limited that you had to create the dlq yourself in aws it was essentially just another queue um so there's nothing special about it and it was these things that we saw as something quite important that seemed to be kind of underdeveloped that we decided to pursue the dlq as our topic and so then once we had that in mind it was just all four of us all focused on the same thing and yeah that's how we came up with bmq yeah um i guess in terms of uh researching specifically as as tony said we were interested in message queuing services so we first kind of had a look um to see what each of kind of the major services as we saw in the presentation what they offered and we realized that dlq was a it was a consistent offering that kind of different services implemented it differently and all of them kind of missed some of the important facts what we thought what we consider to be important functionalities i guess where where the where we had some kind of uh trouble with was actually finding existing solutions um it's quite interesting i think when you research a capstone project that you don't really know what what it is that you're looking for you know there must be something out there that already solves some of the the questions or the problems that you've encountered and it wasn't it was only when we actually changed one word in our in our google search so we started looking for dead letter q kind of monitoring or management services that we came across um the implementations kind of serverless 360 and and service bus explorer and and that's where i think it was the the last stage of our okay there is something here that we could build on and develop as as part of our project [Music] okay so our next question comes from audrey and she says how long did it take for the team to decide between the cdk versus the sdk and then how long did it take to write the sdk it doesn't seem like it would be an easy feat so to begin with this once we knew that we were doing the projects related to aws we looked into the developer kits that they made available and that's when we first became aware that there was two options the cdk and the sdk um like we said in the slides we were kind of torn between the two and we did begin to experiment with writing code of each one and i would say that perhaps after two or three days when we were ready to prototype and we had the spike ready um yeah that took about two to three days and that's when we really started testing it to deploy infrastructure um like we said in the slides once again the cdk probably initially seemed better but it had so many problems related to just how long it was actually taking it took over five minutes to use it it was very inflexible once you had deployed that stack he couldn't really make any changes and so it was after that site that we decided we're going to go down the sdk route and the next part of that question how long did it take and this was definitely for me the most difficult part of the project um it took us almost one week to create each part and then kind of tie everything together and get get it to work in the correct order that was the most difficult bit once we deployed the first resource like making sure everything was in the correct order um that was by far the most difficult part not just of the cd sdk but the whole project to add to on to add to that a little bit um something that really helped us make our decision between cdk and sdk was the command line tool when we started to implement the early versions of that to see which one would go better with the cli so in the initial spike when we were using the cdk we noticed like tony said that the commands for deployment and setup took quite a while at that point we kind of decided okay let's try out sdk and then sdk was a lot smoother and a lot faster but there were still a lot of challenges in terms of making sure all the different resources were aligning in the right way and the promises were executing the right order to create that seamless dlq um deployment um so the cli had a lot of influence on which route we went and yeah we're glad we went with the sdk at the end so that brings us on to there's another question here from gary which just say what was the biggest challenge throughout the process and how did you overcome it i think we've somewhat answered that question like certainly for me it was the sdk and creating the entire infrastructure with that um i'm not sure if anybody else in the team has something they found challenging i think for me it was the sns topic and the changes that he made to the data types kind of under the hood i remember kind of chris and i working on on the dashboard and feeling a little bit like there was something like we were going crazy like oh my god like you know why is this keep coming back and looking completely different than what we had redriven kind of back back to the queue um kind of overcoming this challenge there was a lot of very very carefully reading the aws documentation uh which is um it can be quite it can be quite challenging it's quite dense and quite long um at places and also using a lot of console logs or puts for those of you who are working on ruby at the moment in every place and any place that um you know we thought that we could possibly manage to catch uh that change uh in the data type from number to string that uh sns seemed to do whenever it was sending something to a lambda subscriber okay and our final question comes from anne she says how did you your team navigate working over such different time zones throughout this process um i can take that one so yeah it took a lot of organization and we decided to split our days so that we had asynchronous parts of the day and then synchronous parts of the day so that the time that we spent working together repair programming or group programming and then asynchronously we will continue working in the evening or in the morning and we would just write very detailed notes about what we had done and um kind of pass it on to the the next person that would kind of keep on working the following day yeah i found that that worked really well amongst our team like we did i think we all did a great job considering like as being mentioned there the time zone differences i'm really proud of all of us for how i managed to achieve that and so it seems like that is all the questions that we have today and so i'd just like to say thank you everybody once again for taking time to listen to us and we hope you enjoyed it and that's everything see you 