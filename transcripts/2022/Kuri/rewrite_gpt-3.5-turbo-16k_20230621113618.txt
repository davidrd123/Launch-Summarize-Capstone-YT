Hello everyone, thank you for joining us today. My name is Chris and I'm joined by my colleagues Anna, Arjun, and Tony. We have been working as a fully remote international team on a coding Capstone project. Our project aims to build and open-source dead letter queue (DLQ) as a service for distributed applications. In this video, we will explain what a DLQ is, its use cases, and the existing solutions available. We will then introduce our solution called Query, explain how to install and use it, and discuss the challenges we faced during the development process. Finally, we will touch on future directions for Query. Let's begin by exploring a use case.

Consider a fictional company called ByMe, a growing e-commerce business. ByMe's engineering team recently recommended adopting a microservice architecture for their business application to improve organization, reliability, and speed. Microservice architecture splits the code base into functional units of code, with each unit representing a service. The services can be deployed and scaled independently, providing flexibility in using programming languages and data storage systems tailored to each service's function.

However, adopting a microservice architecture introduces communication complexity compared to a monolithic application. While a monolith can use method calls for communication within the application, microservices require asynchronous communication. This communication is structured using messages held in a message queue located between services. The service that adds messages to the queue is called the producer, and the service that retrieves messages is called the consumer (conser).

Decoupling services through a message queue adds resilience to the system. If the consumer fails, the producer can continue adding messages to the queue, which will be processed once the consumer recovers. For ByMe to work smoothly, it is crucial that the service receiving orders can forward the details to the stock inventory service. The communication between these services must accommodate high throughput.

To address this, the ByMe team considers using Amazon Simple Queue Service (SQS), a fully managed messaging service provided by AWS. SQS is a distributed messaging system that offers high availability and unlimited storage of messages. The team chooses AWS Standard SQS as a suitable option for their application, as it supports nearly unlimited API calls per second, at least once delivery, and best-effort ordering of messages.

After implementing SQS and transitioning to a microservice architecture, ByMe initially sees improvements. However, they soon encounter performance issues. The team discovers that the queue between the ordering and inventory services has an unexpectedly high number of unprocessed messages. These messages should have been processed by the inventory service.

To tackle this issue, the ByMe team begins troubleshooting by considering the reasons messages may fail to be processed by a consumer. Failures can occur due to network errors, dependency outages, or permanent errors such as invalid message formats or bugs in the consumer application code. Messages that repeatedly fail to process are known as poison messages.

ByMe identifies poison messages as the likely cause of the performance issues. Deleting these unprocessed messages would leave orders unfulfilled, resulting in customer dissatisfaction and profit loss. Instead, the team decides to add a Dead Letter Queue (DLQ) to their architecture.

In AWS, a DLQ is a queue to which a source queue can send messages that the consumer application fails to process successfully. It serves as a backup queue for capturing failed messages. Configuring a DLQ involves specifying conditions for moving messages from the source queue to the DLQ, such as the maximum number of processing attempts. ByMe configures the DLQ to move failed messages after a few attempts.

Now, let's delve into the importance of DLQs. They enable proper management of unprocessed messages, providing a workflow for handling failed messages. DLQs also offer debugging capabilities by providing insight into message contents for diagnosing application issues. With a DLQ, failed messages are stored, minimizing data loss and promoting application reliability. Additionally, a DLQ provides various error handling strategies, from simple reprocessing of messages to advanced analytics for real-time insights.

After setting up the DLQ, the ByMe team can start debugging the problem. They encounter obstacles when using the AWS SQS console to examine failed messages in the DLQ. The message retention period, which determines the lifetime of messages in the queue, poses a challenge. Messages sent to the DLQ retain the same retention period as the source queue, potentially causing them to be permanently deleted before they can be properly analyzed.

The AWS SQS console also has limitations in terms of accessing and modifying messages. The entire list of messages is not immediately available, requiring manual polling to retrieve them. Even then, only a random subset of messages is fetched, limiting the team's ability to view all relevant messages. Additionally, the console lacks the functionality to modify individual messages before resending them to the source queue.

To address these limitations, the ByMe team explores DLQ monitoring tools. They come across Serverless360, a leading platform that offers features for viewing messages, filtering them, and accessing failure reasons. Serverless360 enables operating on both individual messages and batches, allowing modifications, deletions, and redriving of messages. Another tool, Service Bus Explorer, is an open-source DLQ solution with some similar functionality but is not compatible with AWS.

Unfortunately, neither Serverless360 nor Service Bus Explorer is compatible with AWS. This poses a challenge for the ByMe team since they want to make use of the rich functionality these tools offer. As they continue their research, they discover Query, our open source DLQ-as-a-service solution designed for small distributed applications using microservice architectures.

Query provides capabilities similar to Serverless360 and Service Bus Explorer but is compatible with AWS. With Query, ByMe can effectively monitor dead letter messages and benefit from features such as message viewing, filtering, modification, deletion, and redriving. Query also allows for setting timers for specific operations, enhancing the debugging and troubleshooting process.

In summary, Query offers an alternative DLQ solution compatible with AWS. It addresses the limitations of existing tools and provides the necessary functionality for effective monitoring and management of dead letter messages. ByMe can now leverage Query to resolve their performance issues and ensure smooth order processing within their microservice architecture.

During the development of Query, we faced several challenges. Firstly, ensuring compatibility with AWS required extensive testing and integration. We also had to optimize Query's performance to handle high throughput and large message volumes. Additionally, designing a user-friendly interface for viewing and manipulating messages proved to be a complex task.

Looking ahead, there are several future directions for Query. We plan to enhance its scalability to handle even larger applications with higher message volumes. We also aim to integrate Query with other cloud platforms to provide DLQ solutions beyond AWS. Furthermore, we will continue to improve Query's user interface and add advanced analytics features for real-time insights into message processing.

In conclusion, Query is an open-source DLQ-as-a-service solution that addresses the limitations of existing tools for monitoring and managing dead letter messages in distributed applications. Compatible with AWS, Query offers features such as message viewing, filtering, modification, deletion, and redriving. It provides ByMe and other users with a comprehensive solution to ensure reliable and efficient message processing in their microservice architectures. Thank you for watching, and we invite you to explore Query and join our community. The team begins inspecting the message body and attributes and suspects that character encoding errors might be causing the message failures. They want to manually modify each problematic message and resend it to the source queue to see if the changes would allow successful processing by the inventory service. However, the AWS SQS console does not provide the option to modify messages before resending them.

Even if they could modify the messages, the AWS SQS console only allows for batch redriving and not individual resending. The team realizes that the built-in functionality of the AWS DLQ does not meet their requirements. They start researching other solutions and come across DLQ implementations by other message queue services.

They discover that other services, like AWS, also have drawbacks in their DLQ implementations. Most services require users to manually create a queue and configure it as a DLQ. At best, users can manually retrieve messages, view them, and batch resend them to the source queue. Operating on individual messages is not common functionality, and the message retention period of the source queue poses an issue if more time is needed to analyze a message.

The team realizes that many others are frustrated by these limitations and that there are tools available for monitoring dead letter messages in testing, development, and production environments. The leading platform in this area is Serverless 360, which offers features fulfilling many requirements. It provides easy message viewing, search options for message filtering, access to the reason for message failure, and the ability to modify, delete, and redrive individual or batch messages. It also allows users to set timers for specific operations.

However, Serverless 360 is not compatible with AWS. Another open-source solution called Service Bus Explorer checks some of the boxes that the team is interested in, but it also lacks the full range of features offered by Serverless 360.

This presents a problem for the team as they are currently restricted by the offerings of the AWS DLQ but cannot make use of any available DLQ monitoring tools, missing out on rich functionality. This is where their project comes in.

During their research for a suitable solution, the team comes across Curry, an open-source dead letter queue as a service specifically designed for small distributed applications that use AWS infrastructure. Curry meets the majority of their requirements, is open source, and compatible with AWS queues.

Curry provides a seamless pipeline incorporating a source queue and a DLQ. The user can set up both queues from scratch or provide details of an existing source queue for Curry to set up a DLQ on their behalf. Once the core infrastructure is configured, Curry is ready to use with just three simple steps.

The Curry dashboard provides easy access to messages without the need for polling. Messages can be sorted based on their timestamp or body text. Individual messages can be modified by altering their body and attributes. They can also be deleted or redriven to the source queue. Batch delete and redrive operations are also available.

Curry's ease of use and functionality make it an ideal tool for the team. However, it is necessary to understand the process of installing and using Curry.

The installation process consists of three steps. The user needs to download the npm package globally, which requires no manual configuration or adjustments. Once the package is installed, the available commands can be checked, which include deploy and view.

To deploy Curry, the user can choose to create a DLQ only, as they already have a source queue. They provide the source queue's URL, AWS region, and instructions for Curry to send notifications to their Slack channel. After confirming the configuration options, Curry provisions the necessary AWS resources.

After deployment, the user can access the Curry dashboard. The dashboard provides a summary of the current state of the DLQ, including the number of dead letter messages. The messages can be viewed in a table format, displaying their ID, timestamp, and body. Navigation options allow for quick browsing of the list.

The messages can also be sorted by timestamp or message body. Sorting in ascending order can prioritize specific messages. For example, moving bulk discount orders to the top of the list can be useful.

If the team wants to give the inventory service another opportunity to process the messages, they can redrive all the messages back to the source queue. One by one, the messages are deleted from the DLQ on their way back to the source queue. The inventory service will pull the source queue for these messages upon their arrival and retry processing them a set number of times.

For individual message processing, the user has several options through the action drop-down. They can view the message details, including ID, timestamp, body, and user-provided properties. They can also delete a message permanently or take other actions based on their requirements.

For example, if the team has communicated with customer services and learned that an order with a specific ID has been attended to, they can simply delete the message from the queue. They can also modify the message body if needed and redrive it to the source queue.

If the user wants to perform further operations on the remaining messages, they have the option to delete them all and empty the DLQ.

Transitioning to the infrastructure aspect of Curry, it consists of six high-level components. The first component is inter-service communication, which involves the producer service, the consumer service, and the AWS Simple Queue Service (SQS) queue.

The producer service is responsible for asynchronous tasks and communication with other services in a distributed system or microservices architecture. The consumer service accesses messages from the producer service. AWS SQS acts as the messaging queue service for decoupling and scaling microservices, distributed systems, and serverless applications.

The failed message handling component comes into play when messages fail to be consumed. It includes the dead letter queue (DLQ), which is directly attached to the source queue. The AWS DLQ captures messages that cannot be processed successfully by the consumer service. Curry automatically configures the DLQ to the user's source queue.

The fan-out component involves a Lambda function that sends messages to a Simple Notification Service (SNS) topic, which enables inter-application and application-to-person communication. Curry uses the SNS to alert developers about failed messages and handle notifications.

The notification persistence component includes two Lambda functions that subscribe to the SNS topic. These functions receive copies of published messages.

Finally, the user interaction component allows developers to view and interact with the Curry dashboard. The dashboard provides a summary of the DLQ and allows users to sort messages, modify message bodies, delete messages, and redrive them to the source queue.

Overall, Curry offers a comprehensive solution to the team's dead letter message management needs. Its compatibility with AWS and its user-friendly dashboard make it an ideal tool for monitoring and interacting with dead letter messages. The Queue Service Dead Letter Queue (DLQ) is designed to capture messages that cannot be successfully processed by the Consumer Service. When Curry is configured, it automatically provides a DLQ configured to the user's source queue. This is achieved by creating an Amazon Simple Queue Service (SQS) queue, setting it as a DLQ, and providing it with the AWS resource name of the source queue.

Curry offers two options for developers. The primary option is the main queue and DLQ option, where developers configure Curry to provide a new AWS queue and DLQ. This is suitable for new projects or existing projects that need a queue. The second option is the DLQ only option, preferred by developers who already have a project with an existing queue and want to add the Curry DLQ to their infrastructure.

For the DLQ design, we decided to use the SQS DLQ, as it effectively collects failed messages and opens the message pipeline for the Curry infrastructure. This allows Curry to provide additional DLQ functionality, rather than consing failed messages directly from the SQS itself, which would be more challenging. The provided AWS DLQ effectively collects these failed messages.

Now that the failed messages are being collected in the DLQ, the next question is what to do with them. We need a way to alert the developer of these messages and provide them with the option to interact with and possibly resubmit them to the source queue. This is where the final component comes in.

The final component consists of a Lambda function that sends a message to a Simple Notification Service (SNS) topic, and two other Lambda functions that subscribe to this topic. We will explore each of these resources and their responsibilities in the following sections.

The SNS is a fully managed messaging service for application-to-application and application-to-person communication. Curry uses both avenues of communication. For the SNS, we had to decide whether to use a standard or FIFO (First-In-First-Out) style SNS topic. We chose the standard SNS topic for its best-effort messaging ordering, at-least-once message delivery, and highest throughput in publishes per second. Additionally, the standard SNS topic provides more subscription protocols, specifically the Lambda protocol which we needed for our Lambdas.

During deployment, Curry creates an SNS topic and adds two subscribers: a Lambda function to post to Slack and another Lambda function to write to DynamoDB. The Publish to SNS Lambda function is responsible for pulling the messages from the DLQ. It reformats the message structure from SQS formatting to SNS formatting and then passes the message to the SNS topic for distribution to the subscribed Lambdas.

Both the Post to Slack and Write to DynamoDB Lambda functions subscribe to the SNS topic. They add a timestamp to the message, ensure that the message attributes are correctly formatted, and then pass the message to Slack or DynamoDB, respectively. Having separate Lambdas that subscribe to the SNS topic helps in decoupling concerns and allows for additional functionality in the future by adding more Lambdas as subscribers to the SNS topic.

Moving on to the Notification component, which is one of the two branches from the Fanout component. This component consists of Slack, a messaging program for teams and organizations to communicate. Curry offers the option of sending DLQ notifications directly to the user's preferred Slack channel. The developer can achieve this by creating a Slack app within the Slack API interface. They then provide the provided Slack webhook path to Curry during the initialization and configuration process before deployment. Once set up, any messages that arrive in the developer's DLQ will trigger a notification in their chosen Slack channel. The notification identifies itself as a Curry notification and provides data about the DLQ message, such as the source queue name, message body, timestamp, and message attributes. This is especially useful for developers who want to closely monitor failed messages without constantly checking the Curry dashboard.

The Persistence component consists of DynamoDB, a fully managed serverless NoSQL database. It stores all the DLQ messages, allowing developers to view, edit, delete, or redrive their dead letter messages back to the SQS. DynamoDB was chosen as the persistent data store because it is well-suited for storing and working with JSON data, which is the format our messages are in. We considered AWS Relational Database Service (RDS), but decided against it because we wanted to keep the messages in JSON format for redriving them back to the main queue.

Finally, we have the User Interaction component, which consists of the Node.js application server. This server provides an API for the React dashboard to consume. The User Interaction component has a two-way relationship with the Persistence component and a one-way relationship with the Inter-Service Communication component. It provides API routes for the React dashboard to interact with the messages stored in DynamoDB. The application server is primarily built with Node.js on the Express framework. It uses the AWS DynamoDB SDK to provide RESTful API routes that the React dashboard can consume. The decision to use REST APIs was made based on the need to interact with the database and provide an easy-to-use dashboard for developers to work with their DLQ messages.

To recap, the message originates from the producer and goes to the SQS queue. The message can be successfully consumed by the Consumer Service or fail and be sent to the DLQ. The Publisher SNS Lambda pulls the messages from the DLQ, reformats them, and passes them to the SNS topic. The SNS topic then distributes the message to the Post to Slack and Write to DynamoDB Lambdas. The Slack Lambda sends a notification to the developer's Slack channel, while the DynamoDB Lambda writes the message to DynamoDB for persistence. The developer can interact with the DLQ messages using the Curry dashboard, which communicates with the Application Server API. Finally, the developer can use the Redrive functionality to move the message back to the source queue.

Curry simplifies the developer's life by handling the setup and configuration of AWS resources needed for the DLQ. During deployment, Curry provisions 17 resources, including queues, topics, Lambdas, DynamoDB tables, and S3 buckets. This eliminates the need for developers to manually configure these resources and reduces the learning curve associated with integrating the AWS SDK. Furthermore, Curry can be used with any software development stack that includes an AWS queue, providing flexibility for developers.

In conclusion, Curry simplifies and automates the setup and configuration of a DLQ for AWS queues. It captures failed messages, alerts developers, and provides them with a dashboard to interact with and persist these messages. Curry handles the provisioning of AWS resources, reducing the development time and learning curve for developers. With its flexibility and ease of use, Curry empowers developers to effectively manage their DLQ messages. The declarative framework provided by AWS, known as the AWS Cloud Development Kit (CDK), simplifies the process of creating permissions and roles for infrastructure deployment. It offers the ability to provision infrastructure as code, enabling easy reuse of code for deploying stacks. While the CDK is a powerful tool, it has some drawbacks, such as slower speed compared to the AWS SDK and limitations on asynchronous operations.

On the other hand, the AWS SDK provides a faster solution for manipulating infrastructure components. It offers a wide range of asynchronous functions, allowing for more flexibility in making changes to the overall infrastructure. However, using the SDK requires creating each component individually, which increases the potential for errors. Additionally, manual integration of infrastructure with lambdas and subscriptions is needed.

Considering the pros and cons of both the CDK and SDK, we determined that the CDK was better suited for our needs due to its ability to reuse code and its compatibility with our infrastructure. However, concerns about the CDK's relative speed prompted us to conduct further research and testing to make an informed decision.

During prototyping, we performed a spike test using boilerplate code from AWS to create a stack with each development kit. Our tests revealed that the CDK's lack of asynchronicity posed difficulties, requiring us to split the initialization process into two commands. The CDK also exhibited slower speed, leading to inconvenient wait times during deployment. Additionally, the terminal output during deployment was either empty or displayed unrelated AWS code, which was not ideal for user experience.

In contrast, the SDK excelled in user experience, offering faster deployment times and real-time progress updates in the terminal. As a team, we felt confident that we could replicate the CDK's functionality using the SDK, while benefiting from its superior user experience. Thus, we decided to move forward with the SDK for deploying our infrastructure.

However, choosing the SDK presented its own challenges. We needed to find the most efficient way to manually create each resource, zip the lambdas, add permissions and subscriptions, and deploy them in the correct order. Furthermore, we had to incorporate the components provisioned by the CDK, such as S3 buckets for storing the lambdas.

Despite the challenges, we persisted, spending many hours debugging and refining the process. In the end, we successfully deployed our infrastructure using the SDK, achieving deployment times of under one minute with a single command. We also ensured that users could monitor the progress in real-time through the terminal.

Another challenge we encountered was how to alert users when a message entered the Dead Letter Queue (DLQ). Initially, we considered using AWS CloudWatch for continuous monitoring of the DLQ. However, CloudWatch proved to be unsuitable for providing real-time updates as there were significant time delays in receiving alerts.

To address this, we leveraged the instantaneous and reliable nature of lambdas and utilized Slice, a webhook service, to send post requests to a user's provided webhook path. This allowed us to deliver immediate notifications to users' devices whenever a message entered the DLQ. By integrating Slack with Slice, we solved the problem of real-time updates while maintaining user-friendliness and high availability.

Additionally, we encountered a challenge related to data types when sending messages to an SNS topic. The SNS topic converted all number data types to strings, which posed a problem in maintaining the integrity of user messages. To overcome this issue, we read the data type of each attribute before sending the message, appended the data type to the attribute, and then removed it upon receipt by the receiving lambda. This approach ensured that we could maintain the data types of all message attributes throughout the entire infrastructure.

With these challenges overcome and our project completed, we look towards future work for Curie. While we are extremely pleased with the final version of Curie and its availability as an npm package, we recognize that there is always room for additional features. Some potential features for future versions of Curie include the ability to access console logs to provide insights into message queuing, creating Slack notifications that allow users to delete or reprocess messages directly from Slack, and implementing a teardown command that allows users to destroy the infrastructure with a single CLI command.

In conclusion, our team thoroughly enjoyed working on Curie, from its initial concept to a fully functional product available for public download. We overcame various challenges and learned valuable lessons along the way, including the power of the AWS SDK and the importance of user experience. As with any software development project, there is room for improvement and additional features that could be implemented in future versions of Curie. We are proud of our achievements and look forward to further enhancing the capabilities of Curie. Thank you for listening to our presentation, and we are now ready to answer any questions you may have. During the Capstone project, our team worked exceptionally well together, setting us up for future success. We thoroughly enjoyed the experience of working in teams and collaborating with others to program. We not only learned to program as a group but also got to know each other's strengths and leveraged them effectively. By complementing each other's skills, we achieved great results. Additionally, managing time was a crucial aspect due to our international distribution. While there was a learning curve in working synchronously and asynchronously, we overcame it and had a wonderful experience.

Moving on to the question about how we decided on the topic of DLQ, we followed a process of ideation. Initially, each team member researched their own areas of interest. We then had daily meet-ups to discuss the topics we had explored and answer any questions raised by team members. Through these discussions and answering questions, we realized that DLQ, specifically in the context of AWS queuing services, was worth pursuing. We observed that the DLQ functionality in AWS was very limited, as it was essentially just another queue that had to be created manually. Recognizing this underdeveloped area, we decided to focus on DLQ as our topic. From that point on, the entire team was aligned on this decision.

In our research phase, we delved into different message queuing services offered by major providers. DLQ stood out as a consistent feature across these services. However, each implementation seemed to miss key functionalities that we deemed important. Finding existing solutions proved challenging during our search. We found that altering one word in our Google search, specifically looking for dead letter queue monitoring or management services, led us to discoveries like Serverless 360 and Service Bus Explorer. This realization marked a significant turning point for us, where we identified a strong foundation to build upon for our project.

Addressing the question regarding the team's decision between CDK and SDK, we initially explored both options after realizing that AWS offered two developer kits. Despite being torn between the two options, we experimented with both and prototyped code for each. However, after two to three days, we prioritized testing the CDK and SDK by deploying infrastructure. Although the CDK appeared more promising initially, it presented significant issues such as slow deployment and inflexibility once a stack was deployed. Consequently, we ultimately chose to proceed with the SDK route. Writing the SDK posed the most challenging aspect of the project, taking us nearly a week to complete each part and integrate them effectively.

A determining factor in our decision-making process between CDK and SDK was the command-line tool. When we incorporated the CLI into our early versions, we observed that the CDK commands for deployment and setup took an excessive amount of time. This prompted us to try out the SDK, which ultimately proved to be smoother and faster. Despite this improvement, ensuring proper alignment of various resources and executing promises in the correct order to achieve a seamless DLQ deployment still posed challenges. The CLI played a significant role in influencing our decision, and we are pleased with the choice to go with the SDK.

As for the biggest challenge we encountered throughout the process, we touched upon it earlier. For me personally, the most demanding aspect was using the SDK to create the entire infrastructure. Other team members may have faced different challenges, but personally, that was the most difficult part of the project. Another challenge we faced related to the SNS topic and the changes it made to data types. This caused confusion and required close examination of the AWS documentation. We also relied heavily on console logs or puts in cases where we anticipated changes in data types from numbers to strings, which SNS appeared to do when sending messages to a lambda subscriber.

In terms of managing different time zones, it required diligent organization. We established a schedule consisting of asynchronous and synchronous periods. This allowed us to work together during specific time slots and continue working individually during off-hours. Detailed notes were crucial for continuity, passing on information to the next team member who would pick up where the previous one left off. Overall, we successfully navigated the challenges posed by our diverse time zones and achieved effective collaboration.

In conclusion, we would like to express our gratitude to everyone who took the time to listen to our experiences and insights. We hope you found the information valuable and enjoyed our presentation. Thank you for your attention.