Welcome to our presentation. I'm Ashish, and together with Aryan, Jordan, and Michael, our team built Triage, a congestion proxy that solves head-of-line blocking for Kafka consumers. Today, we will provide an overview of the larger context of microservices and event-driven architecture. We will then focus on message queues, specifically Apache Kafka, and discuss the problem of head-of-line blocking and its consequences. Additionally, we'll share our research on existing solutions and present Triage, our approach to solving head-of-line blocking, including the design challenges we faced. Lastly, we will touch upon future work and leave room for questions. We're excited to showcase our project, so let's get started.

Microservice architecture has significantly grown in popularity over the last decade, with over 63% of enterprises adopting microservices by 2020. This architecture isolates services into separate entities, allowing for parallel development and independent scaling. Polyglot microservice environments are common, where multiple technologies and programming languages are utilized. The challenge lies in achieving seamless inter-service communication in such setups. One approach is to use a request-response model, commonly seen in web applications. However, this model can lead to head-of-line blocking if a single service experiences a slowdown, affecting the entire request cycle. An alternative solution is event-driven architecture (EDA), which allows services to operate independently. Apache Kafka is a popular choice for implementing EDA, offering scalability, parallelism, and decoupling.

In Kafka, events are referred to as messages and are organized into topics. Messages within a topic are written to partitions, allowing for scalability. Producers write messages to topics, and consumers subscribe to topics to receive messages. Messages can be processed in parallel by multiple consumer instances. Kafka uses offsets to keep track of processed messages, allowing for reliable message delivery even in the event of consumer crashes. This architecture ensures decoupling, scalability, and fault tolerance.

Head-of-line blocking is a common issue in message queues, similar to being stuck behind a slow customer in a supermarket line. It occurs when one slow message delays the processing of all subsequent messages. There are two main causes of head-of-line blocking: poison pills and non-uniform consumer latency. Poison pills are problematic messages that the consumer cannot handle, leading to crashes and blocking the processing of subsequent messages. Non-uniform consumer latency occurs when one message associated with a slow external service delays the processing of unrelated messages.

To address head-of-line blocking, we established five solution requirements: handling both causes of blocking, preventing data loss, working in polyglot microservice environments, being open-source, and freely available. Existing solutions like Confluent's parallel consumer and DoorDash's worker model focused on either poison pills or non-uniform consumer latency but failed to meet all our requirements. Uber's consumer proxy, on the other hand, addressed both causes and fulfilled all our requirements but was not widely accessible.

To bridge this gap, we developed Triage, a proxy for consumer applications that addresses both poison pills and non-uniform consumer latency. Triage ingests messages from the Kafka cluster and sends them to downstream consumer applications. It implements the dead letter pattern, storing problematic messages in DynamoDB for later examination. Triage ensures reliable message processing by utilizing an internal system called commit tracker to manage offsets commits. By calculating the greatest committable offset, Triage commits back to Kafka while maintaining partition health.

Triage's commit tracker works by storing messages in a hashmap, using their offsets as keys. As consumer acknowledgments are received, the commit tracker updates the hashmap. Before updating the commit hash, it ensures that messages have been successfully stored in DynamoDB. To maximize commit efficiency, Triage periodically runs the commit calculator, which identifies the greatest offset with confirmed acknowledgments from all previous offsets. Triage can then commit that offset and remove entries from the commit tracker.

Triage's solution for poison pills involves marking problematic messages with a "negative acknowledgment" from consumer applications. These messages are then stored in DynamoDB, freeing the consumer to continue processing other messages. For non-uniform consumer latency, Triage enables parallel consumption of messages from a single partition. This allows fast messages to be processed independently of slow ones, preventing delays caused by unrelated issues.

In conclusion, Triage addresses head-of-line blocking by acting as a proxy for consumer applications. It handles both poison pills and non-uniform consumer latency, preventing data loss and optimizing message processing. Triage's commit tracker ensures accurate and efficient offset commits, maintaining the health of Kafka partitions. While existing solutions partially addressed the issue, Triage fulfills all our requirements and provides an accessible solution for developers in polyglot microservice environments. Thank you for listening, and we are now open to questions. In this video, we will discuss how the commit tracker works and its central role in the functionality of triage. Triage first ingests a large batch of messages and stores them in a hash map. The keys of the hashmap are the message offsets, and the values are custom structures with two fields: the message itself and a boolean indicating whether it has been acknowledged.

When triage receives ACKs from consumers, it updates the commit hash accordingly. However, it cannot immediately update the commit hash. It must first ensure that the message has been successfully written to the dead letter store, a DynamoDB table. Only then does it update the commit hash.

Next, the rest of the messages are processed by the consumers, including one orange message that takes a long time to process. As a result, the faster green messages are processed and ACKed before the orange one. At this point, whether a message has been ACKed or NACKed is not important. The goal is simply to know that a message has been acknowledged in some way.

To calculate which offset to commit back to Kafka, triage needs to find the greatest committable offset. Periodically, a component called commit calculator runs in the background. It checks the commit hash to see the greatest offset with the value of true for which all lower offsets also have the value of true. Triage can then commit this offset back to Kafka.

Once triage receives confirmation from Kafka that the commit was successful, it can delete all entries up to and including the offset from the commit tracker. These entries are no longer needed.

Now let's discuss how we solved head-of-line blocking due to both poison pills and non-uniform consumer latency. When a conser application receives a poison pill message, it informs triage by sending a NACK. Triage then sends that message to a DynamoDB table to be handled later. This frees up the conser to continue processing other messages.

To address non-uniform consumer latency, triage enables parallel consumption of messages from a single partition. For example, in the case of orange and green messages, the conser application calls the corresponding external service based on the content of the message. Because the orange service is slow, the conser instance at the top takes longer to process an orange message. However, due to the parallel consumption enabled by triage, other healthy conser instances can continue consuming messages, ensuring a steady flow through the queue.

Now let's discuss some of the challenges we faced during the development of triage and our plans for improvements. The first challenge was achieving parallel processing via concurrency. We wanted a one-to-many relationship between triage and conser instances. Go routines in the Go programming language allowed us to achieve this concurrency. Each conser instance runs in its own dedicated go routine, which pulls messages from triage and sends them to the appropriate conser instance.

The second challenge was polyglot support. We needed triage to support conser applications written in different languages. Our solution was to implement triage as a service coupled with a thin client library. The client library manages communication with triage and exists within each conser instance.

Finally, we faced the challenge of making triage easy to deploy. We created an automated deployment script using AWS's Cloud Development Kit (CDK). Developers can use our command-line tool, triage CLI, to easily deploy triage to AWS. The deployment script provisions triage containers using Elastic Container Service (ECS) with Fargate as the deployment vehicle. The use of Fargate abstracts away the management of individual compute resources, making it easier to deploy and manage triage.

In conclusion, we have discussed the functionality of the commit tracker, how triage solves head-of-line blocking, the challenges we faced during development, and our plans for improvements. We are planning to add additional language support for our thin client library, add a cause of failure column to our dead letter table, and implement a simple notification system for poison pills. Thank you for watching and please feel free to ask any questions. In this video, we will discuss the script to deploy Triage to the cloud. We have successfully solved the major challenges and were able to build Triage without compromising on any of our design requirements. For more detailed information on how Triage works and implementation details, please check out our write-up, which we will link in the Zoom meeting description.

Before we open up for questions, we would like to cover some features that we plan to add to Triage. Our first priority is to build out additional language support for our Fin client Library. This should not be difficult as most of the work is already done for us via gRPC code generation. Supporting popular languages like JavaScript or Ruby will help us serve more developers.

Another feature we want to add is a "cause of failure" column to our table that stores dead letter messages. This column will contain reasons that developers can supply when sending a "nap" back to Triage or when encountering poison pills. This will aid in analysis and remedying faulty messages.

Lastly, we would like to add a simple notification system that can alert developers when poison pills are stored in the dead letter table. This will allow for rapid response. We believe this feature is the easiest to implement and is likely to be our next step.

We would like to thank all of you for joining us this afternoon. Now, let's move on to the question and answer session.

Q: Did you consider any alternatives to gRPC?
A: Yes, we did consider HTTP as our first choice. However, after analyzing the relative performance benefits, such as packet size, we decided to go with gRPC. gRPC uses a more efficient data serialization method called Protobuf, which results in smaller packet sizes and higher throughput. Additionally, using gRPC eliminates dependency problems that we would encounter with JSON compression, as each version of the Triage client would need to implement that language's version of JSON.

Q: How easy was it to work with gRPC compared to a typical REST API?
A: Working with gRPC required more time and effort to set up initially, but for our specific use case of a unary service (one-way communication), it was feasible and provided the necessary functionality. While detailed tutorials and reliable resources were scarce, we found that narrowing down our specific use case helped us achieve everything we could with a REST API using gRPC.

Q: What was the most challenging problem to solve in this project?
A: One of the most challenging aspects of the project was working with Kafka. There are over 40 different configurations that can impact how the application interacts with Kafka, and finding the right Kafka consumer library was not intuitive. However, overcoming these challenges was also one of the most interesting parts of the project.

Q: Did you consider other languages or implementations of concurrency?
A: We did consider languages like JavaScript or Ruby, but they do not provide as natural a fit for concurrency compared to Golang. We also explored the option of deploying workers as separate containers on AWS, but the overhead associated with that approach made Golang's simplicity and built-in concurrency features the clear choice.

Q: How did you decide on this problem to tackle, and how did the project change as you delved into the problem space?
A: Initially, we were drawn to Kafka and wanted to explore its limitations, such as out-of-order message delivery. However, as we delved deeper into the problem space, we realized that solving this issue was more theoretical than practical. We then shifted our focus to another problem area in Kafka, namely head-of-line blocking, which lacked robust handling. This change in direction led us to the specific problem domain we addressed in this project.

Q: How did you discover that this was a problem real companies are facing?
A: Through extensive research and reading engineering blogs, we discovered that companies such as DoorDash, Uber, and CrowdStrike have experienced similar issues with Kafka. This gave us confidence that we were tackling a real problem faced by companies at scale.

Q: How does Triage handle messages that rely on the completion of previous messages?
A: Triage does not directly handle messages that rely on the completion of previous messages. Typically, these kinds of applications would consume from separate topics, and it would be up to the application logic to ensure that dependencies are resolved before processing. While Triage does not handle this out of the box, it is something we may explore in future iterations.

Thank you all for attending our presentation and for your questions. We hope you found it interesting and learned something new. For more information, please check out our write-up and repository.