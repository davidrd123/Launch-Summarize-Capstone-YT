foreign and Welcome to our presentation I'm Ashish and together with Aryan Jordan and Michael our team built triage a consumer proxy that solves head of line blocking for Kafka consumers here's a quick overview of what you can expect first we'll address the larger context of microservices and event driven architecture from there we'll take a look at message cues and focus on Apache Kafka with a few details on how it works next we'll examine the problem of head of line blocking and its consequences after which we'll share our research on some existing Solutions at that point we'll present triage and our approach to solving head of line blocking along with some interesting design challenges we faced we'll end with some ideas for future work and leave some room for a q a we're excited to show you what we built so let's get started microservice architecture has really gained in popularity over the last decade and in 2020 it was estimated that over 63 percent of Enterprises had adopted microservices and were satisfied with the trade-offs here's an example of a simple architecture for a shopping app the takeaway here is to show is to notice how the services are isolated into separate pieces so the orders products and stock Inventory Services all have their own logic and data stores and the shopping app can communicate with all of them since Services can be decoupled in this way work can be done in parallel which leads to faster development times additionally there's a benefit in the ability to take individual components and scale them independently often multiple Technologies and programming languages are used in these setups which is known as a polyglot microservice environment given the use of these different languages an important question is how do we successfully achieve the required interest system communication for the system to function properly one option is to use a request response model which is commonly used on the web so imagine a number of interconnected microservices where Services can send a request and wait for responses the issue is that if a single service in this chain experiences a Slowdown the request life cycle of any connected service will also be delayed to overcome this problem a common choice is to implement an event-driven architecture or an Eda edas are centered around events which can be thought of as changes in state or notifications about a change the key here is that Services can operate independently without concern for the state of any other service the service on the left here can communicate with all three services on the right independently this architecture bypasses the problem where a delayed service causes a Slowdown throughout the entire system in order to achieve this decoupling edas can be implemented using message keys here we have two producers to the left of the message queue these applications write events to the queue the consumer which is to the right reads these events off of the queue in a traditional message queue events are read and then removed from the queue an alternative approach is to use log base message queues here all the events are persistent on a log so you don't lose them once they're read among log base message queues Kafka is the most popular over 80 percent of Fortune 100 companies across industries use it as part of their architecture Kafka is designed for scalability and parallelism and it maintains the intended decoupling of an Eda it's worth taking a look at what's unique about Kafka and how it works in the context of Kafka events are called messages and this is how we'll refer to them from here on out so in this image messages are grouped using a named identifier called a topic Kafka achieves scalability by writing all the messages of a topic to partitions so in this example messages in a single topic are written to two different partitions if we add the other pieces of the architecture it'll look something like this so producers seen on the left write messages to a topic consumers on the right are organized into groups with a group ID if a consumer wants to read messages it can subscribe to a specific topic then individual consumer instances can read messages from a partition to achieve more scalability you could simply increase the number of partitions per topic Additionally the use of multiple consumer instances means that the messages can be processed in parallel it's important to note that while a consumer instance can consume from more than one partition a partition can only be consumed by a single consumer instance in other words two different consumer instances can't consume from the same partition Kafka uses commits to know which messages have been successfully processed the way this works is that every message on a partition has an offset this is a number that indicates the position of the message in the queue think of it like an index in an array a consumer periodically commits offsets back to Kafka indicating the last message it successfully processed if a consumer instance crashes Kafka will remember where to resume message delivery from so in this image the consumer commits offset number 50. Kafka now knows that the messages from 48 to 50 have all been successfully processed the consumer can continue consuming before it commits the next offset to recap producers write messages to a specific topic Kafka then routes these messages to partitions consumers subscribe to a topic to receive messages and commit offsets each partition in a topic can only be consumed by one consumer instance now that we've shown the larger context Jordan from our team will explain the problem of head of line blocking in message queues thanks Ashish a real world example of head of line blocking that we are all likely familiar with is when you're at the supermarket and the person in the front of the line is taking a long time to finish paying perhaps they're trying to use expired coupons or have multiple fruits each with their own ID or they're trying to pay with Bitcoin it slows down the entire line and everyone behind them has to wait message cues can also suffer from head of line blocking in this example there are four messages the first green message is processed quickly the orange one though takes longer to process and crucially while it's being processed all of the other messages have to wait once the slow message is processed the rest of the queue can proceed there are two major causes of head of line blocking when it comes to message cues the first is poison pills in this example the circles are regular messages and the skull and crossbones represents a poison pill a poison pill message is one that the consumer does not know how to handle for example if the application developer is expecting an order quantity as an integer but receives one as a string and has not written error handling to handle this scenario the application May crash this will prevent processing of all the messages behind the poison pill message in the queue the first message is consumed quickly but the poison pill message crashes the consumer application no further messages can be processed the second main cause of head of line blocking is non-uniform and consumer latency suppose we have a consumer application that calls one of two external Services depending on the content of a message for green messages the application calls the green service and for orange messages the application calls the orange service the first message is processed normally since the green service is healthy now imagine that the orange service is slower than usual to respond perhaps due to network issues this means that the processing of all the messages in the queue is slowed even though the green messages have nothing to do with the orange service the messages are not able to be processed until the orange service completes once the orange service finishes the block is lifted and the rest of the messages can be processed in determining our desired approach to solving head of line blocking we decided on five solution requirements the first two were handling the two main causes of head of line blocking the third requirement was that data loss was prevented a naive way of handling head of line blocking would be to just drop messages that are causing it this might be appropriate for non-critical scenarios such as tracking likes on social media where it's not critical that every like is captured however for critical situations such as those involving orders it is crucial that every order is captured otherwise potential Revenue may be lost we want a solution that could prevent data loss the fourth requirement was that the potential solution could be easily integrated into polyglot microservice environments lastly the fifth requirement was that the potential solution would be open source and easily available to Developers with these solution requirements in mind we'll now look at the existing solutions that we found that addressed edit line blocking the three solutions we found were confluent parallel consumer door Dash's worker model and Uber's consumer proxy confluent parallel consumer fixes head of line blocking caused by both poison pills as well as non-uniform consumer latency but it doesn't have a way to store poison pill messages and since we cannot tolerate data loss the solution was not viable for our use case also their library is written in Java meaning developers would have to write their applications and job as well this was countered to our goal of finding a solution that worked well in a polyglot environment while using Kafka doordash experienced spikes and latency in their consumer applications individual slow messages were causing delayed processing for all messages in a given partition a real world example of non-uniform consumer latency to address this they introduced something they called Costco workers this solution however failed to address poison pills and with no mechanism to prevent data loss the solution was insufficient lastly Uber's consumer proxy solves head of locking solves head of line blocking resulting from both poison pills and from non-uniform consumer latency poison pills are handled without data loss and non-uniform consumer latency is addressed by parallel consumption of messages Uber built consumer proxy as its own piece of Industry of infrastructure in order to work well in polyglot environments however as an in-house solution it is not available for us or for other developers to use given that none of the existing Solutions fit all of our requirements we decided to build triage next Aryan will discuss what triage is and how it handles both causes of head of line blocking thanks Jordan triage acts as a proxy for Consumer applications it ingests messages from the Kafka cluster and sends them to Downstream consumer applications here's a high-level view of a triage instance in the cloud triage consumes from a single partition just like any other Kafka consumer triage functionality consists of the Apple application logic running in an AWS container and a dynamodb instance problematic messages are stored in Dynamo for examination at a later time this pattern is known as the dead letter pattern in dead letter patterns problematic messages referred to as dead letters are removed from the consumer application and persisted to an external data store for later processing to manage commits back to Kafka triage uses an internal system of acknowledgments with a component we call commit tracker consumers can send an act a positive acknowledgment back to triage indicating that a message was success successfully processed or a knack a negative acknowledgment to indicate a poison pill message using the commit tracker triage can calculate which offsets to commit back to Kafka this ensures that the health of the partition is maintained let's take a look at how commit tracker Works since it's Central to the functionality of triage triage first ingests a large batch of messages and stores them in a hash map the keys of the hashmap are the message offsets and the values are custom struck with two Fields the message itself and the Boolean indicating whether it has been acknowledged as triage receives acts from consumers we update the commit hash accordingly when a message is knacked however we cannot update the commit hash immediately we must first ensure the message has been successfully written to our dead letter store which is a dynamodb table and only then do we update the commit hash next the rest of the messages are processed by the consumers including one the orange message that takes a long time to be processed by the consumer as a result the faster green messages are processed and act before the orange one is it's important to note that since we always wait for confirmation from Dynamo before updating the commit hash at this point whether a message has been act or knacked isn't important we only want to know that a message has been acknowledged in some way so how do we calculate which offset to commit back to Kafka we want to commit as many offsets as we can so we need to find the greatest committable offset periodically a component called commit calculator runs in the background it checks the commit hash to see the greatest offset with the value of true for which all lower offsets also have the value of true triage can then commit this offset back to Kafka once we receive confirmation from Kafka that the commit was successful we can then delete all entries up to and including the offset from that offset from Kafka from commit tracker since they're no longer needed with this understanding of commit tracker and the core functionality of triage let's take a look at how we solved head of line blocking due to both poison pills and new non-uniform consumer latency let's start with poison pills here we can see a consumer application receiving a poison pill message consumer applications tell triage that the message they've received is a poison pill by sending a neck triage sends that message to a dynamodb table so that it can be handled at a later time this frees up the consumer to continue processing messages to address non-uniform consumer latency triage enables the parallel consumption of messages from a single partition here we have two instances of a single consumer application that rely on one of two external Services based on the contents of a message for the orange messages the application calls the orange external service for the greens the green service here you can see that because the orange service is slow the consumer instance at the top is taking an unusually long time to process a message because of the one to many pattern enabled by triage healthy consumer instances are able to continue consumption so the queue keeps moving now that you have now that you know how true our solves head of line blocking Mike will cover some of the challenges that we faced when building triage as well as our plans for some improvements we'd like to build out thanks Aria So based on our requirements and our intended design for triage there were three notable challenge that challenges rather that we like to discuss achieving parallel processing via concurrency poly lock support and ease of a deployment for each of these challenges I'll talk a little bit about them and discuss our respective Solutions let's start with parallel consumption via concurrency we need a one-to-many relationship between triage and instances of a consumer application to solve head of line blocking caused by non-uniform consumer latency our solution was to write the application logic of triage and go go is designed with concurrency in mind via what are called go routines we can think of go routines as non-blocking function Loops several think thousands of go routines can run in the background with very little resource overhead within triage we run a dedicated go routine for each Downstream consumer instance these go routines pull messages and send them to Consumer instances allowing us to consume from a single partition in parallel concurrency via go also allowed us to implement triage as a single application each major component of triage exists as a go routine that themselves utilize other go routines we achieved communication across these go routines using channels channels are strongly typed q-like structures go routines can place messages on the channel for other routines to receive what messages are received it's important to know that they are removed from the channel because message messages are removed we can have multiple senders and receivers without worrying about unintended data duplication let's take a look at some of the major components of triage and how we take advantage of concurrency at a high level we need to process to continually ingest messages from Kafka this go routine is called better in blue it then needs to pipe these messages via the messages channel to a go routine called dispatch and write them to our commit tracker in green well all this is happening we need another process to listen for incoming connection requests from consumer instances we call this go routine consumer manager when it receives a request after authenticating it consumer manager places the network address of the consumer instance onto a new consumer's Channel when dispatch receives the network address via this channel it creates yet another go routine called sender routine that pulls messages from the messages Channel these gender routines as their names imply send messages to their respective consumer instances zooming out a bit really hammers home the benefits we gain from concurrency all the components inside triage that you can see on the screen are go routines many of which rely on other go routines while implementing all this functionality with outgoing is certainly possible go made it very intuitive for us to mention it as the correct language for the job the next challenge we faced was polyglot support as you can see on the right side of the diagram we need a triage to be able to support consumer applications written in a host of different languages our solution was to implement triage as a service coupled with a Thin Client library in addition to our choice of grpg at our primary network communication protocol before choosing our implementation model we consider both a pure client library and a Pure service approach a potential pure client Library implementation would have all the application logic of triage exist as imported code within the consumer application this comes with the benefit of not having to introduce new pieces of infrastructure to a user system and makes testing triage simpler but supporting additional languages would require a complete rewrite of triage maintaining triage would also be pretty difficult since any change to A System's  diversion would require updating all versions of triage we consider these to be poor trade-offs an alternative would be to implement triage out of thermos with the peer service approach triage would act as a piece of infrastructure that sits between the Kafka cluster and consumer applications this allows us to avoid the aforementioned cons of a client Library implementation but we still wanted to make connecting to triage simple for Developers we decided on a hybrid approach the core application logic of triage exists on a container running in AWS consumer applications use a Thin Client library to manage communicating with triage this lightweight client exists within each instance of a consumer application it provides convenience methods for sending an initial connection request and an exposing an endpoint to receive messages from triage while we don't gain the full language agnosticism that a Pure service approach might offer building out multi-language support only requires us to rewrite our simple client library in another language ultimately the client Library only one sends an initial HTTP request to triage to request a connection and two runs a grpc server to receive messages because it's operationally very simple we write in a client library is far more manageable than rewriting triage in its entirety to manage communication between triage and consumer instances we chose grpc as a network protocol primarily for the East which we could build out multi-language support I think it's helpful to talk a little bit about what grpc is grpc is an RPC framework created by Google where RPC stands for remote procedure call we can think of procedure calls as simple function calls or invocation with a local procedure call everything exists on a single host machine in the figure on the left the function do work is executed on machine a resulting in code being executed executed on machine a remote procedure calls however allow us to execute code on a different machine in the figure on the right new work is being called on machine a resulting in code being executed on machine B it's helpful to understand that grpc uses the same client server model that we're all familiar with with triage the triage container act at the grp client and calls process message with a message as an argument the consumer instance runs a grpc server now listens for this procedure call it then executes code to profit the message before sending a response the acronach Arya mentioned back to triage the biggest reason we decided on grpc though is its code generation feature using what's called a grpc service definition client and server implementations can be automatically generated in all major programming languages creating a grpc service definition is pretty straightforward you simply Define a function interface that is what is the name of the function what parameters does it have and what does it return because the most complicated part of building the triage client library is handled for us via this code generation we can write support for other languages with relative ease the final challenge we faced was making triage easy to deploy for application Developers our solution was to create an automated deployment script using aws's Cloud development kit or cdk developers can use our command line tool triage CLI to easily deploy triage to AWS using the cdk script because triage operates on a per partition basis we needed to deploy a container running triage for each partition in a given Kafka topic to do this we used elastic container service specifically with fargate as our deployment vehicle with ECS we can define a minimum number of triage containers running at any given time or when to crash for some reason another will be provisioned to replace it automatically using fargate means management of individual compute resources is abstracted away for our users and allows them to only think about containers the key for us was that by using cdk we could write a reusable script to deploy triage containers via fargate that being said we still needed to answer the question of how to interpolate user-specific information such as Kafka authentication credentials into triage during deployment to do so we created a command line tool called triage CLI it can be downloaded as an npm package and features a two-step deployment process triage init installed any necessary dependencies for deployment and generates a configuration file where developers can supply authentication and kafka-specific information triage deploy interpolates the data in this configuration file into the cdk script it also creates an internal config file used by individual triage containers it then deploys these containers to AWS finally it Returns the network address and authentication key needed for consuming applications to connect the triage using triage CLI a developer can leverage our cdk script to deploy triage to the cloud having solved these major challenges we were able to build triage without compromising on any of our design requirements for a more in-depth exploration of how triage Works in implementation details check out our write-up which we'll Link in the zoom meeting description uh before we open up for questions we'd like to cover some features we'd like to add we'd first like to build out additional language support for our fin client Library as we've discussed doing so shouldn't be difficult since the majority of the work is done for us via grpc code generation supporting other popular languages like JavaScript or Ruby would help us serve more Developers we'd also like to add a cause of failure column to our table that stores dead letter messages it will contain bit like reasons that developers could Supply when sending a nap back to triage or poison pills this would Aid in analysis and remedying faulty messages finally we'd like to add a simple notification system that could alert developers when poison pills are stored in the dead letter table allowing for rapid response we think this is probably the easiest Implement and as likely our next step and with that I'd like to thank you all for joining us this afternoon and uh we'll go ahead and open the floor for questions uh yeah it looks like we have a question uh in the Q a from Catherine awesome presentation and thank you did you consider any alternatives to grpc um yeah we actually uh we're looking at HTTP as our first uh choice but um after spending some time looking at like the relative performance benefits um such as like packet size uh we chose grpc um without getting too in the weeds your PC uses um a much more efficient uh data serialization so the size of the packets we need to send over the network is much smaller which ultimately results in higher throughput there's also the question of like so I guess we would compare these grpc serialization which is called protoba to uh plain text Json um and so a lot of the times Json is compressed Now using something called Giza but even with that even though the compression benefits are less with grpc compared to Json with gcip we still run into dependency problems where every version of the triage client we want to implement we would need to also Implement that languages version of gisa some other questions in the Q a how easy did you find working with grpc as opposed to a typical rest API um Jordan or Ashish do you guys want to handle that you guys kind of spearheaded our GP grpc part uh yeah sure um I think if uh given a certain level of expertise maybe most people are used to creating uh basic rest apis pretty quickly uh with grpc there's definitely more time involved in getting everything set up but for our use case uh we needed what's called a unary service so it's just one-way communication grpc is also capable of achieving bi-directional communication which is uh sort of a more advanced setup um but I think because of kind of the basic functionality that we needed for our specific project uh it was pretty feasible to work with grpc there is a lack of very detailed tutorials and a little bit of confusion here and there because of the way Google handles their libraries for example if you look up what the G stands for in grpc there is an explanation from Google saying that it changes every year for no apparent reason um so stuff like that uh did make it a little bit hard to find um very reliable resources but I think if you really narrow down your use case you can do everything you can achieve in an arrest API with grpc uh we have another question um what was the most challenging problem to solve uh in this project um I think yeah I have something I can probably talk about um honestly working with Kafka was not the easiest thing in the world as we found um I think there's something like over 40 different configs that you need to know that can definitely input or impact how your application interacts with Kafka so we tried out a few different um Kafka libraries Kafka consumer libraries rather um and I think that was one of the more challenging Parts because sometimes working with Kafka is not super intuitive but at least for me that was probably the most challenging but also interesting parts of the project I don't know if the rest of the team has other input um yeah I think one challenging problem is just the idea of thinking about the individual components of the project because as we came across different solutions it would continue to change um sometimes design choices we had in mind about creating certain components for example how do we grab messages from Kafka and send it to the consumer once we discovered concurrency and go that ended up just being a simple go routine but before that we were considering this idea of maybe having a separate container for it and then that led to questions of how would we handle multiple containers in deployment um so I think not being too attached to a structure or a component ideas that we came up with was kind of hard at the beginning but once we got past that we were able to find really good solutions for the problems that we were trying to solve cool I think that kind of leads into the next question um super interesting use of concurrency with go did you consider any other languages or implementations of concurrency uh Aryan do you want to tackle that one sure um so we did look into like for example JavaScript or Ruby um but the there are ways to to have concurrency with them but it's not as natural a fit so one thing we did consider was instead of these workers basically being a little go routines with channels in between was to deploy to AWS and have the the workers be their own containers and these containers would would move from one channel to another so if I can show you this diagram over here where you have a messages Channel and go routines from fetcher to dispatch for example you could have workers that are their own containers that go to fetcher uh pick up a message go to a dispatch drop it off go to fetcher pick up a message and so that was the alternative but honestly the the overhead of doing that uh was high and just the Simplicity of doing like of Performing concurrency uh with go just made it a no-brainer we have another question um from Mitch how did you decide on this problem to tackle how did the project change as you delved into the problem space uh Jordan you want to go ahead and tackle that one yeah sure um so initially you know we were all very excited by Kafka and you know the Innovations it's brought into the message queue space and just started looking into what you know what are some of the limitations and initially we looked into out of order message delivery so in Kafka at a partition level all the all the messages will be delivered in order but across partition so if a topic is split across partitions you don't get that in order guarantee so that was initially something that we were looking into how could we solve that and you know built a prototype and then started looking into what are the use cases for that and besides some theoretical ones we decided that it might have been a solution in search of a a problem and so then we pivoted into a different area of Kafka you know that was that out of the box doesn't have um robust handling which is you know head of line blocking um so that was sort of our journey to how we got to the specific a problem domain uh we have another question I think also kind of this leads into how did you guys discover that this was a problem real companies are facing um yeah I think based off of Jordan's explanation um you know we we we're looking online and basically found a lot of engineering blogs that talked about this so companies like doordash uber I think we found articles by crowdstrike that I'll discuss having these issues um and so that really made helped us feel confident that this was a real problem that companies at scale are experiencing uh we have one other question um outstanding and well thank you appreciate that um from what I understood it sounded like triages is that one to many simultaneous message processing system instead of sequentially taking them one by one from a queue how does it handle messages that rely on the completion of some previous message I don't think that's something we actually address um typically we would expect that those applications I would expect that maybe that these consuming applications um would consume from separate topics so we see a lot of the times at least in our inspiration that uh as part of processing a message sometimes the consumer will actually produce another message to a different topic but that's not something that triage handles out of the box as of yet but it could be something for us to explore in our future work as well okay uh seems like uh that's the end of our presentation uh thanks everyone for attending and uh thanks for asking questions we hope you enjoyed it and learned something new feel free to check out our write up our repo if you're more Curious um the rest of the team I think do you guys want to sign off yeah thank you so much thanks for spending your Friday afternoon with us um I appreciate the kind words in the chat uh it's definitely very encouraging thank you for attending thank you 