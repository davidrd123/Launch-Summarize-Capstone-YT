hi everyone thank you for joining us today today we're going to present to you conifer which we've spent the past few months building my name is aina and my teammates are sam ahmad and lawrence i'm going to walk you through the introduction use case and potential solutions and then sam will talk about using conifer benchmarking conifer and the algorithm ahmad will then talk about the behind the scenes of how conifer works finally lawrence will talk about the implementation challenges we face and future work for conifer so let's get started what is conifer conifer is an open source framework that allows developers to easily deploy an infrastructure that runs cypress tests in parallel conifer reduces the total time it takes to execute a full-track suite for local development conifers aim is to encourage developers to run a full test suite as frequently as needed during development in order to appreciate what conifer does we have to understand some basic ideas on how testing works so what is testing testing is the process of evaluating and validating whether an application is functioning as design and meets the requirements testing also helps reveal any bugs in the application that can then be fixed accordingly testing can be done manually or with automated tools as applications grow in size and complexity manual testing of applications is not sustainable it doesn't scale well and it's a slow process of diagnosing a problem and creating a fix how do we know that this fix doesn't introduce new problems or break other parts of our software well we need to test again here's where automated testing helps instead of manually going through each test case by hand developers can write scripts that execute the same test steps automatically which can be reused over and over again there are many types of tests that developers can run but we'll only cover the three main types of testing first we have unit tests at the most granular level unit tests typically focus on a small individual part of an application which checks whether that code functions according to how it was designed unit tests are written to be independent from other units within a program unit tests typically take a short amount of time to run because of this in a typical test suite it's recommended to write more unit tests because it helps developers with validating code refactoring code and it's the easiest test to write and takes the least amount of time to execute however the disadvantage of a unit test is because it only tests a single unit of work a passing unit test doesn't always mean the code will work as expected when other dependencies are introduced next we have integration tests unlike the independence of unit tests integration tests verify that your code can work well when multiple parts of the program are integrated together integration tests typically live at the edge of your applications since they connect to an external resource like a file system database or race api or more because of this external connection they are slower than unit tests and they can be more difficult to write compared to unit tests a passing integration test gives a higher level of confidence that code works when integrated together the third type of test is end-to-end test it's called end-to-end test because the application is tested through the user interface to replicate real-world scenarios from start to finish real-life user scenarios can be hard to implement from a unit test perspective on the other hand user scenarios are fairly straightforward to reproduce when switching to an end-to-end respect perspective end-to-end tests are written and executed using established testing frameworks that were built for it such as cypress end-to-end tests aren't perfect for the advantage of simulating real-world scenarios the disadvantage is that end-to-end tests usually requires a lot more effort to write it's more difficult to maintain and also it takes a lot more time to execute traditionally end-to-end tests are used much less due to how costly they are to execute and maintain however investing in end-to-end tests is usually something that pays off very quickly modern testing frameworks like cypress were designed to make it easy for developers to write tests because of that more and more companies invest in using end-to-end tests for their modern apps with end-to-end tests we have an even higher level of confidence of catching bugs and regressions before they make their way to a production environment which is known to be costly in terms of company image developer time stress and overall development in productivity now let's take a look at a hypothetical use case drone on is an autonomous delivery platform on its way to success in silicon valley droneon has easily impressed investors and secured their next set of funding for expansion with its current small user base and prototypical nature the developers have been able to get away with more basic front end and less an ideal test coverage a wider range is going to come with greater demands for better ui but with that it will be more likely for bugs to occur and the app is going to need a far more comprehensive set of test suites drawnon's front-end application is built using react and utilizes cypress for their integration test and end-to-end tests they chose cypress because cyprus tests are designed to be written in javascript which is familiar to all genomes developers and due to how easy it is to learn how to use the tool on commit droneon's continuous integration tool automatically triggers all of the cypress test suites however developers also run this test locally on their own machines between commits as progress checks as well as to maintain the efficacy of the end-to-end test so that they can continue to get a high quality and faster feedback loop for their work these local test runs have historically taken minutes to run but with the newly refined ui and test suites they are now taking up to half an hour as a general rule we want our code to fail fast and fail often meaning that we want our code to be tested as early as possible in the development process we want to fail quickly so that we can begin the learning process as fast as possible and we want our code to be tested as often as needed so that bugs are fixed soon after they are introduced if testing the code takes a really long time it becomes less feasible to fail fast and fail often when a test takes a really long time to execute the developer's productivity is disturbed as they weigh eventually developers often have to decide between running less tests or contact switching to another task to make use of the waiting time this type of environment has a hidden cost in the form of developer time stress and overall productivity this problem is only getting worse with every refinement to the ui and test suite according to this research by national institute of standards and technology the cost of software box removal increases increases depending on when it's found note that the cost here is being expressed in developer person hour dollars as we can see in the chart if a bug is found at the requirement and architecture and coding stages it is still relatively cheap to fix them but as you go on to find them in later stages such as the integration testing and system testing stages the cost increases dramatically if a bug made it to the production environment they can cost up to 30 times to remove them this is the reason why failing fast and failing often is an effective testing strategy fail fast and fail often also means failing for a cheaper price if all developers in a company like drone on test less and less during the coding phase the more likely it is for bugs or errors to become more integrated into the code and to be found and fixed later knowing this to be a common problem and knowing the company's image is on the line at this critical growth phase drawnon is looking for ways to minimize the risk of the snowballing effect as much as they can now let's try to understand what are some options that drone on can look into initially testing used to be executed sequentially each subsequent test would run after the previous test was completed while initially sufficient this approach has become less optimal as applications continue to grow more complex and distributed and testing has become more resource intensive executing tests in parallel became one common solution to this problem parallel parallel testing allows for the execution of tests simultaneously in multiple environments in order to reduce task execution time as you can see the following test suite has five day five tests that take roughly five minutes to execute sequentially on one machine by parallelizing the test between five machines where each machine executes one test it can potentially cut down the testing time to a fifth now that we've seen the potentials of parallelizing tests let's take a look at what options exist for parallelization first drone on can look into developing local test parallelization testing in parallel on a local machine is a feature that developers often implement themselves to speed up their testing it is important enough that some testing frameworks offer test parallelization out of the box unfortunately cypress doesn't support localized paralyzed testing secondly even if this option was available for cypress this approach is limited by the developer's computer's performance it only benefits the developer if the computer is powerful enough to run paralyzed as locally with that droneon can purchase more powerful computers for the developers this approach is limiting since computers can only get so powerful before it becomes really expensive we also shouldn't forget that we need to be able to run parallelized tests locally in the first place to best benefit from these more powerful machines next there is also the options of using cloud-based servers to execute the tests in parallel developers can choose and manage servers that are powerful enough to execute the test parallelizing tests require two major components an infrastructure that is powerful enough to execute the test in parallel and the logic to split the test between the nodes in both local and cloud-based solution it requires infrastructure know-how to manage it and to integrate the test orchestrated together building a diy solution is not easy it would require develop developer time and effort to build time and effort which could be better off spent developing business logic of droneon's application next drone can also look into purchasing a subscription for cloud-based testing services cloud-based test services usually provides an easy plug-and-play parallelization for the developers it has the benefit of being highly scalable and being able to test on different types of browsers and on many different mobile devices however this usually comes at a high cost so drone on now starts to explore these potential solutions in more detail let's now discuss the potential solutions drawn on explore firstly let's take a look at existing cloud-based solutions this approach includes solutions such as lambda tests browser stack and source labs they are full-featured cloud-based testing software as a service that provides automated testing of desktop and mobile applications these enterprise solutions offer manual testing and automated testing on virtual machines and real devices they also offer compatibility with various testing frameworks and integration with popular ci cd tools and comprehensive test overview and dashboards this makes them a feature-rich service that would allow droneon to deploy their tests timely they make it easy to deploy and use their platform so droneon's team doesn't have to invest time into building and maintaining the system and focus on their business logic however by exporting your application to a sas like soft labs the trade-off is a lack of data ownership while the offerings are attractive a small company like droneon with only a handful of developers and in a critical growth phase where they are there are a lot of uncertainties it would be difficult to commit to an enterprise solution with a monthly subscription fee that is beyond what they can afford they would also like to keep their data on their own infrastructure and not on a third-party host on the other side drawn on steam could build their own solution to parallelize their tests across any number of instances and generate a report at the end of the test run all the while maintaining full control over their infrastructure and data drawn on would have full control over the feature set and be able to customize it to their specific needs but this would require a significant significant amount of time and resources to build from scratch when their time can be better spent on working on their core business product having built the infrastructure they would also need to spend the time and resources maintaining it in addition droneon could integrate their diy solution with cypress dashboard or currents.dev dashboard an alternative dashboard for cypress both are full-featured dashboards that support parallel testing designed to be integrated with their choice of ci provider the ci is typically only configured to run tests at specific times such as on commit or before a merge it is considered a more reactive approach kind of like a safety net rather than having their developers execute their tests whenever they are able to however if the engineering team wants to use these options to parallelize their tests for use during local development they will still need to build their own infrastructure and integrate it with the dashboard lastly we have conifer conifer was created for companies or developers who want a simple way to spin up their in their own infrastructure to run cyprus tests in parallel and fits into the needs of droneon's team it's easy to set up conifer provides a simple cli to build deploy and tear down aws infrastructure while providing a simple live dashboard to view while tests run in parallel a company like droneon will be able to maintain full control of the infrastructure and that is deployed to aws and can scale up or down as they seep in with conifer a company will have to pay for the resources that they deploy to aws but there are no upfront payments while this costs can increase depending on the usage it's possible to increase or decrease capacity based on their testing demands the low upfront cost and ability to control the cost based on usage makes conifer a cost effective alternative for a company like drone on however conifer is not nearly as feature reached as a sas solution or customized diy platform we designed conifers open source testing framework to fill this niche now that we've seen the potential solution i'm going to pass the presentation to sam to talk about how to use conifer thanks aina so in order to use conifer the user will need to have node npm aws cli and docker installed uh once confer is installed user runs the command conifer init this starts a series of prompts where the user is asked questions about their specific application such as how many parallel nodes do they want to run it can be four five six whatever suitable for the application and what type of ec2 instances to provision they're also asked other questions about their specific app then with these responses conifer will install the dependencies it requires to run next the user ensures that they have a docker file for their own app and runs conor for build to create the necessary image for connor ferderon the conifer build command then uploads this image to aws elastic container registry this command prepares the app for execution and conifers parallelized infrastructure provisioning the infrastructure is made easy through conifers conifer deploy command which automatically initializes the aws infrastructure needed to run tests with conifer with the image and infrastructure set up all the user needs to do is run tests using conifer run the user can monitor the progress of their tests with the dashboard and is launched automatically lastly if the user is finished using conifer and wants to remove all the cloud infrastructure that was previously built using conifer deploy command they can type in conifer teardown this removes all aws resources except for the image in ecr and the database unless otherwise specified so far we've introduced conifer discussed how to use it and who might be interested in using it at this point you're probably wondering how much conifer can speed up n10 testing in this section we're going to examine some of the results one can expect when using converfer to parallelize their cypress end-to-end tests let's start by comparing the total test run execution time for test suites of different lengths with our local machine versus with conifer now keep in mind there could be variation in local test restorations depending on your local machine specs for reference the device we used here was a 2021 macbook pro with an m1 max chip and 32 gigs of ram on the first row we have a small test suite that takes around seven minutes to execute when runs sequentially on the developer's local machine running this test suite on the paralyzed infrastructure provided by conifer results in an execution time of around 5 minutes then on subsequent runs the execution decreases further deformance in 35 seconds next we have medium-sized test suite that takes 21 minutes and 35 seconds to run locally the same test suite takes a little over nine minutes to run on conifer initially and seven and a half minutes for subsequent runs finally the last row features a somewhat larger test suite this test suite takes around 36 to 37 minutes on the developer's local machine but only around 12 minutes when run with conifer for subsequent runs on conifer shades off another two minutes off the total runtime resulting in a total execution time of just under 10 minutes these results while impressive also reveal two key trends first of all the degree to which a test run is sped up depends on the length of the test suite more tests means more significant speed increase both in absolute terms as displayed right now and in relative terms as shown by the run speed multipliers now appearing in the final column of the table the greater speed improvement for longer tests can be explained by the relatively fixed startup time for conor for run and delay that doesn't apply to running tests locally this delay takes up a greater proportion of the runtime in the case of shorter tests but since it is fixed as the test length increases it takes up a diminishing proportion of the test renderation as you can see in the above table as the test suite length progresses from small to large the initial run speed multiplier progresses from 1.39 x to 3.09 x and the subsequent run speed multiplier progresses from 1.55 x to 3.88 x fortunately for our users this means that the most agonizing test suites are the ones that get the biggest boost in speed the second trend is that subsequent runs tend to be faster than initial runs this can be explained by conifer's test splitting algorithm that utilizes metadata from previous test runs to optimize future runs more on that in a moment so you just witnessed a glimpse of how conifer can decrease the time it takes to run as a test suite dramatically now let's explore how conifer achieves these results beginning by exploring how conifers test splitting algorithm works conifer allocates test files to parallel nodes using a two-stage algorithm the first stage is utilized in the initial test run during the initial test run confer naively distributes test files to the various nodes based on the total file count such that each container contains roughly the same number of tests for example let's say that we have a test suite that contains consists of eight separate test files this test suite is to be paralyzed over four nodes the algorithm will go through each test one by one and add it to the node that contains the smallest number of test files this process will continue until all the test files have been allocated though the stage one algorithm splits the files evenly amongst the parallel nodes it does not necessarily represent the most efficient splitting of the test suite this is because it can result in different nodes having longer total run times than other nodes due to the possibility of certain test files taking longer to run than others for instance you can see in the image on the right that even though each node has the same number of test files node one takes much longer than node two which is the problem because the test run is only as fast as the slowest node this brings us to stage two of confirst test allocation algorithm where the test files are allocated based on timing data after the initial run conifer persists metadata about each test file including the amount of time each test takes to run on subsequent test runs the conifer can then use this test data to allocate the test files such that the difference in total test time between each parallel node is minimized using the same test suite as the previous example beginning with the longest running test the algorithm will go through each test file one by one and add it to the node that contains the smallest estimated total runtime rather than the one with the fewest test files this process will continue until all the test files have been allocated we can see this process play out in the animation on the left as we can see from the graph on the right this will result in nodes that take a similar amount of time to finish execution relative to the naive allocation of the previous slide to recap on the initial test run test files will be allocated such that there is an even division of test files among the parallel nodes on subsequent test runs the timing data from previous test run will be utilized to allocate tests among nodes based on total test time it is noteworthy that the naive algorithm will be responsible for the majority of the speed increase this illustrates the power of parallelization making it such that the user can enjoy substantially reduced test suite runtime from the very first run and now i'll pass it off to ahmad we'll talk about how conifer is implemented thanks sam so so far we've missed the extent to which conifer is able to speed up end-to-end testing we've also examined some of the algorithms that allow conifer to divide up the test suite in a manner that facilitates achieving these speed increases now we're going to go behind the scenes to take a deeper look on how conifer is actually implemented before we talk about the specifics of conifer's implementation it would be helpful to define the various responsibilities that have to be fulfilled in order for conifer to function at a high level these responsibilities can be divided into five different categories the first setup refers to the preparation of all the tools that are going to be used to tackle the following responsibilities we also need to be able to orchestrate and direct the testing process ensuring that the correct functions are performed at the appropriate time throughout the process we also need the ability to execute our test suite in a paralyzed manner we need to be able to store the results that are generated from each test in some form of persistent storage and finally we need to communicate the results of the test run to the user in a useful manner we're going to begin with setup what actions need to occur to fulfill this responsibility before we can run any tests we need a blueprint of sorts that specifies all of the files and dependencies that are going to be required to run the user's application and its associated end-to-end testing suite in other words we need a blueprint for a single node in our infrastructure this blueprint is later going to be used to create the nodes that make up conifers parallelized testing infrastructure how can we create such a blueprint for conifer this blueprint is going to be in the form of a docker image docker images are files that function as a set of instructions that can be used to build a docker container what is a docker container it's an executable package software that is going to include everything that is needed to run an application running our nodes as docker containers is going to make it such that we can run our app and its associated tests on any computer that has the physical requirements to run our software without having to worry about configuring the correct environment this is going to dramatically simplify the process of deploying the user's application on general purpose cloud computing infrastructure so now that we have our blueprint let's discuss the provisioning of the infrastructure at its core conifer relies on the power of cloud infrastructure to fulfill its use case like any tool that relies on relies on cloud infrastructure we must first provision this infrastructure prior to using it provisioning the necessary infrastructure is accomplished through the use of aws's cloud development kit or cdk and the graphic on the right is illustrates an example of this process executing the cdk code is going to dynamically synthesize a cloud formation template that is going to detail the specifications of the specific infrastructure components that need to be provisioned the information provided in the initialization process is going will be used to tailor these specifications to a specific end user's needs aws is then going to use this finished cloudformation template to complete the provisioning process so here's a look at the architecture so far that handles the building and building the image and provisioning the infrastructure the next responsibility we want to tackle is the test orchestration process within conifer's architecture the command line interface or cli is responsible for handling the test orchestration process the cli fulfills its responsibility by supporting the following functionalities it's able to initiate the testing process it tracks the status of a single test run as it proceeds and it's able to trigger the recalculation of the test groupings the way the test files are split amongst nodes at the appropriate time so what process need to occur to successfully initiate a test run at a high level we need to be able to one start up the nodes that run our test suite we need to be able to find a way to specify what portion of the test suite what specific tests each node is going to be running we need to be able to link a specific node to a specific test run or specific instance of using the conifer run command and finally we need to be able to reference or access each node for tracking purposes so how does a cli fulfill these stated requirements the cli initiates a test run by using aws's software development kit the software development kit is going to be used to trigger tasks on aws's elastic container service which is going to be the service that actually runs the containers that are going to execute our tests each of these tasks spins up a container using the image we had pushed the ecr earlier and when we actually initiate these tasks we're going to specify certain container overrides in the form of environment variables and these environment variables are going to allow us to dictate the specific test files that are going to be run on a specific node as well as allow us to attach a node to a specific test run via putting in a test run id also upon initiation or the completion of initiation the specific resource identification numbers for the tasks that were initiated are going to be persisted in the configuration file and these later on are going to be used to track the status of a specific task so at this point we've successfully built our image provisioned our infrastructure and initiated the testing process and now our tests are executing on the various parallel nodes while this process plays out there are certain things that we're going to need to be able to do on a regular interval such as check for new test results as they appear in a persistent store and continually monitor the status of the test run to be able to know when it is concluded this process is also going to be accomplished using the aws software development kit at this point the test run is already executing and the sdk is going to be used to check up on the status of the specific tasks using the task resource identification numbers that were persisted in the configuration fire file and the earlier one additionally while the tasks are executing the user dashboard is going to be launched and this is going to allow the user to track the progress of the test run in real time so our tests are initiated and we are able to track their status at this point the test run has concluded and we need to be able to recalculate the test groupings what steps have to occur to enable this process to play out first we need to actually retrieve the timing data from the persistent store in this case metadata from dynamodb which we'll go more into in a later slide once we have this timing data we need to run the stage two of the algorithm that was discussed earlier and this algorithm is going to generate the new test groupings and then finally we need to save these new groupings for subsequent for the subsequent test runs inside the configuration file where they will be read and used on the next test run so here's a look at the architecture so far with the components that handle the test orchestration process now let's take a look at the specific tasks that need to occur to actually execute the test suite meaning the actual code that represents the tests zooming in on a single node three actions need to occur in order to successfully execute our test suite we need to be able to initiate any necessary background processes we need to initiate or start up the application that is going to be tested and once this application has started up and is ready to go we need to start cyprus and execute specific tests so zooming in on a single node let's examine the flow of the testing process upon spinning up the container the node level testing process is initiated by a shell script that lives within the container the first thing the shell script does is initiate a file watcher program which we will discuss in detail following this the user's application itself is going to be launched and since it's critical that the user's application be active when cyprus is run further execution is going to be stalled until the user's app is fully started and finally once the user's app is up and running testing is going to be initiated by launching the cypress framework and here we will use the task level container environment variables that we specified earlier to tell cyprus which part of the test suite we want to run on the specific node so for the completion of these three steps a subset of the total test suite is now running on a single node this is a look at our infrastructure thus far which includes the components that are required to run the code for our tests within these various tasks that represent each single node the next responsibility that has to be handled is persisting the test results that are generated when each test file is run after execution we need to store the actual results of the tests in some form of persistent storage this process can be divided into two main steps first we need to be able to detect when test results are available when they are actually generated after a test has complete next we'll need to be able to save these results into some form of persistent storage that is outside the nodes of the testing infrastructure so it can be accessed later that takes us to our file watcher before we discuss the file launcher let's briefly recap the concept of testing artifacts so each cypress test is going to generate test artifacts upon completion and these artifacts such as json metadata videos of the test itself as well as screenshots at specific points of the test they provide an accounting of everything that occurred during the execution of a single test as well as information about any failure and these artifacts are invaluable for understanding how a test was executed as well as debugging any potential issues or failures the file watcher program lives within each parallel node and it runs in the background on each container it's going to be responsible for detecting when a testing artifact has been generated which usually occurs upon test completion and it's going to do this for watching or it's going to do this by watching for changes in the directories where the cypress artifacts are saved and these are a set of standardized directories such as the results videos and screenshots directory whenever the file watcher detects that a newly created file has finished saving it's going to upload that testing artifact to the appropriate directory inside a conifers s3 bucket and additionally the file watcher program is going to parse certain artifacts and it's going to pull from those artifacts select metadata that it's going to say to our object database or document database in dynamodb so here's a look at our infrastructure now with the dynamodb s3 bucket that into which the test results are going to be persisted and let's briefly recap the process so far we began by performing the setup process and provisioning all the tools that we are going to need we initiated a test run oversaw the execution of each test file in the test suite and extracted all of the resulting test artifacts into persistent storage at this point let's discuss how we might communicate test results back to the end user the process of returning the results to the user can be broken down into three main steps first we need to be able to retrieve the test results from where they're stored once we have them in one location we need to be able to apply some form of processing to transform the data into a format that is easy for the user to understand finally we want to display this data to the user via some graphical user interface conifer handled this responsibility um in two ways with two separate infrastructures the first of which was a live dashboard which uh communicated those test results to the user in real time as they occurred but in a less in a more limited form the other approach um html report generated upon completion of the test run provided a detailed accounting of every thing that occurred in the test room beginning with a live dashboard so the live dashboard utilizes data that's persisted by the file watcher and uploaded into the dynamo database to keep the user up to date on the status of the test run it consists of an express backend and a react front-end if we recall as the test is executing the cli holds dynamodb for updates on the status of the tests and when it finds that there are updates it will send these to the dashboard via a webhook the live dashboard is then going to display these updates to the user as they come in and by doing so it enables real-time monitoring of the progression of the test run allowing the user to monitor stuff such as the status of a single test and the duration of each test whether the test has started failed and so on the user dashboard is also going to provide links to download individual test artifacts from the s3 bucket as they become available and these are things such as screenshots of any failures or videos of the entire test as we can see in the animation the other way we communicated test results to the user was via generating an html report at the conclusion of test run in order to do this we had to first complete two prior steps and that is retrieving the data from the s3 bucket aggregating it into a single file and then saving that file to the user's project directory once created and saved the user can open the html report and they can see a complete accounting of everything that occurred in the test status of each individual assertion what passed what failed and any other data that they would have had access to had they run the entire test suite locally so this is a final bird's eye view of our infrastructure and it shows all the components we just illustrated that are going to be used to fulfill the responsibilities that need to be done for conifer to successfully run so now i'm going to pass it on to lawrence who's going to discuss some of the important design decisions we have to make as well as challenges we face during implementation thanks ahmad so we've talked about the responsibilities the architecture and how we built conifer now we want to take a closer look at two of the main challenges we faced and why we chose the approaches we did we thought these challenges warranted further discussion specifically how do we execute the test in parallel and how do we retrieve these results from the nodes we want to focus specifically on how to perform parallelized testing on the cloud after the consolidated docker image has been built and sent to ecr so now the user's application is on the cloud how do we run the tests in parallel in a parallelized manner our first thought was to use lambda functions lambda is an event-driven compute service that lets you run code without needing to provision or manage any resources we had envisioned using lambda functions as a way to parallelize the test executions where each lambda function can execute each test file in the test suite asynchronously thus the concept would be to invoke n number of lambda functions where each function runs one test file in the complete test suite landa has many characteristics that make it uniquely suitable for our use case firstly it possesses the capacity for infinite parallelization so it is highly scalable secondly it represents a fully managed solution so there is no need for us to manage the deployment of aws resources lastly is a proven solution this approach has been used to successfully paralyze end-to-end testing on selenium which is another popular testing framework aside from these upsides there are a few drawbacks first of all the lambda container size limit is an issue this used to be a bigger problem when the size limit was 512 megabytes but recently it was increased to 10 gigabytes so it is more suitable for applications in any case we thought it was it was sufficient to support applications under that 10 gigabyte limit second of all the lambda function timeout is only 15 minutes in theory it is an issue but because of infinite parallelization it is highly unlikely that a single cypress test fall will take more than 15 minutes and last of all is the cold start time cost stars vary based on what the lambda function is running it could be an issue but we would have to validate it with real world data you may have already noticed that conifers architecture that we we didn't use the laminar functions to parallelize the tests this is because we encountered an issue relating to how a low-level display driver dependency that cyprus needs to run and it had conflicts with running on lambda this issue appeared to be unsolvable as there still is an open issue on github directly related to using landmark cypress which dates back to 2018. we explored a handful of workarounds that attempted to bypass this problem these were typically very complex and always implemented were always implemented to support the testing of a specific application none of the workarounds were intended to be used as part of a general purpose testing infrastructure therefore they were not suitable for use with conifer since using lambda was a dead end we needed a way to run our tests elastic container service or ecs has a few characteristics that make it desirable for our use case being a container orchestration service for docker images it is able to run our nodes additionally it is easily it can easily be scaled in a manner that will allow us to sufficiently paralyze execution of our test suite thus it is able to fulfill the same responsibilities that we had originally envisioned using using lambdas unlike lambdas which are serverless ecs offers two launch types that we can use to run our nodes the first option is the self-managed solution using an ec2 instance as the task runner ecs also offers a serverless fully managed solution called fargate we took the bottom-up approach by trying to get cyprus to work on ec2 instance we conducted initial development with ec2 as a task runner with an immediate goal of ensuring that nodes are able to run and that the same problems with noun that did not apply using ec2 as the task runner made it possible to take advantage of the relative ease of debugging and troubleshooting in a self-managed solution for the initial application development ultimately we decided to stick with using ec2s as our task runner in addition to simplifying the development process ec2 task owners achieved the substantial speed gains that confer aim to provide for e2e testing implementation with a fully managed solution via target was deferred as a future optimization to summarize our first major implementation challenge we needed a way to parallelize the tests on the cloud we first explored using lambdas to parallelize the tests and solved advantages and disadvantages then we discovered there were underlying issues with running cypress on lambda we then explored using ecs and its two launch types and we investigate its pros and cons before ultimately deciding to go with the ec2 route so now that the tests are being run in a distributed environment on the cloud the vc2 let's take a look at the test results of these tests runs but how do we do that normally the user could view the results of their tests in real time through their terminal because convert is executing tests on the cloud they lose this feature what's the point of testing we can't see the results is there another way to view the results besides the terminal output we can create a test report for the user to see these results through an html report after the test run is complete although this is not to this will not let them see the tests in real time as they are executed this can be achieved through cyprus with its built-in reporters unlike viewing the test the test results via the terminal's output these reports can be retrieved and sent to the user when the user gets the report there is a minor problem the reports are generated per each test file which means the user has to go through hundreds of reports to see their test results we managed to solve this problem easily by using a custom reporter plugin called mocha awesome which creates the reports and has a feature to aggregate them before we can aggregate the visual test results we need them to be in one location so let's take a look at the current situation this graphic illustrates the situation after our test suite has finished execution as we can see the test results were produced for each test but they reside in node where the specified the word the specific as file was executed therefore we don't have access to them and we cannot use them to generate a report for the end user in order to send any results to the user we need to first solve the problem of retrieving the test results from individual nodes placing them into one centralized location the easiest approach would be to defer uploading the test artifacts until after all of the tests in on a single node have finished running the conclusion of test execution on a node implies that the cyprus test runner has finished running and thus that all the artifacts have been generated running a script after this to upload the test artifacts would be a trivial process but this is not good enough we want conifer to be able to communicate these test results to the end user in real time functionality that is present when running the cypress test suite locally accomplishing this requires us to retrieve the test results from each individual node in real time as those test results are generated so how can we achieve this let's take a look at two main approaches at the high level we can approach this problem in two ways we can we can attempt to insert the desired functionality directly into the test execution process so the tests are uploaded immediately after the test file runs but before the next test begins or we can create a new process which has the sole responsibility of uploading test results as they are created for the synchronous approach can we configure cypress to upload the results in between each test execution for the asynchronous approach is there a way to detect if test results have been generated if so can we upload that result the first approach would be to direct cyprus to synchronously upload testing artifacts for a single test file immediately after that file finishes running within this the cypress config file we have the ability to specify code blocks to be executed at certain points in testing process including subsequent to the completion of a individual test file's execution this approach is complicated because of the potential for existing code in the end user's cypress config file some of which may be critical for supporting the proper execution of their test suite thus the necessary code would need to be stitched into a pre-existing conflicting file in one of two approaches first by directing the user to add in the necessary necessary code themselves or by injecting the necessary code ourselves however this approach is a no-go the alternative of parsing the user's config file and injecting the necessary code is risky given the complexity of the cypress config file and its importance to successfully execute the end user's test suite the second approach would enable the asynchronous streaming of the testing artifacts through the implementation of a file watcher program this program would be separate from the cypress test runner and would run in the background asynchronously while the cyber tests are being executed to function correctly the cybers the file washer program would need to be able to detect when a test artifact has been both created and fully saved once these conditions are satisfied the file watcher would initiate the process of uploading the specific artifact to proceed to persistent storage this approach would require no additional work from the end user but it is more complex and constitutes an additional point of failure in the testing infrastructure we decided that the implementation of the real-time streaming of testing artifacts would be best achieved through the use of the asynchronous file washer program both of these approaches achieve the necessary functionality the first approach would be undesirable because it burdens the end user with stitching together the configuration files this is especially the case because they they may not even be able they may not even be familiar with the configuration themselves this makes the synchronous approach a non-starter although the file washer program was more of a technical challenge it supports the necessary functionality without requiring additional work from the end user now let's talk about future work for conifer in the future our team would like to investigate other test allocation algorithms that may be useful to the engineers with certain use cases one such algorithm is due to dynamic allocation of tests this the information on this slide illustrates the approach rather than calculate test groupings prior to initiating a test run this approach would dynamically allocate tests by utilizing a queue of sorts to feed tests to the nodes as they become available this approach may prove useful in situations where accurate and or up-to-date timing data is not available such as during the first test run and free and infrequently or rapidly changing test suites additionally we want to investigate fargate as a task runner there is potential for a landlocked parallelization if the kinks can be worked out we would be able to realize the benefits of a fully managed surveillance solution the remaining points are some features that could potentially increase the efficiency of testing for developers when testing locally we want to implement a fail fast feature where we have the option to stop test execution as soon as the first failing test is found we also want to add flaky test detection where the developer can test can detect flag and track so you to test lastly we also want to add analytics on the live dashboard to give users a richer experience under test results this is the end of our presentation thank you all for coming if you have any questions feel free to ask yeah so i have a question from the audience it says the recalculation of test groupings so it's clever can you talk a little bit more about how the idea for this algorithm came about does anyone want to take this one i'll go ahead and take that thanks anna yeah appreciate that audrey um so there are a few ideas uh around that algorithm and um one concept behind it was that we needed some sort of pointer uh towards whichever node had the minimum uh amount of total test time so far just so we know where to most effectively add additional test time to uh in order to optimize or most effectively equalize the distribution of test time between the nodes and then another aspect of it was that we want the decision to order the test files in descending order by test time and the reason for that was because as we added uh these test files to these nodes we wanted to make finer and finer adjustments in a test time edition teach node with the idea that we would converge to a more equal solution between the nodes because if we were doing it in another order we would uh not be converging to a more equal solution it would be easier for us to have uh a more unequal solution at the end there i'm not sure if that answers your question completely or if that was the spirit of the question um uh let us know if there's more we can answer there uh we have a another question what was the most challenging and the most exciting part of working on conifer um yeah i can say something for that so i also start with the most exciting part um so connor for we can really the nice thing about conifer is when it when we got it to work we can we would know right away because we would see that a test suite that you know for example took 40 minutes all of a sudden runs in let's say 10 minutes so when we finally hit that point and um we went ahead and ran this test we deployed it and we got back this result that made it such that it was run four times faster um that was that represented a very exciting part and like validation of our concept uh and all the hard work that was put in um the most challenging bit something that was particularly challenging and i'm sure this is a common thing is figuring out how to appropriately um set the permissions um security groups and roles between all the various portions of the infrastructure um at times it felt like uh you know an endless search you know we fixed one thing uh the next permission you know doesn't work anymore every time we wanted to add something new we had to go in there and you know completely rewire the connections and make sure everything had the correct permissions in order to do what was a otherwise minor you know change to our application code as time went on um finally mastered the general principles behind you know the permissions and this became much easier but in the beginning um it was a very significant time drain and um you know was probably especially at the very start the most challenging thing that we faced thanks so much um we have more questions very neat how connie first speeds up subsequent test runs was that feature planned from the beginning or added on once you had a via version one working what was it like working on it i can answer that i think it was definitely planned from the beginning um we noticed uh like a few other offerings like the sas ones that we talked about like sauce labs they offered something like this and we thought it was a very doable approach it was something that we could do and we saw like a few other approaches in designing the algorithms and so it was definitely one of more fun portions to work on and compared to working on like what i might say like working on the aws cdk stuff thanks laurence uh more questions were there any interesting discoveries as you were initially investigating overcoming the driver issue with lambdas i have an answer for this one um right i know where you're gonna go no wait take it away okay um so i mean interesting discoveries yeah there's definitely a lot of interesting things that we came across while uh trying to figure out how to get past the issue with the lambdas i'm just gonna mention two real fast um one was the so we found a solution that was um more or less like completely mapped out but the nature of this solution was it was so low level there were so many steps involved and like not really any explanation of what was happening um more or less anyone that was trying to attempt it and was like responding to the author the solution couldn't figure it out and these were um you know experienced software engineers with many used experiences in some cases um so taking a look at that you know there was definitely a very brief moment where we were trying to implement it but we had to take a step back because all of these people that were struggling to implement this solution were trying to do it just so they could test their specific application and for us to try to implement it in a way that would work with any person's application um we just we deemed that to be a you know very bad idea and it was like a wake-up call that it's time to move on um another interesting this is more of a funny thing we saw uh and on a similar vein as uh what was what i was just mentioned is a funny github issue title or open issue where someone was like this specific issue essentially is ruining my life and job and like it was very many page long grand of everything they've tried um so yeah seeing that also was a pretty good indication that you know maybe this wasn't the way to do it we have more questions can you can you talk a little bit more about how the file watcher was implemented i can take it also so that's one else yeah um so the file watcher um the i guess at its core the file watcher it wasn't um it's very basic functionality it wasn't uh too complicated of a thing to implement um the challenge came was um for figuring out how to the correct time to upload um an artifact that was generated so specifically this was a problem for videos um the way that cyprus was saving the videos it would create the file um like at the start of the recording essentially um but at that point you know that doesn't contain um what is going to be the complete video and at the end it would conduct the compression process on this video so something that was significant a significant challenge with the file watcher was figuring out how to determine the appropriate time at which this video would be ready to upload a lot of the times in the initial implementation it would be uploaded and you know we'd go ahead and try to play it from our dashboard and you know it was a corrupted video file indicating that it had saved um at a different time or at the wrong time so yeah eventually we figured out that um we would determine that the video has finished um saving by tracking the like files that were being generated by cypress uh almost um adjacent to the video being finished like different metadata that was going to be put in as part of the final result and once all those were erased it was an indication to us that cyprus had finished creating the video and that's the point at which we can upload it okay i think we're out of question so i think that concludes our presentation 