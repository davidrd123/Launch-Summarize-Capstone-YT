Thank you all for joining us today. We are excited to present our coding project called Conifer, which we have been working on for the past few months. I am Aina, and my teammates are Sam, Ahmad, and Lawrence. In this presentation, I will provide an introduction to Conifer, discuss its use case and potential solutions. Sam will talk about benchmarking Conifer and its algorithm, Ahmad will explain how Conifer works behind the scenes, and Lawrence will cover the implementation challenges we faced and future work for Conifer.

So, what exactly is Conifer? It is an open source framework designed to simplify the deployment of an infrastructure that runs Cypress tests in parallel. The main goal of Conifer is to reduce the total time it takes to execute a full test suite for local development. We believe that running a full test suite frequently during development is crucial for developers to fully appreciate the benefits of Conifer. To understand how Conifer achieves this, it is important to have a basic understanding of testing.

Testing is the process of evaluating and validating whether an application functions as designed and meets the requirements. Manual testing is not scalable or efficient as applications become larger and more complex. To overcome this, automated testing using scripts that execute test steps automatically has become more prevalent. The three main types of testing are unit tests, integration tests, and end-to-end tests.

Unit tests focus on testing small, independent parts of an application to ensure they function as intended. They are quick to execute and provide immediate feedback, making them ideal for code validation and refactoring. However, they may not catch issues that arise when multiple units are integrated.

Integration tests verify the functionality of multiple integrated parts of an application, such as connecting to an external resource. They take longer to execute than unit tests and can be more challenging to write, but they provide higher confidence when multiple components are combined.

End-to-end tests simulate real-world user scenarios by testing the application through the user interface. While they offer the advantage of simulating real-world scenarios, they are more difficult to write and maintain, and they take longer to execute.

Let's consider a hypothetical use case to better understand the need for Conifer. Imagine Droneon, an autonomous delivery platform that is rapidly expanding. Initially, they had a small user base and basic front-end with limited test coverage. However, as the application becomes more complex and demands for better UI increase, more bugs are likely to surface. Droneon's current testing approach involves running Cypress tests locally on developers' machines along with automated testing using a continuous integration tool. However, as the UI and test suite have grown, the local test runs are taking increasingly longer, affecting developer productivity.

To overcome this challenge, parallel testing can be an effective solution. Parallel testing allows tests to be executed simultaneously in multiple environments, reducing execution time. Droneon could explore different options for parallelization. One option is to develop their own local test parallelization solution. While some testing frameworks offer this feature out of the box, Cypress does not support it. Alternatively, Droneon could invest in more powerful computers for their developers, but this approach has limitations and can be expensive.

Another option is to leverage cloud-based servers for parallel test execution. This would require infrastructure knowledge and integration with the test orchestration. While it offers scalability and compatibility with various browsers and devices, cloud-based solutions can come with a high cost. Additionally, Droneon would need to decide whether they are comfortable with hosting their data on a third-party cloud platform.

Droneon also considered subscription services that provide cloud-based testing solutions. These services offer parallelization, integration with CI tools, and comprehensive test overviews, making them a convenient plug-and-play option. However, considering their small team and limited budget, committing to an enterprise-level subscription may not be feasible or desirable.

Alternatively, Droneon could build their own solution, parallelizing their tests across multiple instances and generating reports. This would give them full control of their infrastructure and customization options, but it would require significant time and resources to develop and maintain. They could also integrate their solution with existing Cypress dashboards, which offer parallel testing capabilities and integration with CI providers.

Finally, Droneon came across Conifer, which provides a simple way to spin up their own infrastructure on AWS and run Cypress tests in parallel. Conifer offers a user-friendly CLI for building, deploying, and tearing down the AWS infrastructure. It also includes a live dashboard to monitor test execution. The company can maintain full control over the infrastructure and scale it as needed. While there are costs associated with deploying resources on AWS, Conifer offers a cost-effective alternative with no upfront payments and the ability to adjust capacity based on testing demands.

Overall, Conifer provides the necessary parallelization capabilities for Droneon's testing needs while offering cost efficiency and control over their infrastructure. It may not have all the feature-rich options of a SaaS solution, but it aligns well with Droneon's requirements.

In conclusion, Conifer offers a valuable solution for developers and companies looking to optimize test execution time by parallelizing Cypress tests. It addresses the challenges of testing larger and more complex applications, allowing for faster feedback and increased developer productivity. Whether opting for cloud-based solutions, building a DIY solution, or leveraging Conifer, the key is to find a solution that aligns with the specific needs, budget, and goals of the organization. By investing in efficient testing strategies like failing fast and failing often, companies can minimize the risk of bugs and ensure a smoother development process. Their business logic, however, lacks data ownership when exporting the application to a SAS-like soft labs. While the offerings are attractive, a small company like DroneOn, with only a handful of developers and in a critical growth phase, finds it difficult to commit to an enterprise solution with a monthly subscription fee beyond their means. They also prefer to keep their data on their own infrastructure rather than on a third-party host. On the other hand, DroneOn can build their own solution to parallelize their tests across any number of instances and generate a report at the end of the test run, while maintaining control over their infrastructure and data. Building such a solution from scratch would require significant time and resources, which could be better spent on their core business product. Additionally, maintaining the infrastructure would require ongoing investment. 

DroneOn has the option to integrate their DIY solution with Cypress Dashboard or currents.dev Dashboard, both of which are full-featured dashboards that support parallel testing. These dashboards can be integrated with their choice of CI provider, providing a reactive approach by running tests at specific times, such as on commit or before a merge. However, if the engineering team wants to use these options to parallelize tests during local development, they would still need to build their own infrastructure and integrate it with the dashboard. 

Lastly, there is Conifer, which was created for companies or developers who want a simple way to spin up their own infrastructure to run Cypress tests in parallel. Conifer allows DroneOn to maintain full control over the deployed infrastructure on AWS, with the ability to scale up or down based on testing demands. While DroneOn would have to pay for the resources deployed to AWS, there are no upfront payments, and they can control the cost based on usage. Conifer is a cost-effective alternative for a company like DroneOn, although it is not as feature-rich as a SAS solution or a customized DIY platform. The open-source testing framework of Conifer fills this niche. 

To use Conifer, the user needs to have Node, NPM, AWS CLI, and Docker installed. After installing Conifer, the user runs the command 'conifer init', which prompts them with questions about their specific application, including the number of parallel nodes and the type of EC2 instances to provision. Conifer installs the necessary dependencies based on the user's responses. The user then ensures they have a Docker file for their own app and runs 'conifer build' to create the necessary image for Conifer. This image is uploaded to AWS Elastic Container Registry, preparing the app for execution and Conifer's parallelized infrastructure provisioning. The infrastructure is easily provisioned using Conifer's 'conifer deploy' command, which automatically initializes the required AWS infrastructure to run tests with Conifer. To run tests, the user simply needs to use the 'conifer run' command and can monitor the progress with the automatically launched dashboard. If the user is finished using Conifer and wants to remove the cloud infrastructure built using 'conifer deploy', the 'conifer teardown' command removes all AWS resources except for the image in ECR and the database unless specified otherwise. 

Now that we have seen the potential solution, I will pass the presentation to Sam to discuss how to use Conifer. In order to use Conifer, the user needs to have Node, NPM, AWS CLI, and Docker installed. After installing Conifer, the user runs the command 'conifer init', which prompts them with questions about their specific application, such as the number of parallel nodes and the type of EC2 instances to provision. Conifer then installs the dependencies required to run the application. The user needs to have a Docker file for their app and runs 'conifer build', which creates the necessary image for Conifer and uploads it to AWS Elastic Container Registry. With the image and infrastructure set up, the user can run tests using 'conifer run' and monitor the progress with the automatically launched dashboard. Finally, if the user is finished with Conifer, they can use 'conifer teardown' to remove all AWS resources except for the image in ECR and the database, if specified. 

So far, we have introduced Conifer, discussed how to use it, and explored who might be interested in using it. Now, let's examine the effect Conifer has on reducing the time it takes to run Cypress end-to-end tests. We will compare the total test run execution time for test suites of different lengths when running locally versus using Conifer. For reference, the device used for local testing is a 2021 MacBook Pro with an M1 Max chip and 32GB of RAM. 

In the case of a small test suite that takes around 7 minutes to execute sequentially on the local machine, running it on Conifer's parallel infrastructure reduces the execution time to around 5 minutes initially and further decreases it to 35 seconds on subsequent runs. 

For a medium-sized test suite that takes 21 minutes and 35 seconds to run locally, running it on Conifer initially reduces the execution time to a little over 9 minutes and further decreases it to 7.5 minutes on subsequent runs. 

Finally, for a larger test suite that takes around 36 to 37 minutes on the local machine, using Conifer reduces the execution time to around 12 minutes initially and further decreases it to just under 10 minutes on subsequent runs. 

These results demonstrate that the speed increase achieved by using Conifer depends on the length of the test suite, with longer tests experiencing a more significant speed improvement. The initial run speed multiplier progresses from 1.39x to 3.09x as the test suite length increases, and the subsequent run speed multiplier progresses from 1.55x to 3.88x. Subsequent runs tend to be faster than initial runs due to Conifer's test splitting algorithm, which optimizes future runs based on metadata from previous runs. 

Now, let's take a closer look at how Conifer is implemented. We will start with the setup responsibility, which involves preparing the necessary tools for conifer to function. This includes creating a blueprint for a single node in the form of a Docker image. Docker images serve as instructions for building Docker containers, which contain everything needed to run an application. Running nodes as Docker containers simplifies deployment on cloud infrastructure. 

The provisioning responsibility is fulfilled using AWS's Cloud Development Kit (CDK), which dynamically synthesizes a CloudFormation template based on user specifications. This template is used to provision the required infrastructure components on AWS. 

For test orchestration, Conifer's command-line interface (CLI) is responsible for initiating and tracking the testing process. It allows users to start the testing process, track the status of a test run, and trigger the recalculation of test groupings. 

The execution of tests in a parallel manner is enabled by Conifer's infrastructure, which provisions parallel nodes to execute test files. Test results are stored in persistent storage, allowing for easy access and analysis. 

Finally, Conifer communicates the test results to the user through a dashboard, providing real-time visibility into the progress of test runs. 

By examining the implementation details, we gain an understanding of how Conifer achieves its speed improvements and enables efficient parallel testing. The combination of setup, provisioning, test orchestration, test execution, and result communication culminate in a seamless and effective testing process. To run our test suite in parallel, we need a way to store the results generated from each test and communicate the test run results to the user effectively. To achieve this, we first need a blueprint that specifies all the files and dependencies required to run the user's application and its associated end-to-end testing suite. This blueprint will be in the form of a Docker image, which serves as a set of instructions to build a Docker container. The container will include everything needed to run the application, allowing it to be executed on any computer without worrying about environment configuration.

Once we have the blueprint, we need to provision the infrastructure using AWS's Cloud Development Kit (CDK). The CDK dynamically synthesizes a CloudFormation template, which specifies the infrastructure components to be provisioned. This template is tailored to each user's needs based on the information provided during initialization. AWS then uses the CloudFormation template to provision the required infrastructure.

Next, we need to handle the test orchestration process. This is accomplished using the Command Line Interface (CLI), which supports functionalities such as initiating the testing process, tracking the status of a test run, and triggering the recalculation of test groupings. The CLI uses AWS's SDK to trigger tasks on AWS's Elastic Container Service (ECS), which runs the containers to execute the tests. Each task spins up a container using the Docker image pushed to the Elastic Container Registry (ECR). Environment variables are used to specify the test files to be run on each node and link them to a specific test run. The CLI also persists the resource identification numbers of the initiated tasks for later tracking.

With the infrastructure provisioned and the tests executing on parallel nodes, we need to regularly check for new test results and monitor the test run status. This is done using AWS's SDK, which checks the status of the specific tasks using the persisted resource identification numbers. The user dashboard is launched to allow real-time progress tracking of the test run. The SDK is also used to periodically check for new test results and update the dashboard accordingly.

Once the test run is completed, we need to recalculate the test groupings for subsequent runs. This involves retrieving timing data from a persistent store, running the second stage of the algorithm to generate new test groupings, and saving these groupings in the configuration file for future use.

To execute the test suite on a single node, we initiate necessary background processes, start the user's application, and then start Cypress to execute specific tests. The testing process is initiated by a shell script within the container, which also includes a file watcher program. This program detects when testing artifacts are generated and uploads them to the appropriate directory in Conifer's S3 bucket. The file watcher program also parses certain artifacts and saves select metadata to DynamoDB. 

Test results are then persisted by saving them in a structured format to a persistent storage outside of the testing infrastructure. This ensures they can be accessed later. Test results are divided into two main steps: detecting when results are available and saving them. The file watcher program, which watches for changes in the directories where the artifacts are saved, detects when artifacts are generated and uploads them to the S3 bucket. Additionally, it parses the artifacts, extracts metadata, and saves it to DynamoDB.

To communicate the test results to the user, we retrieve the results from the storage location, process the data into a user-friendly format, and display it through a graphical user interface. Conifer achieves this through a live dashboard and an HTML report. The live dashboard uses data persisted by the file watcher and uploaded to DynamoDB. The dashboard displays real-time updates from the test run, allowing users to track the status of their tests. It also provides links to download individual test artifacts from the S3 bucket. The HTML report is generated at the end of the test run, aggregating and processing the data from the S3 bucket into a single file. The user can open this report to see a comprehensive overview of the test run, including the status of each assertion.

In the implementation of Conifer, two key challenges were faced: executing tests in parallel and retrieving test results from the nodes. To perform parallelized testing on the cloud, the consolidated Docker image is built and sent to ECR. The tests are executed in parallel by running multiple containers simultaneously on ECS, each container running a subset of the test suite. AWS's SDK is used to manage and track the status of the containers and retrieve the test results from the persisted storage.

By using Docker containers and cloud infrastructure, Conifer simplifies the deployment of user applications on general-purpose cloud computing infrastructure. The architecture and components of Conifer work together to orchestrate the testing process, persist test results, and communicate them to the user effectively, making it a powerful tool for running tests at scale. The live dashboard is designed to display real-time updates on the status of the test run to the user. It utilizes data persisted by the file watcher and uploaded into the dynamo database. The live dashboard consists of an Express backend and a React frontend. As the test is executing, the CLI holds DynamoDB for updates on the test status and sends these updates to the dashboard via a webhook. The live dashboard then displays these updates to the user in real-time. It allows the user to monitor the status of individual tests, the duration of each test, and whether a test has started or failed. The user dashboard also provides links to download individual test artifacts from the S3 bucket, such as screenshots of failures or videos of the entire test.

Additionally, at the conclusion of the test run, an HTML report is generated to communicate test results to the user. This report is created by retrieving data from the S3 bucket, aggregating it into a single file, and saving it to the user's project directory. The HTML report provides a comprehensive account of everything that occurred during the test, including the status of each individual assertion, what passed and what failed, and any other relevant data. It aims to provide the user with the same information they would have had if they ran the entire test suite locally.

The infrastructure of Conifer consists of various components, including the live dashboard, the file watcher, the DynamoDB database, the S3 bucket, and the ECS task runner. These components work together to ensure the successful execution of Conifer. The live dashboard updates the user on the test status, the file watcher retrieves and uploads test artifacts, the DynamoDB database stores the test status updates, the S3 bucket stores the test artifacts, and the ECS task runner executes the tests in a distributed environment on the cloud.

During the implementation of Conifer, two main challenges were encountered: executing tests in parallel and retrieving the results from the nodes. Initially, the idea was to use Lambda functions to parallelize the test executions asynchronously. Lambda functions are known for their scalability and have been successfully used for end-to-end testing. However, the Cypress framework had conflicts with running on Lambda due to a low-level display driver dependency. So, Lambdas were not suitable for Conifer.

An alternative approach was needed, and Elastic Container Service (ECS) was selected to run the tests in parallel. ECS provides a container orchestration service for Docker images and offers two launch types: EC2 and Fargate. Initially, EC2 was chosen as the task runner for easier development and better speed gains. Although Fargate, as a fully managed solution, offers potential parallelization advantages, it was deferred as a future optimization.

To address the challenge of retrieving the test results from the nodes, two approaches were considered: synchronous and asynchronous. The synchronous approach would have required modifying the Cypress config file and stitching code blocks to upload artifacts after each test file execution. However, this approach was deemed undesirable as it could introduce conflicts with user-configured files. Instead, the asynchronous approach was chosen, involving the implementation of a file watcher program. This program detects when a test artifact is created and fully saved and initiates the upload process. Although more complex, this approach eliminates the need for additional user code and allows real-time streaming of testing artifacts.

Looking ahead, future work for Conifer includes investigating other test allocation algorithms and the use of Fargate as a task runner. Dynamic allocation of tests could be explored to allocate tests dynamically using a queue-based system. This could be beneficial in situations where accurate timing data is not available or when test suites frequently change. Additional features to improve testing efficiency for developers include implementing fail-fast, which stops test execution at the first failing test, and adding flaky test detection to track and resolve unreliable tests. Furthermore, analytics on the live dashboard could provide users with a richer experience by providing insights into test results.

In conclusion, the implementation of Conifer involves a live dashboard for real-time monitoring, an HTML report for test result communication, and an infrastructure consisting of various components working together. Challenges were faced when parallelizing tests and retrieving results, leading to the selection of ECS as the task runner and the implementation of an asynchronous file watcher program. Future work involves exploring different test allocation algorithms, investigating Fargate as a task runner, and implementing additional features to enhance testing efficiency. The purpose of the coding Capstone project is to implement a file watcher program to enhance the testing artifacts. This program operates separately from the cypress test runner and runs asynchronously in the background while the cypress tests are being executed. In order for the program to function correctly, it needs to be able to detect when a test artifact has been created and fully saved. Once these conditions are met, the file watcher initiates the process of uploading the specific artifact to persistent storage.

While this approach requires no additional work from the end user, it introduces complexity and potential points of failure in the testing infrastructure. However, we decided that implementing real-time streaming of testing artifacts through the use of the asynchronous file watcher program would be the best solution. Both approaches achieve the necessary functionality, but the synchronous approach would burden the end user with stitching together the configuration files, which may be unfamiliar to them.

Looking towards the future, our team plans to investigate other test allocation algorithms that would be useful for engineers in various use cases. One potential algorithm is dynamic allocation of tests, where tests are dynamically allocated by utilizing a queue to feed tests to nodes as they become available. This approach can be beneficial in situations where accurate timing data is not available, such as during the first test run or with infrequently or rapidly changing test suites.

Furthermore, we want to explore using Fargate as a task runner. If we can work out the challenges, this could allow for parallelization and leverage the benefits of a fully managed solution. Additionally, we have identified some features that could increase testing efficiency for developers. When testing locally, we aim to implement a fail-fast feature that allows users to stop test execution as soon as the first failing test is found. This saves time by preventing unnecessary further test execution.

Another feature we want to add is the ability to detect and track flaky tests. This would help developers identify and troubleshoot tests that are inconsistent in their results. Lastly, we plan to incorporate analytics on the live dashboard to provide users with a richer experience and deeper insights into test results. This would enhance the overall testing process and provide valuable data for analysis and improvement.

In conclusion, the development of the file watcher program for testing artifacts has been a significant technical challenge. However, it successfully supports the necessary functionality without requiring additional effort from the end user. Moving forward, we have outlined several areas for future work, including exploring new test allocation algorithms, investigating Fargate as a task runner, and adding features to increase testing efficiency for developers.

During the Q&A session, we received some interesting questions. One question was about the algorithm behind the recalculation of test groupings. The algorithm aims to distribute test time evenly among nodes and is based on selecting the node with the minimum total test time. Additionally, the tests are ordered in descending order by test time to make fine adjustments and converge to a more equal solution between nodes.

Another question was about the most challenging and exciting parts of working on Conifer. The most exciting aspect was seeing the significant improvement in test execution time when the project was deployed successfully. It was rewarding to witness the concept in action and validate the hard work put into it. The most challenging part was configuring the permissions, security groups, and roles between various parts of the infrastructure, which required extensive troubleshooting and adjustments.

There was also a question about any interesting discoveries during the initial investigation of overcoming the driver issue with lambdas. One interesting discovery was a low-level solution that seemed promising but was complex and lacked clear explanations. After realizing the difficulties faced by experienced software engineers trying to implement it, we decided to move on and explore alternative approaches. This highlighted the importance of finding a practical and user-friendly solution.

Lastly, there was a question about how the file watcher was implemented. The core functionality of the file watcher program was relatively straightforward. The challenge was determining the correct timing to upload artifacts, particularly for videos, as Cypress saved them at different stages. The solution involved tracking accompanying files generated by Cypress, such as metadata, and waiting for their removal to signify that the video was fully saved and ready for upload.

In conclusion, the Capstone project has achieved its goal of implementing a file watcher program to enhance testing artifacts. The future work outlined includes investigating new test allocation algorithms, exploring Fargate as a task runner, and adding features like fail-fast execution, flaky test detection, and analytics on the live dashboard. The team overcame challenges and made exciting discoveries throughout the project, leading to a successful implementation. Thank you all for your participation and if you have any further questions, please feel free to ask.