[Music] okay hi everybody welcome and thank you for joining us today my name is lisa and i along with my colleagues emily kathy and kyle created q mentis an observability tool for graphql apis so today we'll be covering the following topics first i'll introduce a hypothetical company novels and barnes and walk you through the challenges they're currently facing with their mobile web application then i'll talk about why they decided to develop a graphql api and what are the benefits of doing so finally i'll explain what observability is next emily will talk about why observability for graphql apis is different from a rest api observability tool she will then discuss the existing graphql observability solutions after that cal will walk us through a demo of human tests then kathy will do a technical deep dive of the cumentas architecture and finally cal will come back to talk about the implementation challenges we've faced he will then walk us through a demo installation and share some ideas for future work before introducing novels and barns here's what humanities is in a nutshell humantis is an open source observability solution for graphql apis it abstracts away the complexity of collecting processing story sorry and visualizing metrics and traces data it makes it easy to track errors and analyze the api performance at the resolver level now let's talk about nobles and barns they're an independent bookstore with a few brick and mortar locations and an online store they have a small engineering team with only two people who are full stack engineers so they're doing both back front end and back-end work and they are responsible for maintaining the levels environments monolithic single server app over the past year novas and boring sales have increased significantly and more specifically the online sales have skyrocketed currently about half of their sales are done online and the majority of it is done via their mobile web app so their mobile web app is responsible for about a third of your business revenue however the customer complaints have also increased significantly there have been many complaints about the mobile about being slow and we know that high latency means customers are unhappy and it has a negative impact on sales a customer may end up buying less books or simply giving up and not buy anything at all because the app is low as mentioned before not only a third of their business revenue comes from the mobile web app but sales from that revenue source continue to grow significantly as a result the team has been working hard to improve the mobile app experience and they have recently deployed their first project which was to implement a graphql api but why did they decided to do that you may ask so before the frontend communicated with the backend via a rest api and because of the way their app was implemented when a customer was browsing through books the server was sending a lot of information about books authors etc and it was relying on the client to sort out the relevant information this has worked well for many years and still works well for desktop browsers both this potential increase in mobile traffic and sales it was putting on the necessary string on the client and negatively impacting the shopping experience with a rest api to load a page the client may need to make multiple requests to get the information needed for instance the client may need to make a get request to the author's endpoint and then another one to the genre's endpoint and finally yet another one to the book's endpoint even in this simple example there are already three network calls made between the client and the server and as folks don't always have a good network connection when browsing on a mobile device it may take time to load the page with a graphql api the client only needs to make one request and they can customize the query to fetch exactly the data they need so the heavy lifting is really done by the backhand the engineering team write resolvers which are functions that the graphql server executes to obtain the information requested the server then aggregates the data and sends one response back to the client this improves performance and reduces latency since the client is now only doing one network call instead of three another common issue with rest apis is data over fetching so here for example the client wants to load a book page and in order to do that it needs the author's first and a last name however because a rest api always returns the same resource representation when a client makes a get request to the author's endpoint it receives a lot of information he doesn't need that information would be helpful if the client was accessing the author's page but that is not the case and mobile devices may have to deal with limited bandwidth which makes it harder to handle a large amount of data with graphql bandwidth usage is minimized since requests only fetch exactly the data that they need there is no over fetching while there is a lot of work to be done on the back end to implement the graphql api the nobles and barnes engineering team decided that the benefits outweighed the cost remember that nobles and barnes has a small engineering team of two they're responsible for both the back end and the front tech so while it may take extra work to set up the graphql api on the backhand it will ultimately save time and make it easier to work on the client side of the application so logos environment has already invested a lot of time and money to develop a graphql api and they want to ensure that it is performant the covenants engineers aren't really graphql experts and they want to monitor the health of the app after making the switch to graphql they want to make sure that latency has decreased and that the overall performance of resolvers is acceptable they also want to know how requests are hitting the database they want to track errors and respond to issues quickly so now that we have a better understanding about the reasons why the novels and bonds team develop a graphql api let's talk about what observability is and how we can help the team find the answers to your questions so what is observability it's the ability to measure and analyze a system's current state based on data that the system generates though more observable a system is the more quickly one can go from spotting a potential problem to identifying its root causes without needing to write any code and the goal of observability is to help developers understand what is happening so that they can troubleshoot and debug applications effectively those are the three pillars of observability matrix traces and logs and in this presentation we're going to talk more about metrics and traces since these are the data types we're collecting with qmentis and what are metrics so metrics are numeric measurements aggregated over a period of time they provide developers but like the big picture information so for example the levels environment sales department they too collect metrics is things such as sales volume per day or number of customers in a brick and mortar store per hour or average total sale per customer per hour and this information is essential to guide their business decisions and similarly collecting metrics helps the engineering team maintain and improve an api metrics help answer questions such as is the application performance degrading is latency higher than usual are users encountering errors when are the down times metrics help engineering teams spot potential problems and be proactive like planning maintenance when they know there won't be a lot of requests coming in and humanitas tracks what is called the red metrics so request rate ever rate and duration which is latency the other data type documents generates its traces so it traces the collection of stands and provides detailed information about a request journey so if we go back to a customer visiting a nobles and barnes location from the moment they walk into the door until they leave the store that's a trace and i stand would be for instance do you walk from the door to the non-fiction section then they pick up a book and skim it that's another stand they pay for the book it's yet another span so a trace provides detailed information what sections did they visited what books did they look at if there was a purchase how much was it and in a similar way traces gives us a lot of insight into our request they help developers identify where failures occur and what causes poor performance a span is the building block of a trace a piece of the workflow so a trace has a roots band containing the end to weight latency for the request it's like the top ring line on the right side of this screen and it also has any number of child spends which are the three smaller green lines and these child spends represent operations that take place as part of the request finally traces are often discussed in the context of in the context of microservices however with graphql traces are needed even when the app has a monolithic architecture and we will soon learn more about why that is so now that we have a better understanding of what observability is we can appreciate why for the nobles and bars engineers having an observability service for their graphql api is a must-have next emily will talk about how observability for graphql apis is different from observability for rest apis thanks lisa so observability for graphql apis must be approached differently compared to that of rest apis with rest apis and a monolithic architecture collecting metrics provides good insight about the health of the application for instance error rates can be filtered by endpoint http status code or http method if there's an increase in the error rate having these three pieces of information makes it easier to narrow down where the problem is on the other hand metrics don't provide enough information or context for graphql apis even if they have a monolithic architecture because metrics alone don't give enough context to find the root cause of a problem the three main differences between graphql and res are that with graphql there's only one endpoint most graphql requests are post and most graphql responses have the status code of 200 even with errors because graphql only has one endpoint developers need more information to investigate application issues like high latency is it because of the complexity of the graphql operation is there a performance bottleneck if yes then where is this happening visualizing the latency rate as shown on the left diagram points developers in the right direction as it shows that there's a potential problem worth investigating but this graph alone isn't enough information to figure out where the problem is happening in contrast with rest apis you can quickly narrow down where the issue is coming from the author's endpoint also because rest always returns the same resource representation an increase in latency is probably related to back-end bottleneck we can then filter by status code to get more information about the source of the high latency because the vast majority of requests to graphql apis or post requests filtering the error rate by http method doesn't give any insight into a potential issue also graphql doesn't accept put or delete requests with rest filtering by http method can be helpful in the example on the right there appears to be latency issues coming from put and delete requests with the rest api developers usually track errors by status code if the response has a 400 or 500 status code it is counted as an error with graphql most of the status codes will be 200 even if there is an error so relying on status code to count errors will underestimate the real error rate in this diagram there's a difference between the real error rate versus the error rate with status codes of 400 or 500 this is because graphql handles errors differently with graphql all errors regardless of status code or a handle as part of the response body under a special errors object to have a clear picture of the graphql api's health developers need to track this object instead of the status code adequate graphql observability services require developers to track both metrics and traces metrics give a broad overview of the overall health of the system they help identify potential problems metrics are like the tip of the iceberg traces are essential to finding out what the actual problem is and where to find that problem without metrics it's hard to spot a problem as there will be too many traces metrics provide an easy and quick way to spot a problem traces provide the information needed to find out exactly what is going on and how to solve it this is an example of a graphql specific trace noted here are the trace or operation name the trace id and the total duration of the request and the root span the root span encapsulates the end in latency of the entire request every graphql request goes through parsing validation and execution phases this diagram shows the respective spans for each phase within the execution phase you can see the operation type the names of the parent and child resolvers as well as the associated child spans having these graphql-specific parts of a trace less developers gain a better understanding of the journey of a graphql request and where any bottlenecks or errors may occur after discussing why observability works differently for graphql apis versus rest apis let's now discuss some existing graphql observability solutions novels and barnes has a few options to explore that offer graphql observability they could use fully managed services like apollo studio or hosura cloud or they could take a diy approach using open source tools apollo studio and hasura cloud are two popular fully managed observability and monitoring cloud services for graphql they offer many features and have a ui that's relatively simple to use apollo studio is a cloud platform that helps with every phase of graphql development including monitoring metrics and traces husera cloud also manages the infrastructure of graphql applications offering real-time monitoring and tracing as both solutions are graphql specific they provide users with meaningful information such as the operation name and type this is a view of apollo's monitoring dashboard here you can quickly visualize metrics information such as request rate latency and error rates you can also see which operations were the most requested which ones took the longest and which operations had the most errors on the bottom is a graph that shows the request rate over time making it easy to identify spikes in traffic this is another view of the apollo dashboard the top of the screenshot shows the request latency over time this dashboard helps show when slow requests happened on the bottom is a visualization of the request latency distribution which makes it easy to spot outliers and slow requests here's a view of the husura dashboard it's somewhat similar to apollo's it shows the same three metrics request rate error rate and latency and also shows which operations were the most requested had the most errors and had the highest latency novels and barnes could choose to use apollo studio or hasura cloud as their observability tool however for apollo they will have to pay to access traces for the free version of hasura they're limited to 60 requests per minute novels and barnes is getting more traffic with new customers so they would rather not have this limit also either apollo or hazara would own the collected data and the team would not be able to see data collected past a certain number of days because they're a small company novels and barnes prefers to minimize costs as much as possible also data ownership is a must-have for the team so in this case neither apollo nor hasura would be a best fit for novels and barns aside from fully managed services novels and barns could go for a diy route they would be able to do this without spending a lot of money and would have ownership of their data which are the two most important features they're looking for in their observability tool this approach would also allow them to have full control over the features and how the tool works however a diy approach has a steep learning curve because this approach involves researching many different components and stitching them together it's research intensive resource demanding and time consuming if they had a big team they may choose this approach however with a team of two they would rather spend their time and effort focusing on the business logic of the application so the diy approach is probably not the best option for them there is a third solution that novels and barns could use which is cumintis as discussed earlier keymantis is an open source observability tool for graphql apis with humantis novels and barnes also guests to maintain control over their data without needing to explore it to a third party keymantis is well documented and designed for easy setup reducing time cost users are able to see both metrics and traces all in one dashboard even though kimentos has less features than the managed cloud solutions finally humantis is free there are no time or storage limitations so novels and barns can invest their energy back into their business needs after discussing the pros and cons of various existing graphql observability solutions novels and barnes has found kimantis to be a great fit for their business and now cal will go over how novels and barns can use humantis in more detail thanks emily if novels and barnes chooses to use comantis their small development team can get free real-time metrics and traces coverage made specifically for graphql apis without having to learn or implement any new observability technologies they can check their applications usage by monitoring its real-time request rate error rate and overall latency and can also analyze and distinguish every request trace made to the graphql endpoint here's a quick walkthrough of what this would look like and why it is so important these images show what a humanist dashboard would look like for a user like novels and barnes the dashboard allows users to see metrics and traces side by side being able to view metrics and traces in the same place can provide a lot of insight into what is going on with the graphql api for example here on the top row of the metrics you can see that request rate and error rate move together which is expected but down below that it shows that the average latency of requests is spiking outside of any increase in traffic when seen alongside the traces it's obvious that there are a few major outliers driving up the average latency time these traces should be investigated keep these spikes in mind as i move forward with how to decipher traces using humantis but first to give some context let's see what a full single trace looks like when being viewed in grafana with cumantis a user can easily analyze traces for every request made to the graphql endpoint this is what a typical trace may look like for a sample query when viewed using grafana in this query books from a particular author were fetched along with genres of those books far on the right hand side of the condensed span section you can see that there are multiple spans executing at the same time this is exactly what a developer would want from an efficient query once the author is fetched all books related to that author and genres related to those books are fetched asynchronously the left hand side of the screenshot shows exactly what is being fetched while either span well each span is executing this gives valuable insight into the graphql resolver used to fetch this data and makes it easy to pinpoint any inefficiencies that could be negatively impacting a server's response time okay back to the outliers i tried to make a video of this but the resolution was really bad so i'm just going to talk through it when viewing traces from the dashboard novels and barns can easily spot slow requests with the growing success of their business and their recent surging users they can quickly investigate and address bottlenecks in their application by doing so it allows them to respond efficiently to issues related to high latency and ensure customer satisfaction sometimes an outlying trace is related to an operation that usually runs in line with the rest of the graphql operations in this case then the issue is usually related to some out something outside the graphql api such as a temporary overloaded database here though when hovering over each of the outline outliers in this snippet of traces you can see that they are all related to the same graphql query named all books so it's time to investigate that resolver here is the individual view of one of the outlying traces for some reasons fans are executing synchronously and information for roughly 20 books is being fetched in nearly half a second what would happen if a much larger request was made say for 100 or 1000 books this query defeats the purpose of switching to graphql in order to improve response time and may even increase response time when compared to a rest api this resolver returns a promise which returns a map of the original result which then returns a promise that to populate the genres fields once the authors field is populated for each book just trying to say that out loud should prove that there's a better way to write this resolver okay here's the improved resolver you can see it looks much cleaner and more efficient than the previous resolver in this trace spans are executing asynchronously there are no big gaps between span execution and latency time is cut by over half in the expanded view of this trace that shows all 156 spans you would see that in the first group of asynchronous fans books are being retrieved well in the second group authors and genres related to those books are being retrieved since it's impossible to find the author or the genre of a book before knowing what the book is this is as efficient as this query can get in addition this trace gives us confidence that this resolver could return a much larger larger data set without greatly increasing the response time of the server due to its asynchronous nature this example may seem contrived and a little unrealistic and it probably is for such a simple query but for people new to graphql or when dealing with more complicated resolvers this is a big issue if a company like novels and farns didn't have a graphql observability tool set up an inefficient resolver could easily be pushed into a production environment and go unnoticed for weeks months or even years humantis provides insight into many areas of a graphql api but for the purpose of this demonstration i chose to show an inefficient resolver for two reasons first inefficient resolvers can be very hard to spot in a working api without proper observability and second this demonstrates how humantis can be used to track down an issue all the way from the dashboard to a resolver now that you have seen how humantis works as an observability tool and why graphql specific observability is so important i will pass it over to kathy to explain more about humanist architecture thanks cal this diagram gives an overview of the humantis architecture there are two main overarching and interconnected components humantis express and humantis compose on the left humantis express is an npm package with a configured express graphql server that's responsible for generating the metrics and traces on the right humantis compose lets the user deploy the commantis architecture on their own server using docker in order to process export store and visualize incoming metrics and traces now to provide a better understanding let's break up this architecture into key phases the cumantas architecture can be split up into four major conceptual phases which will be discussed in more depth in phase one we generate traces and metrics with the humantis express graphql server and open telemetry sdks in phase 2 we export data for processing and storage in phase 3 we store data with prompt scale and timescale db and finally in phase 4 we visualize and analyze data with grafana and jager the first phase of the kumantas architecture which is part of the kimetus express component is to generate the metrics and traces in order to eventually visualize the data on some dashboards something first needs to create that data qmantis uses an open source project called open telemetry which offers a range of tools that generate and collect telemetry data such as metrics and traces to create the telemetry data we configured an existing graphql server with observability extensions added custom express middleware functions and added two pre-configured open telemetry node sdks this is a crucial phase of the qmantis architecture because it ensures the data generated by the server is specific to graphql for example the qmantis express component checks the response object to see if there is an errors property if there is one it updates the error rate counter also one of the panels in the qmantis dashboard is all the traces that contain an error as emily mentioned earlier developers can't rely on a response status code to filter traces for errors as most graphql responses regardless of their success return status code 200. to address this we added custom error tags as needed making it easy to filter and visualize all traces with errors additionally as mentioned earlier the majority of graphql requests are post requests the default name of a trace is the corresponding request http method however this isn't meaningful to the user since the majority of traces would be named http post therefore our team altered each trace so that the user could distinguish between traces more easily this middleware fetches the operation name from the request body if the user does not provide a name then the operation name becomes the operation type metrics and traces have to be generated before they're pulled for either processing and or exporting by other services now let's discuss the second phase of the key mantis architecture the export phase which is part of the comet's compose component once telemetry data is created it needs to be exported before anything useful can be done to it in the case of traces the data is exported to the open telemetry collector the collector is a central component that handles additional steps such as batching and processing of data additionally it offloads the responsibility of the application to manage the collected data which reduces overhead using the collector our custom processors add useful graphql information to each trace some of the spans in a trace are given customized names so for example users can distinguish one request field resolver from another without having to click into each span for more information in the case of metrics they are exported to prometheus which is an open source systems monitoring service that collects and stores metrics as time series data meaning metrics information is stored with the timestamp at which it was recorded the purpose of exporting metrics to prometheus is to transform the data into a format compatible for the next phase now that the data has been generated and processed into a readable and more meaningful format making it easier to work with for observability purposes the sata can now be exported to long-term storage which is the following phase so the third phase of the commantis architecture which is part of the humantis compose component is storage before you can visualize and analyze telemetry data on grafana data needs to be stored somewhere long-term storage is useful for users because they can analyze not only current data but also older data which will be very useful if they want to analyze trends in their graphql api as time is a key component of how kimantis generates and reads its data we chose to use a time series database as it is optimized for timestamp data time matters if there is a potential problem it's important to know if it happened at 1pm or 8pm if a user wants to do maintenance during downtime they need to know when there are down times also time series databases support a high rate of inserts and a low rate of modifications and deletions this is a great fit for mantis's use case because when a metric or trace is created there's no need to ever change it the user only needs to insert it and read it specifically humantis uses time scale db an open source time series database powered by sql along with promscale a backend extension that connects telemetry data to timescale db via the connector this database has no time limit or cost involved in storing telemetry data for users and it keeps everything in one place which helps us simplify our architecture since we can store both metrics and traces data on the same database another main benefit of using timescale db is that it leverages postgres to store data an established data store that's widely used among developers the choice to use timescale db means that it is easier for users to understand how their data is stored and they don't have to learn any new technologies also using prom scale and time scale for data storage allows users to easily connect that data to grafana giving them the ability to fully leverage for fonda's built-in queries finally the last phase of the humantis compose component involves visualizing and analyzing data to visualize the data as was demonstrated earlier by cal our team chose to use grafana a popular open source tool for observability dashboards that comes with many options and functionalities qmantis pre-configures an observability dashboard so that users can view and analyze their application's request rate request latency error rate all their traces and error traces grafana connects directly to the prom scale connector and time scale db in order to obtain metrics data as well as to jager an open source tool for end-to-end distributed tracing to query the trace's data when it comes to visualizing data our team considered two options building a custom user interface or using grafana a custom ui would have provided two main benefits we could have branded the ui to add a human to specific design and we could have displayed data more intuitively especially for a first time user ultimately these pros of building a custom ui did not outweigh the cons of actually building it so we chose to go with grafana griffana is an industry standard observability tool that many developers are already familiar with it's also extremely customizable so once the user has humantis running they can change the dashboard and query settings to fit their needs to give an example projects at different points in the development cycle may want to query data by different lengths of time and grafana makes that very easy another advantage of using grafana is that we do not need to maintain a whole extra code base and if we decide to update the dashboard or panels in the future reconfiguration would be relatively simple to recap this is the humanist architecture with its two main components humanist express on the left which consists of the generate phase and humant is composed which consists of the export store and visualized phases again humanist express is an npm package composed of open telemetry sdks and a graphql server observability configuration it's responsible for generating the metrics and traces data qmantis compose lets the user deploy the humantus architecture on their own server using docker it contains all the tools necessary to export store and visualize graphql specific observability data now that we have discussed the kumanta's architecture in depth next up i'll pass it back to cal who will discuss the implementation challenges of designing and building humantis and demonstrate how to set up and install qmantis thanks kathy okay now i'll go over the implementation challenges our team faced namely getting meaningful data from a graphql server and connect connecting metrics to traces before talking about the challenge of getting of generating data from a graphql server i wanted to lay out the choices we considered when it came to graphql servers early on in the process of building humantis our team discussed building a custom graphql server from scratch this would have provided greater control over the implementation of the server and made it simple to generate and collect data exactly as needed however if a developer had an already existing graphql api they would have had to change their code to work with qmanus's less popular server in order to gain observability features another option was to use apollo server a highly popular graphql server note that this is different than the fully managed apollo studio observability service our our team considered using apollo server to give users the option to use a graphql server that was framework agnostic for node.js however this option had a major downside the way apollo server is implemented makes it very difficult to extract meaningful trace information even when creating custom plugins to extract this information in the end we landed on expressgraphql the second most popular javascript server for graphql used by millions of developers around the world expressgraphql does not currently have any out of the box observability options paid or unpaid and it is highly customizable allowing for developers to create extensions with observability features with this in mind we built our first iteration of humanas using an extended express graphql server providing an observability service packaged with an already popular graphql server choosing to work with expressgraphql did not come without significant challenges though as we didn't have control over the implementation of the server our team had to be creative in order to collect the needed data from requests and response objects this is information that is essential to generating accurate graph accurate and graphql specific telemetry data and a key feature of humantis the two most difficult pieces of observability metrics data to obtain were counting errors and collecting latency data for requests that had a 400 or 500 status code to get the accurate information our server needed to access the response object before it was sent back to the client to do that we relied on a function from the expressgraphql server to access the response object however when the status code was 400 or 500 the express graphql server didn't execute that function so it no longer checked for errors or latency information at this point we considered modifying the humanist architecture and even prototyped a custom graphql server to get this data this approach actually worked but it came with the limitations mentioned in the previous slide ultimately we found better solutions to these challenges using an existing npm package to obtain latency data and passing trace data to a custom function in order to accurately count errors now i will explain why we chose not to collect metrics and traces for the time being initially our team wanted to connect mexi metrics and traces in order to make it easy for the developer to go from spotting a potential problem to getting detailed trace information how would this work when the developer notices an outlier metric data point such as an increase in error rate they would click on the data point and it would take them to a list of traces that occurred within that time frame after a lot of research we found that the only way to have this feature in grafana was to use grafana's solution for traces griffon and tempo this would have required us to use a different database to store traces as grafana tempo isn't compatible with time scale and would have added complexity to the humanist infrastructure another option was to build a custom ui which was an idea ideal as cathy already explained to overcome this obstacle we created two trace panels one that shows traces by latency making it easy to spot the outliers while the other only shows error traces because both metrics and traces have time stamps it's easy for the user to filter the traces panel to look for information they need it is also worth noting that the leading paid observability solutions for graphql do not currently support linking metrics and traces together okay now i'm going to go over a quick demonstration on how to install qmanus and get it set up to work with an application first the humanist express package humanist express npm package needs to be installed if express graphql is not currently installed for your project that needs to be installed as well next the humanist compose repo needs to be cloned this repo can be cloned anywhere on your machine and it will run properly so clone it into whatever directory works best for you once the npm package is installed you need to import the qmenas variable into your main file this is the file that imports the schema and starts up the expressgraphql server for your api it is crucial to note that all of the humantus variables needed to be imported before the expressgraphql variable or there may be some issues with collecting traces after the variables are imported it is time to set up the route handlers for your graphql endpoint this is accomplished in a similar way as setting up a route handler to use a package such as cores for any api the main point of difference here as you can see is that the qmantas register latency function needs to be passed to the response time function as an argument and the main humanist function needs to take the graphql schema as an argument and be passed through the express graphql server at this point the humanist express package is set up to generate traces and metrics when your server is deployed the last thing that needs to be done is to initialize the humanist docker container so you can collect export store and visualize the data to do this you need to cd into the qmis compose repo and then just run docker compose up when you do this you will see the docker images starting in the terminal and if you have docker desktop you will be able to view the humanist compose container with all the running images okay now that everything is set up and running you will need some data before you can visualize it if you have a production level server you are using humanos with you can simply deploy it and wait for requests to come in if you if you are still in a development environment you can use graphical which is already set up on localhost 4000 to send requests once requests start to come in you can navigate to localhost 3000 and view view the pre-built humanist dashboard in grafana this is where you will be able to visualize all the metrics in trace data for your graphql endpoint making it simple to keep an eye on the overall health of your app and distinguish where any inefficiencies may be coming from now that you have seen how humanist works let's quickly discuss what our team plans to work on moving forward first the team would like to add functionality that makes q-manas compatible with multiple graphql server types such as apollo secondly humanist wants to expand metrics most notably adding the ability to see how often a certain operation is called and what the average response time for that operation is as discussed before metrics and traces are not currently connected the team has had some discussion on what the best way to do this would be and it is definitely something that could be implemented in the future lastly incorporating alerts it's fairly common for observability platforms to offer alerts via text slack or email when a serious issue arises and this could be a feature that would make humanists more useful okay here's the team that built humanist and that concludes our presentation thanks for coming if you have any questions we would love to answer them at this time okay we do have a question actually first we have a comment which says good job cumentis excellent explanation of graphql observability and the problems that arise at their intersection and now the question what was the hardest part to implement from brandon that's a good question um for me personally this is lisa speaking um and i think the team may agree i think it was getting all the components to work together so after we kind of decided we pieces of infrastructure we wanted to use it was setting up them all together and how they would connect to each other and deploy them on docker yeah i would agree if you go look at our docker compose file it's like 40 lines long but it it took days to get absolutely correct yeah and another difficult aspect is we were using a lot of new technology like um open telemetry and prom scale do not have a lot of you know updated docs or resources that uh are very useful so we kind of had to do a lot of um look at a lot of examples and experiment a little bit to get it to work uh just because a lot of these are very recent uh services that came out just like at the end of last year yeah i think there are some days where we just spent the whole day just debugging or trying to get like one thing to work so that was a little frustrating okay we have another question josh says amazing job can you talk about how you found this topic and why it appealed to you that's another good question let me think about it i think we started by researching graphql and because i thought it was interesting same thing with observability like oh yeah that that looks interesting and starting to ask questions about like why would people transition from rust to grassy well to differences or like having them both together i don't know at some point we're like well what if we did observability for graphql like to answer some of those questions that we like mentioned at first because it's very common like in the graphql community for like people having issues with resolvers and like how to debug it since queries can be so different it's it's not always good the requests are not always the same um so i don't know we kind of went from there and um london on this um project at the end i think it was very um organic i think how the our discussions went from the beginning until like deciding on that project on the topic of the project yeah and i know when we were when we were researching this originally we hadn't quite decided exactly what we were going to do we knew this was an issue and there wasn't a lot of tools out there and we came across the blog that showed an example of kind of the first example that i showed a a bad resolver uh making the response of a server inefficient and it was from a big company that had been running this resolver for months and they were having an average latency time of 600 milliseconds because because of a bad resolver and they got it down to like 180 milliseconds once they realized it but just without the observability tool they had no idea what was even happening and i was like wow this is a real life problem that big companies are actually running into and that's kind of for me at least that's what kind of made me think yeah this is something that's really useful and we we could build would any major parts of your infrastructure need to be reconfigured to connect the metrics with the tracers if so which parts of your architecture yeah we would have had to use a different database because uh to get it to work to grifano which was our ultimate end goal to visualize the traces and metrics we would have had to use grafana tempo um and so that was not compatible with prom scale and time scale so that would have required just changing our entire architecture pretty much to get something that worked um like elasticsearch for example and there's probably more as well okay jason says great presentation the use case you mentioned is for a small business with relatively low request rate and not a huge number of users what features if any would be required to scale up for a more intensive use case or a larger business thank you so the way it this is set up right now it it will only work on single server apps so if we were going if it was going to work on like a for a big big company that was using a cluster or something like that it would need to be totally reconfigured to be able to do that i know there's a way with grafana that you can link multiple servers together and get request rates and all that but we didn't really research that okay katarina says great job with the project what part of working on it did you find the most enjoyable and rewarding um i think the most enjoyable part was probably uh getting things running and being able to visualize the data we're collecting without having errors for the first time yes i was gonna say the same thing i think as someone mentioned before sometimes you spend a day trying to debug one thing and then when it works you're like oh that's awesome yeah and we had fun doing it as a team i think we all kind of struggled with the same things and we're kind of hands-on on every part of the project so i think being able to like quickly like step in if someone needed help or vice versa you know was pretty awesome yeah and i know we had we had a couple days where you you struggle struggle struggle and then in one day like multiple things come together and we we just feel like rock stars on those days so that was probably the most enjoyable part okay another question uh nice work you meant to steam if you have to do this again not that this work has any flow of how or what would you do differently or would you do anything differently i think that's a good question i think for us as a team definitely the things that kyle talked about just at the end in the future work would be things that we would like to add but i can't think of anything that we would have done differently i mean knowing now which things work and which didn't uh we would try the ones that worked first but there's no way of like avoiding this the things that don't work like trying different things until you'll settle into one that fits better to your use gaze and works and kind of respond to the needs that you have i think one thing that i'm curious about doing differently is making the user interface ourselves instead of using grafana i think it would have been nice to build a custom ui that's like a simplified version of grafana since grafana can be overwhelming to you since there's so many features okay the next question from sergio how did you go about dividing the work amongst each other uh well just it kind of this project was kind of natural for it because metrics and traces are collected and exported differently so two of us focused on traces for a lot and two of us focused on metrics at the beginning and then we kind of all came together at at the end i mean we came together every day but at the end it kind of got it all working together yes we made sure that kind of the key parts of that everybody kind of touched the key parts of the project so we all touched a little bit of kind of all the faces that casting talked about um yeah and apart from that as kyle mentioned dividing like metrics and traces and then like meeting again and discussing about the things that we did but yeah again i think it was very natural what we thought worked well was every day we would meet and then we will discuss and adjust if things didn't work in terms of organization or things that we were trying we weren't very inflexible i would say just the office like our team was very flexible with either how we organize the tasks or the things that we wanted to try okay and the final question from julio he says outstanding presentation you mentioned creating an npm library for extending express graphql with instrumentation to emit metrics and traces how does extending express graphql and adding instrumentation work what info do you add to traces so you can add custom extensions to expressgraphql for customizing metrics and traces so we use these extensions to add middleware express middleware um that lets you for example like get latency as well as count the different all the errors um and also the info we added to traces was a custom operation name um as well as the operation type to distinguish them otherwise you could you would just see the graphql endpoint which wouldn't be very useful to distinguish them yeah okay when you see that query like with the name all books that's added by our extension like if if you didn't add that all the traces would either just say http post or operation or something like that where you wouldn't know what the query actually was i mean you could figure it out but it's really helpful to have the name there and for our errors we added a specific graphql error type to each trace that had a graphql specific error yeah so that we can kind of visualize them in graphone because what we did we had a piano ingredient that only shows errors like the traces with errors so that it's kind of easier to visualize them but then we couldn't filter them by status code so we needed to add something like custom to all those traces and um when you asked about how the instrumentation works the instrumentation actually leaves in our npm package and then we execute some functions in the express graphql server so that we can extract kind of the information because we don't really have control over how the express graphql server is implemented but we need data from it to be able to generate the metrics and the traces so we have yeah skeptics are like an extensions functions that we run to obtain that data okay no more questions any final words thanks for coming everybody yeah thank you thank you 