foreign welcome thank you for joining us my name is Carolina and I'm here with my colleagues Julia Marcus and will today we will be presenting Firefly Firefly is an open source observability framework for serverless functions that provides key insights into serverless function Health through the use of metrics and traces let's take a look at what we'll be covering in today's presentation first I'll introduce foundational Concepts so that we can understand the problem that Firefly solves then Julia will talk about serverless observability why it's important along with its challenges she will also be discussing existing Solutions in this space next Marcus will introduce Firefly give us a tour of fireflies architecture and finally will will talk about the challenges we encountered and future work for Firefly with that in mind let's get started what are microservices microservices represent a distributed architectural approach in which a single application is composed of a number of loosely coupled independently Deployable and scalable services each service has a clearly defined role executes its own processes and often communicates via an API because of the abstraction of deployment and management of physical infrastructure away from the end user the ability to build a microservice architecture has become more attainable with the rise of cloud compute microservices are now commonplace and provide an alternative to monolithic architectures let's get into what serverless is serverless is a development approach that allows for applications to be created and deployed without worrying about the underlying infrastructure of the system in this development approach the control in provisioning maintaining and scaling the server is typically handed off to a third-party cloud provider for a fee developers only have to provide application code which will be packaged and deployed by the third party provider a microservices architecture can be composed of services deployed on their own servers as well as Services which are serverless serverless microservices are often built with one or more serverless functions serverless functions are small pieces of code designed to be asynchronously triggered based on events now let's get into what functions as a service is functions as a service or fast for short describes the cloud-based service that hosts and manages serverless functions the deploy a serverless function to a fast provider a developer writes code that fulfills a particular purpose in the application and defines an event that will trigger the function once the event is triggered the service provider called Cloud listener in this diagram starts a new instance of the function work is performed and the result is returned as more events are triggered the fast provider is responsible for automating the provisioning of computation resources and scaling instances in response to demand this eliminates the need for frequent Hands-On management this is contrary to a system where a service is deployed on a server which the developer has to manage the server is kept continuously running ready to respond to any incoming requests if demand increases more than the server can handle requests are ultimately refused and scaling will need to happen manually in this diagram you can see that there is a request response that happens when the function is invoked along with an increase in compute resources compute resources stay stagnant when the function sits idle and Spins down and it'll increase again when the function is invoked and Spins up again but what does this mean fast providers only allocate comp view resources when a function is invoked so when demand decreases and the function sits idle resources are de-allocated and functions do not use compute as a result serverless functions are not required to reserve and hold computational resources in advance as is the case with traditional architectures this Auto skill Auto provision and compute per use model allows developers to save time because they are free from implementing troubleshooting and maintaining servers unfortunately the inability to access or modify the cloud provider's environment and infrastructure makes it challenging for a developer to collect data about serverless functions compared to applications running on traditional servers a different set of strategies is required which we will be discussing later this diagram represents the three most popular serverless function Cloud providers to wrap up the section let's get into what is observability the following quote is an observability definition that we found most helpful to us from honeycomb's O'Reilly book observability engineering observability is the measure of how well you can understand and explain any state your application can get into no matter how novel or bizarre a software system when reduced to its most basic purpose should take an input perform some form of computation and produce an output observability is concerned with the data generated during this process this data is called Telemetry data Telemetry data is a data generated by the system that pertains to Its Behavior using this data we are able to gain a better understanding of the internal state of the system from the outside the inner workings of the system do not have to be known to ask and answer questions about it so the more observable a system is tracking down potential issues and onboarding new Engineers becomes easier in order to generate Telemetry data a system must be instrumented instrumentation is the process of adding code to your system in order for it to generate and emit Telemetry data observability focuses on three main Telemetry data types metrics traces and logs the three pillars of observability metrics are numeric quantifiable measurements that reflect the health of infrastructure aggravated over a defined period of time here's an example of fireflies dashboard showcasing key metrics for serverless functions which include invocations errors and duration these metrics are vital performance indicators for a system enabling Engineers to identify patterns and anomalies while they are able to quickly indicate the presence of an issue they are not sufficient for pinpointing the exact source of a problem within a program a log is a Time stamped message which can either be structured or unstructured it is a record of a specific pre-configured event that happens at a particular time during the request lifecycle here's an example of a dashboard showcasing logs logs provide granular insights that a metric cannot however their volume of generation causes them to be difficult to manage logs aren't useful for tracking a request through its life cycle but they do come in handy once a greater amount of granularity is required moving on to what traces are I'll first introduce what distributed tracing is in order to fully understand traces distributed tracing is the process of tracking a single request as it makes its way through the different parts of an application a trace is a term used to describe the visual representation of this tracked request here is a snapshot of Firefly's dashboard showcasing traces Trace has significantly reduce the complexity that arises from the nature of distributed systems we're reproducing failures locally can be overly complex tracing makes debugging considerably easier as we are able to break down each step in a given request let's walk through this to get a clear picture in this diagram we can see the components that make up the system but we aren't able to see how our requests would travel through it or which point in the request life cycle causes the request to fail this is where distributed tracing comes in this diagram displays a trace and its entirety each component is referred to as a stat a span represents a unit of work and the time taken to complete that work the first Span in a trace is a root stat it generally represents a request from start to finish in this case the root span is a web application the spans underneath are child span each child span represents a unit of work within that request Journey let's go through the request flow for this particular Trace web application calls service a service a call service B and waits for a response and then service a call service C and D in order for spans to become a distributed Trace certain metadata are representing a shared State called contacts needs to be passed at each part of the request execution this can be used to correlate the spans into a single trace this mechanism is known as context propagation requests carry this information throughout their life cycle allowing each generated span to have access to the shared context context propagation is what makes distributed tracing possible across a distributed system now I'm going to hand it off to Julia who will expand on serverless observability why it's important and how it's challenging take it away Julia thanks Carolina so why is serverless observability important well to reiterate again to increase observability the application must be instrumented to emit Telemetry data that data is then used to gain insight about the state of the application and the requests passing through it for an entire system to be observable each individual part of that system must be observable for a system spanning multiple Services interacting with each other only instrumenting one service out of many is insufficient each part of that system must be instrumented and the same applies to any serverless functions in that system let's go over an example here we have a web application which can make requests to two different microservices service a or service B on top service a is composed of a series of serverless functions below that service B is a more traditionally hosted microservice running on a VPS if reports start arriving of a Slowdown in service a troubleshooting would involve more manual work checking each function individually to verify which function might be underperforming if the functions were to be instrumented and observable we can see much more quickly which specific function is a source of the issue based on emitted Telemetry data this allows us to hone in our efforts on the problem faster and optimize a function as required what then is challenging about serverless observability well there are two main problems the first of which being the instrumentation approach in traditionally hosted applications many observability strategies install an agent within the applications environment we can remote into each server and start a process which will send Telemetry data this agent is then responsible for collecting that data and temporarily storing it until it can be admitted to a Telemetry system for services composed of serverless functions the agent-based approach poses a problem because the fast provider takes care of managing underlying infrastructure and function deployment we can't easily access or remote into that function's internal environment separate processes can only be deployed directly by the fast provider regular user installed agents are not suitable for instrumenting serverless functions furthermore serverless functions are ephemeral and stateless there is no built-in built-in persistent storage or state any data saved locally is lost when the function environment spins down normally as long as the generated Telemetry data is emitted to storage external to the function before the instance is destroyed this isn't a problem however if the fast provider destroys the function instance before the Telemetry data is emitted then the data becomes lost due to the lack of persistent memory after the instrumentation approach the second of the challenges in serverless observability is context propagation context as Carolina had already mentioned context propagation is a mechanism by which distributed tracing occurs information about the context for a given initial request is passed along to Downstream Services as the request continues through the system this allows Telemetry data generated by a given request to later be linked together in an observability solution ultimately showing how each service is involved in the request's life cycle for microservices that interact in a well-defined request response cycle the relationship between those two Services is clear and defined service B sends an HTTP request with all necessary information including the context and service C sends a response when the request is processed and complete there's a well-established standard for this communication for serverless functions which are invoked asynchronously via events this relationship is more hazy a request from a client results in an event being sent to enter an event queue functions subscribe to the queue are invoked as a result and the pro and process the events one by one any necessary data that might have been sent with the initial request may not be attached to the event and accessible by Downstream functions the mechanism for ensuring this is vendor-specific and dependent on the fast provider in order for contact's propagation between two services to be successful both sides must agree upon a convention and for a system which may span multiple hosting ecosystems this convention must be vendor agnostic in summary serverless observability is important for overall system observability and optimization but it's challenging to do to due to available instrumentation methods and context propagation fortunately there are available solutions that can be explored developers looking to increase observability of their serverless functions have a few options vendor-specific solutions from the fast provider software as a service solutions or do it yourself different fast providers often have their own Solutions with their own set of conventions dashboards and apis to access the data let's take a look at AWS for example AWS provides cloudwatch as its native observability solution the advantage of using cloudwatch is that there is no instrumentation work required metrics and logs and collectors and logs are collected by default and tracing is enabled at the flip of a switch along with that comes a managed data pipeline that you don't have to set up or scale however despite being easy to set up there are a few disadvantages billing can be unpredictable and confusing due to a variety of different features such as queries dashboards alerts which are all charged separately this results in the amount you ultimately owe to quickly become difficult to predict and track cloudwatch also offers no Global overview of your Telemetry data with each Telemetry type being separated into its own dashboard a key part in being able to increase observability is to be able to make associations between different types of telemetry data to see the state of the service via metrics then flip to traces to see its effects on requests using cloudwatch means you have to switch between dashboards to do this adding unnecessary friction finally using cloudwatch also means your observability data is stuck within AWS this is problematic if the system also contains non-aws hosted Services as observability data across the whole system cannot be cohesively accessed from a single source to get a picture of your overall system Health having one solution that is able to incorporate your entire architecture is of high importance software as a service or SAS Solutions are the next logical choice to explore there are many vendors available as you can see from the logos on the right they range from serverless only to full stack observability Solutions as with fast provider Solutions SAS Solutions have easy automated setup and data pipelines which are managed by the service SAS Solutions also often provide extensive battle tested feature sets which may go well beyond serverless functions all of this is contained in a generally effective and appealing Central UI however similarly to fast Solutions SAS Solutions also come with high unpredictable costs which may take up 30 to 50 percent of your total infrastructure costs and problems of data control and ownership still remain as observability data is all sent and stored with third party creating vendor lock-in if you needed to switch observability vendors down the road it's likely to be a challenging and tedious process a do-it-yourself solution may alleviate many of the issues seen with a SAS provider or a fast specific option hi and unpredictable costs in using the system and vendor lock-in issues can be mitigated by the use of free to use open source tools but while a DIY Solutions provide users with control freedom and flexibility the work required to set up a solution is great considerable prior research needs to be done to understand serverless observability how all the components of a Telemetry system need to interact and the options available for use at each step documentation may be confusing or non-existent adding additional frustration and once research is complete and components are chosen putting it together and tooting it to suit your needs may be additional weeks or months of work so then what alternatives do we have I'd like to hand it off to Marcus to talk about Firefly thanks Judah so as Junior has mentioned there are quite a few options when it comes to observing serverless functions so where exactly does Firefly fit into this picture well firstly it's an open source observability framework for serverless functions but what exactly does that mean in short it means that it can flag key insights into serverless function Health through the use of metrics and traces it does this by automating function instrumentation as well as deploying a Telemetry pipeline once you've deployed Firefly you'll then be able to see all your data in a number of easy to use dashboards so with that in mind let's move on to what the goals were for the project there'll be two main goals for the project and the first was to make sure it was easy to deploy and use we wanted users to be able to get up and running quickly and have no issues with finding the key information required to track down problems secondly we want to Firefly to be vendor agnostic so for example if a user wanted to stream their data to a different observability system they could so what specifically does fireflake focus on well the first to do is built for serverless functions Firefly isn't intended to be a complete observability tool covering your entire stack the second Focus was narrowing the scope to AWS Lambda functions as you probably know AWS is the largest share of the cloud market so there's a good amount of developers who are already familiar with the platform and finally the last focus of the project is concentrating on metrics and traces not logs so if you wanted to start using Firefly it's just a two-step process the first is deploying the Telemetry pipeline which is done by a docker and the second step is running through fireflies command line interface this ensures your functions are instrumented so they can start generating and emitting inflammatory data so once these steps are complete you'll then be able to view the data in fireflies you like we end up using grafana as our monitoring and visualization tool and then build a number of dashboards for serverless functions on top so when you load the UI you'll be greeted with the main dashboard which provides a general overview of all your serverless functions as you can see we have graphs that display each of your functions key metrics as well as a table below highlighting the same date and statistic form when you want to dive deeper into specific function you can simply click it from the function list this brings up the function dashboard which includes additional key metrics and Trace data so along the top you can see staffer implications cold start errors duration and memory utilization and then relevant graphs to those metrics and others as you scroll down has reached the bottom you'll also be able to see all the individual traces for that function this gives you a quick overview of the trace duration but also allows you to see if there is exception on the right hand side of the table if we then click on an individual Trace you'll be taken to the trace View within the trace view you get a much finer overview of the specific Trace as you can see exactly which services are calling which and the associated latency it's a great way to see which services are underperforming in a quick glance now if we then click on an individual span you get even more detail so as you can see the data is a lot more granular you're able to see specific attributes of that request as well as showing any relevant of that information so now that we've seen how Firefly can be deployed and utilized I'll run through the different components that make up its architecture Telemetry systems such as Firefly are made up of three main phases emitting shipping and presentation the first phase is the emitting phase and its purpose to collect an emit Telemetry data and it does this through the instrumentation of application code the second phase is the shipping phase his job is to receive the data sent from the emitting phase process it and then store it within a database the third and final phase of the presentation phase and its all purpose is to retrieve the data stored in the shipping phase and then display it to the user so let's run through each phase in a bit more detail starting with the emit phase you can see the Lambda function at the top left of this diagram now we need to be able to get Telemetry data out of the function this is done in two parts the first part is Trace emission and the second part is metric emission so let's start with trade submission first we hand them a trace Mission through a two-stroke process the first step is instrumenting the serverless function so spans can be created and emitted the second step of the process is making sure spans are assign the correct context when the function is set an event from an upstream servers otherwise known as context propagation now for the first step to be accomplished we need a way to instrument the function and for that we use open telemetry open Telemetry is an open source tool chain that can be used to generate collect and Export Telemetry data it was created due to all the observability providers having a different way of instrumenting application code which meant switching providers was difficult so all the beliefs decided that an industry standard which industry standard was required which thankfully opens Laboratories now become so because one of our main goals was Firefly to be vendor agnostic it made sense to incorporate open Telemetry from the outside now the key component we use from open Telemetry is the collector it allows us to receive process and Export Telemetry data in numerous different formats which is perfect for our use case as you'll soon see the diagram here just shows a high level overview of how an open Telemetry collector is composed so received Telemetry data processes it and exports it to a given endpoint we actually use the collect in two locations within fireflies architecture the first of which you can see on the left of this diagram attach the Lambda function labeled as AWS distro form and telemetry the destroys AWS own version of The Collector that includes additional code for auto instrumentation and this is the key Point as it allows the lamina function to generate and emit spans now it's worth mentioning how the AWS distro is added to the Lambda function as it's also important for the second step in the trace emission process in AWS terminology it's referred to as a Lambda layer and it's just a way to package code that you can use within your Lambda functions they can contain a number of different things such as libraries custom runtimes data or even configuration files so now we know what a Lambda layer is this leads us on to the second step of the process let's just quickly remind ourselves exactly what that is it's ensuring spans are assigned the correct context when they are set an event from an upstream service so we're then able to join these fans together in the presentation phase to form a distributed to form a distribute Trace unfortunately this wasn't handled by the AWS Digital Phone telemetry so we decided to build our own layer which you can see just above the awf distro the Firefly label layer is what enables us to handle contact propagation so when an event hits the function the correct contact from the Upstream service is extracted by this layer and then insert it if I'm going to a downstream service don't worry if this is a little confusing now as well as to go and discuss this in Greater detail in the challenges section so you can view the layers of former middleware they do all the relevant work to the incoming event and allow Firefly to properly handle trade submission let's now have a look at metric mission metric emission is handled by three services and cloudwatch the cloud launch matching stream and the Kinesis data fire hose so let's zoom into that process AWS lab does automatically report metrics to Amazon cloudwatch this is Handy for us as it allows us to extract our metric data from cloud Arch now the way in which we do this is through the cloudwatch metric stream when cloudwatch receives a metric update the metric stream kicks into action and emits the relevant data to a given location that location of Firefly's use case is the Kinesis data fire hose the fire hose receive stream data transforms it and then sends it to a given endpoint not too dissimilar for what the open Telemetry collector does using the file s we can then direct the Telemetry data to the shipping phase and that's it for Metro commission so let's move on to the shipping phase now as I mentioned earlier we use two open Telemetry collectors with an architecture the second of which you can now see at the entrance to the shipping phase it's Firefly's own build of your own Telemetry collector which acts as a gain weight of the sugar phase the reason for using a certain collector is because we have two different emit sources for traces the AWS distro form of telemetry and for metrics the AWS fire hose this means the data is in different formats when it arrives at the Gateway collector not only that but metrics also have to be set to prompt scale they connected for our database in a different format again so if we now visualize oxide and collected you can see we receive Trace data and open Telemetry format a metric data in Json format this data is then processed and explored in open Telemetry format for traces and Prometheus format for metrics this enables prom scale to receive the Telemetry data in the formats it works with and then structure it for entry into the database once the data is stored it's then ready to be queried by the presentation phase the presentation phase is 1A and that's to retrieve and display Telemetry data our dashboard send queries to problem scale to retrieve the necessary Telemetry data and then present it to the user so pulling this all together we have the emit stage on the left which enables trace and Metric emission from the Lambda function in the middle we have the shipping phase which receives the data from the emit phase processes it and then stores it within time series database and on the right we have the presentation phase which as just discussed then retrieves its language to data from the database and presents it to the user so hopefully you've now got a bit of a better grasp on fireflies architecture with that said I'll now hand over to will who will discuss some of the technical challenges we faced thanks Marcus I'll be going over some of the technical challenges we face while developing Firefly namely distributed tracing and context propagation distributed tracing is the act of tracking a request across a system context propagation is the means by which this is achieved as a request goes from service to service a context object is passed along with it this passing of context can be performed using dedicated HTTP headers embedded in the request those headers would contain information that includes a trace ID and a parent spans ID then Services Downstream would be able to parse the information and assign the correct trace and hierarchy as it generates spans there are several different protocols for context propagation and each protocol has its own convention for headers and formatting for Associated values Services communicating with each other have to ensure that they share the same protocol in order for propagation to be successful so now I'll go through how this should work in Firefly the AWS distro for open Telemetry or a DOT layer acts as a middleware for the Lambda it handles tracing and context propagation using open Telemetry API components the layer should parse context information from incoming requests it should create spans with the appropriate context and then it should inject context into outgoing requests but this is not what happens what actually happens is the AWS lambdas and the a DOT layer default to using AWS specific headers to propagate context these headers are not supported by open telemetry when using the ADOT layer our lambdas can only parse open Telemetry supported contacts formats and they can only inject AWS formatted context to outgoing requests so when the ADOT layer sends these AWS headers Downstream and the downstream service is instrumented by open telemetry it can't parse the headers and we lose context but all is not lost instead of using the default context propagation method ADOT allows us to switch to other methods of context propagation and we switched to w3c trace context propagation method which is supported by open telemetry so the w3c propagation method uses a header called Trace parent that header contains the trace ID and parent spans ID that allow us to pass context by switching propagation methods open telemetries context propagation sorry by switching propagation methods open telemetries context propagation API on the ADOT layer is able to look for the presence of the trace parent header it can then parse context appropriately and inject a new transparent header for outgoing requests so we've solved this problem but we're not out of the woods yet while open Telemetry is context propagation API has no problem finding the trace parent header when a requested follows the standard HTTP format such as functions invoked through an API Gateway or via a URL this is not the typical way that serverless functions are invoked we tend to think of serverless functions in terms of events and not requests sqs and SNS messages are the most popular methods of invoking lambdas these more popular methods still have some issues we need to solve in addition functions invoked via AWS SDK also need to work in the case of sqs SNS events the trace parent header is injected in a non-standard location so it has the header but the a DOT layer can't locate that header and we'll assume that it's missing it will use a new context when it creates a span this new context will have a new Trace ID and we lose context we solve this by wrapping the function and intercepting incoming events when the trace parent header is parsed the span is created by our instrumentation and reassigned to the correct context and then fireflies the Lambda layer invokes the function so we've solved our context propagation issue with our lambdas invoked via sqs and SNS events now let's move on to AWS SDK invoked functions so the problem with functions invoked via AWS SDK is slightly different the open Telemetry propagators are able to inject the trace parent header into the request but the invoked function doesn't receive the full request only a designated payload and that payload doesn't contain the trace parent header which means we lose context so we solved this by creating a wrapper function for AWS SDK Lambda invoke functions the wrapper injects the trace parent header into the payload which allows context to be parsed properly so that about wraps up our context propagation issues and I'll just mention a few things that we could improve in the future in a firefly so Firefly currently works well for AWS Lambda functions using node.js but we'd like to extend our language report to python Java and go we'd like to provide observability solutions for serverless function providers in addition to AWS so providers like Microsoft Google cloud and Cloud Fair and we'd like to implement metric emission via Firefly's Lambda layer once open to launcheries Lambda layer support matures okay so now we'll move on to the Q a feel free to ask us questions and we'll do our best to answer them looks like we've got one question just in the uh in the zoom chat and that's uh what was the hardest part of the project you want to take that one Marcus yeah there was uh there was a number of number of different parts to be honest with it proved quite challenging um as they popped up the context propagation was um was definitely one of them um but to be honest with you probably uh one of the things that we had to deal with initially was just the the sheer amount of uh different sort of Open Source solutions that could potentially help us with our problems and having to actually narrow that down so that we could use in a in a in a manageable architecture and you know the amount of documentation that you have to go through and it being very really quite sparse um and unsupported in many ways we often were cropping Acro coming across uh gear to GitHub issues for for the um projects that we're using such as open Telemetry that had been opened like last month for example and we were coming across these problems quite regularly so that was also another uh another part of it but yeah among many others so thanks for the question that was a good one okay so we have another question what's been each of your favorite Parts about building Firefly um and I guess my answer maybe is a cop-out I'm not sure but I just uh really enjoyed like learning everything about context propagation and how to open Telemetry and just how to connect all the pieces together it was very enjoyable for me I can't on to that um I think one of my favorite Parts was like letting go and trying things out trying things out getting your hands dirty um I I there was a point where we were trying to extract metric data and uh like Marcus went in one route and then Julia will and I went another route and just like trying things out reading through documentation trial and error I think that was my favorite part um yeah I don't know if anybody else wants to share I did not enjoy reading the documentation but yeah I'm weird yeah I think my favorite part was just uh that data we first got everything working uh in uh in the system and we started managing to get uh Telemetry data coming through and being able to see it that was yeah very gratifying to say the least yeah I would agree with all of the above I think overall um kind of like stumbling upon this problem domain and then realizing that there is a project to be had here just based on all of our experiences on trying to put together all of the open source tools and um setting up the instrumentation and whatnot um yeah that I think that probably was one of my favorite feelings because I was like oh finally we're doing what actual Engineers have problems with so that was very cool oh okay it looks like we have another question um and then chat um talking about how we use Amazon Cloud watch for the metric stream um so with accessing cloudwatch be a firefly avoid the cost of things like the bass Solutions with their unpredictable costs uh Julia do you want to take this yeah um yeah that's a really good point and that's you and I for detail so yeah ultimately I I think as will has mentioned our future one of our future works is to actually use our Firefly layer for the metrics instrumentation so we don't have to go through cloudwatch um but until we got got to that point our goal was to get metrics in any way possible and so we did decide getting it uh trying to pull it from cloudwatch was the best option um there are still some nominal costs involved but because we're not using all the fuel full feature sets of cloudwatch it's not as high as still using like Cloud watch built-in and um yeah when we we justify that also with the fact that we do provide a much more comprehensive UI I believe compared to what's like natively available in cloudwatch to use so some costs but not all the costs of using the full out cloudwatch solution okay it's a another question a great logo thank you uh did you happen to get a chance to try some alternatives to prom scale or was it perfectly suited for your use case right out of the box um we actually I think we talked about using other Solutions but we kind of we did a lot of research but um Brown scale kind of fit what we were trying to do I don't know if you want to elaborate anymore on that Marcus but yeah we looked at we looked at elasticsearch and time stream um the likes of Victoria Matrix as well um one of the things we found was they were they were very good for that particular Niche but the problem was they were just serving either metrics or traces and then we went once we came across prom scale it just worked perfectly for our use case um as the question says it did just uh seem to fit fit in perfectly so um we did we did a comparison and then once we got prom together working it seemed to it seemed to be the best fit so being able to solve both metrics and traces so yeah and next question did you create documentation for Firefly alongside developing the program or is it easier to put together after Pro after the project is fully functional um I think our project was I mean we were changing so much as we went or that we didn't really make documentation alongside it I would imagine that like a a bigger project or I mean depending on the project you'd you might want to create documentation as you build pieces to it but our I mean yeah it was just we were we kept changing you know the pieces and trying to fit in the best pieces to our overall architecture so we didn't create documentation alongside it yeah there was definitely a lot of a lot a lot of learning and just putting in the code and seeing if it works um configuration I think we went through a lot of configuration uh so for us I think it would make more sense to do it after just because of all the changes um of implement implementation that we went through so good question though uh so I guess that's it I want to thank everyone for attending 