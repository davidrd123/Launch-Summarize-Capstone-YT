Welcome and thank you for joining us. My name is Carolina, and I'm here with my colleagues Julia, Marcus, and Will. Today, we will be presenting Firefly, an open-source observability framework for serverless functions. Firefly provides key insights into serverless function health through the use of metrics and traces. In this presentation, I will introduce foundational concepts, Julia will discuss serverless observability and its challenges, Marcus will introduce Firefly and its architecture, and Will will talk about the challenges encountered and future work for Firefly.

Let's start by understanding microservices. Microservices are a distributed architectural approach where a single application consists of loosely coupled, independently deployable, and scalable services. Each service has a defined role, executes its own processes, and communicates via an API. With the rise of cloud compute, microservices have become common and provide an alternative to monolithic architectures.

Now, let's delve into serverless. Serverless is a development approach that allows for creating and deploying applications without worrying about the underlying infrastructure. The control and provisioning of servers are handed off to a third-party cloud provider. Developers only need to provide application code, which is packaged and deployed by the third-party provider. A microservices architecture can include services deployed on their own servers as well as services that are serverless. Serverless functions are small pieces of code designed to be triggered asynchronously based on events.

Functions as a Service (FaaS) is a cloud-based service that hosts and manages serverless functions. To deploy a serverless function to a FaaS provider, developers write code fulfilling a specific purpose in the application and define an event that triggers the function. The FaaS provider starts a new instance of the function when the event is triggered, performs the work, and returns the result. The provider automatically provisions computation resources and scales instances in response to demand, eliminating the need for manual management. This compute-per-use model allows developers to save time and be free from server maintenance.

However, compared to applications running on traditional servers, collecting data about serverless functions is challenging. Access to and modification of the cloud provider's environment and infrastructure are restricted, making it difficult for developers to gather data. Different strategies are required for observability, which we will explore further.

Observability, as defined by Honeycomb's O'Reilly book, is the measure of how well we understand and explain any state our application can get into. It focuses on generating telemetry data, which provides insights into the internal state of the system. Telemetry data comprises three types: metrics, traces, and logs.

Metrics are quantifiable measurements that reflect the health of infrastructure over a defined period. Firefly's dashboard showcases key metrics for serverless functions, such as invocations, errors, and duration. While metrics indicate issues and patterns, they are not sufficient for pinpointing the exact source of a problem within a program.

Logs are time-stamped messages that record specific events in the request lifecycle. They provide granular insights that metrics cannot offer. However, logs can be challenging to manage due to their high volume. They are useful for tracking a request through its lifecycle and provide greater granularity when needed.

Traces are visual representations of distributed requests as they traverse different parts of an application. Distributed tracing is the process of tracking a single request's journey through an application. Each component involved in the request is called a span. A trace consists of multiple spans, where each span represents a unit of work and the time taken to complete that work. By breaking down each step in a given request, tracing simplifies debugging in distributed systems.

To ensure successful distributed tracing, context propagation is crucial. Context propagation involves passing shared state, called contexts, throughout the system, allowing spans to be linked into a single trace. This mechanism enables distributed tracing across multiple services and makes debugging and issue resolution easier.

Now, Julia will expand on why serverless observability is important and the challenges associated with it.

Serverless observability is essential for overall system observability and optimization. It involves understanding the state and behavior of serverless functions to identify and resolve issues effectively. However, observability in serverless presents challenges in instrumentation and context propagation.

Instrumentation, the process of adding code to a system to generate telemetry data, is problematic in serverless environments. Traditional agent-based approaches are ineffective as cloud providers manage the underlying infrastructure and function deployment. Serverless functions are also stateless and ephemeral, which poses challenges in persistent storage and data loss prevention.

Context propagation is another challenge in serverless observability. For microservices that interact in a traditional request-response cycle, passing context information between services is well-defined. However, in serverless functions invoked asynchronously, this relationship is less clear, making context propagation more complex.

Fortunately, there are solutions available for serverless observability. Fast providers often offer vendor-specific solutions with their own conventions, dashboards, and APIs. For example, AWS provides CloudWatch as its observability solution, which requires minimal setup. However, using CloudWatch comes with disadvantages such as unpredictable billing, separate dashboards for different telemetry types, and lack of cohesiveness in accessing observability data across multiple hosting ecosystems.

Software as a Service (SaaS) solutions also exist, offering more comprehensive observability across different hosting environments. However, they may come with their own limitations or challenges. Alternatively, developers can build their own observability solutions, but this requires more effort and expertise.

In conclusion, serverless observability is crucial for understanding and optimizing serverless functions. However, challenges in instrumentation and context propagation make it complex. Various solutions are available, including vendor-specific solutions, SaaS offerings, or building custom solutions. It is essential to choose a solution that provides comprehensive observability and allows for a global overview of the entire system. In serverless functions, communication between the client and the service occurs through an HTTP request. The service processes the request and sends a response to the client. This communication follows a well-established standard. However, for serverless functions invoked asynchronously via events, the communication is more complex.

When a client sends a request, an event is generated and placed in an event queue. Functions that subscribe to the queue are then invoked and process the events one by one. However, any additional data that was sent with the initial request may not be attached to the event and accessible by downstream functions. The mechanism for ensuring this data propagation is vendor-specific and depends on the chosen serverless provider.

In order for the communication between two services to be successful, both sides must agree upon a convention. This becomes even more crucial for systems that span multiple hosting ecosystems, where the convention needs to be vendor agnostic.

Observability is a critical aspect of serverless systems for overall system optimization. However, achieving observability is challenging due to limitations in installation methods and context propagation. Fortunately, there are available solutions that can help increase observability for serverless functions.

Developers looking to improve the observability of their serverless functions have several options. They can use vendor-specific solutions provided by their serverless provider, opt for software as a service (SaaS) solutions, or develop their own solution. Each option has its advantages and disadvantages.

For example, AWS provides CloudWatch as its native observability solution. Setting up CloudWatch is easy, and metrics and logs are collected by default. However, billing can be unpredictable and confusing due to separate charges for queries, dashboards, and alerts. Additionally, CloudWatch separates Telemetry data into different dashboards, which hinders the ability to make associations between different types of data.

Using CloudWatch also limits observability data to AWS, which is problematic for systems involving non-AWS hosted services. The lack of a single source for accessing observability data across the entire system makes it difficult to assess the overall system health accurately.

Alternatively, developers can explore SaaS solutions offered by various vendors. These solutions provide easy automated setup and managed data pipelines. They often offer extensive feature sets beyond serverless functions, making them appealing for monitoring the entire system. However, SaaS solutions come with high, unpredictable costs and potential data control and ownership issues.

Another option is to create a do-it-yourself (DIY) solution using free and open-source tools. DIY solutions provide users with control, freedom, and flexibility. However, they require significant research to understand serverless observability and the available components and options. Documentation for DIY solutions can be confusing or nonexistent, making implementation time-consuming.

Given the challenges and limitations of existing options, an open-source observability framework called Firefly offers an alternative solution. Firefly is specifically designed for serverless functions and focuses on metrics and traces, excluding logs. It aims to be easy to deploy and use while remaining vendor agnostic.

Deploying Firefly involves two simple steps: setting up the Telemetry pipeline using Docker and running Firefly's command-line interface. Once deployed, Firefly provides easy-to-use dashboards for visualizing data. Firefly utilizes Grafana as its monitoring and visualization tool, allowing users to view metrics and traces of their serverless functions.

Firefly operates within a three-phase architecture: emitting, shipping, and presentation. In the emitting phase, Telemetry data is collected and emitted, including both trace and metric data. To achieve this, Firefly incorporates the open-source tool OpenTelemetry, specifically its collector component. The collector receives, processes, and exports the Telemetry data in various formats.

Within the emitting phase, trace emission involves instrumenting the serverless function to generate and emit spans. AWS Distro for OpenTelemetry is used for this purpose. Context propagation ensures that spans are assigned the correct context when events are received from upstream services.

Metric emission is handled by three services: AWS Lambda, CloudWatch Metric Stream, and Kinesis Data Firehose. AWS Lambda automatically reports metrics to CloudWatch, which is then streamed to Kinesis Data Firehose. The firehose receives, transforms, and forwards the metric data to the shipping phase.

The shipping phase involves a custom-built telemetry collector within Firefly. This collector acts as a gateway and processes the received Telemetry data in its various formats, including open Telemetry format for traces and Prometheus format for metrics. The processed data is then stored in a database to be presented to the user.

By following this architecture, Firefly enables developers to easily observe the health of their serverless functions. The Firefly UI provides a main dashboard for a general overview of the serverless functions, displaying key metrics and statistics. Clicking on a specific function reveals its dedicated dashboard, presenting additional metrics and trace data. The trace view provides a detailed overview of the associated services and latency, allowing users to identify underperforming services. Further inspection of individual spans provides granular information about requests and any relevant associated data.

In conclusion, Firefly offers an open-source observability framework for serverless functions. It simplifies the deployment process, supports various serverless providers, and provides easy-to-use dashboards for visualizing metrics and traces. Firefly aims to be vendor agnostic, allowing users the flexibility to switch observability systems if desired. In the second step of the process, we need to ensure that spans are assigned the correct context when they are set as events from an upstream service. This is crucial for joining spans together in the presentation phase to form a distributed trace. Unfortunately, this functionality was not handled by the AWS Digital Phone telemetry, so we decided to build our own layer. This layer, called the Firefly label layer, allows us to handle context propagation. When an event hits the function, this layer extracts the correct context from the upstream service and inserts it into the downstream service. I will discuss this in more detail later in the challenges section.

Now let's move on to metric emission. Metric emission is handled by three services: CloudWatch, the CloudWatch metric stream, and the Kinesis data firehose. AWS Lambda automatically reports metrics to Amazon CloudWatch, which is convenient for us as it allows us to extract our metric data from CloudWatch. We achieve this through the use of the CloudWatch metric stream. When CloudWatch receives a metric update, the metric stream kicks into action and sends the relevant data to a given location. In Firefly's case, the location is the Kinesis data firehose. The firehose receives the stream data, transforms it, and sends it to a designated endpoint, similar to what the OpenTelemetry collector does using the filehose. This allows us to direct the telemetry data to the shipping phase.

Moving on to the shipping phase, we use two OpenTelemetry collectors in the architecture. The second collector, which you can see at the entrance to the shipping phase, is Firefly's own build of the telemetry collector. This collector acts as a gateway to the sugar phase. We use this collector because we have two different sources for traces: the AWS Distributed Telemetry and the AWS firehose for metrics. These two sources provide data in different formats when it arrives at the gateway collector. Trace data is received in OpenTelemetry format, while metric data is received in JSON format. This data is then processed and transformed into OpenTelemetry format for traces and Prometheus format for metrics. Prometheus can then receive the telemetry data in the formats it works with and structure it for entry into the database. Once the data is stored, it is ready to be queried by the presentation phase.

The presentation phase's main objective is to retrieve and display telemetry data. Our dashboard sends queries to Prometheus to retrieve the necessary telemetry data and then presents it to the user. To summarize, we have three main stages in Firefly: the emit stage, the shipping phase, and the presentation phase. The emit stage enables trace and metric emission from the Lambda function. The shipping phase receives the emitted data, processes it, and stores it within a time series database. Finally, the presentation phase retrieves the stored data from the database and presents it to the user.

Now, let's discuss some of the technical challenges we faced while developing Firefly. The first challenge was distributed tracing and context propagation. Distributed tracing involves tracking a request across a system, while context propagation is the means by which this is achieved. As a request moves from service to service, a context object is passed along with it. This context contains information such as a trace ID and a parent span ID. Services downstream can then parse this information and assign the correct trace and hierarchy as they generate spans. There are different protocols for context propagation, each with its own conventions for headers and formatting. Services communicating with each other must ensure they share the same protocol for successful propagation.

In Firefly, our goal was to use the AWS distro for OpenTelemetry (ADOT) layer as middleware for the Lambda functions. This layer handles tracing and context propagation using OpenTelemetry API components. The layer should parse context information from incoming requests, create spans with the appropriate context, and inject context into outgoing requests. However, the ADOT layer defaults to using AWS-specific headers for context propagation, which are not supported by OpenTelemetry. The default context propagation method resulted in the loss of context when the ADOT layer sent AWS headers downstream to services implemented with OpenTelemetry.

To mitigate this issue, we switched to the W3C trace context propagation method, which is supported by OpenTelemetry. This method uses a header called Trace Parent, which contains the trace ID and parent span ID. By switching propagation methods, the ADOT layer's context propagation API can parse the trace parent header, inject a new transparent header for outgoing requests, and maintain context properly. However, we encountered some issues with serverless function invocation methods such as SQS and SNS events, as well as AWS SDK-invoked functions. In these cases, the trace parent header was either missing or in a non-standard location, causing a loss of context.

To solve these challenges, we created a wrapper function to intercept incoming events and properly parse the trace parent header. For functions invoked via AWS SDK, we created a wrapper function to inject the trace parent header into the payload. These wrapper functions ensure that the context is maintained, allowing us to overcome the context propagation issues in Firefly.

While developing Firefly, we did not create documentation alongside the program. As we were continuously iterating and making changes, it would have been impractical to maintain documentation in parallel. However, creating documentation after the project is fully functional would be easier since we would have a better understanding of the final implementation and can provide comprehensive documentation based on that. During the project, we set up the instrumentation, which was one of my favorite moments. Finally, we were able to tackle real engineering problems, and it felt great. Moving on, there's a question in the chat about how we utilized Amazon CloudWatch for metric streaming. The question is whether accessing CloudWatch can help avoid the unpredictable costs associated with things like the base solutions. Julia, would you like to address this?

Certainly, that's a valid point. As Will mentioned earlier, one of our future goals is to use our Firefly layer for metric instrumentation, which would eliminate the need for CloudWatch. However, until we reach that point, our aim was to obtain metrics in whatever way possible. Therefore, we decided to pull them from CloudWatch, as it seemed like the best option. There are still some costs involved, but since we're not using the full feature sets of CloudWatch, the expenses are not as high as using CloudWatch built-in. Additionally, we justified this decision by considering the fact that our UI provides a more comprehensive experience compared to what's available natively in CloudWatch.

Another question has come up, complimenting our logo design. Thank you for that. The question is whether we looked into alternatives to Promscale, or if it was the perfect fit for our use case right away. Marcus, do you have any further insights on this?

Indeed, during our research, we did explore other solutions such as Elasticsearch, TimescaleDB, and VictoriaMetrics. While those options were excellent for their specific niches, the challenge was that they focused either on metrics or traces, but not both. However, when we discovered Promscale, it turned out to be the perfect fit for our use case. It seamlessly addressed both metrics and traces, making it the best choice for us after a thorough comparison.

Another question is related to documentation for Firefly. Did we create documentation alongside the development process, or is it easier to put it together once the project is fully functional? Considering the constant changes and iterations we made throughout the project, we didn't create documentation alongside its development. In larger projects or depending on the circumstances, it might make sense to create documentation as you build each component. But for us, given the continuous changes and implementations we went through, it seemed more logical to create the documentation afterwards. However, an excellent point and a good question. 

In conclusion, I want to express our gratitude to everyone who attended the session and participated in the discussion. Thank you all for your time and engagement.