foreign thanks for coming everybody today we're going to be covering Armada an open source container orchestration tool that allows administrators to automate the configuration and deployment of developer environments in the cloud our team of four consists of Natalie martos Sergio pachado myself Joey Gilliam and Dean elizardo over the last several months we've been hard at work developing Armada and we're excited to get the chance to tell you about it today we'll be focusing on understanding the problems that our model solves narrowing our Focus to the education space taking a closer look at the process of virtualization examining Cloud environments covering existing Solutions talking through what it was like to create Armada and the challenges that we faced and then lastly we'll highlight some of our plans for our modest future in order to understand the design decisions we've made around our modest features it's important to highlight some of the difficulties that come with setting up developer environments these challenges include configuration overload dependency management and resource availability first let's focus on configuration overload in the world of modern development there's no shortage of productivity enhancing tools such as bundlers Winters and transpilers to accommodate a diverse set of needs these tools typically expose a configuration file allowing their users to fine-tune settings while this may seem ideal it can become quite cumbersome when repeated multiple times across multiple different environments this creates several problems including introducing a web of fragile dependencies increasingly the amount of time it takes to get started on a project and making it much more likely that you'll reach decision fatigue now let's talk about dependency management as we know most applications built today rely on many different packages Frameworks and libraries from reactive Express these tools have greatly accelerated the speed with which an application can be developed on top of that they've also removed a ton of redundancy allowing developers to avoid Reinventing the wheel however dependencies can introduce their fair share of fragility as well for instance when a new project is created it not only has to contend with the dependencies that it introduces but also all of the dependencies that it's in that its dependencies bring with them this creates opportunities for circular dependencies and increases the likelihood that a single misconfigured or improperly version dependency could break the entire chain lastly let's highlight some of the issues with resource availability depending on the type of work you're doing you may need access to a significant amount of computation such as CPU GPU or Ram tasks that fall under this umbrella typically include creating virtual environments running emulation software for a specific deployment Target or using test Runners and compilers in watch mode these type of tasks dictate the hardware needed for the development of your project and unfortunately Hardware can be especially costly while every developer must base these challenges at some point students often face an even greater burden as they strive to learn and grow instructors too struggle with these challenges as they try to guide and support their students in their Learning Journey here we'll get a glimpse of what it's like to be a new student learning to code a journey that we're all quite familiar with so you want to learn how to code but then all of these questions start popping up like what kind of Peter what kind of computer do I need which text editor should I use what language should I learn how do I configure my editor do I need to learn GitHub am I even installing the right thing all of these questions can get overwhelming quickly putting a major damper on your motivation rather than nope rather than spending time on setup new students should focus on core programming skills limited time and attention make it harder for students to complete tasks if they are already exhausted before even starting as an instructor troubleshooting students development environments is often a time-consuming guessing game additionally solving these problems on a student by student basis isn't really efficient in this scenario we've highlighted there are really two viable Solutions the instructor can maintain a document that contains some of the most common issues encountered by students or the instructor can manually intervene working with each student individually but what if an instructor could guarantee that every student had access to the same hardware and software well that would make things considerably less complex but how could we accomplish this instead of buying a new computer for every student the instructor might consider using a technique known as virtualization according to IBM virtualization uses software to create an abstraction layer over computer hardware that allows the hardware elements of a single computer including its processors memory storage and more to be divided into multiple virtual computers commonly known as virtual machines or VMS for short each VM runs its own operating system and behaves like an independent computer even though it's running on just a portion of the actual underlying computer hardware one other important thing to note here is that the server running the virtualized environments is sometimes referred to as a host or host machine one of the key features of virtualization is its ability to create an image a snapshot of a computer that captures its overall state these images can be used as the starting point for a virtual machine allowing you to return a VM to a specific State time and time again with a tool like this instructors could configure a single functional workspace with all the necessary files and software for a given class if updates need to be made all the instructor would have to do is create a new image however there are a few flaws with this approach the type of Hardware used to perform virtualization at this scale can be cost prohibitive managing servers and virtual machines can be Troublesome due to the need for frequent and varied maintenance additionally if you need to add more capacity you'll have to purchase and configure that Hardware yourself and last but not least it introduces a single point of failure meaning that if the server goes down none of the environments will be reachable what would an ideal solution look like well an ideal solution would abstract away the hardware ensuring that we weren't responsible for its management and maintenance it would also ensure that our troubleshooting efforts could be replicated for every student fortunately a solution like this actually exists and it's called the cloud Cloud environments are a type of third-party service that provides on-demand access to networking and computational resources what does this mean for us we no longer need to worry about managing any of our underlying Hardware and if we want to add additional capacity all we have to do is make a request to the cloud provider meaning we can scale Up and Down based on demand so great we've solved all of our problems we can just provide each student with their own virtual machine and move on with our lives well actually it's not that simple providing each student with their own virtual machine would be the easiest approach but it's definitely not the most efficient most of the time students won't use the full capacity of a machine's memory or CPU which means that instructors are paying for more resources than they're actually consuming unless you've got cash to burn this isn't ideal how can we improve we can optimize our virtual environments by using containers without getting too much into the weeds containerization is a lightweight type of virtualization that enables applications to be bundled with their dependencies the optimizations used to create containers enable them to be much smaller in size and more portable in addition they're designed to be ephemeral which is a fancy way of saying that we can create use and discard them without much concern that being said containerization does present challenges regarding data persistence as data is often tied to the life cycle of the container one of the most common ways that we can create containers is with a piece of software known as docker with the docker ecosystem Docker files are used to design images enabling developers to configure how a container is constructed including specifying a base image that can be extended these images are then consumed by the docker engine which is a runtime environment that is responsible for creating containers on your local machine if an instructor wanted to take advantage of both the cloud and containerization on their own there are several steps they'd have to take such as creating or sorry choosing a cloud provider learning about the different services that their cloud provider offers provisioning and connecting all the services needed to create containerized development environments managing security and permissions to limit access and developing a scalable process to create and Destroy environments fortunately there are several commercially available products that achieve these goals without requiring the instructor to be a cloud expert next up Natalie is going to talk to you about some of the existing Solutions within this space thanks Joey there are a few existing solutions that provide pre-configured developer environments and three of the most popular ones are coder gitpod and GitHub code spaces we looked at a few criteria When comparing gitpod coder and GitHub code spaces the first criteria we looked at was how easy is the product to set up next we looked at whether the product is easy to use we also looked at whether or not the product supports different integrated development environments other than vs code another criteria was custom templates whether or not users can customize the base environment to meet their needs several of the solutions are cell coastable so we looked at that as well and finally we considered cost the first solution we looked at was coder coder is moderately priced and offers all of the core functionality of an integrated development environment but it can be difficult to set up next we looked at gitpod gitpod has extensive features but unfortunately has a slow startup time like code or gitpod isn't super easy to set up or install and it can be overwhelming at first gitpod is available for self-hosting but the docker image is no longer actively maintained gitpod's cost is also about the same as coders GitHub code spaces was the last solution we looked into code spaces integrates easily with the rest of the GitHub ecosystem and it's also a turnkey experience your code is automatically imported into the browser-based IDE without having to perform much configuration some of the downsides of code spaces are that it only offers vs codes it's not self-hostable and it's also the most expensive option on the list after looking at these options we came up with our own solution our motto we spent some time reviewing coder gitpod and GitHub code spaces and we realize that all of them are aimed at working software developers and not people learning how to code for the first time or instructors teaching software engineering on top of that the current Solutions require some level of background knowledge so they can be difficult to get started with with that in mind we built Armada to provide an easy to use low-cost option that was made exclusively for the education space so to recap here are some of the challenges for students learning how to code and for their instructors for students they need to have knowledge of environment configuration and dependency management which can get complicated quickly they also need to have access to the right Hardware then the issue that instructors face is that they need to navigate between all the different Hardware configurations software and experience levels of each one of their students which is difficult time consuming and frustrating after considering all the challenges that students and instructors face when learning to code we developed our motto with the following goals in mind we wanted to create easy to use development environments for students we wanted to make it easy for instructors to manage and deploy those environments we wanted to provide the ability to scale to meet student and instructor demand and finally we wanted to minimize the cost for instructors we're going to run through some demo videos Ramada this first demo video shows an instructor signing into the Armada app you can see when they first sign in they have a dashboard view where they can add or edit new students cohorts courses and workspaces first you can see that I'm adding a student next you see them adding a cohort a cohort is a group of students like in a semester at school and instructors can add students to a cohort and a course the next part is creating the course as you can see there are places throughout the user interface to edit update and delete and then finally you can see the instructor here creating workspaces for students in a course you can also see that once the workspace is created the instructor can use buttons to start and stop the workspace they can also click the preview button to actually View and edit our particular student's workspace this next demo video shows a student signing in an access accessing a workspace you can see they have a simplified version of the dashboard with the two buttons to start their workspace and also access a workspace through the resume button once they click the link they are taken to their development environment in the browser where can where they can get access to all their files they created previously and then finally you can see at the bottom of the screen they also have access to the terminal now that we've covered our modest features we'll take you on an in-depth review of our motto's architecture in order to create and manage development environments there are eight Milestones that we needed to accomplish first we had to containerize a workspace then provision Cloud infrastructure then deploy a single workspace followed by deploying several workspaces then we had to find a way to access those workspaces from dedicated URLs our next step was persisting workspace data in other words making sure the students work would be saved next we had to manage that data and Implement a UI and finally we had to provide user Authentication our first goal was to get a working copy of an IDE running in a local container that could be accessed from the browser when building our Mata our Focus was making it easy to create and manage developer environments we weren't actually trying to construct the core development environment ourselves luckily both coder and gitpod offer open source versions of their coding environment and their platform either solution could have been used as the basis for a modest workspaces but there were trade-offs for each the first one we'll discuss is git pod even in its open source form gitpod offers many features for configuration customization and integration but unfortunately this comes at the cost of both speed and size gitpod's container was about seven gigabytes and its typical start time range from 5 to 10 minutes next we looked at coder coder's container called code server was less than a gigabyte and had all the functionality we would need to provide for an idea to students the code server image is both lightweight and fast with load times under 20 seconds in the end we chose to use the coder image as our base since it would allow us to provision more instances of the workspace container per server than the gitpot image and with that decision we had our first step completed we now had containers that would run on our local machines and could be accessed from the browser next we turned our attention to the Second Step running workspaces in the cloud but before we jump into how we accomplish that we'll look at some of the decisions that need to be made when building Cloud applications in order to begin building our Mata we needed to make a key architectural decision when we focus on a single cloud provider Cloud native or ensure that our model Works across all platforms Cloud agnostic a cloud agnostic approach revolves around designing applications that can be deployed to any Cloud environment so your application gains a lot of flexibility this flexibility comes with its downside stuff a lot of work has to be done to adapt the application to run on each Cloud provider's environment so even though the cloud agnostic approach improves flexibility and portability it can also introduce logistical and maintenance issues Cloud native is referring to building an application with technologies that are native to a specific cloud service provider in other words building an application exclusively for AWS Azure or Google Cloud this allows developers to focus on building the application's core features instead of managing other IT services the problem with doing this is that developers become locked into that provider's framework also known as vendor lock-in after considering the advantages and disadvantage of disadvantages of a cloud agnostic versus Cloud native approach we decided to go Cloud native and close our application with AWS AWS helped us focus on development instead of system administration AWS comes with many tools that sped up our development out of the box including the software development kit the SDK and the cloud development kit the cdk the cdk allows developers to create and customize any of aws's products and made it straightforward for us to create share and gradually extend our architecture we'll talk more about the SDK later on with the decision to use AWS along with its cdk and SDK we were now able to provision Cloud infrastructure and were able to cross the second step off our list the next logical step was that we needed a way to deploy and manage a single workspace in the cloud to understand how our containers would interact with their host we first focused on deploying a single container to a server to a server in the cloud aws's elastic Cloud compute ec2 service provides and manages virtual servers that we could use as a host for our workspaces we started by using an ec2 instance and installing Docker on it so we could use the code server image for the workspace then using nginx we were able to make the workspace accessible over the web although this did work there wasn't a good way to automate the process and even worse it wasn't scalable so we needed to find a better solution this led us to looking at container orchestration tools container orchestration is the process of automating the deployment management and scaling of containerized applications we looked at a few open source and Cloud native tools to do the job including kubernetes Docker swarm and Amazon's elastic container service ECS this chart summarizes the differences between kubernetes Docker swarm and ECS on the left we have kubernetes kubernetes is a well-known open source technology with extensive features but all those features also introduce a lot of complexity and take a lot of time to learn in the middle we have Docker storm it isn't as complex as kubernetes and if you're already familiar with Docker it's easier to learn how to use one of the downsides of Docker swarm is that it doesn't offer container logging and health checks so we would have had to implement that ourselves finally on the right we have Amazon's elastic container service or ECS for short ECS is what we ended up using for Armada it's a cloud native tool that has a relatively low learning curve and eliminates the need to manage individual containers ECS integrates with other AWS services and gave us the ability to Auto scale both containers and the hardware required to host those containers also ECS comes with container monitoring logging and the ability to automatically replace containers that have crashed in order to understand the way armada's workspaces eventually functioned within ECS it's important to understand some key terminology that ECS uses the ECS provides the ability to create containers with what's called a task definition task definitions are similar to Docker files or Docker compose files they specify what images are used in the ECS containers and are also used to configure Port mapping volumes and memory usage within the containers after a container or a set of containers has been has been defined with a task definition ECS all allows you to run these containers in what they call a task a group of these tasks running together is called a service and then finally a cluster is the group of tasks or services that together Encompass an entire application and with that I'm going to hand it off to Sergio who will be further explaining our path towards deploying a single workspace thanks Natalie in its initial form Armada use ECS Dasa finishes and tasks to create containers that could be accessed over the internet this allowed us to deploy and run our developer environments by making HTTP requests to the AWS elastic container service it would look something like this first we deploy the test definition using the ECS SDK then the ECS service would use the test of ignition as a blueprint to create a new task running a code server container and finally the task will be accessible over the internet via an IP address and Port combination although we were able to run individual workspaces as tasks tasks did not give us enough control over reliability and scalability of our containers so far we've seen how we were able to get started by deploying simple ECS tasks now let's take a look at how we conquer our next Milestone and we're able to launch multiple workspaces working with plain tasks created two main issues for us one if a failure occurred it would stop our workspaces and we have to manually restart them again two if we wanted to deploy another workspace container we'd have to know an available host Port beforehand as you can imagine this was not sustainable therefore we began searching for a solution that would allow us to manage multiple workspaces with scalability and reliability in mind if we wanted to operate our Mata as a fully fledged product we needed to be able to accomplish a couple things one serve an arbitrary number of users simultaneously and to ensure that workspaces would recover automatically in case of failure eventually we concluded that we need to use ECS services to fulfill our mother's requirements ECS Services provide a straightforward way of automating monitoring and managing tasks in the context of formata we only needed to run one test per service for each student's workspace using ECS Services enabled us to start stop or scale individual tasks by manipulating the desired task count in the test definition with services in hand we were able to pave our way to more reliable and deterministic workspace deployments another important aspect of ECS is how easy it is to configure health checks ECS performs health checks by sending several requests to a predefined URL which responds with a 200 status code if the container is healthy if a container is deemed unhealthy ECS will automatically destroy it and replace you with a healthy one now with services in our pocket we needed to address an underlying issue in the way that our compute infrastructure was being provisioned although ECS manages Individual Services and tasks the actual containers run on individual ec2 instances which have a finite amount of available resources most importantly memory locally AWS provides a service for creating a pool of resources that can scale horizontally with demand known as Auto scaling groups we Define another scaling rule for ec2 instances hosting our workspace containers and configured it to add or remote capacity based on the amount of memory reserved for each individual service as you'll see in the following animation when an ec2 instance reaches a predetermined resource utilization threshold the auto scaling group will notice that changing state in the cluster and will automatically provision a new ec2 instance ECS service scaling on the other hand is a bit simpler essentially we're trying to pack as many services as we can on an ec2 instant that has the most available CPU or memory let's take a look foreign will find the instance with the most available resources and places Services there doing this allows today to reduce unnecessary expenditures by minimizing the number of instances we had in use so far we've learned how we deploy multiple workspaces and how we manage to scale resources in our cluster however in order to fully take advantage of scaling we first needed to take a step back and focus on routing so that students would be able to connect to their workspaces finally foreign with workspaces now running as Services we were able to easily turn them on and off as well as manage unexpected failures automatically but it's important to emphasize that that ECS services offered one very important benefit scalability up until this point we've been hard-coding the container Parts in our test definitions this allowed us to deploy individual workspaces on separate ec2 instances however this was not an efficient way to use ec2 instances since each ec2 has sufficient resources to host multiple workspaces simultaneously to deploy multiple containers in our ec2 instance we needed to make sure the health scores that our containers use did not conflict next we'll see how we achieve that and briefly explore the distinctions between static and dynamic part mapping when we Define a container we're able to determine which parts on our host operating system map to those in the container itself we can either Define a set port for the container to use which is known as static Port mapping or we can delegate this work and allow the host operating system to automatically choose a random port for us and this is known as Dynamic part mapping as we've learned we previously relied on static for mapping and once again the problem with study for mapping is that we could not deploy containers using a host Port that was already being used by another container therefore in order to temporarily bypass this roadblock we continue tearing down redeploying our infrastructure and spinning out multiple ec2 instances at a time in order to avoid poor conflicts while still searching for a viable way to dynamically deploy multiple workspaces on the same post this is what our process looked like first we need to deploy an ec2 instance then we deploy a container inside of the ec2 instance finally we'd get access to the container if we needed to launch another container we need to repeat the same process all over again we eventually landed on a potential solution to our networking challenges with static load balancing it allowed us to place a load balancer in front of our ECS cluster that would serve as the entry point into our application we selected the application of balancer because it seamlessly integrates with ECS and provides the ability to use Dynamic for mapping now armed with an application load balancer Dynamic Port mapping enabled us to deploy multiple tasks on the same ec2 instances enabling the ALB to use Dynamic formatting was quite easy all we had to do was update our test definitions to indicate that the selection of the host port for each task will be delegated to the application load balancer let's take a look first a test definition is enabled for dynamic Port mapping then the ALB selects a random host Port from the instances ephemeral House Port pool finally a container is deployed on an ec2 instance already running another container even though we were able to evolve from a single task to now being able to launch multiple tasks on the same ec2 instance using Dynamic part mapping we stumble upon a new challenge imposed by the application load balancer typically a load balancer is used to distribute traffic across a fleet of horizontally scale machines however our modest workspaces are stateful each one belongs to a specific student and by default albs use a round-robbing routing algorithm which ensures that traffic is routed to each of the load balancers Targets in a livingly distributed manner this meant that every time the ALB endpoint is hit the incoming requests will be forwarded to a completely different workspace which would result in a student connecting to a random environment instead of the one dedicated to them which definitely wasn't what we intended to overcome the new challenges presented by the load balancers routing algorithm we mapped each individual workspace to its own Target group a Target group is a collection of resources that can receive traffic from a load balancer which let us divide traffic to resources based on a specific path in addition the ALB enabled us to attach a listener that allow us to dictate how the traffic it received will be proxied we accomplished this by introducing an ALB rule each time that the test definition for a workspace is created allowing us to use a consistent naming convention now with the with Target groups configured we could uniquely identify each workspace and Route traffic accordingly let's look at an example when the connect button is clicked a request is trigger and sent to the application load balancer then the up the application of balancer uses the path in the request and matches it against a path pattern included in the ALB listener rules and finally the request is forwarded to the correct Target group I've seen as you have seen throughout the presentation our progress was quite organic first we launched a single task then we'll launch multiple tasks inside of the same ec2 instance they will launch a service and then we overcame the challenges posed by the load balancer by configuring Target groups for each workspace but we still needed to do one more thing and here's what we did in order to map the user from a URL to their workspace each workspace was linked with its own nginx reverse proxy because it enabled us to remove the portion of the URL previously used by the Target group for path pattern matching with this last configuration in place the code server container was now able to match the root path of the request and students were now able to connect to their very own workspace with each of these pieces in place we were able to successfully automate the distribution of our developer environments we could efficiently host multiple environments on a single ec2 instance scaled to more easy to instances if if needed and provide a consistent method for accessing and the correct environment next Dean will tell you all about persisting data thank you Sergio at this point our overarching infrastructure was largely solidified but to provide a reliable way for our users to persist their data independent we needed a way Force users to be able to persist their data independent of the life cycle of their workspaces containers are inherent with stateless so each development environment that exists in a container does not save user data when the container stops running or otherwise shuts down volumes can be used to persist data outside of the container and on the host file system but in the case of armada it wasn't as simple as designating a mounted volume for workspaces we explored two major Avenues to address this issue using elastic block storage EBS in conjunction with S3 buckets or the elastic file system EFS EBS is a service that creates a virtual hard drive that can be mounted to a server and treated as a block device but these EBS volumes are tied to the life cycle of the ec2 instance turning off the ec2 instance would still cause data loss in order to make this process work we would have to do the following first we would query the ECS and find the target container then we would query ECS again to find the target ec2 instance after we've gotten all of the requisite information we would create a new EDS volume and then attach that EBS volume to the specific ec2 instance from our second query once we needed to shut down the workspace we would create a snapshot of the EBS volume preserving changes made to the file system and store that snapshot and an S3 bucket for later retrieval when the student needed to re-instantiate the workspace while this process works it has quite a few problems using EBS and S3 largely prevented us from being able to scale workspaces easily and efficiently EBS mounts directly to a virtual server in much the same way that a USB drive can be plugged into your desktop managing which Mount points would be available for a volume on any given ec2 instance at any given time is extremely complex ultimately we decided to abandon EBS and S3 as a way to save user data between sessions and instead we turned our attention to Amazon's elastic file system EFS EFS is a serverless cloud-native storage service that provides an on-demand scalable file system EFS works by monitoring a specific directory on a Target system and uses serverless functions to mirror changes made to files in that directory on the target system with EFS we could much more readily predict and control how a workspace would connect to its storage solution the EFS proved to be the best fit for our use case to successfully attach EFS within Armada there were two key components that needed to be present first the specific folder or directory being used on EFS must exist within the EFS volume itself before it can be mounted to a given container we needed to tell EFS where within itself the mirror changes from the target needed to be written and second each task definition for a container must specify a mount point for the EFS volume that will be attacked when the container is created enabling the location of the directory to be configured based on the container's needs we have to tell each container where it could find the EFS volume on the ec2 host but to mount that volume and what to call that mounted volume internally we use AWS Lambda to meet these two requirements Lambda enabled us to integrate serverless functions into our workflow which we could then use to create uniquely named directories that would contain the EFS volumes mounted for each individual workspace in our case the Lambda function for creating the EFS Mount point is triggered whenever an instructor creates a new task definition for a specific workspace the directory is created as Mount points for EFS follow the naming Convention of cohort name Dash course name dash student name coder and within the container definitions for each workspace that directory would be mapped to home slash coder here's a diagram of where Lambda fits into our overall architecture as you can see Lambda communicates directly with the elastic file system so that it can create the directory where changes from the target systems will be written okay to sum up the steps taken to establish a persistent storage solution for a new workspace are as follows first we create a task definition referencing a volume that's a uniquely named directory on EFS then using Lambda we create the directory within EFS that the task definition references in its EFS volume configuration with the naming Convention of cohort name course name student name and finally we run the task instantiating the workspace with the EFS volume mounted to a container directory called home coder with a data persistent solution in hand we were able to successfully save students work from one session to the next now students can be sure that their data would be there regardless of any interruptions or failures looking forward we needed a way to coordinate and expose all of these established resources to our instructors and students in a straightforward and intuitive way we needed to give instructors and students the ability to manage create maintain and interact with workspaces without having to use the command line or interfaces provided by AWS to do this we began constructing a full stack application that would eventually become the Armada app armada's backend is an express application written in typescript which acts as the central control Hub connecting each component of provisioned architecture to ensure that students can can provide sorry to ensure that instructors can provide and manage student workspaces at scale the application itself is containerized and managed as a service in the ECS cluster Amazon offers a software development kit the SDK that provides libraries for interacting with resources at runtime by wrapping our feature set around the functionality defined within the SDK we were able to automate core workflows and streamline actions that instructors would regularly take for instance when an instructor creates a workspace for a specific student Armada performs the following actions under the hood using the SDK first we create a task definition specific to the student outlining the configuration of the required containers then we create a Target group for the student's workspace and attach a rule to The Listener on the ALB allowing the workspace to receive traffic once the workspace is operation then we create a service specific to the students on ECS allowing its availability and health to be managed and monitored automatically finally we store information relevant to the creation and organization of that data with all of these elements in place we can now serve both types of end users instructors and students instructors can create individual accounts for students organize students into cohorts and use those cohorts to enroll students into individual classes each workspace is automatically associated with a given student and accessible through each student's Student Portal or for the instructor's preview button Armada provides a platform for students to access their workspaces if a student's workspace is in an inactive state or is not yet entered a running State the student can resume their workspace without instructor intervention and most importantly the student can connect to each of their workspaces and access all of their files serving our monitor instructors or workspaces to students required a database within our architecture that would help us manage the relationships between workspaces students courses and cohorts we considered implementing an instance of postgresql on the server that would act as the host for a modest back-end but dismiss this approach as too fragile instead we decided that the database should be implemented on its own server we began to think about using our Amazon's managed solution the relational database service RDS as a managed service RDS would ensure that our data would be automatically backed up and in the event of a crash then RDS would automatically provide a replacement configuring the RDS instance to be accessible to the ec2 host only required us to use a naming convention specified in the cdk's documentation however we still needed a way to initialize the schema of our database and insert our first user we accomplished this by spinning up an additional ec2 instance That Was Then configured to communicate with our RDS instance when deployed this admin node would connect to our RDS instance and take care of all of the initial setup via a script that we wrote to configure postgresql once it does its job the admin node is then spun down and with that our RDS instance is ready to respond to queries from the Armada app at this point we've managed to establish a database that can handle all of the relationships between our end users and their data and we can serve an easy to use front end our last step was to provide a way to authenticate our users we determined that there was no point in Reinventing the wheel and decided to use aws's Cognito to provide authentication and differentiated access to the Armada app Cognito provides applications with authentication authorization and user management when an instructor adds students through the Armada interface this adds a user to Cognito along with their credentials adds the student to the RDS database and sends an email to that student with a temporary password on login the instructors are directed to the admin interface while the students are only able to see their designated workspaces with authentication in the database in place we had a way to associate users of all types with the resources that they might want to access in Armada this Associated data meant that we could use all of the other components of armada to serve students their specific workspaces in the browser and allow them to keep track of their work without having to perform any configuration or installations on their own machines instructors can also create custom development environments to deploy to an arbitrary number of students without needing to directly interact with AWS and finally our modest optimizations ensure the lowest cost for instructors maximizing efficiency or preserving usability why we accomplished a lot while building our motto there are a few things that we would like to do to extend our work here's a few of those things we would like to create a one-click deployment for AWS via cloudformation I mean generating the infrastructure easier rather than using cdk Deploy on the command line we would also like to integrate with GitHub repositories to allow data from the repositories to be automatically inserted into generated workspaces we'd like to integrate existing Solutions either through vs code's liveshare or other paid extensions that allow users to work collaboratively with a single environment simultaneously and we would also like to save some more costs and we could do this by implementing measures such as spinning down ECS services in those containers are no longer in use and with that on behalf of the Armada team I would like to thank you for your time and attention we hope that you enjoyed learning how we built Armada and do you have any questions how did your group decide to tackle this particular problem how did your project evolve as you delved into the problem space um Joey you want to take this and talk about uh how we let's definitely yeah definitely um so prior to joining Capstone I actually worked as a TA in The Core Curriculum as many of you may know um and this is a common student or a common problem that most students have is getting their development environment set up uh we're dealing with a bunch of different machines that can also make it really hard to be able to figure out like how to solve a specific problem because the answer could be different if the students on Mac or if this is on Linux or if you're on windows so it was a problem that many of us have and even getting started with a new project you have to think about all the different tools you want to reach for even if it's just like eslant and prettier to get set up on a JavaScript project so that was one of the reasons that we kind of like went this direction okay um let's see as far as how our project evolved as we went uh further into the problem space um I think really the product that we wound up with was really similar to our initial conception of what we wanted to build uh if there was I I personally I think that the thing that we evolved the most along was the the number of cloud services and technologies that we had to incorporate to accomplish this I don't know that the rest of the team want to chime in on that anybody have any different thoughts yeah I would agree I think the amount of services we had to use for from AWS was the big thing we didn't anticipate as much of other than that it kind of was what we wanted from the beginning yeah and I uh I know that Sergio's uh a pretty devout Cloud devotee uh I think he's managed to probably convert me I was I was very much in the camp of like do as much as we can for ourselves um and that I that was wrong we uh I think we took the right tack using as much Cloud as we did let's see um what would you say challenged you the most about this project EBS versus EFS that took us a long time um to figure out how that would work um and it was the most difficult in my opinion yeah I I got pretty Stuck In The Weeds on that uh as well yeah I would say routing too like because we use the aob to like route to like stateful containers uh that made it difficult to understand or not difficult to understand but more difficult because we needed to route two specific containers every time we couldn't just round robin to just whatever container was available yeah routing is tough um I I think I don't think I really did too much with routing um I think I was stuck on EBS at that point you guys were working with routing Sergio you did a lot of uh on ALB right oh yeah um I would say um yeah routing was kind of tricky in the beginning um also maybe the AWS documentation was kind of difficult to figure out right but it would eventually figure out the pattern and we made it work um also um how to untangle a circular dependencies you know like that was a little bit tricky too with routing especially when deploying Cloud infrastructure um other than that um I don't know yeah let's see um uh what someone says I found the Eds versus EFS comparison to be interesting could you go into more detail about why EBS just didn't work for you is there any way to make it work um I I think I did the majority of the work on finding out why EBS wasn't gonna work did you guys do any additional work on that or was that was that my rabbit hole I think that was mostly your focus okay um so the main problem with um EVS was really because it acts very much like a uh like a USB drive then the ec2 instance that UPS mounts to it expects to have a reserved device name for its you know for its Mount points so like uh Dev slash SDF through there's like 40 of them um if you're only going to run one or two uh workspaces on a single machine and then you can more or less just randomly pick um which one of those which one of those Mount points is not going to be available but when you're trying to deal with a scenario where there could be like 30 or 40 of these volumes all mounted across various machines spread out across the cluster then figuring out what's available on any one ec2 turns into a bit of a scripting nightmare um because you like we don't we don't know when a student's gonna fire up a workspace or when they're going to shut it down and every time they turn it off and turn it on again then the list of Mount points that were going to be available was going to be something that we would have to manage there's a lot of logic that we were gonna have to run we weren't sure where we would be able to host to that logic uh reliably because it was going to have to be available to every ec2 instance um so I think we decided to go with EFS just as a matter of scope um there was we it probably could have been done but it probably would have taken another couple of weeks to like really get it working reliably get it fully debugged um but that answers that question uh was there a particular part of the project that each of you enjoyed the most uh go ahead Joey I got you I was gonna say and I think Natalie probably agree with me possibly um the AWS Lambda getting that to work with EFS and being able to make the folders um it was really interesting to see that work and to know that you could like mount a hard drive essentially to it like a serverless function um so it was like a really very unique challenge to that because EFS built in allows you to generate access points but that only goes up to about 120 excess points and we would hit that limit pretty quickly so we needed another solution that we kind of had to figure out ourselves yeah I thought Lambda was was a fun part of the project as well but it actually wasn't what I was thinking of it was there before we even did that when we first created the API we were learning how to use the SDK and kind of programmatically create the tasks and the containers um just to see that work um I don't know it felt magical like it was really fun that was my favorite yeah I really enjoy working with the cloud without with the cdk particular I like how we incrementally improve our architecture and use you know everything that the cdk pretty much had to offer you've seen like nested stacks and constructs and things like that really enjoyed that part yeah I I think I have a twofer i i really um like once I was able to wrap my head around to the conventions of the cdk I really enjoyed it that was uh Natalie's right that was kind of magical like I could just in a couple of lines of code describe what I want this thing to look like on the cloud and then it just kind of it does for my terminal I thought that was cool um and I hit I also really liked uh getting the database initialized um that was a that was a fun little problem to work on I I freely admit I had to write some ugly code to make that work anytime um let's see what sort of optimizations could you make with more time that's a good question let's see I think if we had if we kind of like extra time you know like a couple more weeks we would have gone with um with an API Gateway on nbpc thing solution that way we don't have to use it like the nginx on on the last part you know which is routing to the students yeah and that's this but to the Armada app you know would have made things a little bit easier so essentially we would just take whatever request is being sent from API Gateway map that to a VPC link and then just use the application or balancer as a proxy that would send the request to a specific Target group yeah this way we could have um had maybe one target group for the front end another one for the for the back end and then another one for the workspaces so maybe it would have simplify things a little bit Yeah but I'm pretty happy with the with the way that things turned out and the work that we did in the time that we had yeah yeah I think getting rid of that nginx container with an API Gateway would also say there's quite a bit of resource on agency2 instance so we may be able to break down that way um I think probably like the one that seems most obvious to me is um tuning the memory reserves on the auto scaling group if we could tighten that up a little bit so that we wouldn't uh so we could shut down ec2 instances when when they're vacant and maybe not spin up an extra one if we just like absolutely didn't need to but I don't know if that would be a trial and error kind of a thing or if that's something that we could calculate yeah and I think one of the things with that is that coder has built in like that Health checkpoint that we talked about and it actually gives you the last time I think in seconds that it was accessed um so we could figure off that to get those individual servers to shut down or individual containers to shut down to kind of add to that cost savings and facilitate that yeah okay let's see Jason says we mentioned ECS Services provide automatic error recovery and we'll restart Services if they fail if several students are working on an instance that fails are all of those individual workspaces restored okay um somebody I I could do this one unless somebody else wants to um I can describe if you want like kind of what happens sure yeah so if if the workspace is deemed unhealthy ECS will automatically take it down because that health check endpoint won't be responding with the 200. it'll instantaneously almost especially with the load times with coder bring that right back up and it'll use the exact same Mount point from EFS to um to access our files so as long as their work was saved um it should just be just as they left it encoder kind of caches heavily so you should see probably the same files open that you had before as well right I kind of think I'm not sure Jason may be asking like okay so if if jimothy and Anna are both working on the same uh on workspaces that are hosted on the same ec2 instance and jimothy's container uh kicks the bucket does that mean that the entire ec2 is gets rebooted I think is what he's asking uh no it's only at the container level right yeah uh let's see was there any discussion of using another IAC tool like terraform over the cdk yes yeah we we had that discussion really early on um where we were trying to figure out if we wanted uh if we wanted to be able to eventually Port our Mata to run on any other kind of cloud um and we determined at the beginning like at this stage focusing on AWS was going to be was going to be sufficient yeah we tried to play with the different levels of complexity and we kind of fine-tuned it and realized that even the cloud native solution would be the most ideal given the you know time we had and the purpose we were trying to serve yeah okay I don't see any other questions I'll give a couple more minutes in case somebody thinks it's something else see I have a lot of fun guys I'm a little sad it's open but I'm also kind of Happy really enjoyed that process okay yeah does it look like there's going to be too many more questions in a couple of seconds all right folks we really appreciate you coming out to listen to our story hope you enjoyed it hope you got something out of it alrighty thanks everybody thank you thank you thanks 