Welcome, everyone! Today, we are going to discuss Armada, an open-source container orchestration tool designed to automate the configuration and deployment of developer environments in the cloud. Our team, consisting of Natalie Martos, Sergio Pachado, Joey Gilliam, and Dean Elizardo, has worked tirelessly over the past few months to develop Armada. We are thrilled to have the opportunity to share it with you today. Throughout this presentation, we will delve into the problems Armada addresses, focus on its application in the education sector, explore virtualization processes, examine cloud environments, discuss existing solutions, detail the creation of Armada and the challenges we faced, and conclude by outlining our future plans for Armada.

In order to understand the design decisions behind Armada's features, it is important to grasp the difficulties involved in setting up developer environments. These challenges include configuration overload, dependency management, and resource availability. Configuration overload occurs due to the multitude of productivity-enhancing tools available to developers, each with its own configuration file. While flexibility is desirable, managing multiple configurations across different environments can become burdensome, leading to fragile dependencies and decision fatigue.

Dependency management is another challenge. Modern applications rely on numerous packages, frameworks, and libraries to expedite development and minimize redundancy. However, dependencies can introduce fragility, circular dependencies, and the risk of a single misconfiguration or incorrect version derailing the entire chain.

Lastly, resource availability is a major obstacle, particularly for computationally intensive tasks that require high levels of CPU, GPU, or RAM. Such tasks include creating virtual environments, running emulation software, and utilizing test runners and compilers. Hardware costs can be prohibitive, especially for students and developers on a budget.

Now, let's shift our focus to the education space. Learning how to code can be a daunting experience for students. They are bombarded with questions, such as what type of computer and text editor to use, which programming language to learn, and how to configure their environment. These queries can quickly overwhelm students, dampening their motivation and impeding their progress. To compound matters, instructors often spend valuable time troubleshooting individual student environments, hampering their overall efficiency.

To address these challenges, there are two possible solutions. Instructors can either maintain a document containing common student issues or work individually with each student. However, what if instructors could ensure that every student had access to the same hardware and software? This is where virtualization comes into play. By utilizing virtualization, instructors can create an abstraction layer over their hardware, effectively dividing it into multiple virtual machines (VMs). Each VM runs its own operating system, functioning as an independent computer while actually utilizing a portion of the physical hardware. This approach allows instructors to create an image, capturing the state of a computer, and use it as a starting point for a VM. Instructors can then configure a single functional workspace and easily update it. However, traditional virtualization methods suffer from cost issues, maintenance complexities, and potential single points of failure.

The ideal solution lies in the cloud. Cloud environments provide on-demand access to networking and computational resources, alleviating the need for hardware management. Additionally, instructors can scale up or down based on demand, without purchasing and configuring additional hardware. Cloud environments offer a turnkey experience and minimize the risk of a single point of failure. However, simply providing each student with their own virtual machine is not the most efficient solution. Students rarely utilize the full capacity, resulting in wasted resources for instructors.

Containerization offers a way to optimize virtual environments. Containers are lightweight and bundle applications with their dependencies, making them more portable and smaller in size. They are also designed to be ephemeral, meaning they can be created, used, and discarded without much concern. Docker is a popular software used to create containers, allowing developers to configure container construction through Dockerfiles, which specify base images that can be extended.

Implementing cloud-based containerization can be complicated and time-consuming. Instructors would need to choose a cloud provider, learn about its services, provision and connect the required services, handle security and permissions, and develop a scalable process for creating and destroying environments. Fortunately, there are commercially available solutions that simplify this process, enabling instructors to focus on teaching rather than becoming cloud experts.

Now, let's discuss existing solutions in this space. Three popular solutions for pre-configured developer environments are Coder, Gitpod, and GitHub CodeSpaces. When comparing these options, we considered factors such as ease of setup, ease of use, support for integrated development environments other than VS Code, customizability with templates, scalability, and cost.

Coder is moderately priced and offers all the core functionalities of an integrated development environment. However, it can be difficult to set up. Gitpod, on the other hand, has extensive features but has a slow startup time and is not easy to install. While Gitpod is available for self-hosting, its docker image is no longer maintained. GitHub CodeSpaces integrates seamlessly with the GitHub ecosystem and provides a turnkey experience. However, it is limited to VS Code, cannot be self-hosted, and is the most expensive option.

Based on a thorough review of these solutions, we developed our own solution, Armada. Unlike existing solutions, Armada is specifically tailored for the education space, providing an easy-to-use, low-cost option. Armada simplifies environment configuration and dependency management, ensuring students have access to the right hardware. It also streamlines instructors' workflows, minimizing the time spent managing individual students' diverse environments.

To demonstrate Armada's capabilities, we have prepared a few demo videos. In the first video, an instructor signs into the Armada app and accesses the dashboard. The instructor can add and edit students, cohorts, courses, and workspaces. Key functionalities include creating, starting, stopping, and previewing workspaces for students.

In the second demo video, a student signs into Armada and accesses their workspace through the simplified dashboard. The student can start their workspace and access files and the terminal through the browser-based development environment.

Now, let's delve into Armada's architecture, which encompasses eight milestones. First, we containerized the workspace, allowing it to run in a local container accessible from the browser. Next, we provisioned cloud infrastructure, followed by deploying a single workspace. Subsequently, we focused on deploying multiple workspaces and creating dedicated URLs for accessing them. To ensure data persistence, we implemented mechanisms for saving workspace data. Managing the data and implementing a user interface were our next milestones. Lastly, we provided user authentication.

In conclusion, Armada addresses the challenges students and instructors face when setting up and managing developer environments. Our goal is to provide easy-to-use, scalable, and cost-effective development environments exclusively tailored to the education space. Thank you for joining us today, and we look forward to the future of Armada! The purpose of our coding Capstone project was to address the challenges faced by students and instructors when learning to code. With this in mind, we developed our motto, which aimed to achieve several goals. First, we wanted to create user-friendly development environments for students. Second, we aimed to make it easy for instructors to manage and deploy these environments. Third, we wanted the ability to scale our solution to meet the demands of both students and instructors. Lastly, we wanted to minimize costs for instructors. In this video, we will demonstrate our progress by showcasing some demo videos.

In the first demo video, we see an instructor signing into the Armada app. Upon signing in, the instructor is presented with a dashboard view. From here, they can add or edit students, cohorts, courses, and workspaces. The video demonstrates how the instructor can add a student, create a cohort (which is a group of students), and then create a course. Additionally, the instructor can create workspaces for students enrolled in the course. Once a workspace is created, the instructor has the ability to start and stop the workspace and even preview and edit a specific student's workspace.

The second demo video focuses on a student signing in and accessing their workspace. The student has a simplified version of the dashboard, where they can start their workspace or access it through the reset button. Clicking on the provided link takes the student to their development environment in the browser. From here, they have access to all their previously created files and even a terminal for executing commands.

Now that we have covered the features of Armada, let's take a detailed look at its architecture. Our journey consisted of eight milestones that needed to be accomplished. First, we had to containerize a workspace. Then, we had to provision cloud infrastructure. After that, our focus was on deploying a single workspace, followed by deploying multiple workspaces. Next, we needed to find a way to access those workspaces through dedicated URLs. We also had to persist workspace data to ensure the students' work was saved. Managing that data and implementing a UI were our next tasks. Lastly, we needed to provide user authentication.

Our first milestone was achieved by getting a working copy of an IDE (Integrated Development Environment) running in a local container that could be accessed from the browser. Instead of building the core development environment from scratch, we explored existing solutions. Two options we considered were Coder and Gitpod, which both have open-source versions of their coding environments. While Gitpod offers extensive features for customization and integration, it has a large container size and longer startup times. On the other hand, Coder's container, called code server, is lightweight and fast, with load times of under 20 seconds. Given its advantages in provisioning multiple instances per server, we chose the Coder image as the basis for our workspaces.

The second milestone involved running workspaces in the cloud. When building cloud applications, an important architectural decision must be made: whether to go for a cloud-agnostic or cloud-native approach. The former provides flexibility but requires customization for each cloud provider, whereas the latter focuses on technologies native to a specific cloud service provider, leading to potential vendor lock-in. After weighing the pros and cons, we decided to go with AWS (Amazon Web Services) and its cloud-native approach. AWS offers tools like the Software Development Kit (SDK) and the Cloud Development Kit (CDK) that expedited our development process. The SDK and CDK allowed us to provision cloud infrastructure and create and customize AWS products.

Once we had set up the necessary AWS tools, our attention turned to deploying and managing a single workspace in the cloud. Initially, we used AWS EC2 (Elastic Compute Cloud) service to host a code server container. Nginx was used to make the workspace accessible over the web. However, this setup lacked automation and scalability. To overcome these limitations, we explored container orchestration tools like Kubernetes, Docker Swarm, and Amazon's Elastic Container Service (ECS). After considering their features and complexities, we opted for ECS as it offered a low learning curve, integration with other AWS services, and easy container management, including scaling and replacing crashed containers.

To deploy workspaces as containers, we made use of ECS tasks and services. While tasks allowed us to run individual workspaces as containers, services provided automation, monitoring, and management capabilities. With services, we could start, stop, and scale tasks by manipulating the desired task count in the task definition. This gave us more control over reliability and scalability.

Next, we focused on resource scalability. ECS services interact with individual EC2 instances that host the containers. To efficiently utilize EC2 instances' resources, we leveraged AWS Auto Scaling Groups, which enable horizontal scaling based on resource utilization thresholds. By integrating ECS service scaling with Auto Scaling Groups, we could pack multiple services on the instances with the most available resources, minimizing unnecessary expenditures.

Routing was another vital aspect we tackled to allow students to connect to their workspaces. By dynamically deploying multiple workspaces on the same port, we addressed conflicts that arose when using static port mapping. Additionally, ECS provided health checks to automatically replace unhealthy containers, ensuring workspace recoverability in case of failure.

In conclusion, through our journey, we successfully containerized workspaces, provisioned cloud infrastructure using AWS, deployed and managed both single and multiple workspaces using ECS tasks and services, scaled resources using AWS Auto Scaling Groups, and implemented routing for students to access their workspaces. These accomplishments kept us on track to achieving our goals of providing user-friendly development environments, easy management for instructors, scalability, and cost minimization. Our focus on AWS allowed us to leverage their tools, SDK, and CDK, streamlining our development process. Memory is important in our project, and we needed a way to scale resources efficiently. AWS provides a service called Auto Scaling Groups which allows us to create a pool of resources that can scale horizontally with demand. We configured a scaling rule for our EC2 instances hosting workspace containers, which adds or removes capacity based on the amount of memory reserved for each service.

When an EC2 instance reaches a predetermined resource utilization threshold, the Auto Scaling Group detects the state change in the cluster and automatically provisions a new EC2 instance. On the other hand, scaling ECS services is simpler. Our goal was to pack as many services as possible on an EC2 instance with the most available CPU or memory.

To achieve this, we implemented a strategy to reduce unnecessary expenses by minimizing the number of instances in use. We learned how to deploy multiple workspaces and scale resources in our cluster. However, before fully leveraging scaling, we needed to focus on routing to ensure students could connect to their workspaces.

By running workspaces as services, we were able to easily turn them on and off and manage unexpected failures automatically. But one important benefit offered by ECS services is scalability. Up until then, we had been hard-coding the container ports in our test definitions, which limited deploying individual workspaces on separate EC2 instances. To efficiently use EC2 instances, we could deploy multiple containers and make sure the health scores used by the containers did not conflict.

We explored the concept of static and dynamic port mapping. Static port mapping involves defining a specific port for a container, while dynamic port mapping allows the host operating system to choose a random port automatically. Previously, we relied on static port mapping, but it was not efficient since each EC2 instance had sufficient resources to host multiple workspaces simultaneously.

To temporarily bypass this roadblock, we kept tearing down and redeploying our infrastructure and spinning up multiple EC2 instances at a time. This helped us avoid port conflicts while still searching for a viable way to dynamically deploy multiple workspaces on the same port. Finally, we found a potential solution using static load balancing.

We placed an Application Load Balancer (ALB) in front of our ECS cluster to serve as the entry point for our application. The ALB seamlessly integrated with ECS and enabled us to use dynamic port mapping. This allowed us to deploy multiple tasks on the same EC2 instances, as the ALB could now choose a random host port.

Enabling dynamic port mapping with the ALB was straightforward. We updated our test definitions to indicate that the host port for each task would be delegated to the ALB. With this configuration, we could deploy containers on EC2 instances that were already running other containers. We were able to evolve from deploying a single task to launching multiple tasks on the same EC2 instance using dynamic port mapping.

However, the ALB presented a new challenge. By default, load balancers use a round-robin routing algorithm, which distributes traffic to each target randomly. Our workspaces were stateful and assigned to specific students, so we needed a solution that would route the traffic correctly. To overcome this challenge, we mapped each workspace to its own target group.

A target group is a collection of resources that can receive traffic from a load balancer, allowing us to divide the traffic based on a specific path. Additionally, we attached a listener to the ALB, which allowed us to control how the incoming traffic would be proxied. We created an ALB rule for each workspace's test definition, using a consistent naming convention. This allowed us to uniquely identify and route traffic to each workspace based on their designated target group.

With this configuration, when the connect button is clicked, a request is sent to the application load balancer. The ALB matches the path in the request against path patterns in its listener rules and forwards the request to the correct target group. This ensured that students connected to their dedicated environment instead of a random one.

Our progress in developing these features was organic. We started with a single task, then progressed to launching multiple tasks on the same EC2 instance. We then introduced services and addressed load balancer challenges by configuring target groups for each workspace. However, there was still one more thing to be done.

To map the user from a URL to their workspace, each workspace was linked with its own NGINX reverse proxy. This allowed us to remove the portion of the URL that was used for path pattern matching with the target group. With this final configuration in place, the Code Server container was now able to match the root path of the request, and students could connect to their own workspaces.

With all these pieces in place, we successfully automated the distribution of developer environments. We could efficiently host multiple environments on a single EC2 instance and scale to multiple EC2 instances if needed. We provided a consistent method for accessing the correct environment.

Next, we needed a reliable way for users to persist their data independent of the lifecycle of their workspaces' containers. Containers are stateless, meaning that user data is not saved when the container stops running. To address this, we explored using Elastic Block Store (EBS) in conjunction with S3 buckets or the Elastic File System (EFS).

EBS creates a virtual hard drive that can be mounted to a server. However, EBS volumes are tied to the lifecycle of the EC2 instance, which would result in data loss if the instance is turned off. To make EBS and S3 work, we would have had to query ECS to find the target container and EC2 instance, create a new EBS volume, attach it to the specific instance, and create a snapshot to preserve changes made to the file system.

However, using EBS and S3 would prevent us from scaling workspaces easily. Managing mount points for volumes on EC2 instances is complex. Because of this, we decided to abandon EBS and S3 and turned our attention to Amazon's Elastic File System (EFS).

EFS is a serverless cloud-native storage service that provides an on-demand and scalable file system. It monitors a specific directory on a target system and mirrors changes made to files in that directory. EFS allowed us to predict and control how a workspace would connect to its storage solution.

To successfully attach EFS within Armada, two key components needed to be present. First, the specific folder or directory in EFS needed to exist before it could be mounted to a container. Second, each task definition needed to specify a mount point for the EFS volume. We used AWS Lambda to meet these requirements.

Lambda enabled us to integrate serverless functions into our workflow. The Lambda function created a uniquely named directory within EFS. This directory contained the mounted EFS volume for each individual workspace. The Lambda function was triggered when an instructor created a new task definition for a specific workspace. The directory followed a naming convention based on cohort name, course name, and student name.

With the persistent data solution in place, students' work would be saved from one session to the next. No interruptions or failures would cause data loss, providing a reliable user experience.

To coordinate and expose all the established resources, we needed a straightforward and intuitive interface for instructors and students to manage, create, maintain, and interact with workspaces. To achieve this, we developed the Armada app, a full-stack application with an Express backend written in TypeScript.

The Armada app acted as the central control hub, connecting each provisioned architecture component to ensure scalable management of student workspaces. The app was containerized and managed as a service in the ECS cluster.

To interact with AWS resources at runtime, we used the AWS SDK, which provided libraries for this purpose. By wrapping our feature set around the SDK's functionality, we automated core workflows and streamlined actions that instructors would regularly perform.

For example, when an instructor creates a workspace for a student, Armada performs several actions using the SDK. It creates a task definition outlining the configuration of the required containers, creates a target group for the workspace, attaches a rule to the ALB listener to allow traffic to the workspace, and creates a service specific to the student in ECS to manage and monitor availability and health.

Armada served both instructors and students. Instructors could create individual accounts for students, organize them into cohorts, enroll them in classes, and associate workspaces with specific students. Students could access their workspaces through their student portal or through the instructor's preview button.

Armada provided a platform for students to access their workspaces. If a workspace was inactive or not yet running, students could reset it without instructor intervention. Most importantly, students could connect to their workspaces and access their files, ensuring a seamless development environment.

To address the need for data persistence, we leveraged a database within our architecture. This allowed us to provide a reliable way for users to persist their data independent of workspace lifecycles.

With the infrastructure in place, we successfully automated the distribution of developer environments. We achieved efficient resource utilization through scaling, provided a reliable way to persist user data, and built an intuitive application interface for instructors and students. All these components together formed the Armada project, allowing for easy management and accessibility of workspaces. During this project, we focused on utilizing EFS and Lambda to create a directory within EFS that the task definition references in its EFS volume configuration. We followed a naming convention of cohort name, course name, and student name. By running the task, we were able to instantiate the workspace with the EFS volume mounted to a container directory called home coder. This allowed us to implement a data persistent solution, ensuring that students' work would be saved from one session to the next. This ensured that their data would be available regardless of any interruptions or failures.

To simplify the process and provide an intuitive experience for instructors and students, we developed a full stack application called Armada. The Armada app's backend is an express application written in TypeScript. Acting as the central control hub, it connects each component of the provisioned architecture to allow instructors to provide and manage student workspaces at scale. The application is containerized and managed as a service in the ECS cluster.

We utilized AWS's Software Development Kit (SDK) to automate core workflows and streamline actions that instructors frequently perform. For example, when an instructor creates a workspace for a student, Armada performs several actions using the SDK. It creates a task definition specific to the student, configures the required containers, creates a target group for the student's workspace, attaches a rule to the ALB listener to allow traffic, and creates a service specific to the student on ECS for availability and health monitoring. We also store relevant information for data creation and organization.

By implementing these features, we can serve both types of end-users: instructors and students. Instructors can create individual accounts for students, organize them into cohorts, and use cohorts to enroll students into classes. Each workspace is automatically associated with a student and accessible through their Student Portal or the instructor's preview button. Armada provides a platform for students to access their workspaces, even allowing them to reset their workspace without instructor intervention.

To manage the relationships between workspaces, students, courses, and cohorts, we needed a database within our architecture. Instead of implementing an instance of PostgreSQL on the server, we opted to use Amazon's managed solution, the Relational Database Service (RDS). This decision ensured automatic backups and replacement in case of a crash. We configured the RDS instance to be accessible to the EC2 host, initializing the schema and inserting our first user using an additional EC2 instance connected to the RDS instance.

With the database in place, we proceeded to provide authentication for our users. We decided to utilize AWS Cognito, which provides authentication, authorization, and user management. Instructors adding students through the Armada interface add users to Cognito, RDS, and send the students an email with a temporary password. Instructors are directed to the admin interface upon login, while students can only access their designated workspaces. With authentication and the database in place, we associated users with their resources, allowing Armada's components to serve students their specific workspaces in the browser.

Throughout the project, we encountered several challenges. One notable challenge was the decision between EBS and EFS. EBS was initially considered, but its limitations made it difficult to manage multiple workspaces across different machines. The requirement to manage and mount multiple volumes became complex, and we eventually decided to use EFS for its ease of use. Another challenge was routing, especially when dealing with ALB and stateful containers. Configuring the routing to specify containers added complexity, but we managed to overcome this difficulty.

Among the team members, several parts of the project were particularly enjoyable. Joey and Natalie found working with AWS Lambda and EFS to be fascinating, as it allowed them to mount a hard drive to a serverless function. Creating the API and programmatically creating tasks and containers using the SDK was both magical and fun. Sergio particularly enjoyed working with the CDK, appreciating the incremental improvements in the architecture and the use of nested stacks and constructs.

Given more time, there are several optimizations we would consider. Firstly, we would create a one-click deployment for AWS using CloudFormation to simplify infrastructure generation. We would also integrate with GitHub repositories to automatically insert data into generated workspaces. Additionally, we would explore integrating existing solutions, such as Visual Studio Code's Live Share or other paid extensions, to allow collaborative work in a single environment. Finally, we would implement cost-saving measures, such as spinning down ECS services when containers are no longer in use.

In conclusion, we have achieved a lot with Armada, addressing the challenge of setting up development environments for students. We constructed a full stack application utilizing AWS services and optimized workflows for instructors and students. Although there were challenges, we successfully implemented solutions and enjoyed various aspects of the project. Given more time, we would further enhance our work by implementing additional features and optimizing costs. We appreciate the opportunity to share our journey in building Armada and are open to any questions. During the project, we needed to manage the list of Mount points that would be available. There was a lot of logic that we had to run and we were uncertain about where to host that logic reliably, as it needed to be accessible to every EC2 instance. Ultimately, we decided to go with EFS to keep the scope manageable. While it probably could have been done differently, it would have required a few more weeks to get it working reliably and fully debugged.

When asked about the favorite part of the project, Joey mentioned that both he and Natalie enjoyed working with AWS Lambda and getting it to work with EFS. They found it interesting to see how they could essentially mount a hard drive to a serverless function. They explained how EFS allowed them to generate access points, but since there was a limit to the number of access points available, they had to come up with their own solution.

Natalie, on the other hand, mentioned that even before working with Lambda, her favorite part was creating the API and learning how to programmatically generate tasks and containers using the SDK. She described it as a magical experience and found it truly enjoyable.

Joey added that he especially liked working with the AWS CDK, appreciating how they could incrementally improve their architecture and utilize the various features offered by the CDK, such as nested stacks and constructs.

Another team member expressed a similar sentiment, stating that once they understood the conventions of the CDK, they found it quite enjoyable. They were amazed at how they could quickly describe their cloud infrastructure using just a few lines of code.

Additionally, they mentioned that they liked the challenge of initializing the database, admitting that they had to write some messy code to make it work.

Given more time, they mentioned several potential optimizations. One idea was to use an API Gateway on an VPC endpoint solution to eliminate the need for an Nginx container, making things simpler. They also noted that tuning the memory reserves on the auto scaling group could help optimize resource usage.

Furthermore, they discussed leveraging the Health checkpoint feature of Coder, allowing individual containers to shut down based on access timestamps, thereby increasing cost savings.

The question about workspace restoration in case of failure was addressed. They explained that if a workspace is deemed unhealthy, ECS will automatically bring it down and almost immediately bring it back up. As long as the work was saved, the files should be as they were, thanks to Coder's caching system.

There was also a discussion about using another Infrastructure as Code (IAC) tool like Terraform instead of the CDK. They mentioned that they had explored the idea early on but decided to focus on AWS as their choice of cloud platform for this project.

In closing, they expressed their gratitude to the audience for listening to their story and hoped that the audience had enjoyed and gained something from it. They concluded by thanking everyone for attending the presentation.