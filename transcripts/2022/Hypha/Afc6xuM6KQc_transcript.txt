hello everyone and thank you for joining us my name is yulia and together with my teammates josh steve and isaac we built haifa hypha is an open source observability framework it sets up distributed tracing and a telemetry pipeline that aggregates logs and correlates logs with traces this facilitates a powerful debugging workflow using haifa's simple ui let's take a look at what we'll cover in today's presentation i'll first talk about observability and explain some foundational concepts then josh will tell us the story of a fictional health tech startup so that we understand the problem haifa solves then isaac will tell us about what's involved in creating a telemetry pipeline existing solutions in this space and give us a demo of fifa then steve will give us a tour of haifa's architecture finally i'll talk about engineering challenges we encounter and future work for haifa so let's get started in the software industry there isn't a clear agreed upon definition of observability depending on where you look or who you asked you might get a slightly different definition we preferred this definition originating from control theory which lays out the core principle observability is a is a measure of how well internal states of a system can be inferred from its external outputs let's unpack what that means a software system boils down to inputs and outputs a developer user or another system gives the input and the system computes and gives it gives us an output but what if we know if we what if we want to know what's happening on the inside our software systems are much like a black box we can see the input and output but it's difficult to know the internal states of the application such as intermediary steps in a process or exceptions being raised but we need to know the internal states of the system to debug issues and keep the system operational so what do we do well there's a hint in the definition of observability internal states of a system can be inferred from its external outputs so if we want to better observe our systems we need to output more data data that is output to increase system observability is called telemetry in the software industry we generally speak of three types of telemetry logs metrics and traces these are often called the three pillars of observability you may recall that hypha correlates logs and traces i'll explain those next in detail but first how do we output telemetry for our application to output telemetry we need instrumentation instrumentation is any code that outputs telemetry for example calling a method which which outputs a log logs are just lines of text they provide human readable specific detailed information about an event these can logs can often be errors or any event for example completing a processing step often logs are structured to be easily parsed by machines for example in json they're great for providing fine-grained detailed information originating from a single application traces on the other hand provide a higher level view when compared to logs they show the path a request took through a system for example on the right we see a trace visualized as a waterfall as a waterfall chart a trace is composed of spans that each represent a call to a service and the span of time it takes for the request to exit so in this case the trace starts with the root span where the requests enter the system so the api gateway from there we can see how subsequent requests were made and the relationship between components of the system traces gives us a system level view and through them we get the context of how system components interact over the network josh will show us why you might want you may remember that hypha correlates logs and traces josh will show us why you might want to correlate them through the story of a developer at a fictional company labra cadabra thanks elio so labra cadabra is a small health tech startup company their platform for patients and diagnostic labs to communicate they provide scheduling and payment for lab tests as well as the ability to view test results and the analyses of these results amy is a senior engineer at labrador here she's working on her local computer adding a feature to their application she's written some code takes an input and she expects a certain output but this isn't exactly what's happening and just based on this output it's not obvious what's wrong so she can use logging to understand why the code is behaving this way she writes a few lines of code to output the extra information she needs in other words she instruments her code to emit logs and with these extra details she can pinpoint the issue and fix it now that the issue is fixed she's ready to deploy to labrador's production server let's take a look at what this will mean for her ability to understand her code through log in development logs are usually output to the current process meaning they show up on the console and are not saved anywhere this is fine since the developer is actively watching the box but in production no one is actively watching logs and if they were the volume of logs would be overwhelming this means in production logs need to be stored often to a file answer when running locally the developer can add logs run the program see the output and make changes and run the program again within a matter of seconds or minutes to get additional logging into production the process is much more involved it may include testing code reviews and deployment to the server the feedback loop is much longer this means that the developers need to think carefully about the logs their production applications will produce you can't go back and replay a user's request so it's important to capture the needed data the first time this is also true of the structure and formatting of the logs locally a developer doesn't necessarily need a lot of structure to understand logs they just wrote but in production logs need to have structure and formatting that anyone can read and understand so here's library cadavers server they have a simple architecture with an application server and a database server users make requests receive responses and the logs are stored to a file on the server now if a user reports an error amy can ssh into the server search for the logs with errors and diagnose the issue amy and your team begin using logs extensively they invest time and effort into making sure their logs have all the information they need to debug issues let's fast forward a little bit and labrador cadaver has been experiencing rapid growth their product is growing in size and so is their engineering team they make the decision to transition to a microservice architecture now this comes with many benefits services can scale individually which allows for better resource utilization each team can focus on a single service meaning they can be responsible for a smaller code base they're also more independent and they can make different technology choices that are best for their service all of this often speeds up development time of course like everything else in software engineering this comes with some trade-offs the first is that previously amy was able to ssh into one server and find the logs she needed now that services are separated logs are produced in many nodes and it's tedious to ssh into each service this slows down the button a solution to this is to aggregate logs in one place this would allow them to find what they need in one location making debugging quicker now there are several ways to do this they could have their logging library directly send the logs to the central location or they can continue writing logs to a file and have separate agent program that scrapes those files and ships the logs to that central location this will solve the issue of having logs distributed across many nodes there's another challenge they face as the company grows with the distributed nature of the new system it becomes more difficult to see how execution flows through the different components requests come into the api gateway but then several services might be called cart service may call the auth service and then have several calls to the lab test service or another flow might be that the api gateway takes the initial request again calls the cart service which needs to authenticate but then calls the analysis service and then some lab test service if there's a problem with a request how do you know from that services logs how which service is the culprit and how this execution has flowed this is especially difficult for new hires who are still learning about the system amy and other senior engineers have had the time to build up enough knowledge of the system that they usually can narrow down where the issue is but even this is becoming more difficult as they continue to add services they want a solution that will help everyone get the context they need quickly now luckily julio already mentioned a possible solution distributed tracing we know that the distributed tracing shows the high level overview of the execution path of a request while the architecture diagram here shows the layout and connections of the system the trace diagram shows how an individual request actually flowed through the system so we can start at the top left and see that the api gateway calls the cart and then the auth service and then we can see those three calls to the lab test service we see how long everything took and the relationship between all of these spans so what does distributed tracing actually give us so it's an intuitive visual overview even if you don't know much about the system now you know how things are connected and it makes debugging much easier and as an example of this let's imagine this scenario amy's been getting complaints about a delayed request to the cart service so she takes a look and most of the traces look something like the picture on the left but then she finds a trace that looks like the diagram on the right now it's easy to see that the last call to the part service is a lot longer than the others now these traces don't have all the information to tell the root cause of the issue but now she knows where to look she needs to figure out why that lab test call is longer or here's another example there are reports of some errors in the analysis service let's say that she goes to investigate a typical trace looks like the one on top but then she discovers another and something strange the lab test service keeps getting called repeatedly and these fans are very short compared to the others once again she doesn't have all the information she needs but she knows exactly where to go to find them to cause the issue so amy and her team have decided to aggregate their logs and implement distributed tracing in their system as they begin researching their options amy also thinks about how they will use these two types of telemetry data together she realizes that to get the most out of both logs and traces they'll often want to go back and forth between the two because logs contain the specific details about events and the traces contain that high level picture of the execution path as we saw earlier you often want more details when you're looking at a trace let's take a look at how she might go back and forth between the two so let's say amy is investigating a user-reported error that is occurring in the lab test service she's found a log that shows the error this gives her details but she needs more context about the requests that caused the error perfect this is exactly what traces are good at the question is how does amy find the trace associated with the specific logs she's looking at so here's the search ui of a popular open source tracing tool jager let's see vlog has a time stamp so she can set a custom time range to capture traces that are around that time she also knows what service it came from so she can search only traces that contain that service and she knows that it was a 500 error so she can filter by error code now depending on how busy that service is and how many 500 errors occurred during the time range amy might have narrowed down the number of traces to a manageable amount but she also might need to keep bigger now this process can be pretty tedious especially if amy's trying to do this in a stressful situation like in the middle of trying to debug an incident the other direction can also be difficult once amy finds the tracing question she'd like to be able to look at the specific logs associated with the single request in the other services it touched in distributed systems the root cause of an error might be an event that occurred in a different service and that service may not have even thrown an error but there could be important details that will help in debugging the error that did occur so how does she get from traces to logs well she can search the logs for that service and filter the time stamps based on when the span occurred but there might be dozens or hundreds of logs in that time frame if they have included user request data in the traces she may also be able to correlate based on those the core problem here is that there's really one single piece of data that is present in every log in every trace that are connected i like the way that ted young the co-founder of open telemetry put it he said that it can be a bit like looking for a needle in a stack of needles all of your logs can look the same a lot of them have a lot of the same information and we often don't have the way we often don't have a good way to differentiate easily so we've seen there are a lot of ad hoc ways trying to find the logger trace that we need using the time stamp for service or error codes what amy and her team would like to have is a way to seamlessly connect their log data and their trace data they need a single context that gets attached to every log and trace as it's created and will travel with the request as it moves through their system this will make it possible to go from log to the associated trace and then from that trace back to all the logs that were generated in every service for that request and they'll need a system to create the telemetry attach the context and then handle and use all of this data so next isaac is going to talk about what would go into building a system that achieves all of these things thanks josh so amy and her team are really excited about the benefits that observability could give them they're ready to start upgrading their observability system so that they can get these things like log aggregation distributed tracing and correlation between logs and traces and now they're faced with a big question which is well how do we actually do this what are we going to need in order to set up this kind of observability system one of the major pieces they're going to need is a telemetry pipeline that can handle their telemetry data so we want to give you an overview of the major stages that make up a telemetry pipeline so that we have a better idea of what's involved before moving forward we're going to use the model outlined by jamie riedesell in her book software telemetry which describes three major phases emitting shipping and presentation let's take a closer look the first stage is emit this is where we add the code or the instrumentation that's responsible for actually generating the telemetry data that we're going to use for example to generate traces one approach is to use a software development kit from a framework such as open telemetry when a new request hits the service this code is responsible for generating the data that's needed to start tracing the journey of that request through the system for application logs it's common to use a logging library that generates and emits logs in a consistent customizable format the emitting stage is also where we prepare the telemetry data for shipping this preparation can include formatting or enriching the data with additional contextual information so the approach taken here will determine the initial format and content of your telemetry data before it enters the next stage and next is shipping so this stage is concerned with collecting the data that's been emitted processing that data as needed and storing it so that it can be used effectively in presentation depending on the use case this might also be where your telemetry data is transformed or enriched to be more useful for example we might want to parse data into a format that's better suited for the database we're using or we might add contextual information that makes our data more helpful later on now you may recall that this kind of enrichment is also sometimes done in the emitting stage and this is just one of the variations in implementation that can exist there are many other considerations here ranging from the type of storage we use to how we aggregate and export our data to how we process our data and why and all of these choices are going to have a major impact on what we can do next the final stage is presentation this stage is focused on querying data from our storage and transforming that data in a useful way this usually means visualizing data into charts tables and graphs and providing a user interface that can be used to interact with the telemetry data produced by our system so for people interested in using this pipeline this is the stage you're going to interact with and see the most for example this is where a developer would actually go to investigate and debug an issue later on steve is going to do a deeper dive into each of these sections as he discusses the approaches we took with haifa for now hopefully this gives you a little more context for what labracadabra is going to need let's pick back up with amy and her team as they investigate some of the existing solutions in this domain one place they might start is with sas solutions or software as a service sold by third-party vendors now one of the main advantages of sas solutions is that there's very little setup required to use them in most cases you just copy some instrumentation code in your applications and the vendor takes care of all the other stages in the pipeline they also manage that pipeline for you so you don't have to worry about deploying scaling or maintaining that infrastructure and finally these solutions come with feature-rich uis that can handle many data types and are highly customizable however these benefits come with some drawbacks first by sending their data to a third party labra cadabra would no longer have sole ownership of that data as a healthcare company that needs to protect the privacy of their users this is a pretty big drawback to consider another difficulty with using sas solutions is that the huge feature set can actually be a double-edged sword while very powerful they can also be overwhelming and difficult to learn and amy and her team would like to start using telemetry without such a steep learning curve so another approach they might look at is to use open source telemetry tools which they could connect together to create a diy solution one of the major appeals of this approach is that by using open source tools labra cadabra would have control over their telemetry pipeline they could customize it to fit their needs and they could update and expand their pipeline as they grew this would also give them the data ownership they've been looking for since they would own the pipeline itself but this also comes at a cost the biggest challenge with this approach is really just the amount of time and effort required to build a pipeline this way the logos on the right represent just a portion of the tools that can be used for telemetry and this is a constantly evolving field it can be difficult to stay up to date on what tools are best suited for certain use cases amy and her team would need to invest a huge amount of time and effort in researching these tools not just in learning which tools to use for different stages of the pipeline but also around which tools are compatible with one another and this is before they even get to the work of configuring and customizing these tools to do what they want which of course presents its own set of challenges so both of these solutions have some major drawbacks sas solutions are managed and easy to set up but the team wouldn't have control over the code and they wouldn't have the data ownership they want diy gives them more control and ownership but that control comes at a heavy labor cost so they'd really like to have another way and this is where hifa fits in hyfo was designed to combine the ease of setup that sas provides along with the data ownership and control of a diy approach haifa provides a ready-made telemetry pipeline it automates setup and deployment processes and it provides a ui that's easy to learn and start using our code is open source which means that teams have the option to customize and control the code if they want but they don't have to change anything to get started open source code also means they have ownership of the data flowing through their pipeline now there are a couple of trade-offs with using hifa one is that we're not as feature-rich as some of the sas solutions but this is actually suited for amy and her team who are looking for something easy to learn that they can get started with quickly the other trade-off is that teams will have to manage the hi-fi infrastructure themselves so this isn't as convenient as a sas solution but for labradcadabra self-management is worth the trade-off in order to have control and data ownership so now we have more context for what haifa does and why we built it let's see how it looks to actually use haifa's ui to interact with logs and traces so here's the dashboard of the haifa ui it gives you an overview of your system health showing you metrics like errors by service logs by level or type and also a list of logs that contain errors you can filter the data displayed on the dashboard and you can do this by changing the time range or by filtering through service name if you want to take a closer look at one of the logs you can click into it and this will expand to show you more detail about that log this includes the main body of the log and it also includes the trace id of the trace that's associated with that log which we use for correlation to see this trace you can simply click the yeager button and you'll be taken to the trace overview here we see a waterfall chart representing the trace that is associated with the log we were just looking at now if you want to take a closer look at one of the spans in this trace you can click into the span and you'll see expanded information particular to that span to see even finer detail by looking at the logs that were produced during the span you can click the logs for the span button and a new frame will open up alongside showing the logs that were produced for that span and you can click into one of these logs as well to get more detail again you'll get an expanded view of that log's data and you can continue exploring in this way you can also format these logs so that they're a bit easier to read so going back and forth between logs and traces this way gives you a lot of flexibility and power in terms of how you investigate we also have dedicated tabs that allow you to search your logs and traces individually our goal here was to provide a ui that is focused and easy to get started with and we wanted this demo to highlight that core functionality but there are also a lot of options for customization and expansion because our ui is powered by grafana and we include a link that allows you to access grafana directly as well and that concludes the demo next up steve is going to talk about how we built haifa by giving us a tour of the architecture and discussing some of the major choices we made along the way thank you isaac there are four key design goals i'd like you all to keep in mind as we talk about haifa's architecture when we set out to build haifa we agreed on the following it should have drop in functionality be interoperable be easy to use and be ready to deploy that said let's dive deeper recall the three phases of the telemetry pipeline that isaac took us through earlier emit ship and present within each phase of ifa's architecture we chose specific tools and configurations let's start with the emit phase hyphen needs to generate traces and logs for each application and collect and emit telemetry data hyphen needs to accomplish both of these while also being drop-in and unobtrusive to existing application code and logging tools that means there shouldn't be any code changes to existing services manual setup or complex configuration in order to understand how hyper provides drop-in instrumentation and emission of telemetry data let's start with an example here we have a node.js application running in a virtual machine the application already uses a logging library such as winston which outputs application logs to local log files in the vm since our application already generates logs we next need a way to collect them we could change the logging library to send the log somewhere else such as directly to an agent however this would require considerable changes to the locking implementation violating our design goal in order to instrument logs while also leaving the existing logging system and log files intact we need a way to read from the log files that are ready in use this is where the open telemetry agent collector and specifically its file log receiver comes into play the open telemetry agent collector tracks reads and processes application log files to a standardized spec now that we have logs instrumented we need a way to generate and collect traces instrument instrumenting traces can be pretty complex it's usually a manual process of editing application code to define where to start and end traces and spans and then specify what details or attributes to attach to them this takes time to learn and implement correctly and remember it must be done for each application therefore we need a way to instrument traces without touching application code this is where open telemetry auto instrumentation comes in with auto instrumentation we can wrap javascript libraries and frameworks to attach use case specific details to traces all without changing application code one example is the network protocol library for http the instrumentation wraps the library attaching http request and response details to the current trace another example is the logging library winston the tracing implementation wraps here too automatically injecting the current trace id into each log that's created ulyu will talk more on the importance of this correlation later lastly the auto instrumentation sends trace data directly to the agent collector with instrumentation for logs and for traces in place the agent collector can be configured to emit and tran to emit the telemetry data to hyphas back-end since this process is needed across multiple services hyphen provides a script to automate downloading and configuring the tracing libraries and agent collector next we have this next we have the ship phase hyphen needs to aggregate logs and traces across services and write and store high throughput data hyphen needs to accomplish both of these while also being interoperable and flexible should users need to change should user needs change in the future it should be scalable to allow for growth and it shouldn't cause vendor lock-in to provide a flexible interoperable back end hyphen is a central gateway for all the agent collectors to admit to using a proper gateway would abstract away the downstream height of back-end services from all the agents but hypha's gateway needs to do more than just receive traces and logs it also needs to process and export them this is where the open telemetry gateway collector comes in it provides a central vendor neutral collection service allowing us to process and enrich our telemetry data in one location open telemetry's collector design also means configuration changes are easy for example changing the endpoint for where to send traces means swapping out an exporter essentially a single line change in the configuration file as a result open telemetry's collector has become the standard for gateway collectors in the observability world with most platforms now adopting the standard ifa configures the gateway collector so it's ready to go however there's nothing preventing users from adding to their configuration as their needs grow now that we have our gateway collector in place we need a storage solution for our traces and logs hypha storage solution should be able to handle high throughput the instrumented applications generate and emit a lot of time machine data it needs to write data quickly and should be able to scale hypha separates out the trace and log storage needs choosing two tools that are best suited for our use case loki is designed to query logs it's part of grafana lobbed grafana labs and integrates well with open telemetry and with grafana for ui we'll talk more about grafana shortly yeager is designed to query traces and is a well-respected tool with extensive community support it also integrates well with open telemetry and grafana lastly haifa ensures loki and jaeger are scalable and platform agnostic we've configured both to run as containerized services with read write deployment configurations vendor agnostic database stores and compo docker compose configuration next we have the present phase here hyphen needs to query logs and traces from the loki jager data stores and provide a seamless user interaction between logs and traces hypha needs to accomplish both of these while also being easy to use i shouldn't have a steep learning curve or require training prior to using that we have loki and jager api set up hyphen needs a way to query correlate and visualize these powerful data sources within a central interface furthermore the ui needs to leverage our correlated data to produce correlated visuals so that users can flow between related logs and traces this is where grafana comes in hypha leverages grafana to proxy api calls to both loki and jager data sources grafana also produces feature feature-rich interfaces for querying both traces and logs lastly and most importantly haifa configures grafana to link related logs and traces within debugging flows grifana's powerful visualization comes with one downside however grafana is designed for many use cases and as a result includes many features that can be overwhelming to haifa users this is where haifa ui comes in it focuses grafana's powerful log and trace features ensuring hyphen ensuring the hi-fi interface is easy to use now that we have our hyfo pipeline in place we need to make sure that hyphen is ready to deploy to meet our goal we designed hypha's back end using scalable dockerized services and configured it with docker compose this makes haifa easy to deploy across many environments and cloud platforms and while it's important for hyphen to be platform agnostic we also wanted to provide an automated deployment option for users looking to get started right away hifi leverages docker compose integrated with aws elastic container service to automate deployment users can deploy hifa to ecs from their terminal in three easy steps create an ecs context switch to that context and compose up easy next i'd like to hand it over to yulio who will take us through an important challenge we faced when designing and building haifa correlating logs with traces thanks steve remember from earlier in the presentation amy and her team found that correlating logs with traces would really be helpful in their debugging workflow let's look at how we accomplished correlation in detail to explain the approach we took to solve this problem i have to first distinguish between different types of correlation first you can correlate by the origin of telemetry here correlation is by the resource that emitted the logs or traces for example a particular microservice or a vm instance this correlation is rather general second you can correlate by the time of execution a log is associated with a trace if it occurred at the same moment in time or a time range for example a one a one minute window this type of correlation doesn't necessarily allow you to filter to the exact logs associated with a trace since many logs associated with different request executions might have occurred at a point in time third you can correlate by the execution context that means correlating traces precisely with the logs output during the same request execution to enable a smooth debugging workflow we want to correlate by the execution context so how do we accomplish this one option for correlating by the execution context involves replacing logs with span events span events are a form of structured logs appended directly to spams like logs they're for recording fine grain events at a point in time so instead of being separate types of telemetry logs are appended to spans as events and direc and directly inherit the trace context this option has a great advantage of simplifying the telemetry pipeline architecture we only need to collect and store traces however this would require users to refactor their logging instrumentation to append span events instead also for engineer used to working with logs this would be a fundamental change in how their tools work since logs are now embedded in the span data structure another option for correlating by execution context is to inject the trace context into existing logs so this means accessing the trace id and span id and appending it as metadata to each log so just directly dropping it in there the pros for this approach is that if the user already has login instrumentation they wouldn't need to refactor any of their code also if engineers like working with their logs they get to keep them the downsides are that the telemetry pipeline becomes more complex to implement since we need to collect and store both logs and traces also we need to combine the two data sources when it comes to querying and visualizing them in the ui adding complexity to the present stage as well given we want our users to not have to change their existing logging system to correlate logs with traces we went we went with the option of injecting trace contacts into logs instead of using span events to implement this we could create a log appender library that extends popular logging libraries and ejects the trace context however it turns out that otel has instrumentation libraries for popular node loggers so as part of the auto instrumentation code haifa sets up for tracing we also add instrumentation for logging libraries these append the trace contacts directly to logs now we can correlate logs with trace by the execution context i'll quickly mention what our team plans to work on moving forward we want to add instrumentation support for more languages we want to add support for additional login libraries and formats we want to containerize haifa the hifi agent to support more deployment options such as kubernetes or even docker containers next we want to automate adding tls certificates to the hifa backup so here's the team that built hyphen and that concludes our presentation thank you for listening if you have any questions we love to answer them you can put your questions either in the question and answer or in the chat and we've got our first question what kind of ecs configuration did you use for aws deployment does hyphen get deployed as a single ecs cluster when you use the cli app command with the ecs suffix um and i can start with this one and have anybody else jump in so we leveraged the docker compose and ecs integration um and so docker provides provides a way if you add that context um then you're able to just write docker compose up and uh and it will actually provision all of the resources and everything and set set it all up and get it running um and there's there are integrations for some other cloud services and so we wanted this to be flexible and that's why we chose that so there are some considerations in terms of changing some of the configurations to make them uh uh what's the word i'm looking for uh work with both ecs and locally or just regular doctor compose but for the most part it was all done through that dr compose integration yeah and one thing i'd add to that i think maybe the question um kind of maybe misunderstood one of the pieces that we were using a cli app command there where that command of like creating a new context a new ecs context is actually part of that docker integration and so that um basically what happens is like docker converts your docker compose file to a cloud formation script so it takes something that's around 100 lines of code and turns it into something that's i think like closer to like a thousand lines of code and then that gets used to actually deploy all right our next question from wes anderson says good job haifa very impressive breakdown of observability and observability pipelines i might have missed the answer to my question but i'll ask it anyway for the emission part of the infrastructure did you assume the user is already using the open telemetry collector or were you modifying their virtual machines steve do you want to take that one sure so part of the emit phase is and part of what we automate with our setup script in each of those vms is setting up an agent collector and so that runs as a binary and and you have one of those running per vm so that handles the the collection of traces and logs and it also handles the emission to the gateway and wes says thanks in the chat richard blue said great job guys this was the first time i felt fully capable of following along a capstone presentation and i remember what that feels like so congratulations richard um do you support java logging and i'll just quickly say no right now um we are only working with node.js but in the future we would like to expand to support other languages and then sophie asks what were the main factors that made you choose to use jager does anybody want to jump in on that i'll do that um so in terms of some of the other options uh the main other option we considered was grafana labs which has tempo which is their uh trace storage solution and um we tried that actually at first and it it made a lot of it it was really easy to configure with grifana labs but it was a little bit limited in terms of the search options um it generally supports searching uh with trace id and then you can configure it with loki to use like loki tags to search in the um in tempo um but we just found that the to be able to really get an overview of your traces and do some of the things we wanted our users to be able to do to get the benefits of tracing that jaeger was a little bit more full-featured in that way so that was i don't know if anybody else remembers some of the reasons there yeah yeah maybe one other thing to add is that jager also provided a lot of different deployment options they they have a lot of different images that they provide and you can configure them to run uh more scalably uh so we specifically have a read and write configuration for jaeger and and for loki that allows us to off put some of the some of that work on uh those two different uh parts of the of the system and so that improved performance a fair bit as well and that was something that was part of our consideration there just to be clear i think that tempo does have a scalable infrastructure but it was either running as one binary or like a full microservices deployment and uh so that was we liked the middle ground of having jaeger be able to have just those couple components that provides scalability without having to you know learn kubernetes at this point yeah thanks i wasn't clear on that part one quick thing i'd add to that too is um grafana tempo is a really new solution um so part of just one more thing to add to that is jaeger has a lot more community support and just history around um i think just being more familiar with that tool and like josh was saying kind of giving us a little bit more flexibility and what you're able to search was a major consideration um so michael has a question he says that was very interesting thank you um the question is how does the collector agent know where the logs are going from any given app i'm assuming this means like where they're stored and do the logs have to be going to a specific predictable place for them to be noticed and collected somebody want to jump in on that i can answer that um so uh haifa has a a script we provide for users to set up the collector agent and as part of that script you provide an argument uh the path which can include like a splat star arguments so you provide the path to the logs and then the script installs the collector and configures it so that it knows where to look for your logs and yes they need to be in a file at a set file path and the user can specify that file path yeah and one thing that's a little confusing about um the open telemetry collector is we have sort of two like versions of that code in a sense where one is the agent that yulia was just talking about that collects logs from the file um but i think part of the question was also asking about how does the collector know um and that is actually a little bit different where the gateway collector um where like all of this stuff gets aggregated is using um a another receiver so that all these different agents are kind of sending to one central place we have another question that i think i'll pass to julio how did you integrate grafana with your ui yeah so to do this we used um first on the architecture side we had to use njinx reverse proxy that served a haifa ui but also proxied requests to grafana and then in the browser we have an iframe where we load grafana and and then we have because they're running on the same origin the iframe and the hi-fi ui we could manipulate uh the code inside the iframe inside grafana and again we did this to uh because we found using grafana can be a bit overwhelming there's a lot of like tabs you can click on uh a lot of things you can customize and if you were we wanted our user when they get started to just have to something simple where they know what to pay attention to and where to look for things we kind of viewed it as as like curation of grafana so we provided some custom dashboards um and one that allows you to search and one kind of sees that overview um and just give a landing place for that simple experience thanks everybody for coming and reach out if you have any uh any other questions or want to chat about it yeah thanks everybody josh thanks eliot you 