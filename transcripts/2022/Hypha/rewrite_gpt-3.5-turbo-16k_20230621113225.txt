Welcome, everyone! Thank you for joining us. My name is Yulia, and together with my teammates Josh, Steve, and Isaac, we have developed Haifa, an open-source observability framework. Haifa facilitates powerful debugging workflows by setting up distributed tracing and a telemetry pipeline that aggregates logs and correlates them with traces. In this presentation, we will cover various aspects of Haifa. I will start by discussing observability and its foundational concepts. Then Josh will present a fictional health tech startup story to highlight the problem that Haifa solves. Following that, Isaac will delve into creating a telemetry pipeline, exploring existing solutions, and providing a demonstration of Haifa. Steve will then guide us through Haifa's architecture, and finally, I will discuss the engineering challenges we faced and outline future work for Haifa.

In the software industry, there is no universally agreed-upon definition of observability. Different sources and individuals provide various interpretations. For our purposes, we prefer the definition originating from control theory, which states that observability is a measure of how well the internal states of a system can be inferred from its external outputs. Let's unpack this definition further. Software systems primarily consist of inputs and outputs. Users, developers, or other systems provide inputs, and the system processes these inputs to produce outputs. However, understanding what's happening inside the system, the internal states, can be challenging. It is akin to a black box where we can see the inputs and outputs, but not the intermediate steps or exceptions within the application. To address this, we need to output more data, specifically telemetry, which is data that increases system observability. In the software industry, telemetry is often categorized into three types: logs, metrics, and traces. These three pillars form the basis of observability. HiFA focuses on correlating logs and traces.

Logs are lines of text that provide human-readable, detailed information about events within a software system. They are often structured for machine parsing, such as using JSON format, and are valuable for providing fine-grained information. Traces, on the other hand, offer a higher-level view by showing the path a request takes through a system. Traces consist of spans, which represent individual service calls, along with the time taken for each request. By visualizing traces as a waterfall chart, we can understand the flow of requests and the relationships between different system components. Traces give us a holistic view of the system, allowing us to understand how its various components interact over the network.

Now let's explore why correlating logs and traces is essential through the story of a fictional health tech startup called Labra Cadabra. Amy, a senior engineer at Labra Cadabra, is working on adding a new feature to their application. Despite her expectations for certain outputs, the behavior of the code is not as anticipated based solely on the output. To understand the issue, Amy introduces logging into her code and outputs additional information related to the specific events. This allows her to pinpoint and rectify the problem. However, when deploying the code to the production server, the dynamics change. While logs can be easily observed in a local development environment, in production, where no one actively monitors the logs due to their overwhelming volume, storing logs becomes crucial. Amy and her team invest time and effort in ensuring that the logs contain all the necessary information for debugging purposes.

Labra Cadabra experiences rapid growth, prompting a transition to a microservice architecture. This move leads to several challenges. Firstly, with services spread across multiple nodes, finding specific logs for debugging becomes tedious. To alleviate this issue, Labra Cadabra decides to aggregate logs in a centralized location, making debugging more efficient. Secondly, as the system becomes more distributed, understanding how execution flows through different components becomes increasingly challenging. Requests pass through various services, and identifying the source of errors and comprehending the execution path becomes time-consuming, especially for new hires. The solution to this challenge is distributed tracing, which provides a visual overview of the execution path of each request. With distributed tracing, developers can better understand the connections and dependencies between different components, simplifying the debugging process.

In the Labra Cadabra context, distributed tracing proves valuable. For instance, Amy investigates a delayed request to the cart service by examining traces. By comparing different traces, she can identify anomalies, such as longer response times for specific service calls. While the traces do not reveal the root cause, they guide her in narrowing down the problem area. Distributed tracing also helps in cases where errors occur in services like the analysis service. By examining the traces, Amy can identify patterns, such as repeated calls to a particular service, highlighting the area where the error originates. Consequently, Labra Cadabra decides to aggregate logs and implement distributed tracing to improve their debugging capabilities.

Amy and her team understand that for optimal observability, logs and traces work together. Logs provide specific details about events, while traces offer a high-level picture of the execution path. Integrating these two data types allows developers to gain comprehensive insights into the system. Switching between logs and traces helps users obtain the necessary context for effective debugging. However, finding the desired trace associated with a particular log can be challenging due to the abundance of logs and the lack of differentiating features. Developers typically rely on ad hoc methods such as using timestamps, error codes, or filtering by service. This process can be especially demanding during high-pressure situations like incident debugging. Similarly, it can be challenging to trace logs associated with a specific span in the trace. This issue arises because logs may look similar, and distinguishing between them becomes cumbersome. Amy and her team hope to find a seamless solution that effortlessly connects log data with trace data.

 The transcript discusses how Amy, a developer, investigates a user-reported error occurring in the lab test service. She finds a log that provides details about the error, but she needs more context about the requests that caused the error. To find the trace associated with the specific logs, she uses a search user interface (UI) in a popular open source tracing tool called Jager. The UI allows her to set a custom time range based on the timestamp of the log and filter by the service and error code. By searching for traces with these criteria, she can narrow down the number of traces to a manageable amount.

However, finding the logs associated with a specific trace can be challenging. In distributed systems, the root cause of an error may lie in a different service, and that service may not have thrown an error. Amy needs a way to navigate from traces to logs in order to investigate the error thoroughly. She can search the logs for the specific service and filter the time stamps based on when the span occurred. However, if there are many logs within that time frame, it can be difficult to find the relevant ones. Amy could also correlate the trace with the logs based on the user request data included in the traces, which would provide additional context.

The core problem is that while logs and traces contain valuable information, there is no easy way to connect them efficiently. Logs often look similar, and there is a need for a single context that can be attached to every log and trace, allowing for seamless navigation between the two. This context should travel with the request as it moves through the system.

The solution to this problem is to create a telemetry pipeline that connects log and trace data. This pipeline would attach a single context to every log and trace as they are created, enabling easy navigation between them. The telemetry pipeline would handle the creation, attachment, and usage of this data.

The transcript then introduces Isaac, who discusses the major stages of a telemetry pipeline: emit, ship, and present. In the emit stage, code or instrumentation is added to generate the telemetry data. The emitted telemetry data is then prepared for shipping, which can include formatting or enriching the data with additional contextual information. The shipping stage involves collecting, processing, storing, and transforming the emitted data. Finally, the presentation stage focuses on querying and visualizing the stored telemetry data in a useful way.

Amy and her team understand the benefits of observability and want to upgrade their system to incorporate log aggregation, distributed tracing, and correlation between logs and traces. Now they face the question of how to achieve this. They consider using SAS solutions offered by third-party vendors. These solutions require minimal setup and offer feature-rich UIs, but they come with drawbacks such as data ownership and privacy concerns.

Another approach they consider is using open-source telemetry tools to create a do-it-yourself (DIY) solution. This would give them control over their telemetry pipeline and data ownership, but it requires a significant amount of time and effort to research, configure, and customize the various tools involved.

Given these trade-offs, Amy and her team are looking for a third option that combines the ease of setup provided by SAS solutions with the data ownership and control of a DIY approach. This is where Haifa comes in. Haifa is a ready-made telemetry pipeline that automates setup and deployment processes and provides an easy-to-use UI. It offers the option for customization and control while still allowing teams to get started quickly. However, it requires self-management of the infrastructure.

The transcript then proceeds to demonstrate how the Haifa UI can be used to interact with logs and traces. The UI provides a dashboard with an overview of system health and metrics. Users can filter the displayed data by changing the time range or filtering through service name. Clicking on a log expands it to show more detail, including the trace ID associated with that log. Clicking on the Yeager button takes users to a trace overview with a waterfall chart representing the trace. Users can further investigate spans within the trace and view the logs produced during those spans.

The transcript also mentions that Haifa provides dedicated tabs for searching logs and traces individually. The UI is powered by Grafana, which allows for customization and expansion. Despite some trade-offs compared to SAS solutions, Haifa offers a balance between ease of use and control, making it a suitable choice for Amy and her team.

The final section of the transcript introduces Steve, who outlines the four design goals of Haifa's architecture: drop-in functionality, interoperability, ease of use, and ready deployment. The architecture follows the three phases of the telemetry pipeline—emit, ship, and present—and each phase incorporates specific tools and configurations.

In the emit phase, Haifa generates traces and logs while ensuring drop-in instrentation and emission without requiring code changes to existing services. The ship phase involves collecting, processing, and storing telemetry data, and Haifa allows for customization and configuration in this phase. The present phase focuses on querying and transforming the stored data into useful visualizations, utilizing Grafana for customization and expansion.

The architecture of Haifa aligns with the design goals and provides a solution that combines the benefits of SAS solutions and DIY approaches. It automates setup and deployment processes, provides an easy-to-use UI, and offers data ownership and control. Although Haifa may have some trade-offs, it is a suitable choice for Amy and her team.

In conclusion, the transcript discusses Amy's investigation of a user-reported error, the need to connect logs and traces, and the challenges of finding the relevant trace and logs. It introduces the concept of a telemetry pipeline and explores different approaches—SAS solutions, DIY using open-source tools, and the Haifa solution developed by the team. The transcript also provides a demonstration of Haifa's UI and explains its architecture. Throughout, it emphasizes the importance of ease of use, control, data ownership, and effective visualization for observability in a distributed system. The logging feature in our coding Capstone project video allows users to click into a log to reveal more details. This includes the main content of the log and the associated trace ID for correlation. To view the trace, users can simply click the "Yeager" button and they will be taken to the trace overview. Here, they can see a waterfall chart representing the associated trace. For a closer look at a specific span within the trace, users can click into the span to view expanded information specific to that span. To examine even finer detail, users can click the "Logs for the Span" button to open a new frame showing the logs produced during that span. Clicking into one of these logs will provide more detailed information, expanding the view of the log's data. This exploration can continue in this manner. Users also have the ability to format logs for easier reading. This flexible back-and-forth exploration between logs and traces provides users with powerful investigation capabilities. 

In addition to the log exploration feature, our project includes dedicated tabs that allow users to search for logs and traces individually. Our goal was to provide a user interface that is focused and easy to use, showcasing the core functionality. However, there are also options for customization and expansion as our user interface is powered by Grafana. We provide a direct link for users to access Grafana, allowing for further customization and exploration. This concludes the demo section. 

Now, I'd like to pass it over to Steve who will discuss how we built Haifa and give a tour of the architecture, as well as discuss some of the major choices we made during the process. Thank you, Isaac. When we set out to build Haifa, we had four key design goals in mind. First, we wanted it to have drop-in functionality; second, we wanted it to be interoperable; third, we wanted it to be easy to use; and fourth, we wanted it to be ready to deploy. Let's dive deeper into the architecture of Haifa and the choices we made to achieve these goals. 

As Isaac explained earlier, the telemetry pipeline in Haifa consists of three phases: emit, ship, and present. For each phase, we selected specific tools and configurations to fulfill the requirements. In the emit phase, Haifa needs to generate traces and logs for each application and collect and emit telemetry data. It should accomplish this without requiring any code changes to existing services or complex configuration. To achieve this, we use the OpenTelemetry agent collector, specifically its file log receiver, to track, read, and process application log files to a standardized specification. For instrenting traces, we use the OpenTelemetry auto instrentation feature which allows us to wrap JavaScript libraries and frameworks to attach use case-specific details to traces without changing application code. The instrented trace data is then sent directly to the agent collector for emission. To simplify the setup process across multiple services, we provide a script that automates the downloading and configuration of the tracing libraries and agent collector. 

In the ship phase, Haifa needs to aggregate logs and traces across services and write and store high throughput data. This should be done in an interoperable and flexible manner to accommodate future changes. To achieve this, we use the OpenTelemetry gateway collector as a central vendor-neutral collection service. This collector receives traces and logs from the agent collectors, processes and enriches the data, and exports it. The gateway collector is configurable, allowing for easy changes such as modifying the endpoint to which traces are sent. For storage, we selected Loki for logs and Jaeger for traces, both of which integrate well with OpenTelemetry and Grafana. To ensure scalability and platform compatibility, Loki and Jaeger are configured to run as containerized services with vendor-agnostic database stores and Docker Compose configuration. 

In the present phase, Haifa needs to query logs and traces from the Loki and Jaeger data stores and provide a seamless user interaction between them. It should be easy to use and have a simple learning curve. To achieve this, we leverage Grafana as a proxy for API calls to Loki and Jaeger data sources. Grafana provides feature-rich interfaces for querying and visualizing traces and logs. However, to make the user experience more seamless, we have focused Grafana's powerful log and trace features specifically for Haifa, ensuring that the interface is easy to use. 

To ensure that Haifa is ready to deploy, we designed its backend using scalable Dockerized services and configured it with Docker Compose. This makes Haifa easy to deploy across different environments and cloud platforms. Additionally, we provide an automated deployment option using AWS Elastic Container Service (ECS). Users can deploy Haifa to ECS from their terminal in just three simple steps: creating an ECS context, switching to that context, and running the compose up command. 

Now I'll pass it over to Yulio who will discuss an important challenge we faced during the design and development of Haifa: correlating logs with traces. 

Thank you, Steve. As mentioned earlier in the presentation, the ability to correlate logs with traces was identified as a valuable feature in the debugging workflow. Let's delve into the approach we took to tackle this challenge. First, it's important to understand the different types of correlation. First, there is correlation by the origin of telemetry, which means correlating logs or traces by the resource that emitted them, such as a specific microservice or a VM instance. Second, there is correlation by the time of execution, where logs are associated with traces if they occurred at the same moment or within a time range. However, this type of correlation does not guarantee filtering to the exact logs associated with a trace, as multiple logs from different request executions may occur at the same point in time. Third, there is correlation by the execution context, which involves correlating traces precisely with the logs output during the same request execution. 

To enable a smooth debugging workflow, we wanted to correlate logs with traces by the execution context. Two options were considered for implementing this correlation: replacing logs with span events or injecting trace context into existing logs. The first option involved appending structured logs, called span events, directly to spans. This simplified the telemetry pipeline architecture, as only traces needed to be collected and stored. However, it required refactoring logging instrentation to append span events instead of using separate log telemetry. This fundamental change in how logs were handled could be challenging for engineers accustomed to working with logs. 

The second option, injecting trace context into existing logs, was ultimately chosen to maintain compatibility with existing logging systems and workflows. By accessing the trace ID and span ID, the trace context was appended as metadata to each log, enabling correlation by the execution context. This approach allowed us to leverage existing logging libraries and formats without requiring code refactoring. However, it did introduce some complexity to the telemetry pipeline, as both logs and traces needed to be collected and stored, and the two data sources needed to be combined for querying and visualization in the user interface. 

Moving forward, our team plans to expand Haifa's functionality in several areas. We want to add instrentation support for more languages, as well as support for additional logging libraries and formats. Containerizing the Haifa agent is also on our roadmap to provide more deployment options, such as Kubernetes or Docker containers. Additionally, we plan to automate the addition of TLS certificates to the Haifa backend. 

That concludes our presentation. Thank you for listening. If you have any questions, please feel free to ask them in the question and answer or chat section. We would be happy to answer them. In this approach, users will not need to refactor any existing code if they already have login instrumentation. Additionally, engineers who prefer working with their logs can keep them as well. However, implementing the telemetry pipeline becomes more complex because it requires collecting and storing both logs and traces. Moreover, querying and visualizing the data from these two sources in the UI becomes more complex as well, thus adding complexity to the current stage.

To address the need for correlating logs with traces without requiring users to change their existing logging system, we decided to inject trace context into logs instead of using span events. We developed a log appender library that extends popular logging libraries, which allows us to inject the trace context directly into logs. Fortunately, OpenTelemetry (otel) already has instrumentation libraries for popular node loggers, so we leveraged them as part of the auto-instrumentation code our team developed for tracing. With these modifications, logs can now be correlated with traces based on the execution context.

Moving forward, our team plans to focus on several key areas of improvement. First, we aim to add instrumentation support for more programming languages. This will allow us to expand the compatibility of HAIFA to a wider range of applications. Second, we intend to add support for additional logging libraries and formats, providing users with more flexibility in terms of their logging preferences. Third, we plan to containerize HAIFA to enable deployment options like Kubernetes or Docker containers. This will enhance the flexibility and scalability of HAIFA in different environments. Lastly, we aim to automate the process of adding TLS certificates to the HAIFA backup, further improving its security features.

We want to thank you for taking the time to listen to our presentation. If you have any questions, please feel free to ask. You can submit your questions either in the question and answer section or in the chat. Our team is here to provide answers and assistance. Now, let's move on to the questions.

The first question is regarding the ECS configuration for the AWS deployment. Specifically, the question asks whether HAIFA is deployed as a single ECS cluster when using the CLI app command with the ECS suffix. To answer this question, we utilized the Docker Compose and ECS integration. By setting the appropriate context and running the "docker-compose up" command, all the necessary resources are provisioned, and HAIFA is deployed and running. This approach provides flexibility since it can work with other cloud services as well. However, some configurations might need to be adjusted to support both ECS and local or regular Docker Compose environments. Overall, the integration simplifies the deployment process while offering flexibility.

We received positive feedback from Wes Anderson, who praised our breakdown of observability and observability pipelines. Although Wes mentioned potentially missing the answer to their question, we encourage them to go ahead and ask it. We are here to help and address any inquiries.

Wes Anderson's question is about the emission part of the infrastructure. They want to know whether we assumed that the user is already using the OpenTelemetry Collector or if we modified their virtual machines. In response, we did automate the setup process for each VM by running a script that installs an agent collector. This agent collector handles the collection of traces and logs, as well as the emission to the gateway. Therefore, to achieve the emission functionality, we incorporated the OpenTelemetry Collector as part of our setup script.

Richard Blue expressed appreciation for our presentation, mentioning that it was the first time they felt fully capable of following along with a Capstone presentation. We are glad our presentation was clear and comprehensible.

Regarding Java logging support, as of now, HAIFA only works with Node.js. However, in the future, we plan to expand our support to include other programming languages, such as Java.

Sophie asked about the factors that influenced our decision to use Jager. In response, Jager offered a more full-featured solution compared to other options we considered, such as Grafana Labs' Tempo. Although Tempo was easier to configure with Grafana Labs, it had limitations in terms of search options. Jager provided a better overview of traces and offered more functionalities that aligned with our users' needs. Additionally, Jager offered various deployment options, which allowed us to select the most suitable one for HAIFA. Jager's community support and familiarity were also significant factors in our decision-making process.

Michael asked how the collector agent knows where the logs are coming from in each application. The setup script provided by HAIFA allows users to specify the file path to their logs. The collector agent is then configured to collect logs from that specified file path. Thus, the logs need to be stored in specific, predictable locations for the collector agent to notice and collect them.

It is worth clarifying that there are two versions of the OpenTelemetry Collector: the agent collector for collecting logs from files and another version used for aggregating the collected data. The latter, known as the gateway collector, receives data from multiple agent collectors and aggregates it into a central location.

Overall, our team is grateful for the positive feedback and questions received. We hope we were able to provide satisfactory answers and clarification. If there are any more questions or if anyone would like to continue the discussion, please don't hesitate to reach out to us. Thank you again for your time and engagement.