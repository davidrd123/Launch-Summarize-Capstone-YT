Welcome everyone, thank you for joining us today. We will be presenting Waypost, an open-source feature flag management platform that focuses on A/B testing. Our team, consisting of Julia, Sean, and Caleb, has spent the last few months developing Waypost. In this presentation, we will provide an overview of the problem that Waypost addresses, discuss A/B testing and feature flags in detail, explore implementation solutions, and analyze the pros and cons of each option.

To begin, let's define some key terms. In this context, a "feature" refers to a distinguishing characteristic of a software item, such as performance, portability, or functionality. A "new feature" represents a change in the functionality of an application, whether it's an addition or an upgrade to an existing feature. These changes can range from small UI modifications to complex algorithmic improvements.

Development teams creating new features need to ensure that their design decisions align with the business goals. This requires making data-driven decisions by analyzing user behavior and understanding the impact of design choices. To achieve this, teams run experiments by serving two versions of the application to users: the existing version and the new feature. Collecting data from both versions allows teams to gain insights on the effectiveness of their design choices. In addition, feature flags enable developers to easily toggle off new features that may have bugs or performance issues.

Now, let's consider a hypothetical example to illustrate why an organization would use Waypost. Imagine a solar panel company called Solar Flare. One way Solar Flare generates sales is through a feature that estimates the cost of a solar panel installation. However, the company discovers that 80% of users who start filling out the installation form do not complete it. To address this issue, Solar Flare designs a new version of the form that utilizes third-party software to calculate the quote based on the user's address and satellite and climate data. The company wants to determine whether this new feature improves the completion rate of the form and needs a way to measure the difference between the old and new versions.

One option is to temporarily roll out the new feature and measure the metrics. However, this approach has drawbacks. The new feature has not been tested in production and may contain bugs, so it shouldn't be rolled out to all users at once. Additionally, external factors like time may influence user behavior, potentially skewing the results. Therefore, Solar Flare needs a controlled experiment where the only difference between the user experiences is the feature being served to them. This is where A/B testing comes into play.

A/B testing, also known as experimentation, is a common method used by product development teams to test the impact of new features. It involves randomizing the assignment of users to different versions of a feature and comparing metrics between the groups. By analyzing the differences in metrics, teams can make informed decisions about whether to ship the new feature, continue experimenting, or stick with the old version.

While A/B testing is a valuable tool, it comes with challenges. Randomly assigning users to different groups is necessary, but it can be complex to implement. Additionally, tracking user assignments, logging user events, and analyzing the results require careful consideration. To simplify this process, many companies use feature flags.

Feature flags are toggles that control the activation of specific functionality without requiring a redeployment. With feature flags, developers can write conditional statements in the application's code to determine which version of a feature to display. In the case of Solar Flare, they can render the new form when the feature flag is on and the old form when it's off. This flexibility makes feature flags ideal for A/B testing scenarios.

To implement A/B testing with feature flags, developers need a platform that manages feature flags and provides a way to collect and analyze user event data. This typically involves storing feature flag data in a database and using a server to handle queries. Developers also require a UI for managing feature flags, allowing non-technical personnel like product managers and marketers to collaborate. Furthermore, an SDK is needed to communicate between the backend server and the application, ensuring that flag updates are received and features are toggled accordingly.

Now that we understand the core concepts, let's discuss two options for implementing these functionalities: building your own platform or using a third-party service. Building your own platform offers control and customization, but it requires significant time and resources, as well as ongoing maintenance. Third-party services, such as LaunchDarkly and Optimizely, offer feature-rich solutions and save time, but they may be too expensive or lack the flexibility of customization.

Depending on the size of the organization and its specific needs and budget, either building a custom platform or utilizing a third-party service can be the right choice. However, for smaller companies like Solar Flare, these options may not be suitable due to cost or limited feature requirements. As a result, we saw an opportunity to develop Waypost, a self-hosted, open-source feature flagging platform tailored to the needs of small businesses.

Waypost provides a customizable solution while still offering core features necessary for managing feature flags and running experiments. Now, I'll hand it over to Sean, who will discuss the architecture of Waypost and demonstrate how to deploy it using Docker.

Thank you, Julia. Let's dive into Waypost's architecture and deployment using Docker. Waypost consists of several components, color-coded in blue, which are part of the Waypost architecture. Additionally, there are red components that developers are responsible for creating.

Starting on the left, we have the manager application, an all-in-one solution with a UI, a backend server, and a PostgreSQL database. This application allows users to manage feature flags, start and stop experiments, and view results. The manager app sends flag data to the flag provider, which stores and distributes the data to the applications running the Waypost SDK.

The Waypost SDK, embedded within the application, evaluates feature flags at runtime, enabling the serving of multiple versions of the website to users. User events are logged in an events database, and when an experiment is complete, the manager's backend queries the events database, performs statistical analysis, and displays the results in the UI.

To deploy Waypost using Docker, start by cloning the Waypost Docker repository from GitHub. Then, run the Docker Compose file, which will set up instances of the manager app and the flag provider. Next, install the Waypost SDK in your application and provide the necessary address for the SDK to connect with the flag provider.

Let's now focus on the manager app. It consists of a UI, a backend server, and a PostgreSQL database. The UI serves as the dashboard for managing feature flags. When you first access the manager app, you'll see the Flags Dashboard, where you can create, update, and delete flags. The UI also allows you to start and stop experiments, analyze results, and perform statistical analysis.

In summary, Waypost provides developers with a self-hosted and open-source feature flag management platform tailored to the needs of small businesses. With its customizable architecture and essential features, Waypost simplifies A/B testing, enabling data-driven decisions. Back to you, Caleb, who will discuss engineering decisions and discussions we faced during the development of Waypost.

Thank you, Sean. In the development of Waypost, we encountered several engineering decisions and discussions. One of the key considerations was whether to use an SDK or an API for communication between the manager app and the applications running the Waypost SDK.

We chose an SDK approach because it offers better control and flexibility. The SDK allows us to embed the code within the application, reducing network latency and minimizing the impact on application performance. Additionally, using an SDK simplifies the implementation process for developers.

Another decision we made was to use Docker for deployment. Docker provides a standardized and reproducible environment, making it easier to set up and deploy Waypost across different systems. It also facilitates scalability, allowing additional instances to be spun up as needed.

Throughout the development process, we focused on making Waypost self-hosted and open-source. By doing so, we ensured that users could customize the platform to their specific needs, maintain full control over their data, and benefit from contributions from the open-source community.

In conclusion, we have discussed the problem Waypost solves, the concepts of A/B testing and feature flags, and the different implementation options. We presented Waypost as a self-hosted, open-source alternative that caters to the unique needs of small businesses. We also explored Waypost's architecture and demonstrated how to deploy it using Docker. We hope that Waypost can help organizations effectively manage feature flags and make data-driven decisions. Thank you for your attention.

Now, we will open the floor to any questions you may have. In this video, Sean will discuss the architecture of WAPIs (Waypost APIs). Now that we have an understanding of how Waypost addresses our use case, let's delve into the details of Waypost and its architecture. Firstly, we will provide an overview of the Waypost architecture and explain how to deploy Waypost using Docker. Next, we will discuss the individual components of Waypost and their roles in the overall architecture. Lastly, we will demonstrate how Waypost allows developers to perform A/B tests on their applications. Let's begin by looking at the complete view of the Waypost architecture. While it may seem overwhelming at first, don't worry as we will dive deep into each component, how they interact, and how they aid developers in managing feature flags and running experiments.

Now, let's discuss the different colors used in the architecture diagram. The blue components represent the parts of the Waypost architecture, while the red components are the developer's responsibility. Starting from the left, we have the Manager Application, which is a full stack application responsible for managing feature flags and experiments. The Manager Application sends copies of flag data to the Flag Provider, which saves the copy and forwards it to each application running the Waypost SDK. The SDK, embedded into the developer's application, evaluates flags at runtime, enabling the serving of multiple versions of the website to users. User events are logged using the developer's existing solution, and these events are sent to an Events Database. When an experiment is finished, the Manager Application's backend queries the Events Database, performs statistical analysis, and displays the results in the user interface.

Now, let's explore how to deploy Waypost using Docker. To begin, you will need to clone the Waypost Docker repository from GitHub. Next, run the Docker Compose file, which will start instances of the Manager Application and the Flag Provider. Install the SDK in your existing application and provide it with the address to connect with the Flag Provider. This will establish the connection between the Manager Application, Flag Provider, and your application.

Let's dive deeper into the Manager Application. It consists of three components: a UI, a backend server, and a PostgreSQL database. The Manager Application stores your feature flags, allowing you to create, update, and delete them. It also manages experiments, runs statistical analysis, and provides updated flag data to the Flag Provider.

Now, let's explore how to manage feature flags in the Manager Application UI. When you first visit the Manager Application, you will see the Flags Dashboard page. You can return to this page at any time by clicking the "Flags Dashboard" button in the navigation menu. The Flags Dashboard displays all the flags used by your application. From here, you can toggle and delete flags. To create a new flag, click "Create New," and enter the required information, with the most important being a descriptive name. The other fields are optional. To view a flag's details, click on its name in the Flags Dashboard, where you can see its description, rollout percentage, and make edits if needed. The Flag Events page shows when any flag was created, edited, or deleted. This page is useful for tracking changes to your feature flags, and you can filter events by flag.

Now let's discuss the Flag Provider and its role in serving real-time updates to the SDKs. The Flag Provider is a lightweight and scalable service responsible for sending updates to the SDKs in real-time. When changes are made to the flag data, the Manager Application sends an HTTP request, also known as a webhook, with the updated flag data. The Flag Provider saves this data and forwards it to the SDKs using server-sent events (SSE) connections. The SDKs store this up-to-date flag data.

The SDK is a package that is installed in your application and is responsible for evaluating flags and rendering different versions of your application. Currently, we have SDKs for React (client-side) and Node (server-side). To install an SDK, use npm. Now, let's explore how Waypost SDKs render multiple versions of a website. For server-side rendering, developers install the Waypost Node SDK into their application. When a request comes in, the SDK evaluates flags during the application rendering process, and the page is then sent in the response. For client-side rendering, developers install the Waypost React SDK into their application. When a request comes in, the React app is served to the client. The React app runs on the client, and the SDK evaluates flags while rendering the page.

Now, let's consider what the SDK needs to consider when evaluating flags. The most obvious consideration is the flag status. However, sometimes you may want a flag to be on for specific users. To ensure consistent flag evaluation per user, Waypost passes in a user ID into a hashing algorithm. This allows consistent flag evaluation and ensures that users are served the same version of the application even if they leave and come back. Each user ID needs to be unique for each user or visitor to your application. User IDs can be based on various factors, such as IP addresses, usernames in a login system, or a random string stored in a cookie.

If you want to customize how a flag evaluates for specific users, Waypost provides custom assignments. This allows you to tailor flag evaluation based on specific user IDs. For example, you may want beta testers to receive a new feature, while others see the old version. You can view and edit a flag's custom assignments in the Manager Application UI. Waypost evaluates flags based on flag data attributes, including the flag status (true or false), custom assignments (user ID and status pairs), and a rollout percentage to determine what percentage of users receive the new feature.

Now, let's discuss how flags are evaluated using an example. In our example, we have a feature flag in a React component. The "evaluate flag" method determines whether to render the new or old quote form based on the flag status. The SDK client object, which stores the flag data and the user ID, utilizes both data points to evaluate the flag. To integrate your application with the Waypost SDK, you need to generate an SDK key and insert it into your application. This key is used to verify connections between the Flag Provider and the Waypost SDK embedded in your application.

Now, let's look at how to integrate a React app with Waypost's React SDK. Before adding feature flags to individual components, you need to add code to your top-level app component. Import the "config" and "WaypostProvider" from the npm package. Call the "config" constructor, passing in your SDK key and the address of the Flag Provider service. This will return a config object. Wrap everything in the return statement inside the WaypostProvider component, passing in the config object as a prop. This allows the rest of your app to access the SDK client and its methods using React context. When serving a feature flag or using a feature based on a flag, import the Waypost context from the npm package. Instantiate the SDK client object using React's useContext hook. Call the "evaluate flag" method to get the flag status, passing in the feature flag's name and a default value if the data is unavailable. Use the boolean value to determine which feature to display in your code.

Lastly, let's discuss running experiments with Waypost. To run experiments, an organization must have user event data stored in a PostgreSQL database known as the Events Database. Read access to this database must be granted to Waypost. The developer needs to send data from their application to the Events Database whenever a user is exposed to an experiment or performs an action to be measured. For example, when a user signs up for a quote. To start an experiment, the Manager Application must be connected to the Events Database. Metrics need to be set up to determine experiment success. The developer can then create and start an experiment on a flag. To determine when to end an experiment, it's common to wait for enough users to be exposed since larger sample sizes yield more reliable results. The Manager Application provides updates on the number of users exposed to an experiment through the Exposures Pipeline. This pipeline aggregates user data each night and displays it in the UI.

In conclusion, Waypost's architecture consists of components such as the Manager Application, Flag Provider, and SDKs. The Manager Application is responsible for managing feature flags and experiments, while the Flag Provider serves real-time updates to the SDKs. The SDKs evaluate flags and render multiple versions of your application. Custom assignments and rollout percentages allow for personalized flag evaluations. To integrate the Waypost SDK into your application, generate an SDK key and configure it with your app. Running experiments with Waypost requires an Events Database to store user event data, and metrics can be set up to measure experiment success. It's important to wait for sufficient user exposure before ending an experiment. To measure an experiment, such as when a user signs up for a quote before starting, the developer must connect the manager app to the events database. They need to create the metrics they want to measure, and then they can create and start the experiment on one of their flags. Setting up experiments in the UI involves connecting the manager app to the events database. The button shows whether you are connected or not, allowing you to connect or disconnect as needed. Input the credentials to connect to the events database, along with a query string for querying the data related to the experiments. After submitting, the manager app verifies the credentials and query, saving the connection.

Before creating an experiment, it is necessary to set up metrics that will determine the experiment's success. To do this, go to the metrics page and open the new metric form. There are four types of metrics to choose from, as explained in the documentation. The metric query requires specific column names for data related to the analysis. Once the necessary metrics are set up, an experiment can be created on the flag from the flags dashboard. Select the desired flag, click "create experiment," and set the percentage of users to test. A duration can also be set, but note that the experiment does not automatically stop. Select the metrics for the experiment and click "start new experiment."

After starting an experiment, it is important to know when to stop it. Waiting for enough people to be exposed to the experiment is a common approach, as more participants provide more reliable results. The manager app provides regular updates on the number of users exposed through the exposures pipeline. This pipeline runs daily, aggregating user data into cohorts and storing it in the manager app's database. The frontend displays this aggregated data as a line chart, showing the sample size of each cohort over time. The developer can use this chart to determine how much longer the experiment should run.

To end the experiment and view the results, click on the "refresh results" button in the manager app's UI. This triggers the statistics pipeline, which queries the events database for the tracked metrics. The data is then analyzed using statistical tests, with different tests applied depending on the type of data. If everything goes well, the results are stored in the manager app's database and displayed in a table, with one row per metric attached to the experiment. The p-value is the most important column in the results, indicating whether the difference in metrics was caused by the new feature or random chance.

Next, let's discuss some of the engineering decisions made during the design of Waypost. Designing and building Waypost presented several engineering challenges, including communication between the flag manager and flag provider, communication between the flag provider and SDK, and efficient statistic running. The first challenge is to establish communication between the manager app and the flag provider. In the initial implementation, the SDK clients connected directly to the flag manager, but this approach had two main issues. First, the API would need to handle two major functions: handling requests from SDK clients and processing API requests from the flag manager UI. Dealing with the high volume of requests could create a bottleneck. Second, scaling both services together would be inefficient since the number of external users is typically much larger than internal users.

To address these concerns, an intermediary service called the flag provider was implemented. This service acts as a cache for flag data and separates the functionality of providing clients with flag data from managing feature flags. The flag provider allows for horizontal scaling to handle a large number of SDK clients and reduces latency by creating instances closer to users. The drawback is the need for extra maintenance and increased architectural complexity due to moving flag data twice. Despite these drawbacks, the benefits of scalability outweighed the costs.

To update flag data between the flag provider and flag manager, three communication methods were considered: polling, a message broker, and webhooks. Polling involves sending HTTP requests at a set interval, but it generates unnecessary traffic and lacks real-time updates. A message broker allows event-driven communication but increases complexity and can hinder debugging. Webhooks, HTTP requests sent in response to an event, were chosen for simplicity and real-time event-driven communication. The drawbacks include potential missed updates if the flag manager is unavailable and the need to handle a request for every flag edit.

For communication between the flag provider and the SDK, two methods were considered: websockets and server-sent events (SSE). Websockets allow bidirectional communication through a single TCP connection, but require more code and limit HTTP functionality. SSE, on the other hand, uses HTTP to establish a persistent connection for one-way communication from the server to the client. SSE was chosen because it allows real-time event-driven communication, is simpler, and does not require two-way communication. Connection drops are handled by the SDK, ensuring reliability.

The statistics pipeline, a critical part of the architecture, consists of three steps: querying the user event database for metrics, analyzing the data using statistical tests, and storing the results. The pipeline's runtime could be long if there is a significant amount of user event data. To balance the need for frequent updates and computing resources, a button was added to trigger the statistics pipeline as needed. This approach reduces the need for frequent updates and allows developers to access results once the experiment is completed. By avoiding the need for consistent result refreshes, a dedicated server for the statistics pipeline was deemed unnecessary.

These engineering decisions reflect careful considerations to address the challenges faced in designing and building Waypost. The chosen approaches aim to provide efficient communication and reliable statistical analysis while balancing system scalability and resource utilization. The implementation of these decisions contributes to the robustness and effectiveness of Waypost in conducting experiments and obtaining valuable insights from user data. During the first phase of the project, the client sends a get request with special headers to the server to establish a connection using the WebSocket protocol. In the second phase, the client and server communicate with each other by sending messages back and forth through the WebSocket connection. Finally, in the third phase, the connection is closed when either the client or the server calls a closed connection method.

WebSockets are useful in this project because they enable real-time communication between the client and server with only one connection being established. However, the downside is that two-way communication is not necessary for this particular use case. Another minor concern is that using WebSockets adds some development overhead as it requires more code than other methods and restricts the use of certain HTTP functionalities.

An alternative means of communication that was considered was Server-Sent Events (SSE). SSE uses HTTP to establish a persistent connection that allows for one-way communication between the server and client. In Waypost, the client first sends a request to the server, which responds to confirm the connection. The server can then send messages to the client in response to events. One challenge with SSE is handling dropped connections, which can occur when a client exceeds six open connections or after 30 seconds of inactivity. To address this, the SDK is designed to automatically reestablish lost connections.

SSE was ultimately chosen over WebSockets because it provides real-time event-driven communication and is simpler to implement. In addition, SSE allows for one-way communication from the flag provider to the SDK. However, it is important to note that SSE is not suitable for scenarios where two-way communication is required.

The team also faced a challenge regarding the implementation of the statistics pipeline. This pipeline consists of three steps: querying the organization's user event database for tracked metrics, analyzing the data using statistical tests, and storing the results in the Waypost database. The concern was that running this pipeline frequently could consume a significant amount of computing resources. However, it was determined that frequent updates were not necessary as it is generally recommended to wait until an experiment is finished before looking at results. As a result, a button was implemented on the experiments page to run the statistics on command, saving computing resources.

To test the pipeline's performance, the team timed how long it took with a large dataset of approximately 10,000 users in an experiment with multiple metrics. It was found that there was hardly any delay, leading to the decision to have the statistics feature on the same server as the manager app.

Looking to the future, there are a few features the team would like to include in future versions of Waypost. Currently, applications running the SDK receive all flag data, regardless of whether it pertains to that specific app. The team aims to separate flags so that applications only receive the flags relevant to them. Additionally, the team plans to introduce a login functionality to allow users to add permissions to individual users or groups. This feature would also enable tracking of edits made to the flag data.

In conclusion, the team appreciates the audience for listening to their project presentation. For further information about Waypost, links to their GitHub page and case study will be provided. The team invites questions from the audience regarding Waypost and is grateful for the opportunity provided by Launch School to showcase their project.

During the Q&A session, the team is asked about the type of statistical analysis conducted in the statistics pipeline. They explain that for continuous metrics, such as total time spent on a site, a t-test is used, while for discrete metrics, such as conversion rates, a chi-square test is employed.

Another question arises regarding whether feature flags are the only way to implement the splitting required for A/B testing. The team confirms that feature flags are indeed the most effective way to accomplish this task. They discuss alternatives such as canary deployment for splitting infrastructure or an API-based approach for smaller UI changes. However, these options have limitations and are not suitable for larger feature changes.

The team is then asked whether users will notice being redirected to a different URL with the new version. They explain that the observation of such redirection depends on the implementation of the feature flag and how pages are rendered, particularly in React, where URL specification can differ.

A question is raised about whether Waypost adds any additional load to the existing event database and whether users need to consider any configurations. The team clarifies that Waypost's impact on the event database is minimal, as it only queries specific tables once a day, and the runtime is very short. This allows for efficient resource utilization, and the product is designed for small businesses with manageable data sizes.

Lastly, the team is asked how they managed to complete the project within the given timeframe. They credit their talented team of coders and mention that they took inspiration from existing projects like Pioneer and Growthbook. The team also expresses gratitude to their mentor for providing guidance and assistance throughout the project.

In closing, the team expresses their appreciation to the audience for attending the presentation, encourages potential employers to reach out, and thanks Launch School for providing the platform to showcase their work. They once again invite any further questions from the audience.