Welcome everyone! Thank you for joining us today for our presentation on Artemis. We are thrilled to show you what we have been working on. Before we dive in, let's introduce ourselves. Our team developed Artemis as a fully remote team spanning four different cities and three different time zones. Sophie is in Seattle, Washington, Mia is in Fairfax, Virginia, Raul is in St. John's, Newfoundland, and I'm Charles, calling in from Portland, Oregon.

Today, we will be talking about Artemis, an open-source serverless framework designed for scalable, load testing of APIs. We will begin by defining some key terms and then show you a quick demo of an existing load testing tool. Next, we will discuss the use case for Artemis and delve into its architecture, including scaling load generation, data aggregation, data storage, and visualization. Towards the end, we will touch on ideas for future work, and of course, we will save some time for questions.

Let's start by defining load testing. In general, load testing involves simulating the usage of an application, website, or collection of resources to test their behavior under certain conditions. It allows us to identify and fix errors, bugs, and bottlenecks before deployment. Load testing also helps validate assumptions about the performance of existing infrastructure. Specifically, API load testing focuses on simulating raw network requests without considering the graphical user interface. This helps us measure the performance of the system and identify any bottlenecks.

To demonstrate, let's look at an example load test script. Load testing tools, like the one shown here, simulate how real users interact with applications or websites by generating load and making requests. The goal is to simulate a realistic number of users and their interactions. In this example, the script gradually ramps up from zero to 200 virtual users over two minutes, maintains 200 users for one and a half minutes, and then ramps down to zero in the final 30 seconds. Each virtual user executes a sequence of actions defined in the script. After the test completes, various metrics, such as total requests made, average requests per second, request duration, and request failures, can be analyzed to evaluate the performance.

API load testing is critical because API calls account for a significant portion of network traffic. Companies heavily reliant on APIs, such as Stripe, need to ensure high availability and performance. Downtime during peak periods, like Black Friday, can result in significant financial losses. Load testing helps identify the breaking point for APIs and ensures that they can handle expected traffic. This is particularly important when the API is the primary product of a company, as in the case of Stripe.

Now, let's discuss some common challenges when it comes to load testing tools, such as K6. While K6 is renowned for its performance and low memory footprint, it still has limitations. One limitation is the amount of load that can be generated locally, which is limited by the machine's memory and CPU. The CPU resources needed depend on the test script, and if CPU resources are overwhelmed, metrics may be reported incorrectly. Similarly, the memory requirements depend on the number of virtual users and the endpoints they interact with. Running tests locally also lacks real-time result visualization and may require additional effort to process and interpret data.

Cloud-based load testing tools, like K6 Cloud, address some of these limitations. They offer scalability to simulate a larger number of virtual users and provide real-time visualization of test results. However, cloud-based tools come with subscription-based plans that impose limitations, such as test duration and the number of virtual users. This can be restrictive and costly, especially for small and medium-sized companies.

This is where Artemis comes in. With Artemis, you can easily deploy the necessary infrastructure for API load testing on your AWS account using a single CLI command. Artemis leverages K6, an open-source load testing tool, allowing developers to write load tests in JavaScript. It eliminates restrictions on the number of virtual users and test duration, enabling you to run comprehensive load tests. The aggregated test results are visualized in near real-time using the provided dashboard, and the results are stored for long-term analysis.

In conclusion, Artemis offers a solution for scalable, open-source API load testing. It addresses the limitations of local testing and the restrictions of traditional cloud-based solutions. With Artemis, you can efficiently and confidently test the performance of your APIs and ensure high availability. We hope you found this presentation informative and we are open to questions from the audience. Thank you for your attention.

[End of transcript] Transcript:

We will now discuss the subscription tiers offered by Kasich's cloud-based load testing tools. These tools typically require a subscription that imposes limitations based on the chosen tier. For instance, if you subscribe to tier one and want to run a continuous 30-minute test, you would need to upgrade to the next tier or settle for the 15-minute limit. Similarly, if you expect an increase in traffic to your API and want to simulate over 1,000 virtual users, tier 3 would be the only option, even for a 15-minute test. However, if your testing needs span multiple tiers, you would be forced to choose the least restrictive option, potentially overpaying for functionality you don't need.

Now let's compare in-house and cloud-based solutions. Building an in-house solution using an open-source tool like k6 can be a good starting point for developing a minimally viable product or for occasional load testing. However, as the application and testing framework grow, so should the load testing solution. Building a scalable load testing application is not easy, as it requires long-term maintenance and comes with known risks. These risks include technical debt and project delays due to a lack of domain expertise and the efforts spent on building a reliable tool. Even if these challenges can be overcome, there is still a risk of reinventing the wheel, unless a highly customized tool is necessary. The main advantage of building a solution from scratch is the flexibility of implementation, but this comes at the cost of managing several unknowns.

On the other hand, cloud-based SaaS load testing solutions are well-established and provide the desired traits of reliability and scalability. These solutions abstract away much of the complexity required to automate tests, scale clusters, and produce visual results of the test data. However, due to their feature-rich nature, these cloud-based SaaS tools can be expensive and may provide unnecessary functionality. The subscription-based model used by these companies limits users to simulate a certain amount of virtual users, restricts test duration, and limits the total number of tests performed over a given time period. While larger companies can choose the most expensive option as long as it gets the job done, small and medium-sized companies do not have this luxury. They must make a choice between their testing needs and their budget.

Now, let's discuss where Artemis fits in. Artemis allows users to deploy the necessary infrastructure for performing API load tests on their AWS account using a single CLI command. It leverages k6, an open-source load testing tool that enables developers to write load tests in JavaScript. With Artemis, users can run load tests without restrictions on the number of virtual users or the test duration. The aggregated test results can be visualized in near real-time through a provided dashboard, and the results are retained in long-term storage, allowing users to further process, transform, or query the data as desired. While Artemis is not as feature-rich as some existing cloud solutions, it provides a flexible and scalable solution for users who cannot build a dedicated local testing environment or for whom tiered cloud-based SaaS solutions are too restrictive or costly.

Moving on to the architecture of Artemis, it consists of four components: load generation, aggregation, data storage, and result visualization. The load generation component utilizes load testing containers to simulate virtual users that perform requests against the API being tested. The aggregation component collects and processes the test results generated by the load testing containers. The data storage component stores the aggregated test results in a time-series database. And finally, the result visualization component provides a dashboard to visualize the meaningful metrics generated by the load test.

Artemis's infrastructure is accessible to the user through the Artemis CLI. This CLI allows users to run tests, visualize results, and deploy or tear down their load testing infrastructure on AWS. Deploying the infrastructure can be done with a single command, "Artemis deploy," which utilizes the AWS Cloud Development Kit to model and deploy the cloud infrastructure. The user will receive confirmation in the terminal once the infrastructure has been successfully deployed. This command is typically used once when first using Artemis and before running any load tests. When load testing is completed, the infrastructure can be removed using the command "Artemis teardown," except for the database.

To start a test using Artemis CLI, the user needs to provide the local file path to the test script and the number of load testing containers to spin up. The command "Artemis run test" uploads the script to an S3 bucket and invokes a Lambda function to spin up the designated number of load testing containers. S3, which stands for Simple Storage Service, is AWS's object storage service. In this case, we use S3 to store the test script since Lambda functions can't directly access a user's local files. By utilizing S3 as a stepping stone, the test script can be accessed by the AWS infrastructure. AWS Lambdas, on the other hand, are functions as a service that allows for on-demand response to events. The start-test Lambda is responsible for spinning up the specified number of containers, with each container creating a local copy of the test script and starting the test.

Scaling the load generation to achieve the desired load is accomplished through multiple load testing containers running in a Virtual Private Cloud (VPC). A VPC is a secure and isolated portion of the AWS infrastructure where users can deploy their own resources. Each load testing instance pulls a custom image from AWS's Elastic Container Registry (ECR), which we have configured and uploaded. This custom image includes k6, the load testing tool at the center of Artemis's solution, and a Node.js application called Artemis.js. K6 provides the functionality for creating API load testing scenarios and generating the desired number of virtual users. By scaling the load generation across multiple instances, Artemis allows for a greater number of total virtual users than any single instance could provide.

To achieve synchronization across multiple instances, the starting of a test is decoupled from the spinning up of instances. This synchronization is achieved by introducing a delay to the running of the test script. A timestamp is generated three minutes from when the start test Lambda is invoked. Each test container calculates a wait time based on its creation time, using the initial timestamp as a reference. This wait time acts as a container-specific timer, enabling test start synchronization.

After the load tests have been executed across multiple containers, the data aggregation component comes into play. A single container is responsible for collecting and processing all the test results generated by the load testing containers. This is done to provide meaningful insights into the performance of the API. By aggregating the results, the metrics displayed to the user reflect the overall load testing results instead of the results from individual containers. Aggregating the data also reduces noise by eliminating unnecessary data points that could hinder the user's ability to interpret the results correctly. Additionally, aggregating the data reduces the number of read and write operations to the database, resulting in improved performance.

To aggregate the data, multiple approaches were considered. One approach involved using AWS Step Functions to coordinate a series of lambdas for running tasks concurrently. However, this approach introduced unnecessary complexity. Instead, Artemis implemented a delay in running the test script, allowing for the synchronization of test starts. This delayed start ensures that all virtual users run simultaneously, accurately simulating the desired load.

In summary, Artemis provides a scalable solution for API load testing by leveraging AWS infrastructure and utilizing k6 as the load testing tool. It allows users to deploy the necessary infrastructure with a single CLI command, run load tests without restrictions, visualize test results, and store data for further analysis. With its flexibility and cost-effectiveness, Artemis is a viable option for users who cannot build a dedicated local testing environment or find tiered cloud-based SaaS solutions too restrictive. Up next, we have the load generation aspect of our architecture. To generate the desired load, the user is presented with a three-minute spinner that represents the timestamp coordination explained on the previous slide. After the spinner, the user is presented with a confirmation that the test has started. This provides clarity regarding when the user can expect test results to be stored in the database and visible in our provided Grafana dashboard. 

Now that our test is running simultaneously across multiple containers, we need to aggregate the resulting data from multiple sources. To address this, we use a single container to collect and process all the test results generated by the load testing containers. Aggregating the data allows us to provide meaningful insights into the performance of the API. 

There are three main reasons why we need to aggregate result data. Firstly, when generating the desired load, we spin up multiple containers, and each container runs its own load test. However, the results generated by each test do not take into account the results generated by tests in other containers. This means that each data point presented to the user does not represent the load test as a whole but rather the performance of the API from the view of a single container. 

Secondly, aggregating data helps to make it easier for the user to understand and interpret the results. If each data point generated by each container is shown individually, it creates a lot of noise and makes it difficult to determine how the API is performing. By aggregating the data, we can reduce noise and present a more concise representation of the test results. 

Lastly, aggregating data reduces unnecessary database reads and writes. With aggregation, the number of rights to the database decreases as we reduce the amount of test results before they are stored. Similarly, the number of reads decreases as our dashboard doesn't need to query as many data points to display meaningful results. 

To aggregate the data, we considered two approaches. The first was a polling-based approach, where k6 would perform the load test and write the results into a file. However, this approach had memory issues and posed challenges for aggregation. The second approach we considered was a streaming-based approach, where each individual load testing container would send data in real-time to a single server. We chose this approach because it allowed us to stream the data from every container into a central location and aggregate it to generate more meaningful results for the user. 

We chose to use a tool called Telegraph for data aggregation. Telegraph is a server-based agent for collecting and sending metrics and events from specified sources. It can receive data in line protocol format, making it compatible with k6 through built-in plugins. Telegraph provides filtering, transformation, and aggregation capabilities, allowing us to aggregate the data from multiple containers. We selected k6 and Telegraph because they provided the necessary functionality to stream data from each container to a central location and aggregate it. 

The aggregated data is then inserted into a time series database for long-term storage. Time series databases are designed to handle rapidly accumulating data, making them ideal for our needs. We chose to use AWS Timestream as our time series database solution because it is serverless and integrates well with k6, Telegraph, and our overall architecture. 

With the aggregated results stored in the time series database, we needed a way to visualize the data in a user-friendly manner. For this purpose, we implemented a result visualization component. The Artemis CLI can spin up a Grafana container that automatically queries the Timestream database. This allows us to visualize the results of our tests in near real-time. 

In the Grafana dashboard, we display metrics such as virtual user simulated, HTTP request duration, requests per second, and request errors. These metrics provide a baseline understanding of how the API is performing. We also provide summary panels at the top of the dashboard, which show statistics to enable a quick overview of the test. 

The HTTP request duration panel displays the time it takes to perform the entire request-response cycle in different percentiles. This helps users understand the typical request duration experienced by most users. The HTTP failures panel shows the number of errors that occurred during a specific time period, helping us identify when the system starts to break. The virtual users panel displays the total number of users simulated across all containers. 

The Grafana dashboard is connected to Timestream through a plugin provided by Grafana Labs. Users can also customize the dashboard to display additional metrics as needed. To access the Grafana dashboard, users can run the "artemis grafana start" command from the CLI, which will provide an IP address to view the dashboard. 

In conclusion, Artemis is an open-source serverless framework for scalable load testing of APIs. It deploys the necessary infrastructure for performing API load tests to a user's AWS account. Artemis leverages k6, an open-source load testing tool, to allow developers to write load tests in JavaScript. Load tests can be run without restrictions on the number of virtual users and test duration. The aggregated test results can be visualized in near real-time using the provided Grafana dashboard. Test results are also retained in a time series database for long-term storage and further analysis. 

While Artemis does not have some advanced features like parallel tests or load generation across multiple regions, it provides a flexible and scalable solution for users who cannot build a dedicated local testing environment and find existing cloud-based solutions too restrictive or costly. 

In the future, we plan to scale the aggregation step to handle an even greater number of virtual users. We also aim to implement an automatically generated executive summary of the test results that can be easily shared across a user's team or organization. Additionally, we plan to add functionality to schedule tests alongside on-demand testing. 

Thank you for attending our presentation. We hope you found it informative. Now, we would like to open the floor to any questions you may have. The longest container startup time observed was around two minutes and ten seconds, which was rounded up to three minutes to accommodate for potential longer startup times. 

To configure the containers to communicate with each other, we initially faced an issue where the load testing containers could communicate with each other and with the single telegraph container. However, the communication between the load testing containers and the telegraph container from different task definitions posed a challenge. To resolve this, we utilized a service called Cloud Map. Cloud Map allowed us to expose certain containers using a private API within AWS. The load testing containers then used this private API to communicate with the telegraph container.

Configuring the metrics for the Grafana dashboard and selecting the appropriate queries to display the desired data on the dashboard was another interesting and challenging aspect of the project.

In terms of the engineering challenge we are most proud of, we found the implementation of the three-minute startup delay for all tests to be a particularly elegant solution. Although it may seem like a simple task, there were numerous details that needed to be considered, such as passing the correct timestamp from the lambda to the containers.

Working on Artemis as a team was a challenging but highly rewarding experience. Each team member brought their unique perspectives, making the collaboration rich and fruitful.

Lambda serverless functions were not a feasible option for load generation due to the maximum execution time limit of 10 to 15 minutes imposed by Lambda. Since our load tests could range from a few minutes to several hours, Lambda was not suitable for our requirements. Additionally, coordinating load generation scenarios and varying the number of virtual users and endpoints would have been significantly more difficult with Lambda.

Load testing piqued our interest due to the vast amounts of data available and the dramatic increase in API traffic. We were particularly intrigued by the role APIs play in various aspects of technology. Furthermore, a previous capstone project focused on load testing single page applications, which further drew our attention to the specific market for API load testing.

Determining the maximum number of virtual users per Fargate task involved considering several factors. We needed to strike a balance between an underpowered and overpowered task to optimize resource usage and cost efficiency. We conducted various tests to determine the optimal CPU and memory allocations, eventually settling on 200 virtual users per Fargate task.

The criteria for determining whether a load test passed or failed using our tool are subjective and depend on the user's own specific circumstances. Our tool provides a visual representation of HTTP failures in the Grafana dashboard, allowing users to identify specific requests that may have failed. However, the user defines their own pass/fail criteria, such as setting thresholds for response times or error rates.

In our tool, virtual users and requests have distinct definitions. A virtual user is an instance created by k6, which repeatedly executes a defined scenario. Each virtual user generates multiple requests over a specific period of time. For example, in one of the Grafana dashboards, we observed that 2000 virtual users generated around 250,000 requests.

Thank you to everyone who attended our presentation and asked insightful questions. We hope you found the information valuable and informative.