[Music] welcome everyone thank you for coming to our presentation on artemis today we're very excited to show you what we've been working on first we'd like to start off by introducing ourselves we developed artemis as a fully remote team with members across four different cities and three disparate time zones sophie is in seattle washington mia is in fairfax virginia raul is in st john's newfoundland and i'm charles i'm in portland oregon today we're here to talk about artemis an open source serverless framework for scalable load testing of your apis first we'll define some terminology and we'll do a quick demo of an existing load testing tool then we'll talk about the use case for artemis we'll explore artemis's architecture at a high level then go deeper into problems and solutions around scaling load generation data aggregation and data storage and visualization we'll conclude by briefly touching on ideas for future work and then we'll take some time for questions let's start by explaining some key terms first let's define load testing generally speaking load testing is the process of simulating usage of an application website or collection of interconnected resources or services to test their behavior under conditions that a user defines as you can see in the slide a single computer is simulating a number of virtual users or vu's that perform requests against a server that they wish to test results of this load test are then generated based on responses received and specific metrics can be visualized based on those results so why load test load testing allows for errors bugs and bottlenecks to be identified and fixed before they are deployed into a production environment additionally load testing can confirm assumptions made about the performance of a company's existing infrastructure and its capacity now that we've talked about load testing let's discuss what api load testing is more specifically api load testing is the process of simulating raw network requests without regard to the graphical user interface to measure the performance of a system a good api load test will generate the type of load you would expect to see in production this gives you a realistic assessment of how your application architecture will behave and highlights any performance bottlenecks that need to be fixed prior to release now let's look at an example of what running a load test entails in this slide we see an example of a simple load test script test scripts such as this are executed with the help of a load testing tool load testing tools are used for load generation simulating how real users would interact with an application website or other network resource that accepts protocol based requests when you write a test script the goal is to simulate a realistic number of users that will interact with the api and the manner in which they will interact with it virtual users or vu's can be used to mimic the behavior of a real user virtual users are used to perform separate and concurrent executions of your test script they can make http requests against a web page or api on line six the target attribute with the value of 200 refers to the number of vu's to simulate in the first two minutes of the test as you can see on the right side of the slide this particular test is designed to ramp from zero to 200 virtual users over the course of the first two minutes then gradually decreases the vu number to 100 over the next minute and a half and finally decreases the virtual user count to zero over the last 30 seconds of the load test a virtual user is not limited to making simple single requests but rather can execute an entire scenario of actions against a network resource in this context a scenario is used to describe a sequence of actions that might be taken by a real user lines 12 through 17 outline a scenario based on the endpoints that every virtual user will repeatedly execute for the duration of the test after the load test has completed execution the user is presented with the results across a predefined collection of metrics as shown in the console some of these key metrics include total requests made average requests per second request duration and request failures request made indicates how many total http requests were generated over the course of the test and reports the average request per second request failures identify the total amount of requests that failed and the failure rate request duration is a measure of how long the remote server took to process the request and respond along with totals and averages it is common to report the min median max and percentile values for a more complete view of the results for example the p90 value for request duration indicates that 90 of the requests made lasted less than 48 milliseconds now that we've had a high level look at load testing let's briefly talk about the importance of load testing apis more specifically calls to api endpoints make up an increasing amount of public and private network traffic networking giant cloudflare has reported that more than 50 of the requests they process are api calls and that api call traffic is growing twice as fast as browser-based web traffic for some companies apis are the very focus of their business companies like this may expose several endpoints that perform tasks or retrieve data from upon receiving requests since their apis are their primary product it is important to know at what point they would break global payment provider strike is an excellent example of this type of company stripe provides an api that allows businesses to access its services without needing to know anything about how stripe is implementing its core functionality of processing payments users of the stripe api are provided with an interface that accepts credit card debit card or mobile wallet information across multiple currencies to use stripes api business owners don't need to know the internal processes of how stripe works they just need to prevent should they just need to provide the relevant information to stripe using the rules defined by stripes api it is then up to stripe to ensure that the user receives timely and meaningful responses to their requests high availability is essential for companies like stripe and the customers that rely on they face severe consequences for any degradation of service when it comes to the performance of their apis as an example depending on its size a single minute of downtime on black friday could cost a business thousands tens of thousands or even hundreds of thousands of dollars conversely a company that has done adequate load testing and is able to perform as expected during peak times such as this puts themselves in a great position to attract new customers and increase sales given that companies need a flexible performance load testing solution to ensure high availability of their apis what options do they have in general api load testing tools share similar benefits and drawbacks let's look at some common issues you might encounter when getting started with a load testing tool such as k6 k6 is an open source load testing tool originally developed by load impact now part of grafana labs k6 is written in go which is built with performance in mind its low memory footprint means it can run more virtual users and generate more load on average when compared to other load testing tools k6 allows users to write their test script their test scripts in javascript making it easily accessible for many developers and k6 provides the functionality for creating api load testing scenarios and generates the number of virtual users specified by their scenarios now i'll pass it over to sophie thanks charles charles just defined some key terminology and gave us a demo of load testing tool now let's explore some load testing approaches the largest issue when using an api load testing solution locally is the amount of load you generate is limited by your machine's memory and cpu even though k6 is performant compared to other load testing tools these cpu and memory limitations remain a concern the amount of cpu needed depends on the test script it can be assumed that the larger the test the more cpu power is needed if your chosen load testing tool uses too many cpu resources to perform requests or generate load and there are not enough resources left to measure the responses adequately metrics will be reported incorrectly the amount of memory needed depends on the load test scenarios which as mentioned earlier is a combination of the amount of virtual users simulated and the endpoint that those users will be sending requests to a simple k6 test will use around one to five megabytes of memory per virtual user this means if we simulate 1000 virtual users the memory consumed ranges from 1 to 5 gigabytes if we take a look at the image on the left a user is performing a test that simulates 600 virtual users this test performs around 63 000 requests total and it seems the api was able to hold up under this load the user then decides to perform a test that simulates 1000 virtual users if we take a look at the image on the right we can see how the total request performed by this test is also around 63 000. why is it that both 600 virtual users and 1000 virtual users generate the same amount of requests lack of resources for this particular user the maximum amount of virtual users they can simulate on their local machine is 600 the only way to simulate more virtual users is to upgrade the machine's memory and cpu another limitation in using a local load testing tool is the lack of available results while the test is running while this may not pose much of a problem in a short test of just a few minutes when running longer tests the user has little to no information about how the test is going until it completes and a summary view is output to the console as we saw in a previous slide and here again on the left the open source version of k6 provides no default graphical representation of test results the user may specify an output file in json or csv format to capture the more granular results of their tests a short example of some results in csv format is pictured here on the right however the user will still need to come up with a way to process and pass that data and visualize it in a meaningful way many open source load testing solutions provide enterprise level software for performing large-scale tests from the cloud these solutions address a number of the previously mentioned shortcomings of running tests locally as an example k6 cloud is the commercial software as a service offering of k6 open source a major benefit of using a cloud-based solution like k-6 cloud is the ability to simulate a much larger number of virtual users than could have been achieved with a local load testing solution on this slide we can see the results of a sample test run on k6 cloud unlike using k6 locally the results are visible as the test is running which provides some indication of whether there are performance concerns imagine running a six hour long test without any insights into the results only to find out that there was an unexpectedly high failure rate within the first 30 minutes in addition to providing live graphical representation of the results many of these cloud tools provide visualization of similar metrics for individual endpoints within a given scenario however all of these features come at a price here we show the subscription tiers offered by kasich's cloud cloud-based load testing tools typically require a subscription that imposes limitations as a function of that chosen tier for example if you are subscribed to a tier one and wanted to run a continuous 30 minute test you would need to upgrade to the next tier or settle for the 15 minute limit likewise if you're expecting an increase in traffic to your api and you want to simulate over 1 000 virtual users tier 3 would be the only option even if the test lasted 15 minutes although your testing needs span multiple tiers you'd be forced to choose the least restrictive option and could possibly be overpaying for functionality you don't need now let's take a look at the comparison of in-house and cloud-based solutions building an in-house solution around an open source tool such as k6 can be a good starting point for developing a minimum viable product or if load testing is not a regular part of development as the application and testing framework grows so should the load testing solution building a scalable load testing application is not an easy undertaking aside from long-term maintenance there are known risks with developing in-house tools that are not mission critical technical debt and project delays can be expected due to the lack of domain expertise and efforts spent on building a reliable tool assuming these challenges can be surmounted you are still at risk of reinventing the wheel unless a highly customized tool is a necessity the biggest benefit of building a solution from scratch is flexibility of implementation which you pay for with managing several unknowns cloud-based sas load testing solutions are well established and provide the sort after traits of reliability and scalability they abstract away much of the complexity required to automate tests scale clusters and produce visual results of the test data due to their feature-rich nature cloud-based sas tools can be costly and may provide unnecessary functionality the subscription-based model these companies use locks users into only simulating a certain amount of virtual users restricts them into tests of a certain length and limits the amount of total tests performed over a given time period big companies don't have to choose they can simply select the most expensive option as long as it gets the job done small and medium-sized companies do not have this luxury they must make a choice between testing needs and budget now let's talk about where artemis fits in with a single cli command artemis deploys the necessary infrastructure for performing api load tests to a user's aws account artemis leverages k6 an open source load testing tool that allows developers to write load tests in javascript users can run load tests without restrictions on the number of virtual users and the test duration aggregated test results can be visualized in near real time with the provided dashboard and the test results are retained in long-term storage allowing the users to further pass transform or query the data as desired artemis is not as feature-rich as some of the existing cloud solutions it does not perform parallel tests or generate load across multiple regions the results dashboard displays a limited set of metrics and scheduled tests are not a built-in feature however artemis is a flexible scalable solution for users who are unable to build out a dedicated local testing environment and for whom tiered cloud-based sas solutions are too restrictive or costly now that we've spent some time introducing the problem and providing a high level overview of artemis let's dive into how artemis was built artemis is made up of four different components load generation aggregation data storage and result visualization the first component is concerned with load generation the load testing containers in this component simulate virtual users that perform requests against the api you wish to test the second component performs aggregation of the test results a single container collects and processes all the test results generated by the load testing containers the third component is focused on data storage the aggregated test results are stored in a time series database the fourth and final component is for result visualization this dashboard allows us to visualize meaningful metrics generated by our load test here we have a full picture of our architecture and all of its components artemis's infrastructure is accessible to the user through the artemis cli this enables the user to run tests visualize results deploy and tear down their load testing infrastructure in aws artemis's initial deployment includes a number of aws resources including lambda functions a time stream database a virtual private cloud an elastic container service cluster and more we will explain what these services do later on for now let's take a look at how to deploy the artemis infrastructure from the cli deploying the infrastructure can be performed with a single command artemis deploy under the hood this makes use of the aws cloud development kit which is a framework for modeling cloud infrastructure via code that can then be synthesized and deployed to a user's aws environment the user should receive confirmation in the terminal that the infrastructure was successfully deployed a user would typically run this command once upon first using artemis and before running any load tests when load testing has concluded the infrastructure can be removed using artemis teardown all of the components but the database will no longer exist now i'll pass it over to mia thanks sophie sophie just gave a high level overview of artemis's architecture and how to deploy it now we'll take a look at how to start a test our load testing tool of choice and scaling load generation using the artemis cli the user provides two inputs the local file path to the test script and the number of load testing containers to spin up the start command artemis run test triggers the script to be uploaded to an s3 bucket and invokes a lambda function to spin up the total number of load testing containers specified by the user s3 is aws object storage service object storage provides a way to store unstructured data in a structurally flat data environment there are no folders directories or complex hierarchies as in a file-based system while cloud-based object storage is ideal for long-term data retention we used it as a means to an end we needed a way for a user's test script to run inside the load testing containers since a user's local files can't be read by a lambda function s3 serves as a stepping stone for getting the test script onto the aws infrastructure aws lambdas fall in the realm of functions as service functions as a service implementations allow you to make functions or chunks of code available on demand to respond to events as they occur the functions as a service provider manages the underlying infrastructure and the user is charged by execution time in our case the start test lambda pictured in the slide spins up the specified number of containers each application in the container then creates a local copy of the test script and starts the test let's examine how we scale to generate the necessary load to execute a test here we see a high level overview of our load generation component the load testing containers identified as tasks in the diagram are running in a virtual private cloud a virtual private cloud or vpc is a secure logically isolated portion of the aws infrastructure where you can deploy your own resources these tasks generate virtual users that perform requests against the api you wish to test next let's take a closer look at the elements that make up our load testing containers each of the load testing instances pulls a custom image from aws's public container repository known as elastic container registry or ecr the custom image is one that we configured and uploaded to ecr it includes k6 and a node application k6 is the load testing tool at the center of artemis's solution k6 provides the functionality for creating api load testing scenarios and generates the number of virtual users specified by those scenarios artemis scales the load generation across multiple instances to allow for a greater number of total vu's than any one instance could provide the node application artemis js fetches the previously uploaded test script from the s3 bucket waits for a prescribed period for test start coordination then uses k6 to run the test based on the provided script the test results are then sent to a separate container for aggregation during a regular load test a user may be spinning up one or more of these containers where are these containers running and how is the scaling of these containers handled the answer is aws fargate and aws elastic container service aws fargate is a serverless compute engine for containers fargate eliminates the need to provision and manage servers and lets you specify and pay for resources per application all of our containerized applications are running in fargate instances ecs allows us to manage deploy and scale the containerized applications provisioned by fargate the number of instances specified by the user when running a test is used by ecs to determine the number of fargate instances to spin up through testing and analysis we determined the sweet spot of maximum virtual users per container to b200 this gives the user the greatest flexibility in terms of overall total vu's and length of test duration our users write their test scripts for up to 200 vu's and then specify the number of tasks desired for a particular test run with this information we can generate load across multiple containers to achieve a large total number of virtual users for example if a user wants to test for 5000 bus they simply write their script based on 200 vus and specify a task count of 25 likewise if a user only wants to perform a load test that simulates 200 or fewer users they can use the same test script and specify just one task although ecs and fargate make it easy to spin up serverless instances we still needed a way to coordinate the starting of load tests across these multiple instances once we solved the problem of spinning up more than one instance we noticed that containers in the ready state would automatically run the test script the question to answer then became how can we decouple the spinning up of instances from the starting of a test ideally you would want to have all of the virtual users running at the same time to accurately simulate the desired load otherwise you could end up with skewed test results that wouldn't accurately reflect the test input and therefore invalidate the test itself we first considered using aws step functions to coordinate a series of lambdas for running tasks concurrently aws has a distributed load testing implementation guide that makes use of step functions using worker tasks and a leader task to ensure no tests have started until all instances have been provisioned using this approach would require a complex workflow and introduce unnecessary complexity to our use case another option we explored and how we ended up addressing the problem was introducing a delay to the running of the test script this was achieved by generating a timestamp three minutes from when the start test lambda is invoked each test container then calculates a specific wait time based on when it's created using the initial timestamp as a reference this wait time can be thought of as a container specific timer enabling test start synchronization let's take a quick look at initiating a test using our command line tool through the artemis cli the user can specify the path to the test script and the number of containers to spin up to generate the desired load the user is presented with a three-minute spinner that represents the timestamp coordination explained on the previous slide next the user is presented with confirmation that the test has started this makes it clear to the user when they can expect test results to be stored in the database and visible in our provided grafana dashboard now that our test is running simultaneously across multiple containers how do we aggregate the resulting data from multiple sources for the answer i'll pass it off to raul thanks man mia just talked about the load generation aspect of our architecture now i'm going to talk about how we aggregate that data from multiple instances reduce its size and make the resultant data set most meaningful to the user and yes we have a lot of data here we see a high level overview of our data aggregation components a single container collects and processes all the test results generated by the load testing containers before we go deeper into this part of our infrastructure let's examine three reasons why artemis needs to aggregate result data first in order to provide meaningful insights into the performance of the api the metrics displayed to the user should reflect the load testing results across all containers as we mentioned before to generate the load desired by the user we are spinning up multiple containers with each container running their own load test the downside of spinning up multiple containers to generate load is that the results generated by each test do not take into account the results generated by tests in other containers this means that every data point presented to the user when visualizing does not represent the load test as a whole instead it represents the performance of the api from the view of a single container we also want to aggregate data so that the user can make sense of it when visualized plotting too many data points produces too much noise and by noise we mean any data that a user cannot understand and interpret correctly even interpret correctly if every single data point generated by each container is shown to the user it is harder to immediately determine how the api is doing aggregating data also prevents us from having unnecessary rights to and reads from the database when we aggregate rights to the database go down since we are reducing the amount of test results before they hit the database reads also go down since our dashboard does not need to query as many data points to display meaningful results we needed a solution that will allow us to funnel test results from all containers into a central location where we will then proceed to aggregate set results we consider two approaches to solve this problem first we looked into a polling based approach in this approach k6 would perform the load test and have the test results written into a file in either json or csv format we would tail the results file take those results and send them to be aggregated an issue with this approach is that k6 consumes a large amount of memory when test results are read into the file every virtual user simulated by k6 stores a copy of this file in memory this presented a problem because simulating a large number of virtual users for performing long tests would deplete the container resources prematurely and terminate the container even if k6 did not have this memory issue there was still the challenge of aggregation for us to solve then we looked into a streaming based approach instead of bi-directional communication a streaming approach entails a one-way data flow in near real time each individual lotus container sends data as it is generated to a single server the server is configured to combine the results received at a regular time interval k6 provides the ability to output data in line protocol format through a built-in plugin after evaluating a number of data aggregation solutions we chose a tool called telegraph telegraph is described as a server-based agent for collect for collecting and sending all metrics and events from specified input sources telegraph has the ability to receive data in line particle format making it compatible with k6 through the use of built-in plugins telegraph can filter transform and aggregate data as needed for example telegraph's basic stats plugin calculates the mean min max count and sum of all the incoming data these metrics were of interest to us for accurately representing the test results as a whole we chose to use k6 and telegraph since they allowed us to do exactly what we needed stream the data from every single container into a central location and aggregate that data to generate more meaningful results for the user the telegraph container aggregates the data every 10 seconds based on these metrics and then inserts it into a database for long term storage here we see a high level overview highlighting our data storage components we were faced with the problem of how to best store data long term for historical monitoring purposes while retaining the scalable and serverless quality of the rest of artemis's architecture let's take a look at how we solve this problem we chose to store our aggregated results in a time series database time series data is a sequence of data points collected over time intervals giving us the ability to track changes over time time series data can track changes over milliseconds days or even years time series data accumulates very quickly and normal database are not normal databases are not designed to handle that scale we chose aws time stream for our time series database solution since it is serverless and integrates well with k6 telegraph and our overall architecture implementation at this point we haven't seen any of the results of the load tests we needed a means to make sense of the data in a way that was easy to understand this leads us to our fourth and final component result visualization let's take a look at how we decided to implement this part of artemis's infrastructure the artemis cli can be used to spin up a graphonic container that will automatically query the time stream database this will allow us to visualize the results of our tests results are updated every 10 seconds as the tests are running we wrote custom database queries to display the following metrics in the dashboard virtual user simulated http request duration request per second and request errors these metrics are often displayed in other load testing result visualizations and provide a baseline understanding of how the api is performing the for summary panels shown at the top of the dashboard provide statistics to enable a user to see how their test is running at a glance the bottom panel total requests per second shows the number of requests made every second to all endpoints in the test script on its own this metric does not tell us much about the performance of the api but when request duration increases or errors start to appear it lets us know at what point our system starts to break on this slide we are focusing on the last three panels shown by our dashboard http request duration http failures and virtual users the top panel http request duration displays how long it takes to perform the entire request response cycle this metric is represented in four different ways min p90 p95 and max the percentile values allow the user to determine the request duration that most users are experiencing if these percentiles are complemented with minimum and maximum measurements then it is possible to have a much more complete view of the data and better understand how the system behaves the bottom left panel http failures displays dots that represent any response every response that returned with a status code in the 400 or 500 range this allows the user to quickly determine how many errors occurred within a certain time period the bottom right panel virtual users displays the total amount of users simulated across all containers to summarize grafana provides a modular easy-to-use dashboard for visualizing many different types of data we connected grafana to amazon timestream via a plugin available from grafana labs although we configured a default dashboard view the artemis user has the flexibility to display additional metrics that they deem important let's take a look at the terminal on the left to use the rafana dashboard run artemis grafana start from the cli once the container is up and running you'll receive an ip address that you can use to see the dashboard on the right side of the screen we can see a user interacting with the grafana dashboard artemis's default dashboard view displays the results of the most recent test and the user can zoom into specific portions of each test by highlighting the areas of interest if you no longer need to use a dashboard simply run artemis or find a stop to terminate the container at this point we've covered the four main components of artemis load generation aggregation data storage and result visualization again artemis is an open source serverless framework for scalable load testing of your apis artemis deploys the necessary infrastructure for performing api load tests to a user's aws account artemis leverages k6 an open source load testing tool that allows developers to write load tests in javascript users can run load tests without restrictions on the number of virtual users and the test duration aggregated test results can be visualized in near real time with the provided dashboard and test results are retained in long term storage allowing the user to further parse transform or query the data as desired artemis is not as feature rich as some of the existing cloud solutions it does not perform parallel tests or generate load across multiple regions the results dashboard displays a limited set of metrics and scheduled tests are not a built-in feature artemis is a flexible scalable solution for users for users who are unable to build out a dedicated local testing environment and for whom tiered cloud-based sas solutions are too restrictive or costly let's briefly talk about some things you would like to add to artemis in the future our current infrastructure allows for testing up to 20 000 views this limitation is a function of our aggregation implementation we would like to explore ways to scale this aggregation step to accommodate an even greater number of vu's we would also like to implement an automatically generated executive summary of the results of each test that could be easily shared across a user's team or organization currently tests can be started on demand only we would like to add functionality that allows users to schedule tests as well this is the end of our presentation thank you for attending and we hope you enjoyed it we would now like to open it up to questions thanks earl uh we have one question right now how do you handle the potential issue of instances not being ready within that three minute window anyone like to say that yeah sure i'll try to answer that um so when we performed um testing well the way we would determine that three-minute window was for by performing some testing um and we tried to spin up containers um and kind of search for the period in which it would take a container to spin up and most of our containers were kind of able to spin up within that three minute window even if we were running a large test so you know we didn't experience any containers that started outside of that window but that is certainly something that we would want to be sure of in the future all right we've got a few more i think that answered it um specifically the longest container startup time i observed was around two minutes ten so we kind of rounded it up to three minutes to accommodate for the slightly longer startup times that might have happened um next question how did you configure your containers to communicate with each other i can uh i can take that question oh it's okay so um yeah that's uh that's actually a great question um we so initially the as we mentioned in in the presentation we have our load testing containers and we have the single telegraph container so one of the issues that we were encountering was that the containers that the load testing containers could communicate with each other with no problem and any telegraph container was also able to communicate like if uh if a container was from the same task definition as that one they would also be able to communicate between them so we actually had to use a service called cloud map cloud map allows you to expose certain containers using a private api within aws the load testing containers would then use that private api to communicate with the telegraph container and that's that's basically how we did it charles i don't know if you would you would want to add to uh to that um no i think that's a that's a good explanation basically there's a there's a service our telegraph container is a is is on a service rather than just a task itself and there's a service discovery function and you actually use um aws as sort of internal dns um there's actually like a route 53 component to it and so um all of our test containers are pointing to that service which has its own sort of like raul said it has its own private ip or its own you know sort of named private dns uh next question um what engineering challenge that you tackle the most proud of or do you think is most interesting or elegant um i think this might be personal preference but i think the implementation uh for our startup delay of that three minutes to just make sure the all the tests start at the same time having a hard-coded three-minute timer doesn't necessarily seem like the most elegant but i think it was straightforward and also we also had to make sure that we passed the timestamp from when the lambda started to the containers as well so for such a small part of just making it like a three-minute timer there was a lot of small details that we had to implement correctly to get that to work how we wanted did anyone else have anything different i'd like to mention yeah i would agree with that and i think also maybe just configuring the metrics that we wanted to show on the grafana dashboard and being intentional about the metrics we wanted to choose and have as a default for the user and just just kind of trying to figure out the the right queries to perform on the time stream database um to display some of those metrics sure um a couple more questions what was the most challenging part of working on artemis and working together as a team um i can say something about that i i think that um i can't think of a single most challenging part i think the whole the whole process was pretty challenging um in terms of working on a team i feel like we had to really exceptionally good team experience um i don't i was just thinking about this as i was watching you know my team members present and what came to mind for me was i don't i don't know what i did when i only had like one brain to rely on i don't i don't i don't know my life before with just one brain was was uh lonely and limiting so um yeah i think it's i think it was just a great experience to sort of to build something together and just to realize how how rich it is to you know experience other people's viewpoints even even if you're not necessarily like open and ready for them all the time when you look back you realize that uh you know that they're just so so additive to the whole process thanks that's very nice charles we've got a few more questions still um would it be impossible to use aws lambda serverless functions instead of aws fargate with ecs for load generation and if so why did you choose to use serverless containers with fargate instead of serverless functions so i think i think the main issue with with lambda sits there uh total execution time which i if i'm not if i'm remembering correctly it's either 10 to 15 minutes and we are potentially having tests run from you know they can be as short as you need one minute five ten minutes to hours so it just it just uh lambdas just would not would not work in that in that case now there is also there is also the case where you could you could possibly use lambdas to generate constant requests right or constant load that could be an option but um we are offering the users the option to either ramp up their tests ramp down their tests there's these different scenarios and you know you can have different number of uh vu's at different times you're hitting different endpoints at different times in that sort of coordination across lambdas across different scenarios it just wouldn't uh from my perspective it would be incredibly incredibly difficult to pull up uh next one uh what is it about load testing that made you interested in creating a solution for it um i'll answer this i think well a lot of my research early on ended up focusing in um load testing i think it was just my general interest in the vast amounts of data that are out there right now and also what we mentioned in the presentation about api traffic increasing dramatically as well that alongside a previous cohort um capstone project that also focused on load testing but they were specifically interested in load testing of single page applications so once i read up about their project i saw the there's also this specific market for api load testing and it was just um how apis function in so many different parts of technology is what really focused me in on being interested in this area i don't know if anyone else wants to add to that i would also i would also say that it just seemed that this this uh problem of flexibility of existing cloud solutions seemed like something that i don't know i guess it seemed obvious that we could talk tackle right like we looked at this problem and we we thought yeah we can we can do this right we can we can solve this issue which which was was pretty cool as well sure um how how was the max number of virtual users per fargate task determined i can i can answer this or they start an answer um we we looked at a number of different factors um so a fargate instance um is still you know you can think of it as sort of i guess the serverless version of an ec2 instance so there's still a um you're still specifying a um cpu and memory sort of limitation or you know attribute on on the fargate tasks so you want to choose something that you know isn't too underpowered or too overpowered because obviously the user is still going to be paying for that compute so we wanted something that you know many users might you know for a number of users you know 50 or 100 vu's may be enough um in which case you would you wouldn't want to have you know some hugely provisioned um task to do a smaller number of users um and uh it also just gives us a lot of flexibility so you can you know you can really you have to do a little bit of multiplication but you can really um sort of dial in what you want and we did a lot of testing around like the right size for we also didn't want to like have to dynamically determine you know the fargate task attributes in terms of cpu and memory um uh based on the test um we just wanted to like find that sweet spot and 200 was the sweet spot where we could provision a relatively powerful but not super powerful fargate container and then just allow people to you know run more it would be better to run you know if you want 2 000 virtual users it's better to run 10 of those sort of modest containers than have like a default container that's overpowered and be paying for that extra compute got a couple more questions how would you decide if a load test passed or failed using your tool i can answer that one um so if you're looking at the output in the grafana dashboard there's a specific panel for http failures so you can kind of visually see [Music] if specific requests fail and at what point they fail but typically if you're running a load test you would probably define a threshold so some kind of pass fail criteria that you decide on as as an individual or as a team um so that could be like you know you want 95 of your requests to be below 500 milliseconds have a response time of less than 500 milliseconds so um you know those are those thresholds is something that you would kind of define before running the load test yeah our tool is more about providing the means of determining whether what you're testing is past or or failing for your own specific circumstances we're just providing the means to test it and then the dashboard gives that overview and it's going to be up to the user whether they find that an acceptable response or not next question you mentioned that virtual users is different from requests could you explain that a little more and how many virtual users can artemis run yeah so um a virtual user is is uh is essentially an instance created by k-6 that repeated that repeatedly executes the scenario as as we should as we show as we show as we sorry as we uh shown before in the in the presentation so for example in in one of the graffana dashboards we displayed we were seeing how 2000 users was not really equivalent to 2000 requests each of those users over a single over a certain period of time can generate many many requests right so over so for example on that uh panel we saw 2 000 users generated over 250 000 requests so a single user just repeatedly calls that function that we call a scenario in the test script that was all the questions we had so far thanks again for coming everyone really appreciate the question so great and i hope you enjoyed it you 