hello everyone thank you for joining us today and welcome to our presentation of seymour an open source easy to configure active monitoring solution my name is katarina and i'm here today with my colleagues scott tim and miles and we are very excited to introduce you to the project that we work on for the past several months as a fully remote team spread over the us and europe but what is seymour you may wonder siever is an active monitoring solution that allows users to simulate requests from globally distributed locations to test their api endpoints seymour measures performance uptime and correctness of the payload data we built seymour to help engineering teams bolster existing testing approaches and handle the challenges of monitoring their increasingly complex systems our solutions enable rapid detection of issues in production before users experience them and moreover cmos infrastructure is easily deployed with a few cli commands and a quick setup but before we continue let me just give you an outline of our presentation structure we'd like to begin with an overview of what problems we designed seamless to solve after which we'll explain what active monitoring is and why it is essential in assuring the help of a system next we're going to explore existing solutions in the active monitoring space the advantages and limitations and where exactly seymour puts in in the picture we'll then present a quick demo of seymour and how it is deployed followed by a discussion about its underlining architecture afterwards we'll talk about some of the design decisions we made and challenges we encountered along the way next we're going to cover some additional features we'd like to work on in the future and finally we'll finish with the q a but before we dive further into c-more and active monitoring let's take a step back and talk in general about how development teams minimize the risk of bugs reaching production many of the techniques from accomplishing this follow the category of testing software testing is the process of evaluating and verifying that a software product or application does what it's supposed to do good testing help to prevent bugs and can sometimes even help to improve performance how is software normally tested but typically tests are either run locally during development or in staging environment just prior to deployment software tests are generally categorized based on purpose and scope and there are many different types of tests two common examples that we'll talk about are unit tests and integration tests unit tests are type of test which focuses on a small part of an application the unit might be one or many classes or functions these tests are often run by an individual developer frequently after making even small changes to a code base unit tests can also be run as a part of cicd pipeline another example are integration tests in this approach different part of an application are tested together as a whole these tests can find backs in the interaction between components in a way that unit tests are not designed to remote systems however are often mocked in spite of developers best efforts issues still do arise in production how can this happen you may wonder well first of all conditions change integration tests are often performed only on deployments meaning these tests are run only at a moment in time secondly replicating production isn't always possible sometimes only nearly perfect replication occurs in pre-production tests and this is especially true of modern distributed systems and finally third-party service dependencies can prove to be unreliable on martin fowler's website a contributor role this person and calls in the age of small independent services and frequent deployments it is very difficult to test pre-production with the exact same combination of versions as they will later exist in production so what happens when issues do arise in production well the end user's experience may be degraded or even completely broken for example response time can be higher than expected the response data can be incorrect and maybe worst of all an api can be unavailable altogether rather than waiting for users to experience and surface such issues it would be convenient to test api endpoints in production continuously and this is exactly where active monitoring comes into play but what is active monitoring specifically active monitoring also known as synthetic monitoring is a technique that runs automated tests against a live production system in order to detect failing business requirements it helps to continuously monitor server and application performance even during periods of low user engagement active monitoring is generally used to test three things availability or uptime performance which is a measure of latency and correctness of data tests are written with abi consumer expectations in mind to ensure the api provider continues to meet its commitments let's zoom in a bit and have a look at how a single test might look like this test called my api comprises of two assertions here a get request sent to our example endpoint should return a response within 500 milliseconds the actual response was completed in 632 milliseconds so the assertion would be considered a failure the other assertion expect the response status of 200. in this case the expected value was 200 meaning the assertion is considered passing but because our first assertion failed the overall test is still considered failing in the event a given test fails on-call engineers can be notified through various channels such as slack or email and begin troubleshooting the underlying problem now that you know what active monitoring is let me talk briefly about why is it even important but simply stated things are bound to go wrong in any software production environment but rather than view this as a bad thing however engineering teams can see it as an opportunity they can use active monitoring to identify problems in production before users do active monitoring is especially important in the context of microservice architecture take for example an api endpoint that is dependent on many different services of internal and external each of which may be deployed multiple times per day including a replication of all these services in local or ci tests would be slow or brittle at best including some services may not be visible at all so even with a well-maintained test environment functional and data errors can still slip through the cracks and make their way into production for the users for the company exposing public apis the source of truth is what the experience is when they hit one of these endpoints furthermore active monitoring solutions often provide the ability to originate tests from locations around the world this provides insight into how a system is performing for users in specific geographic locations and help to better diagnose if an issue is network related by introducing active monitoring in a holistic testing strategy engineering teams can sleep better at night knowing that vaccine production will surpass fast and be fixed before users are impacted i'm now going to pass it to spot we'll start by discussing existing active monitoring solutions thanks katarina now that we better understand what active monitoring is and what problem it solves let's get an overview of the existing solutions and see where c-more fits in there are quite a few products offering active monitoring solutions some prominent vendors that offer this service are new relic and datadog followed by some smaller scale companies like uptrends checkly and run scope there are also a handful of open source solutions such as artillery which is more commonly known for load testing and monica besides using a sas or open source active monitoring product some teams may decide on a diy approach and build their own in-house solution let's weigh these options by considering their various capabilities and feature sets in other words what are the pros and cons let's take a look in general sas active monitoring products offer geographic distribution of request locations typically these solutions are managed meaning there is very little to zero setup required from the customer to configure their first test additionally most of the interactions with these sas products including new relic and checkly are through a ui which makes setting up new tests easy for all users including non-engineers one drawback of a managed solution however is the lack of data ownership this might be a deal breaker for some companies who don't want their data residing on a third-party provider's infrastructure sas solutions typically offer a wide variety of features including native alerting over a variety of channels some additional features include tear down scripts and integration into ci cd pipelines all of this functionality some of which might not be needed for a given use case does come at a direct financial cost sas solutions are the most expensive option on the other hand choosing an open source active monitoring product allows users to maintain complete control over their data one drawback of these solutions is that they do not typically offer geographic distribution of tests at least in their free tiers additionally these solutions are more laborious to set up and use as compared to their sas counterparts both monica and artillery for instance are set up via a multi-step process and tests are configured via yaml there is no ui on the other side of the spectrum we have in-house solutions this could either entail building a system from the ground up or wrapping an existing open source product like monica a major appeal of this approach is that by definition the user can customize the product they build to fit their specific needs which can of course include maintaining complete control over their data by hosting it on their own infrastructure a significant drawback of this approach however is the amount of time and effort needed to develop such a solution it would require engineering resources that could be otherwise spent on more mission critical tasks this brings us to c-more having surveyed available active monitoring solutions what we found was lacking was an open source self-hosted easy to configure option that allows running tests from globally distributed locations and that is why we built c-more c-more has geographic distribution users can configure tests originating in 22 global locations c-more is easy to use although not as simple to set up as a managed sas product as we'll discuss later in the presentation provisioning c-more requires only two cli commands once it is set up all tests and alert configuration are done through a ui although c-more also exposes an api enabling programmatic configuration if that's what the user prefers because c-more is open source and self-hosted it allows users to maintain sole ownership of their data c-more enables users to configure alerts to be sent over one or more of three channels email slack and discord and finally since the only cost of seymour is the cost of the aws infrastructure it runs on it's inexpensive especially relative to its sas peers now that you know a bit about active monitoring and c-more let's take a tour through the c-more ui and explore how to create a test and view the results the first thing i see when i visit the c-more homepage is a table view of the existing tests this table includes some basic information about these tests including the name of the test a summary of the three most recent test results and how often the test is scheduled to run this table also includes buttons enabling the user to delete edit or trigger a run of the test i can begin the process of configuring a new test by clicking the create test button which takes me to a form with several inputs first i need to name the test i'm going to call this c-more demo next we'll configure a request to hit the specific api endpoint that we want to test i want to test an endpoint that some members of our team worked on that's part of a task management api modeled after trello let's test the endpoint used to create a new card this is a post request to this url this endpoint expects a request body of a particular shape so let's add that now we don't need to add any custom headers for this request so we can skip this section now that i've configured the request i can add some assertions about the response received from the server hosting the api let's begin with the response time i want to make sure that the response time is less than 500 milliseconds next i expect a status code of a successful request to this endpoint to be a 201 so let's add an assertion for that i also want to make some assertions about the response body first that the response body contains a title property equal to what we indicated in the request body the dollar sign here represents the body of the response so dollar sign dot title is how we indicate that we want the title property of the response body next the response body should contain a property completed that's equal to false and include a null due date property the server should generate an id for the card and attach that to the response body let's add an assertion for that finally let's add an assertion for the connection header it should have a value of close that's it for assertions now i can choose the locations from which i want the request to originate although the api we're testing doesn't yet have users all over the world let's pretend that it does and that we want to make sure that any clients of this api in a variety of geographies will not experience unacceptable latency when interacting with this endpoint let's select north virginia stockholm and montreal finally we'll configure the frequency of the test and how we want to be alerted in the case of an assertion failure let's configure the test to run once every minute if any of these assertions fail i want to be alerted on discord i now need to paste in a web hook address for the discord channel on which i want to be alerted although i chose to be alerted on a particular discord channel i can add as many alerts as i want and can choose from other alert channels such as email or slack with that we've completed the test configuration so let's save we can now see that c-more demo has been added to the top of the table of tests i could trigger a run of that test by clicking the run now button or wait a minute in order for the results to start to populate either way we need to give it time to run so for now let's click into another test that already has results and we can come back to the test we just configured at the end of the demo so here you have a summary of the test meaning an aggregation of the results from each time the test has been run we can see that 100 of requests have succeeded which means no 400s or 500s but don't be mistaken a successful request does not mean that the test assertions passed it only means that the response to the request was successful as indicated by its status code next we can see that we have had many assertion failures we will dig into what might have been the cause for one of these soon and then we have a basic summary of response times including the median and the 95th percentile values we also have some basic information about the three most recent test runs including the region where the request was made from the status code the response time the number of assertions passed and when the run of the test was completed below we have a graph of the response times broken down by each of the different regions that the requests are originating from so we can see here that the response times from stockholm for instance are higher than those originating in montreal and north virginia this makes sense since the endpoint that we're hitting is served from north america to see more than just the three most recent runs we click see all test runs let's click into the most recent run of this test as it has a failed assertion this test was completed on august 10th and the request originated from stockholm although we received a response this test is labeled a failure due to the actual response time of 747 milliseconds exceeding the target value of 500 milliseconds all of the remaining assertions have passed and we see a summary of the target values and the actual values we can also review a collapsible response body and the response headers okay let's head back to the tests view we can now see that there are results for the test that we configured including several failures let's click in the failure of this run of the test was caused by a response time again the actual response time was 921 milliseconds whereas the target was 500 milliseconds just like we configured and that means we should have been alerted on discord so let's go check that out sure enough we can see here that we have alerts for this test the details of the failure now that you've seen the ui demo let's talk about how to get started with c-more an aws account is required but once that's set up all that's left is to one clone the c-more infra setup repo two run two cli commands to spin up the necessary resources in your aws account three enter the generated url in your browser and with that done you are ready to configure your first test one thing worth noting however is that the provisioning of aws resources while automated may take upwards of 20 minutes primarily due to the provisioning of resources in 22 remote regions i'm now going to hand it off to miles who will take you through the details of seymour's architecture hey thanks for walking us through that scott all right now that we've seen what it's like to interact with seymour's ui let's take a look under the hood at the infrastructure and see what it's doing to make everything work the infrastructure can be thought of as loosely organized into four main sections which together make up the life cycle loop of a single test first we have test configuration which you got to see a demo of before but we'll look into what is going on behind the scenes and then there's a pre-processing phase which preps each test run with the test configuration and proper routing to the geographic regions where it's needed to run now we get to the main event test execution where the actual http request is made at the end point and the response is checked against the assertions in the test configuration the results are packaged up and sent to the final phase post processing where we handle storing test results and alerting for failed tests before going on let me address home and remote regions real quick when you deploy seymour you select your main aws region in which to set up the infrastructure and that's what we are referring to here as the home region and it in fact holds most of the infrastructure everything needed to power the ui the back end server as well as everything for the test configuration pre-processing and post-processing phases the test execution needs to happen in any number of the aws regions selected when configuring a test so the infrastructure for that phase is set up in all other available aws regions which we refer to here as remote regions next let's take a turn in each phase to walk through the details of a single test journey through the loop the journey of a test through the infrastructure begins as a json configuration file sent from the ui when a user first schedules a test the back end app stores test config in the database assigns an id and then passes it to eventbridge a serverless eventbus often used in aws based event-driven architectures we are using the event bridge as our scheduler service each test is stored in our scheduler as an event bridge rule a scheduled action that fires at a fixed recurring interval the test configuration is saved to the rule as a payload that will be sent to its target every time it fires that's it here and in fact as we'll see the next several slides the back end app isn't needed at all during the ongoing life of a test once it has been scheduled this the pre-processing phase is where each scheduled test run fires off and where our json test config kicks off through the loop we previously walked through what scheduler is doing so we know that the rule fires on a schedule but i left off the target for where the rule fires to the target destination is a lambda function we refer to as route packager and it's the same target for every rule that is scheduled lambda is an aws serverless compute service that lets you run code without having to provision or manage servers route packages purpose is to read all the location data from the json test configuration payload and prep it for routing to the correct regions it then publishes a message with the test to a topic in sns amazon's simple notification service a queue in each region is subscribed to the topic and will only accept messages if they are intended for that region up to this point all the infrastructure we've looked at has been within the home region this the test execution phase is a look at what is going on in each remote region as i mentioned previously there's a queue that first receives the message in each remote region the message is pulled from the queue by a lambda function we refer to as testrunner now the json test config has made its journey to the point where it's actually needed for making the http request testrunner uses the config data to make a request to the endpoint and then compare the response to the expected response values in the assertions if any of the assertions fail then the whole test is marked as failed it then packages up the results and sends them back to the home region back in the home region again the test results are received in a queue they are processed from the queue by a lambda function we refer to as test result handler regardless of whether the test passed or failed result handler writes the results to the database however when it first examines the results for whether the tests pass or not if it finds that the test failed it immediately invokes another lambda which handles alerting when the alerting lambda is invoked it receives an its payload information about the test which failed it then queries the database to get that test alerting information and sends an alert message to each configured destination here you can see an overview of the entire system from the test configuration coming from the ui and making it to the database and into an eventbridge rule then how that rule fires off for each test run and is routed to the correct region for the actual test all the way back to storing the results in the database and alerting for failed tests i'll now pass things off to tim who's going to discuss some of the decisions and challenges we encountered while building seymour thanks miles let's dive right in during our project research phase after the team settled on our problem domain we were quickly faced with a critical decision should we wrap an open source project to achieve our goals or should we build something from the bottom up let's take a closer look at monica one of the open source projects scott discussed in his overview of existing solutions this project was especially compelling for us because it offered a lot of the functionality that we wanted to provide to see more users monica is an active monitoring tool designed to send out user-defined http requests on a schedule and assess the responses out of the box monica can run complex api tests on scheduled intervals handle alerting through integrations with a number of popular platforms and being open source users are able to maintain ownership of their data we use monica quite a bit in our research and while it works well it was still lacking critical features that were important for our use case first and foremost monica lacked the ability to originate tests from a variety of geographic locations it's limited to sending http requests from the server instance the app is running on we also honed in on the difficulty of using monica which is a command line tool and does not come with a ui as scott mentioned earlier one of our goals was to make tests easy to configure with a solid understanding of monica's capabilities and limitations we then took a hard look at wrapping and extending monica to achieve all of our requirements if monica could provide many of the components for c-more we could focus more time on additional features or so we thought after assessment we felt that we could add a ui and expanded metrics with relative ease a much bigger challenge came into focus when we looked at multi-region testing because monica requires a constantly running process in order to give our users the option to generate multi-region tests we would have had to deploy the app to dedicated virtual machines in each of the 22 aws regions we plan to support this approach presented a number of thorny questions first each virtual machine would require security networking and application configuration upon deployment how would we make that easy for the user next to generate tests a monika instance must read from a local yaml configuration file how would we make sure that the config files were all in sync across the virtual machines lastly 22 virtual machines would be inherently difficult to maintain how would we ensure the reliability of what was supposed to be an easy to use monitoring tool furthermore even if we were able to answer these questions and monica was distributed across regions how would we then send the test results back to the home region where the data would be processed after researching potential solutions to these problems we determined our entire project architecture would have to be oriented around monica ultimately we determined that bending the infrastructure to accommodate only a portion of the functionality was sub-optimal and that by developing our own solution we could avoid these challenges as miles demonstrated in his overview of seymour's architecture we ended up utilizing lambda functions to achieve our requirement of multi-region testing instead of deploying an entire virtual machine running monica to remote regions seymour simply deploys the testrunner lambda function coupled with a queue although we had to write more custom code we found the setup to be superior in the following ways first the testrunner lambda function is easily deployed to all regions with aws's cloud development kit or cdk this eliminates the need to configure any virtual machines the testrunner lambda function is stateless all of the scheduling and configuration logic remains in the home region and is communicated to testrunner via sns aws message broadcasting tool and by placing a cue in front of the test runner lambda we get a high degree of assurance that testrunner will process incoming sns messages communication of test runner's results back to the home region is simple thanks to lambda's ability to send invocation records to a queue destination in another region and rather than a constantly running virtual machine the test runner is invoked on demand this reduces maintenance needs and also reduces costs finally lambda functions scale on demand out of the box if a user configures dozens of tests to run on the same region at the exact same time testrunner can handle the load in the end by building seymour from the ground up we were able to fully take advantage of aws cloud native products resulting in an easy deployment and low maintenance user experience our journey to building the route package or lambda function also comprised an interesting set of design decisions if you recall from the walkthrough of seymour's architecture rat packager is the target for all scheduler rules also recall that rules in the context of seymour are simply scheduled events with an attached json test configuration we knew from the outset that we wanted to enable geographically distributed testing locations with an efficient use of resources deploying testrunner to all remote regions achieved part of this plan but how to communicate scheduled test configurations to test runner was not immediately apparent one option we looked at was directly invoking remote region test runner lambdas from the scheduler rules while eventbridge is technically capable of this there is a 5-5 target limit for any given rule because we wanted to support 22 targets one for each region we quickly discarded this approach we also tried using eventbridge's eventbus functionality which is essentially another interface for communicating events in aws to connect home and remote regions this approach denied us the ability to easily pass through test configuration json data however and was also discarded ultimately we landed on using sns as the medium through which scheduler events were distributed because of its ability to widely broadcast a message containing the test configuration across regions while this setup was effective it lacked efficiency because for every rule that fired a corresponding message would be delivered to every test runner lambda this was not ideal because every message would invoke all test runner lambdas causing each to spin up and execute some code to check if it was an intended target resulting in wasted compute time in order to make this message distribution more targeted we added message filtering logic to the sns topic subscriptions for each region so that only messages intended for a given region were added to the queue for processing by testrunner we still needed one last piece of functionality to make this work however in order for the subscription filters to work we needed to add some region metadata to the message functionality that eventbridge does not provide this is why we decided to build the route packager lambda function in seymour's final architecture each fired rule first travels to route packager which translates the location data from the test configuration into attributes that it adds to the message before publishing to the sms topic the location attributes are then used by the subscription filters to ensure that the test configuration is only sent to the relevant remote regions so by combining route packager and sns in the pre-processing phase of seymour's test loop we were able to achieve our goal of efficient distribution of tests to the remote regions at this point i'd like to highlight how our collective design decisions resulted in a high level of overall system resiliency resiliency was at the top of mind at each phase of building seymour because after all what good would an active monitoring tool be if it was brittle or prone to crashing there are three pillars in seymour's overall approach to system resiliency first we reach for aws's serverless offerings including lambda functions throughout the test loop as discussed in the wrap versus build section using serverless allows seymour to shift a good portion of operational responsibility in infrastructure management to aws furthermore by using aws's managed messaging services we were able to decouple individual components within the test loop as well as the test loop from the rest of the system this design provides assurances that test runs will be processed eventually even in cases of extremely high volume or if one of the loop components fails as miles mentioned earlier once a test is configured it is able to run and generate alerts even if the ui and backend app go down lastly we chose to deploy the back end app using aws's elastic bean stock service which reduces the risk of failures through automatic capacity provisioning load balancing and auto scaling because the underlying infrastructure is fully managed by aws elastic bean stock also provides automatic failover for terminated or unavailable instances in addition to some interesting decisions that our team had to make while building seymour we also encountered some challenges along the way one of the foremost challenges we faced in building seymour was automating the deployment of aws resources across multiple regions because ease of deployment was one of our main goals for the project we wanted to limit as much as possible the amount of infrastructure configuration required of the user the main tool we used to deploy resources was the cloud development kit or cdk which is an aws service used to programmatically set up modify and tear down resources the cdk was relatively straightforward to work with for deploying and connecting infrastructure within the home region however this same approach did not work when it came to connecting resources between the home and remote regions one way of implementing a multi-region deployment is using a cdk pipeline however we continued to look for a different way because of the additional burden of complexity caused by adding the pipeline there would be the one-time technical burden on us to implement the pipeline but it also added several more pieces of complexity for the eventual user just to get seymour deployed on their own account technically this would work and it remained a fallback option for us ultimately we were able to avoid having to use the pipeline we did this by using a different aws tool the software development kit or sdk while the cdk is specifically purposed for setting up modifying or tearing down resources the sdk enables cross-region communication at runtime we added some code to the test runner lambda function so that when invoked it can use the sdk to connect to the home region this kept the deployment process lean and the complexity to a minimum for the installing user while we are pleased with the capabilities that c-more offers there are still a few things that we would like to add to the system as it's currently configured c-more will trigger an alert for every failed test in practice this means that if a user has a test running from 10 different locations and the same assertion is failing for each distributed test the user will receive 10 different alerts for each configured alert channel if this test is configured to run on a tight interval say every one minute well you get the idea you're going to have a whole lot of things chirping at you going forward we'd like to implement a feature allowing users to select alert thresholds as well as alert aggregation options tear-down scripts are custom scripts supplied by the user that are intended to execute after a test is run a teardown script could be used for example to send an additional request to clear out any test data generated from a post request test as it stands the user will have to rely on other mechanisms to clear out data generated by seymour in their production system lastly c-more tests are designed to run on a set schedule but can also be triggered on demand with the run now button in the future we'd like to give users the ability to trigger tests from a cicd pipeline so c-more can be fully integrated with devops processes and that's all we have for you today thanks for listening and now we'll take any questions all right let's just wait a minute see if we get some questions come through the q a all right i was quick all right uh question we have come in here what was the most challenging and the most exciting part of working on seymour uh yeah i can i can start off here i guess i imagine we should probably have our own opinions here uh the most exciting part that first comes to mind uh was the working on something with the overall geographic distribution of tests it it just seemed kind of cool to be able to um have to build something that could uh test and make sure that this thing is working from all these different locations around the world and that kind of fits into some of the most challenging as we uh talk through in working with the cdk part getting the home region and remote regions connected together it had some challenges in there working with uh with with the cdk but it was it was very rewarding i can go uh i think yeah from a from a technical perspective uh learning the the various aws resources that we could use to implement our our design uh was really interesting uh in an area where i learned a lot what was most exciting for me about the project and this is at sort of a high level is that it it actually feels like c-more does fill this this niche that's not addressed by the existing solutions um as far as we can tell it it is unique in in the marketplace uh so that was that was pretty cool um there's still work that would need to be done to get it to a level where you know it uh it could be used by a company but we feel like it actually does address a an unmet need i can jump in so for me i think the most exciting part where we as a team was where we as a team that were deciding about how similar architecture would look like uh where we had to wait or the pros and cons of every engineering decisions we made and um i feel like it was very exciting to see how the whole infrastructure grew with time as we were adding those new components and functionalities i think at the same time it was uh also the most challenging parts as we knew that every decision we make will have its own consequences uh which kind of the weight might be hard to predict beforehand so each of the decisions we had to take needed to be carefully thought through so that was quite challenging i think yeah just to piggyback a little bit on what's already been said um you know during our research phase the global distribution of tests really sparked my imagination for this project and you know got me excited and motivated um not to say that i wasn't already but uh also uh speaking for myself i had never used aws before and so you know the the team as as a whole had to ramp up quickly uh in terms of you know learning how aws aws works and also you know how to connect the different pieces and uh felt like towards the end we were speaking an entirely different language than uh than where we started yeah okay i have another question here uh how did you come up with your project idea someone want to jump in on that sure i can jump in uh yeah we we originally were interested in the over the concept that things do go wrong in production which led us to look into to testing that led us to this concept called fuzz testing um which is designed to is different than what seymour is but it's an interesting topic for generating edge cases in your tests that developers might not think of and and that led us to to active monitoring and then once we discovered this area and and started to learn about the uh the products that were offered by uh the sas solutions and the open source uh projects uh we thought we might be able to build something interesting yeah and that kind of hits uh i see another question here where did the inspiration to build seymour come from i feel like you kind of hit that there scott i also have another question here did you choose eventbridge only for scheduling functionality have you considered other options i could take this actually we we started off originally just thinking about the scheduling functionality you know what's something we can do to have this uh test fire off every one minute or five minutes whatever it is and when we we're looking at the different services or products that aws has eventbridge jumped out for that um but as we got into it it it's really it can do a whole bunch of different things and tim kind of briefly talked about it in his uh discussion of how we were developing route packager in that it is used for event driven architectures and it can send like event messages and so we played around with using that part of the event bridge as well uh we considered building something for ourselves for the scheduling part of it but in the end the solution kind of just coalesced as we looked at combining a few different things into the final architecture that we ended up with okay i have another question here you mentioned data ownership uh as a con for your larger enterprise solutions such as new relic and checkly can you speak about what you mean here a bit more what data would normally be sent in these larger enterprise solutions i can take a stab at that one um so when we when we talk about data ownership um what we had in mind particularly was some sensitive sensitive data like healthcare data or financial data so when it comes to testing and production you know that means exactly like it sounds live data is going to be flying around to for lack of a better phrase um when we looked at the sas solutions including you know data dog and relic checkly um what happens is that uh the the data generated from you know an api endpoint actually uh makes its way to the sas provider servers where the sessions checks are run and that that data is also saved so that you can view results in a future date so we you know we surmised that that could potentially lead to some you know privacy issues in certain contexts and so we like this idea of allowing the users to maintain control of the data and self-host so that that would you know potentially solve a an issue for them yeah and uh i have another question here what's the most valuable thing you learned as a software engineer while making seymour that you will take on to future jobs i would say working in a team like uh it took a little bit of uh uh time to to ramp up and get our processes all in sync in terms of like get workflow and collaboration in terms of like the architecture and weighing design decisions and trade-offs and things like that uh you know but towards the end we were firing on all cylinders and everyone was bringing something different to the table and uh yeah it was a very good experience yeah i have to second uh tim's comment there uh the teamwork just it was at times challenging but just seeing the way that uh everybody's different strengths and weaknesses you know started and then morphed and we each kind of fell into different roles and worked together that was uh that was really satisfying i enjoyed i enjoyed that process too yeah i agree with both team and miles definitely working as a team was very important and there were obviously many challenges with that but i feel like um our team did really good job with communicating um apart from that i think that also that uh researching technical topics was for me something that was very interesting and a very valuable thing to learn how to research a project how to find idea for a project and how to finally create something together as a team something that other people can use and have um uh using a daily life so yeah i think that was very valuable yeah i agree with the uh the rest of the team here that the the teamwork aspect of the project is something uh that i learned a lot from that i'll take with me into whatever is next um and then on like a very tactical level related to the teamwork implementing the like having processes in place with our repos where we would have automatic deployments just solved a lot of headaches we had early on uh with you know some lambdas being out of date and uh and you know only some members of the team knowing that and so once we smoothed out those processes uh things became a lot easier yeah i agreed all right i have another question here to connect to remote aws regions does seymour pass along an arn during the processing phase if that is the case how the heck do you keep track of all the multi-regional infrastructure uh yeah so connecting to uh remote aws regions uh it has to do with the queue in each of the remote regions subscribing to an s s topic and so it's kind of like this pub sub model where the s s messenger in the home region is publishing a message to any subscribers that are there and so each of those cues during the deployment phase we have uh set up to subscribe to the it uses the arn of the topic uh which is created in sns um and yeah that the only improvement we made to that was the filtering uh which uh tim walked us through that prevents like the needless distribution so that all the cues aren't receiving the message it's only the ones that have the specific property for its location okay i have another question uh what was the hardest bug or challenge you had while building seymour so i want to take this i can take uh yeah i can i can try to answer this question although other members of the team definitely jump in uh it was a it was a we were all working on independent parts of what miles referred to as the the test loop when we began development on seymour uh and then we had a a day or two where we all needed to jump into the same uh chat because we were trying to connect all those parts um and and see a test go from configuration all the way to being written to the having its results written to the database and just connecting all of those pieces making sure that the uh the test configuration json was uniform throughout the entire loop uh which it originally wasn't there were small differences in naming uh some some of us had used underscores or dashes uh and so like making all that consistent was required for the the test to make it all the way through the loop but eventually we we sorted that out and um learned from the experience that setting up the the contract beforehand to have consistent naming would solve a lot of headaches uh but once we got through that um things were things were a little bit smoother all right uh can seymour monitor applications that are not hosted on aws uh i can a step on that one uh so the short answer is yes um because we designed similar to we well we designed simult to be cloud specific uh because we wanted to take the full advantage of all aws services that had to offer um but with that being said cmos infrastructure is completely separated from what user might have on their own aws accounts or any other cloud providers accounts so it is not required at all for the user's infrastructure to be hosted specifically on aws um however the user need to have their own aws account in order to host some more but not to yeah yeah okay i see a question here down towards the end what are some tips for researching topics and finding ideas i mean first thing that comes to my mind is uh following something interesting you know as scott i think you were hitting an earlier question talking about some of our our research phase uh i there was a lot of these api adjacent type things i remember personally i was really interested in grpc and kind of like went down this this trail and uh the fuzz testing and just as we kicked around different uh ideas and challenges i don't know just seeing the team conversation back and forth uh just with some input i don't know it kind of there's one phrase you said tim uh it it fired up your imagination like the geographic distribution i think you were talking about and yeah it's a good a good phrase i just wanted to take because getting that imagination kind of that interest and following all the way through i don't know i think that's kind of what brought us together on this and i would even caveat that a little bit like follow follow what's exciting but at first you know cast a wide net because it's easy to kind of dig in and go deep on like you know the first handful of things that you research um but it's often the case that you know it's dead end or you know it doesn't work for x or y or z reason um and actually you know going back to the build versus rap discussion um i was actually i was on the like rap side of things at first and i was like you know lobbying hard for that approach um but you know it's just important to keep an open mind and my teammates you know were able to persuade me that that you know was not ideal for you know the reasons i discussed earlier so i felt like that that was uh you know one of the the biggest takeaways for me during this research is you know just have an open mind and even if even if you hit a dead end it it informs you know the path that you ultimately take yeah and i see oh sorry go ahead jump in katarina i just wanted to add to this one that um what we have actually did with our team was uh we decided to research as many topics as possible so we met every day and maybe research for two hours try to find a topic and then we present it to each other uh and if that was interesting then maybe we were keeping researching day after but if other team members didn't find it uh interesting at all then we would just leave the topic and try to find something else but the first week of our research was the phase when we are actually didn't dig two where we actually didn't dig too deep into specific topics but we try to go very broad to kind of have a bigger picture of what's out there yeah and i see uh this last question here uh it's a good question how did you determine which team members worked on which of the independent parts of seymour and what strengths did each team member bring someone want to start on this i can i can start um i think we were we were really lucky uh in that we yeah all of us were some there was overlap in some of the the things that we were interested in uh but there also was a lot of of um mutually exclusive things that we were interested in and so each of us kind of was able to work on the thing that most excited us and still be able to complete the project there was i don't think really a moment or a particular item that nobody wanted to work on that you know one of us needed to put up our hands and say like okay i'll do it it just kind of worked out that each of us had a had a slightly different set of of interests in the implementation all right nobody else has anything to add there we can just give it another minute see if any final questions roll in okay did you guys have any technical disagreements or conflicts in your group and how did you handle those uh we did tim kind of mentioned uh one probably the one of the first big conversations uh we'd kind of gotten to a point where we had all agreed on all right let's let's dig in to this idea that you know became seymour and at the beginning you might think that you're all in agreement but as you start having conversations and dig into well what would this look like how would we design this part um you know what about this piece of the architecture you realize that maybe your thoughts aren't quite an agreement as you thought and we'd probably not halted but we definitely took a a couple days where we really looked at all the different technical aspects of either wrapping monica or building something from the ground up because it it really felt like it would be this maybe a longer trajectory like if we wrap monica well then that might lead to this well then how would we solve that and kind of walking down the whole road of what that would mean okay and if we don't then we'll take more time to build something what what does that mean will we have to sacrifice some features and that was uh i won't say heated i i feel like the conversation was actually really really productive but just through a lot of conversation we to have conversations do research come back you know just turn turn the problem around a little bit and look at it from a different angle as a team that's probably how we handled that anybody else want to add anything on that i'll add one thing i think we're really good at not becoming personally attached to any ideas uh like the person who suggested something in general was not emotionally attached to that and we were able to rely when it came to disagreements on listing pros and cons of the uh the options we were considering and as a team once those pros and cons had been presented uh deciding on the the best option uh and normally like after we had done gone through that process uh it was it was obvious uh which thing we should choose whether that was a database or an infrastructure decision or even something that was was not technical um in terms of like how we organize our efforts all right i think we're winding down here if anybody has any last questions you go ahead and ask them we'll give it about a minute and then that's it all right well i think that is it thank you everyone we appreciate you coming to the presentation today and thank you for letting us share see more 