Welcome and thank you for joining us. My name is Ann and my teammates are Dawn, Olga, and Pauline. Together, we developed Arroyo, a lightweight framework for the granular rehydration of logs. Today, we're excited to introduce Arroyo and show you how it can be used.

But before we get into Arroyo, let's start by talking about Meamly, a fictional image and video hosting service. Meamly is a popular site that consolidates the most shared content from across the web. As Meamly experienced growth, they realized they needed better oversight of their infrastructure. They also had a specific problem to address: investigating potential click fraud on their site.

To address these challenges, Meamly set up an ELK stack, which stands for ElasticSearch, Logstash, and Kibana. This stack helps them manage and analyze their logs. However, as the number of logs increased, they faced performance issues with ElasticSearch.

To optimize their system, Meamly decided to store their logs in AWS S3 and keep only the most recent two weeks of logs in ElasticSearch. This allowed them to search logs quickly while still having access to older logs for auditing purposes. However, they soon realized they needed a way to rehydrate or reintroduce their archived logs back into ElasticSearch.

At this point, Pauline introduced the concept of observability and explained the three pillars of observability: metrics, traces, and logs. Logs are detailed records of events that occur in an application and are essential for troubleshooting and auditing. However, managing logs can be challenging, especially as the volume increases.

To address this challenge, many organizations use the ELK stack. Logstash is used to transform logs into a standard format and send them to ElasticSearch, which stores and indexes the logs. Kibana provides a graphical interface for searching and visualizing the log data.

While the ELK stack works well for Meamly, they faced issues with searching logs as the volume increased. This is where Arroyo comes in. Arroyo is a lightweight framework that allows users to perform granular rehydration of logs. It provides an easy way to transfer archived logs from S3 back into ElasticSearch for searching and analysis.

Arroyo offers two options for log rehydration: bulk rehydration and conditional rehydration. Bulk rehydration allows users to select a timeframe and re-ingest entire log files back into ElasticSearch. This is useful for analyzing trends and deriving metrics with historical context.

Conditional rehydration, on the other hand, allows users to search logs by date range or provide specific queries to selectively re-ingest logs. This offers more flexibility and customization for the audit process.

During a demo, we showed how the Meamly team could use Arroyo's bulk rehydration option to re-ingest their archived logs and have them available in ElasticSearch. The process was straightforward, and once the rehydration job was completed, the team had fully searchable logs within the desired time frame.

However, it's important to note that bulk rehydration does have some limitations. In the case of Meamly, they rehydrated logs related to a specific client IP that they suspected of click fraud. But the majority of the rehydration job included logs from other clients, which the team was not investigating. This can impact the capacity and cost of the rehydration process.

To address this, Arroyo also offers conditional rehydration. This allows users to search for specific logs using queries and selectively re-ingest only the relevant logs. This provides a more targeted approach for auditing purposes.

In summary, Arroyo is a lightweight framework that provides a solution for the granular rehydration of logs. It offers both bulk and conditional rehydration options, allowing users to tailor the process to their specific needs. While Arroyo may not have all the features of a commercial observability solution, it is easy to use and integrates seamlessly with an existing ELK stack.

Moving forward, the Arroyo team has plans to continue improving the framework. They are considering adding more advanced search capabilities, as well as exploring ways to optimize the rehydration process for large-scale systems. They also aim to provide better documentation and support for users.

In conclusion, Arroyo offers a flexible and cost-effective solution for log rehydration. Whether it's for trend analysis, auditing, or deriving new metrics, Arroyo can help organizations like Meamly make the most of their log data. Thank you for joining us, and we welcome any questions you may have. Arroyo provides two options for rehydrating archive logs back into Elasticsearch. The first option is bulk rehydration, where users can specify a timeframe and receive a list of log files inserted into their S3 bucket during that timeframe. To use this option, users need to specify the name of their AWS S3 bucket during the setup and launch of Arroyo. From there, users can select log files from the list and ingest their contents back into Elasticsearch.

The bulk rehydration option is useful for reintroducing a batch of archived data into Elasticsearch to analyze trends or derive new metrics with historical context. For example, the Meanly team could use Arroyo's bulk rehydration to assist with their audit. They can specify a start and end date and select their Nginx logs to ingest. Once the job is submitted, the Arroyo UI will notify the user of the progress and completion, allowing the Meanly team to access fully searchable logs within a previously unavailable timeframe.

While bulk rehydration is useful for trend analysis and developing metrics, it has some trade-offs. The donut chart in the Kibana dashboard shows the breakdown of rehydrated logs during a bulk rehydration job. It can be seen that over three quarters of the logs rehydrated were generated by clients that the Meanly team is not currently investigating. This introduces irrelevant data and consumes capacity in Elasticsearch. To address this, a more granular approach is recommended to rehydrate only the most relevant data.

Arroyo offers another option called query rehydration, which allows users to specify a timeframe and include two log attributes for conditional reingestion based on their values. In the Meanly team's case, they could specify the spammy client IP and an HTTP request method of GET. By submitting the job and returning to their Kibana dashboard, they can gather the necessary information without introducing any additional data. This focused approach gives users more control over the logs they reingest into Elasticsearch.

Index cleanup is another important aspect of using rehydrated logs. Arroyo provides a sample Logstash HTTP input template that users can implement. The goal is to store all rehydrated information in a separate index, called "logs-arroyo," to maintain separation from a user's day-to-day logs. Once users are done using the rehydrated logs, they can simply delete the "logs-arroyo" index, ensuring that unnecessary data is not cluttering Elasticsearch.

To get started with Arroyo, users need to clone the Arroyo deployment repo from the team's GitHub page. They then install the necessary Node packages and follow the instructions in the readme file to provide deployment credentials, such as AWS credentials and the archive bucket name. After that, users can run the Arroyo deployment command to provision the required AWS infrastructure for the rehydration tasks.

Launching and initiating log rehydration is straightforward once the AWS infrastructure is deployed. Users can issue the command "docker-compose up" to fetch and build the necessary Docker images for the Arroyo UI and backend. An environmental variable file needs to be filled out with information like the archive bucket name and AWS credentials. This approach minimizes the additional work required from the user, keeping the deployment and launch process simple.

After performing the necessary rehydration tasks using Arroyo, cleanup is also straightforward. Users can unprovision the AWS infrastructure created during deployment by using Arroyo's destroy command. Once the destroy script finishes executing, all the AWS infrastructure created by Arroyo will be removed, requiring no manual cleanup from the user. This ensures that the entire process remains easy and user-friendly.

Now, let's dive into the architecture of Arroyo and understand how it processes rehydration requests. Arroyo provisions various AWS resources during installation, such as rehydration SQS, status SQS, S3 bucket for lambda deployment, and an IAM role for lambda function. The Arroyo client and server applications are containerized using Docker and Docker Compose.

Conceptually, the Arroyo application can be split into three phases: rehydrate, re-ingest, and notify. During the rehydrate phase, the user submits a request via the Arroyo client, which then forwards the request to the Arroyo server. The request includes file names and additional data about the rehydration request. For each file, the server sends a message to the rehydration SQS, containing information such as the S3 bucket name, file key, logstash endpoint, and optional SQL expression for querying.

Once the message is placed on the queue, a Lambda function is invoked. Depending on the contents of the message body, Lambda either uses the SQL expression to query the file using Amazon S3 Select or fetches the entire file. The ability to query log files allows Arroyo to retrieve a subset of the file contents, reducing the amount of data reingested into the user's Elasticsearch cluster.

In the re-ingest phase, Lambda makes a POST request to the logstash endpoint provided by the user. The rehydrated logs are then forwarded to Elasticsearch by Logstash for indexing. Once indexed, the logs become searchable in Kibana. The Arroyo client needs to be notified when the re-ingestion job is completed, which is where the notify phase comes in.

After Lambda forwards the rehydrated logs to Logstash, Arroyo needs to be notified of the rehydration request outcome. Lambda creates a message containing the response data and places it on the status SQS. The Arroyo server continuously monitors the status queue, saving the message bodies and deleting the messages once processed. When there are no more messages on the status queue, the Arroyo server sends a single notification to the Arroyo client using server-sent events. The client then notifies the user via a text message, and the rehydrated logs can be viewed in the Kibana dashboard.

During the development of Arroyo, several engineering decisions had to be made. The first challenge was ensuring controlled and non-overwhelming rehydration of logs. Instead of directly accessing the S3 bucket or using HTTP requests for each rehydrated log, alternative solutions were considered. One option was to introduce Kafka into the architecture. However, the current approach relies on SQS to ensure controlled rehydration without overwhelming the Logstash service.

Another challenge was achieving precision in rehydration by querying specific log files. Amazon S3 Select was utilized to enable SQL-like querying of log files, allowing for filtering and retrieving specific data within the S3 bucket. By parsing the data into records and returning only the matching records, Arroyo minimizes the amount of data reingested into Elasticsearch.

Designing Arroyo with simplicity and user-friendliness in mind was a priority. The process of setting up and launching Arroyo was made easy by providing a GitHub deployment repository with clear instructions. Full infrastructure provisioning, launching the Arroyo UI and backend, and providing AWS credentials and the archive bucket name can be done with just one environmental file. Additionally, cleanup is simplified by using Arroyo's destroy command, which removes all Arroyo-created AWS infrastructure.

In conclusion, Arroyo provides two rehydration options: bulk rehydration and query rehydration. Each option serves different use cases, allowing users to reintroduce archived data into Elasticsearch with control and precision. The architecture of Arroyo leverages various AWS resources, Lambda functions, SQS queues, and Logstash to facilitate the rehydration and re-ingestion of logs. By making well-considered engineering decisions and focusing on user experience, Arroyo aims to simplify the process of retrieving and utilizing rehydrated logs in Elasticsearch while mitigating data bloat and ensuring efficient cleanup. The first phase of the project involves rehydrating logs and posting them. Once the logs are rehydrated, they are forwarded to Elasticsearch using Logstash. After being indexed by Elasticsearch, the logs are ready to be viewed in the Kibana dashboard. But how does the Aurora client know when the rehydration job is completed so that it can notify the user?

In the last phase, known as the notify phase, the Aurora client needs to be notified about the outcome of the rehydration request. Lambda plays a crucial role in this phase. After Lambda receives a response from Logstash, it grabs the response data and packages it into a message. This message is then placed onto a status SQS queue.

The purpose of the status queue is to hold the status of the ingestion tasks for the Aurora server to pick up. On the Aurora application server side, once the server receives messages from the status queue, it saves the body of each message and deletes it from the queue. This process continues until there are no more messages on the status queue.

When there are no more messages on the status queue, the Aurora server sends a single notification to the Aurora client via server sent events. The Aurora client receives the notification event and notifies the user through a test message. This way, the user can view their rehydrated logs in the Kibana dashboard.

During the development of the Aurora application, our team faced several engineering decisions. The first challenge was to ensure controlled and manageable log rehydration without overwhelming the Logstash service. The initial solution was to introduce Kafka into the architecture. Kafka would act as the middleman between the Aurora application and the user's ELK stack. However, setting up Kafka proved to be a non-trivial task, leading us to consider other options.

We decided to combine Amazon SQS and AWS Lambda as a solution. SQS acts as a buffer, while Lambda handles the rehydration and ingestion tasks. This combination provides better performance, scalability, and reliability. SQS and Lambda are easily integrated and can be programmatically configured, making our backend server more lightweight. However, setting up Lambda programmatically requires significant effort.

To ensure that the queue is not blocked by problematic messages, we explored two approaches. The first approach involved adding more error handling and retry logic to the server code. However, upon further investigation, we realized that this could lead to complex logic. The second approach was to use a dead-letter queue. This queue holds failed messages separately, preventing them from blocking the main queue. It also simplifies troubleshooting for failed messages.

Notifying the client about the completion of the rehydration job posed another challenge. Initially, we considered using a webhook. This would involve setting up an API endpoint that accepts data payloads and triggers events. However, this approach tightly coupled the notification layer with the rest of the logic. Instead, we opted to use SQS. Once Lambda receives a response from Logstash, it puts a message containing the status on the queue. The backend server picks up and processes this message. Once all messages are processed, it notifies the client.

In terms of future work, we aim to improve visibility into the jobs dispatched by Aurora. Additionally, we want to enhance searching capabilities by allowing full text log searching and more robust query searches. Furthermore, we plan to expand integration capabilities to fetch logs from multiple cloud storage vendors. These efforts will increase flexibility and expand the benefits of log rehydration to a wider range of users.

In conclusion, the project addresses the challenges of log rehydration and ingestion. It employs a combination of AWS services, including SQS and Lambda, to ensure controlled and manageable log processing. The architecture allows for notification of the client and provides potential for future enhancements, such as improved visibility and expanded integration capabilities. Thank you for joining us for this presentation.