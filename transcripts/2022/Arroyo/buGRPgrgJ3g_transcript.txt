welcome and thank you for joining us my name is ann and my teammates are dawn olga and pauline and together we developed arroyo a lightweight framework for the granular rehydration of logs we're excited to tell you about it we'll start off by introducing you to meanly a fictional image and video hosting service we'll talk about observability and then we'll zoom in a little bit and talk about the elk stock and go over some existing solutions for rehydrating logs after that we'll introduce arroyo and we'll go through some demos during our technical deep dive we'll explore how arroyo works we'll go through some of the engineering decisions we grappled with while we were developing arroyo and finally we'll cover some of our future plans we'll wrap up by opening the floor for any questions before we start what is an arroyo an arroyo is a dry creek bed or gulch that temporarily or seasonally fills and flows after sufficient rain similarly our project arroyo is a framework that performs rehydration arroyo rehydrates logs that have been stored in aws s3 to aid with our presentation our team would like to introduce everyone to meamly.com and its engineering team memeli is a fictitious image and video sharing and hosting service that scavenges the internet to consolidate the most popular content being shared across the web memeli strives to provide a one-stop shop for non-stop laughs memely.com was not an overnight sensation but they now have a consistently growing user base this has allowed meanly.com to start generating pay-per-click ad revenue by hosting clickable ads on its front page the ad revenue has allowed mainly to reinvest in the site by hiring a small team of full-time developers the memelia engineering team is currently focusing their efforts on scaling the meanly infrastructure to accommodate their growing user base as the memele team began scaling their infrastructure they started to sense that they did not have the level of oversight desired to move forward confidently the infrastructural complexity grew during the team's initial scaling phases eventually a problem requiring an audit occurred the ad agency that memey.com hosts ads from has asked the memele team to perform an audit to investigate potential click fraud that occurred on their site within the past month in short the number of clicks or user engagements with the ads that the agency generates and that memeli.com hosts well exceeds the historical amount for the site how can meanly perform the click fraud investigation given the complexity of their infrastructure next paulian will cover what observability is and how it can help mainly perform their audit thanks ann hi i'm pauline i'll be reviewing basic observability concepts today first some definitions to observe is to watch carefully observability is the ability to measure a system's current state using the data the system outputs note that observability is a property of a system while monitoring is accessing the data provided about the system to make decisions we use three types of data in observability that are sometimes called the three pillars first metrics provide the big picture measurements about a service captured at run time for example the number of requests per second next traces so the path a request takes through a distributed system finally there are logs let's go into more detail blogs are text records ideally with a time stamp they can be structured or unstructured logs often come with metadata and are usually the most finely detailed view of what is happening in an application logs are emitted by many applications because of the level of detail provided logs are useful for both troubleshooting and auditing however blogs don't scale well well why not how can we find the information we need when we are overwhelmed with a large amount of log entries ideally we will need a way to centralize logs search logs and analyze the results of the search one common way to manage logs is to use the elk stack elk stack stands for elastic certs log stats and kibana let's dive deeper into what this stack looks like we'll start with log stats move on to elasticsearch and finish with kibana log stash is a data collection engine with real-time pipelining capabilities it has three stages inputs filters and outputs inputs are where events are generated in our case the log files from every node filters are where we modify or transform those events here we will transform the events from text files to json outputs are where the filtered inputs are shipped to we will ship the json to elasticsearch now we'll review elasticsearch elasticsearch performs many functions elasticsearch is a distributed document store a real-time search engine and an analytics engine let's break down the meanings of each of these terms first elastic search as a distributed document store distributed means each elasticsearch cluster has many different nodes for increased reliability document store means elasticsearch stores serialized json files as complex data now let's explore elasticsearch as a real-time search engine real time means it takes one second or less to index a document using an inverted index search means support for structured sql-like queries text queries and complex queries that combine both finally let's review elasticsearch as an analytics engine analytics means creating complex summaries of data to gain insight into key metrics patterns and trends now let's move on to kibana kibana is a collection of graphical user interface applications that wrap around elasticsearch inside kibana you can search data using kibana query language make charts and dashboards and manage the elk stack from a graphical user interface let's move back to meanly to see how they've implemented their elk stack thanks pauline memeli has set up logstash to transform text logs to json and send the transformed logs to the elasticsearch cluster niemly has elasticsearch running on its own ec2 instance for better performance additionally memeli has kibana running on a separate ec2 instance however as the number of logs shipped to elasticsearch increases elasticsearch is starting to run very slowly fast searching is a key benefit of elasticsearch but as the number of documents in elasticsearch increases searches become much slower the stale documents in order to keep searching fast old or stale documents are removed from elasticsearch using an index lifecycle policy the stale documents can be stored elsewhere one common choice is to store them in cloud storage memeli decides to keep two weeks worth of fresh logs and elastic search with stereo logs shipped to an aws s3 bucket the decision to store two weeks worth of logs has been working well for the memele team their ability to search logs is fast and reliable they have the added peace of mind knowing older logs are stored in aws s3 which is a cheaper and more secure storage option next we'll explore how the meanly team can use their searchable logs within elasticsearch to begin their audit during a preliminary audit of their indexed web server logs the meanly team noticed the following patterns the donut chart shows us each client ip as a share of total requests to memeli.com within the past 14 days we'll notice one client ip holds a drastically disproportionate amount of weight within the number of requests that were made within the given time frame at this stage of the audit the mainly team chooses to focus their investigation around logs that involve this seemingly spammy client ip is the mainly team dealing with an extremely active user or something more malicious the memeli team analyzed all of their indexed web server logs involving the specific client ip to understand how the client has been interacting with the website the team discovered the following behaviors all requests issued by this particular client were http get requests all requests were sent to the root page of memeli.com where the majority of the ads memey hosts are served additionally no attempts were made to browse the rest of the site as a typical user would at this stage of the audit the meanly team is confident they are dealing with a malicious user reinforcing suspicions that meme.com experienced bouts of click fraud within the past 30 days the next question the mainly team would like to answer is how long has this user been interacting with their site can the meanly team pinpoint when the suspicious behavior began in order to move forward with their audit but because the memele team decided to index only the most 14 most recent 14 days of logs they don't currently have all of the necessary data within elasticsearch to complete their audit ideally what the meanly team would like to accomplish is to transfer the logs they currently have archived in s3 back into elasticsearch in order to search older logs and a feature-rich familiar environment while conducting their audit the task of transferring archived logs back into elasticsearch for easier searching is often referred to as log rehydration and the mainly team has a few options at their disposal next we will take a look at some of the existing solutions the meanly team could use to rehydrate their archived logs there is no shortage of commercial observability solutions that offer log rehydration features these are feature-rich all-in-one solutions meaning that users do not need to concern themselves with scaling complexities or infrastructure management while a commercial solution is an option the mainly team could explore they have reservations about cost because they may end up paying for features that they don't currently need complexity because they would need to transfer all their current logs to a new unfamiliar technology stack and time because it might take a while for the meanly team to get up to speed with all of the features offered all this while attempting to perform their audit if a commercial solution doesn't quite fit meanly's needs there are many open source tools available to aid in their rehydration goals the main benefit of a diy solution is that the meanly engineering team could continue to use their existing elk stock and log archival solution another benefit is that the team could save on cost by looking to utilize free open source tools and plugins however because the meanly team is small they may not have the time and resources to dedicate to researching all of the open source tools they could use to build a rehydration tool in-house the scarcity of turnkey free rehydration tools is one of the primary reasons we decided to build arroyo so far we've explored two options the commercial route the diy route and soon we'll explore arroyo both the commercial route and the diy route are valid solutions for log rehydration but they don't quite fit the mainly team's use case ideally the meanly team would like to strike happy medium between the ease of a commercial solution and the ability to keep their existing elk stack that a diy solution offers another option the mainly team could explore is arroyo while arroyo is not as feature rich as a commercially available solution the ease of use and ability to integrate with existing elk stacks may make arroyo the right tool for them next we will explore arroyo highlighting what arroyo is and how it might be used to help me team rehydrate their logs i'll hand it over to don thanks ann so we hinted at a log rehydration early in the presentation and the first question we need to ask is what is arroyo so to start arroyo is a lightweight framework for the granular rehydration of logs and we're going to revisit this idea of granular rehydration a little further along in the demo allow users to search logs by date range or provide a date range and up to two queries to conditionally reingest logs and those search results are pulled from s3 back into elasticsearch so let's take a look at arroyo in action by going through a series of demos arroyo offers two options to facilitate the rehydration of archive logs back into a user's elastic search the first option is a bulk rehydration option with this option a user can specify a time frame and receive a list of log files that were inserted into their s3 bucket within that time frame in order to generate this list of available log files a user is going to need to specify the name of the aws s3 bucket where their logs are currently archived while setting up and launching arroyo which we'll cover a little later in our demo but from there users can select from a list of available log files and re-ingest the entire contents of the selected files back into elasticsearch a great use case for the bulk rehydration option is when a user might want to reintroduce a batch of archived data into elasticsearch in order to analyze trends or derive new metrics with some historical context let's take a look at how the mainly team could use arroyo's bulk rehydration to help with their audit so within the royal ui a user can specify a start and end date and from there they can select from a list of archive files that they would like to ingest so for the meanly team they could select their nginx logs now once the job has been submitted the arroyo ui will notify the user that the task is in progress and they'll also be notified once the job is completed and they can return to their keyboard dashboard so now that the meanly team has a bunch of archive logs within their elasticsearch cluster they can continue with their audit and as we can see the meanly team now has access to fully searchable logs within a time frame that was previously not present within elasticsearch while the bulk rehydrate option is a great tool for trend analysis and developing new metrics with some historical context it does come with some trade-offs so this donut chart was generated in the kibana dashboard and represents the breakdown of the number of logs rehydrated during arroyo's bulk rehydration job the melee team wanted to rehydrate logs that were generated by the suspicious client ip that was identified during their audit and they've managed to do so but we'll notice that over three quarters of the rehydration job includes logs generated by clients that the meanly team is not currently investigating so what this graph is is a re-visualization of these results with some added context in that context being the impact on capacity the size so in order to generate this graph we're making some assumptions let's assume that the meanly team has performed multiple bulk rehydration tasks and they've ingested around 10 000 logs in each of these logs each individual log line is 297 bytes in size what we're going to notice is that irrelevant data or data that the mainly team is not currently deriving any value from is contributing 645 megabytes of capacity with an elastic search to better suit the meanly team's use case it would be beneficial to rehydrate logs on a much more granular level reducing the amount of excess data that's reintroduced back into elasticsearch with each job striving to rehydrate only the most relevant data so let's rewind the clock a bit and demonstrate how the mainly team could use arroyo's query rehydrate option to get the information they need without any additional data bloat with arroyo's query ingest option users can specify a time frame just as they would the bulk rehydration option but then in addition they're able to include two log attributes that they want to conditionally reingest logs based on so for the melee team's use case they could specify the spammy client ip as well as an http request method of get since this behavior seems to be following a very specific pattern now once the job has been submitted the meanly team could return to their keep on a dashboard and continue with their audit now what we're going to notice is given a similar time frame in addition to being able to provide two log attributes the meanly team was able to gather the information they need without any excess additional data and that's what that singular stick represents it's only the information that pertains to that spammy client ip so this donut chart was generated in the cubana dashboard and represents the breakdown of the number of logs ingested during arroyo's query rehydrate job and as we can see only the logs pertaining to the malicious client ip were re-ingested for this specific time frame the ability to query logs before re-ingestion allows users a much more granular level of control with respect to the logs that they ultimately reingest back into elasticsearch which we think is a very important tool so next we'll take a look at index cleanup after a user is done using their rehydrated logs one of the challenges using elasticsearch is striking a balance between having too much information on hand and too little information naturally rehydration reintroduces data back into elasticsearch which leads to some questions what can a user do once they've rehydrated their logs and derived the information they need do these logs simply stay in elasticsearch and if so doesn't that contribute to the problem that arroyo is seeking to help solve how do we reconcile with these questions arroyo provides a sample logstash http input template that users can implement into their elk pipeline the goal is to store all rehydrated information in a completely separate index compared to a user's day-to-day logs that way a user can simply delete the index once they're done using those rehydrated logs so by default if a user chooses to utilize the input template that the arroyo team includes in our deployment repository the rehydrated logs will be stored in a separate index called logs dash arroyo the template we provide is simply a guide and users can tweak their logstash http input plugin as it suits their use case next let's take a look at how to get started using arroyo to get started using arroyo a user will need to clone the arroyo deployment repo available from our team's github page install the necessary node packages and then follow the readme instructions to provide the deployment credentials important things like your aws credentials and your archive bucket name but from there the user can run a royals deployment command in order to provision all the necessary aws infrastructure arroyo is going to need to perform rehydration tasks next let's take a look at how to launch and get started rehydrating logs so once the necessary aws infrastructure has been successfully deployed a user can simply issue the command docker compose up and the docker client will fetch and build the necessary images in order to launch the arroyo ui and back end on their host machine part of launching the application will require the user to fill out an environmental variable file with information like the archive bucket name which is where the user's logs are currently archived and their aws credentials providing these variables in an environmental file allows users to provide the minimum amount of information a royal will need to perform rehydration tasks the goal was to keep this process easy we wanted the aws infrastructure deployment to be simple and we also wanted launching the royal ui and back end to be simple as well so filling out one environmental file allows arroyo to do everything it needs without much additional work from the user so at this stage let's assume that the users deployed all the aws infrastructure they've performed all the rehydration tasks that they needed to perform what about cleanup let's take a look at how arroyo can help in that department as well so once a user has finished the rehydration tasks they can easily unprovision the aws infrastructure that was created during deployment by issuing arroyo's destroy command once the destroy script has finished executing all arroyo created aws infrastructure will have been destroyed leaving no artifacts for the user to clean up afterwards we wanted this entire process to be as easy as possible next i'm going to hand the presentation over to olga who will give a technical deep dive into how royal works as well as some design considerations that we made along the way thanks don now let's dive into aurora architecture and see how our application processes rehydration requests first let's take a look at a high level overview of our application and aws resources that are used by the application during the installation arroyo provisions a number of aws resources this is done programmatically using aws sdk arrow provisions rehydration sqsq and status sqsq as well as their corresponding deadlotter queues s3 bucket is created for hosting lambda deployment package arroyo also creates an imroll for lambda function along with appropriate permissions attached to it arrow also creates the lambda function itself both the royal client and the os server applications are containerized using docker and docker compose conceptually we can split our royal application into three key phases rehydrate re-ingest notify let's take a look at the first phase which is rehydrate during the rehydrate phase the user first submits the request via royal client which is a react application that provides the royal ui a royal client then forwards the rehydration request to the auros server which is a node application that interacts with aws resources and facilitates rehydration and re-ingestion of logs the request contains names of the files to be rehydrated as well as some additional data about the rehydration request for each file that needs to be rehydrated server sends a message to the rehydration sqsq within the body of the message the server includes information about s3 bucket name where logs are stored key that represents the name of the file to be queried or fetched log stash endpoint where the rehydrator logs will be forwarded to and sql expression which is optional and will only be included if the user chooses to perform a query instead of rehydrating the whole file once the message is placed on the queue lambda function is invoked lambda will do one of two things if message body contains a sql expression lambda will use that sql expression to query the file otherwise lambda will fetch the whole file now i'm going to describe how the file is queried in a bit more detail we perform the query by utilizing amazon s3 select feature s3 select allows for filtering and retrieving data contained within an s3 bucket it allows to query structured data using simple sql statements the log data stored in s3 bucket must be in json format although s3 select also supports csv or apache park a currently arrow only supports json format to perform querying of the logs via s3 select feature we invoke select object content action to create and submit a query request this is done via aws sdk we send the user generated sql expression as a part of the request and also specify that we would like any matches to be serialized and returned in json format once amazon s3 receives the request it parses the data into records and then returns the records that match the provided sql expression being able to query log files allow us allows us to retrieve a subset of the file contents and therefore reduce the amount of data that we reingest in user's elk stack let's take a look at the next phase which is rangest during the ringest phase lambda makes a post request to log stash endpoint provided by the user once the rehydrated logs are posted they are forwarded to elasticsearch by logstash after they're indexed by elasticsearch the logs are ready to be viewed in kibana dashboard but how does the aurora client know when the reject injection job is completed so that it can notify the user let's see what happens during the last phase which is the notify phase once the logs are rehydrated and then forwarded to logstash by lambda a raw application needs to be notified about the outcome of the rehydration request after lambda gets a response from logstash lambda grabs the response data and packages it into a message it then places that message onto a status sqsq the goal of the status queue is to hold the status of the ingestion tasks for the ros server to pick up at the same time here is what happens on the arrow application server side once the ros server finishes sending all of the messages to the rehydration queue it starts following status queue for each message it receives from the status queue it saves the body of that message and then deletes that message from the queue once there are no more messages on the status queue the error server sends a single notification to the url client via server sent events our client receives the notification event and then notifies the user via a test message now the user can view their rehydrated logs in the kibana dashboard let's discuss engineering decisions that our team had to make while developing a royal application the first engineering challenge we had to solve was how to make sure that we rehydrate logs in a controlled manner and without overwhelming logstash service in order to reingest logs from s3 we could simply configure logsdash s3 input plugin to access the s3 bucket directly and re-ingest all of the logs that it contains the problem is that we do not need to readjust the entire bucket contents instead we need to be able to ingest the logs with precision potential alternative approach is to use an http input plugin instead with http input plugin we could send each rehydrated log or a whole log file as a post request however if we go this route we can run into an issue is as the number of rehydration tasks and a number of post requests to logstash increases dramatically we could potentially overwhelm the lockstar service for example if the user decides to ingest thousands of files in a single rehydration job with the current approach we could we would have to send thousands of post requests one right after the other this may result in lockstar service getting overwhelmed in order to solve this challenge we considered a couple of different alternatives the initial solution we came up with was to introduce kafka into our architecture kafka is a distributed event streaming data store using for use for ingesting and processing of data in real time kafka would serve as the middleman between aurora application and user's elk stack in order to integrate kafka we'd have to create a kafka producer and we'll also have to configure logstash to be able to accept kafka events in this case we would have full control over which logs are ingested and also ensure that rehydration and regression layers are decoupled from the server logic this approach will work well but after conducting more research we came to a conclusion that setting up kafka producer is not is a non-trivial task we decided to continue evaluating other options the solution that we decided to implement was a combination of amazon sqsq and aws slam the function sqsq would serve as a buffer and lambda would perform rehydration and re-injection tasks now let's take a look at both of these technologies and discuss the pros and cons amazon's simple queue service is a fully managed solution that allows to easily create use and manage a queue using a queue would allow us to create a buffer between the os server and the re-ingestion and rehydration layers this would provide for better performance scalability and reliability of the whole application additionally it is easily integrated with aws lambda and is fairly simple to create and interact with using aws the downside of this choice is that introducing new components like sqsq increases overall complexity of the application aws lambda is a serverless compute service that allows to run code without provisioning servers it is easily integrated with sqs in our case we are able to programmatically create both sqsq and the lambda and then configure sqsq as lambda trigger so that once queue receives a message lambda automatically spins up and performs the rehydration and re-ingestion tasks benefit of using lambda function is that it takes on rehydration and ingestion functions therefore making our back-end server more lightweight the downside of using lambda is a deploying lambda programmatically still requires quite a bit of work once we decided that using sqsq combined with a single lambda works best for our use case next issue we had to consider was how to ensure that the queue is not blocked by any problematic messages the first approach we came up with was simply to add more error handling and retry logic to the server code but upon further investigation as to what that logic should be we realized that it might end up being very complex the second approach we considered was using a dead ladder queue at that letter q is a queue that is created specifically to hold messages that can't be processed it allows us to put aside messages that failed so that they don't block the queue also having a deadline queue makes troubleshooting for failed messages much easier once the main queue in our case rehydration queue receives a message it attempts to process the message a specified number of times if the message processing continues to fail and message retention is set to a relatively long period of time or volume of messages is large there is a chance of failed messages blocking the main queue having a dead letter queue ensures that the main queue is not blocked and recovers quickly while developing a royal we faced the challenge of notifying the client once the rehydration job was completed in order to achieve that we needed to somehow access the result of each post request sent to logstash combine the results of all requests and pass it to the aurora application the first solution we considered involved setting up a webhook a webhook is an api endpoint that would accept data payload from a different service which in turn would trigger some other event our backend would expose an endpoint webhook that would accept post request each post request would contain a status of a single re-ingestion job in this case we would also need the user to provide a host or an ip address of the machine where a raw application is deployed and then we would need to pass that information to the lambda so that it knows where to post the status this approach seemed like a viable solution a downside of this approach is that notification layer would be tightly coupled with the rest of the logic we decided to continue researching other options the next approach we considered ended up being the solution we chose to implement we decided to use an sqsq once lambda received response from logstash it would put a message containing the result of the request on the queue the backend server would pick up and then process the message once all messages are processed it would notify the client one important positive of this approach is that if we were to build a more robust notification system in the future we already have a way to access the status of each reingested log now i'm going to turn it over to don who is going to talk about future work thanks olga some of the areas we would like to focus on in the future include number one being improved visibility into the jobs that arroyo dispatches right now the arroyo ui will inform the user when a job has been dispatched and when it has been completed but we would like to provide some additional insight into the overall process another area we would like to focus on in the future is more robust searching capabilities while we currently support searching logs that are formatted in json we would like to expand that ability to perform full text log searching we would also like users to be able to perform more robust query searches by expanding the number of log attributes each user can specify and implementing some additional logic to perform more complex queries finally we would like to expand our integration capabilities and be able to fetch logs from multiple cloud storage vendors the goal would be to increase flexibility and be able to expand the benefits of log rehydration to a wider range of users so that concludes the informational section of our presentation next we're going to hold a brief q a so if anybody has any questions they would like to ask the team now is the time to do so and what i'm going to do is open up the q a and the chat sections so we have one from elaine that says how did we come up with the idea to build arroyo i'll take this one on okay um so while we were researching we found a blog post written by netflix about creating their own observability tools in this post netflix writes about choosing the stream logs as events and only persisting selected logs in memory in order to deal with the large amount of log volume netflix generated we became curious as to how else this problem of log volume could be solved commercially some observability platforms offered to store stale logs on a cloud storage solution for some time for an extra fee but only a select number of these solutions like datadog logzio etc offered the ability to search and restore these logs and we did not find any open source solutions with this capability so we knew that it was a good use case okay well if there are no more questions on behalf of team arroyo i'd like to thank everybody for stopping by and letting us show you all the work we've done over the past few months you 