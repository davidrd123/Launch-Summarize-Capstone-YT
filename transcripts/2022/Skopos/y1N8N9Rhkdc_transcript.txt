foreign Welcome to our presentation on scopos a free and open source API monitoring tool we are a team of four I'm Hans Ellie and I'm here with my teammates Catherine Ebel gagan sabkota and Michaela Dotson so first what is scopost in short scopos is an open source API monitoring tool designed for multi-step testing API workflows and running multiple tests in parallel before we get too deep into scopost though we're going to start out with an introduction to API monitoring then we'll take a look at some different approaches to API monitoring before moving on to some existing API Monitoring Solutions that we considered before building scopost then we'll introduce scalposts and take a quick look at how to use it then we'll explore how we built it starting with how we built our core components then diving into our core infrastructure finally we'll wrap up by sharing some future work we'd like to do in scopos and at the end we'll take questions okay so we start with the first section introduction to API monitoring in this section we'll explore two things the consequences of API failures and core functionalities of every API monitoring tool but first what is an API apis are at the heart of every piece of modern software and use and they act as the connective tissue that binds together application services consider our weather application when a user clicks a button to check the weather in Seattle the weather app sends a get request to the weather API next the weather API sends back a response with the requested data in this case that's the current weather in Seattle the weather app then uses that data to display the current weather to the user it is common for an application to rely on multiple API endpoints though if apis are Central to Modern applications then what happens when they fail when one API fails other dependent apis May Fail too these cascading failures can have unfortunate consequences let's take a look at one example where an API failure had a noticeable impact in 2019 ubereats customers were able to order huge amounts of free food because one of ubereats payment services paytm changed one of their API endpoints before the change the endpoint Returns the same response every time a customer placed an order with insufficient funds to pay for the order in other words the endpoint was item potent ubereats could expect the same result no matter how many times they made the same API call after the change the endpoint was no longer item potent after the first attempt to place an order with insufficient funds on subsequent attempts the paytm endpoint returned a new unexpected error message now because of the way ubereats had integrated their application with paytm this unexpected error message allowed these orders to go through even though payment had not actually been processed successfully before ubereats realized the error many customers had already ordered thousands of dollars worth of food for free clearly it's important to identify issues like this as early as possible if and when they occur in production fortunately API failures can be traced and when apis fail their failures manifest really in just a few different ways maybe like we just saw with ubereats the date of payload sent back in the api's response is something unexpected or maybe the failure shows up as an unacceptable response time say you need a response back in under 400 milliseconds but is taking three times that or maybe the HTTP status code is wrong if you need a status code of 200 okay and you're getting back 500 internal server error something's probably wrong so now that we know what to look for to detect API failures what do we do well to catch failures as quickly as possible companies invest in tools that track the performance of API endpoints and look for these signs of failure these tools are called appropriately API monitoring tools API monitoring tools detect API failures by making requests to endpoints as specified intervals and checking the validity of their response data what do we Define as a failure well whenever there's a mismatch between expected response data and the actual response data the endpoint is considered to have failed and by response data here remember we just discussed three aspects in particular if API responses that we can track data payload response time and status code but how do API monitoring tools do this we think it's helpful to think of how API monitoring tools work in the context of a set of core functionalities that are shared by pretty much all API monitoring tools these core functionalities can be broken down at a broad level as a series of distinct steps definition execution scheduling and notification an API monitoring tool allows users to Define tests execute those tests on a schedule and notify various targets when the tests fail let's explore each of these steps typically looks like for defining tests a user provides the information necessary for communicating with the API endpoint this might include the HTTP method endpoint URL headers and request body a user then defines assertions to compare expected and actual responses so an assertion is how a user actually specifies what status code they expect what response time is reasonable or what they expect in the response body definition functionality is generally offered either through a graphical user interface or a command line tool so now that the tests are defined in API monitoring tool has to execute the tests this means that the tool must be able to send requests to the specified API endpoint receive response and check assertions associated with that test against the received response the tool must also be able to schedule tests to run and scheduling here doesn't just mean time-based scheduling it can also refer to arranging for tests to execute from different geographical locations or setting a deployment trigger to execute tests as part of a CI CD pipeline for instance tests scheduled based on time can be set to execute every minute every 15 minutes every hour depending on the use case and how quickly API failures should be responded to finally when apis return unexpected responses the monitoring tool must be able to alert interested parties about the failures this could include internal notifications that alert self-healing processes or external notifications to users of the monitoring tool such as via SMS or Slack now while API monitoring tools have these core commonalities they are not generally one size fits-all products and these functionalities definition execution scheduling and notification can be fine-tuned to suit different use cases knowing which features to Target with an API monitoring tool depends on which approach to API monitoring makes the most sense for that specific use case with that said Kathy is now going to discuss some of these different approaches to API monitoring thanks Hans well not an exhaustive list we identified four approaches to API monitoring let's take a look look at each each of these approaches first let's take a look at multi-step tests these tests are designed to simulate complex workflows that consume multiple apis consider a user workflow like the one on the slide adding an item to a cart making a payment scheduling a delivery and updating the database when all these steps work is expected the test passes however when one or more of these steps do not work as expected for example the payment step fails then the test would fail another approach is parallel tests although it's necessary to simulate multi-step workflows by running tests sequentially this has a disadvantage in that it takes extra time for example executing three tests that take 200 milliseconds each would take 600 milliseconds meanwhile the same three tests sent in parallel would only take 200 milliseconds executing tests in parallel can save time when the requests don't depend on each other furthermore making parallel requests can also be used for load testing because you can configure multiple tests to make a request to the same API endpoint in parallel to see how the endpoint performs next we have geolocation based tests applications with global users may want to test the performance of their apis from various geographical locations this approach provides insight into how an API is performing for varied users that are distributed across the world finally we have CI CD integrated tests some API monitoring tools allow also allow integration with CI CD Pipelines this is often done by linking tests to multiple environments and setting deployment triggers that run these tests in response to new builds this practice helps to ensure that API Integrations don't break during frequent code base changes in summary the approach chosen to monitor apis will depend upon the use case whereas one company May prioritize geolocation based tests another might want to prioritize integration with their cicd pipeline next let's let's talk about the specific use case for our requirements and the use cases existing tools may be used for we wanted an API monitoring tool that would allow us to monitor multi-step API workflows and execute groups of tests in parallel the tools we looked at each will filled a subset of our requirements let's start off with Newman Newman is a CLI tool for Postman and allows setting up multi-step tests that can run in parallel however it has a steep learning curve Newman requires a developer to write their own test scripts also collections have to be created in Postman and then exported to be used in Newman next we have checkley checkley has an easy to use GUI however implementing multi-step functionality requires a developer to write their own setup and tear down scripts on each test it also did not have the ease of use we were hoping to provide finally we have tetsfully teslie came the closest to satisfying our requirements however it locked key parts of its functionality behind High monthly subscription costs we wanted an open source solution ultimately we did not find a great fit for our use case so we set out to build our own introducing scopos does not include functionality for cicd integration or geolocation based tests scobos would also not be a good fit for users who want to have flexible scripting scopos however is open source has multi-step and parallel test functionality and offers a user-friendly GUI that meets the needs of our use case let's continue by doing a quick demo of how to use scope boasts to Define multi-step tests a user can click the add collection button to create a new collection from there they can add a test to the collection by providing details on where to send the request and adding assertions about the properties of the response for multi-step test the user can add additional tests which can refer to a previous test in the collection by its step number after all the tests are defined the collection is ready for execution to execute the test once our user can click the Run collection button immediately to receive the results to execute the tests on schedule a user would create a monitor and specify how often the tests are to be executed and how they would like to be alerted when one of the tests fail users can choose to be notified by email slack or pagerduty if a test fails scopo immediately sends a notification of the failure which includes information regarding the tests that failed you've now seen how to define execute schedule and notify using scopost in the next section Michaela will discuss how we implemented the core functionality of our application thank you Kathy all right now let's take a look at building the core components of our application specifically how we built the definition and the execution components we knew we needed to build the definition functionality and that we wanted to provide a way to set up multi-step tests an important part of this is referencing values from previous tests this was challenging because those values are not accessible until after the previous test has completed for example if the redfish above wants to know the color of the orange fish the redfish will only know its color after the orange fish comes by we also noticed we are working with a large amount of data storing and retrieving that data quickly became a complex challenge let's look into the first Challenge and how we reference values Downstream we decided to group tests that reference previous values from other tests together into what is called a collection a collection is a group of tests this way when the tests within a collection are run sequentially values needed Downstream will be available in this example a value from the response of test 1 can be accessed by test two or three but not by testing another collection such as tests four five and six now how does a test know that the user wants to interpolate a previous value well we decided to create a reference flag to identify where to interpolate values using an at symbol followed by two and closing curly braces here is an example of accessing data from a previous test in the same collection while using scopos step one retrieved an API key that is used here in Step 2. you may also notice the reference flag where the at curly brace curly brace is used for interpolation this is how we solve the challenge of defining where a user wants to access previous values to then be interpolated later on now let's take a look at storing and querying our data during development we started by working with rust endpoints however those became limiting sometimes we were not getting all of the data we needed from one endpoint or under fetching and other times we are fetching more data than we needed or over fetching first we tried to adjust our queries to Target the specific data we needed however query started growing in complexity and the custom endpoints we used started to move away from proper rest implementation so we decided to use graphql using graphql allowed us to retrieve the precise data we needed no more no less in particular we added a policy server to the back end of our application any components that would need to communicate with the database could then use Apollo client and the back end running Apollo server would act as the single gateway to the database this was a great start however while implementing our data model we ended up making frequent updates to our schema to mitigate this issue we decided to use Prisma Prisma is an object relational mapper or orm it allowed us to not only interact with the database as if it were an object but also update and migrate our schema with ease here's our final data stack which consists of Apollo client communicating from the front end a back end running Apollo server integrated with graphql and Prisma that then communicates with our postgres database next let's take a look at our execution functionality One Core problem we faced when implementing the execution functionality where we are actually making API calls came with handling the complexity of the code we decided to group this functionality into what we call the collection Runner here are the steps for making that possible initially a post request is sent to an Express endpoint on the collection Runner next data for the tests including requests and assertions that belong to the collection are fetched from the database from here the process of actually sending requests to API endpoints is started for this the requests are first processed by interpolating values the user has defined in the test this is where our at curly brace curly brace reference flag for accessing values from previous tests comes in then the first request is ready to be sent to the specified API endpoint once a response is received from the API assertions are checked for the test if they fail interested parties are notified however if the assertions pass the process is repeated with the next request after this process has been repeated for each request in the collection the execution phase is complete as you can see this requires complex logic and keeping track of variables that change at different parts of this process because of this we started looking at implementing a state machine to run collections so what is a state machine a state machine helps declaratively model application logic it defines States and application can exist in and the actions that take the state machine from one state to another State machines also keep track of context as you can see on the light blue clipboards the values saved to the state machines context can be updated by different states we decided to use this and began at moving our complex execution logic to x-state which is a library for creating and using State machines the process begins in the same way we looked at previously with our collection Runners Express server receiving a post request with collection ID as a parameter from there the express endpoint invokes our main collection Runner machine as you can see the collection Runner machine has six distinct States at this point the machine moves from the idle state to the querying State and Saves The Collection ID to its context in the querying State data for the collection is set from the database and again saved to the machine's context The Collection Runner machine then moves into the initializing state where collection run ID is generated that will be used to later save the results to the database from here each test that belongs to the collection uses a child State machine to handle the logic of processing the request sending the request and making assertions on the response here's a little more detail on how that works when the collection Runner machine reaches the running State the request processor machine is invoked here we interpolate values for tests that reference previous requests now this state machine has its own States in context and when the request processor machine reaches the complete State the context is sent back in an event to the parent machine here you can see that the request in the collection Runner machine's context have been updated based on the data sent back from the request processor machine then the request Runner machine is invoked which makes a request to the specified API endpoint waits for the response and passes the data to the parent machine to be saved as responses once those responses are saved the next child station is invoked at this point the collection vendor machine has all the information necessary for checking assertions against the saved responses the assertion vendor machine is invoked which uses the api's response to evaluate assertions defined for the current test assertion results are then saved to the collection runner's context now let's take a moment and look at how we handled failures you may have noticed that each state machine includes a failed state if an assertion does not pass or an error occurs at any point during a state machine's invocation the parent collection Runner machine is notified and enters the failed State similarly if the collection Runner machine does not receive an event from a child State machine The Collection Runner machine will enter the failed state at this point the remaining tests in the collection will not run and an appropriate error message is sent to any interested parties now this process repeats until all tests in a collection have been completed at that point the collection Runner machine moves to the complete State and the collection rent is done in summary The Collection Runner receives a post request with a collection ID and invokes multiple State machines that go through a sequence of states to complete the tests in a collection this process allows for the implementation of multi-step tests while organizing the complex logic that comes with that at this point we had implemented a large part of our API monitors definition and execution components with this scopos is able to Define and execute multi-step tests but is unable to perform parallel testing scheduling or notifying we decided to solve most of these problems at an infrastructure level God gonna walk us through our Cloud infrastructure thank you Nicola we are now going to discuss how we build the cloud infrastructure in particular we will discuss how we implemented parallel testing scheduling and notification in the next slide I'm going to show a diagram of the full architecture don't be alarmed it will all make sense by the end we're just taking a brief glance at what the final product will look like here it is it might be too complex two parts at the moment but we will build this piece by piece let's break this down into simple components as you might remember every API monitoring tool needs four key tasks Define schedule execute and notify we're going to build a full architecture starting with this simple diagram now let's take a closer look at how we move the definition component to the cloud we stored the front-end build file in an S3 bucket we hosted the back end on a Docker container on an ec2 instance fronted by an elastic load balancer and we used RDS for our database before moving to the cloud The Collection Runner was already able to execute multi-step test we also wanted to execute the task in parallel let's do a quick recap of the problem we set out to solve suppose we want to monitor 50 collections every minute but executing all the tests in one collection takes around 10 seconds that means executing all collections sequentially would take 500 seconds if I'm getting my math right that should be just about eight minutes we set out to solve this problem by executing collection of tests in parallel so we needed a way to run multiple instances of the collection runner in peril we could accomplish this in two ways we could take the single tenant solution or the multi-tenant solution let's go over what these terms mean as you can see on the left side of the diagram each Tenon gets a house as the diagram illustrates single tendency means running one server on one machine as you can see on the right side of the diagram three tenants said a house as the diagram illustrates multi-tenancy means running multiple servers as distinct processes on the same machine for a single tenant solution we would provision multiple virtual machines each hosting its own collection Runner process then we would add a load balancer to direct traffic to available collection runners this approach would enable parallel testing for our needs however this solution has some downsides first provisioning and managing each virtual machine as complexity second we will be wasting resources The Collection Runner is a relatively lightweight process it would not utilize the full resources of even the cheapest Amazon ec2 instance in other words single tenant solution would be quite wasteful we still have the other option though multi-tenancy containers are the perfect Solutions here to implement this approach we could run multiple instances of the collection runner on Docker containers and multiple Docker containers could reside in a single machine then to enable parallel testing we only had to use a load balancer to direct traffic to different containers since the single tenant approach is wasteful we decided on a multi-tenant solution we could implement the multi-tenant approach in two ways we could use AWS fargate which is a serverless container service managed by AWS or we could use elastic container service with ec2 let's consider fargate fargate abstracts away the complexity of managing virtual machines AWS dynamically scales the forget instance depending on the demand but we found two main drawbacks to using fargate first Target's Auto scaling can take up to 15 seconds to spin up new containers which would increase compute time second target was a pay as you go service since the collection Runner makes multiple HTTP requests and weights in between to receive a response using fargate means paying for all the time waiting for a response for our needs a better solution was to run containers on ec2 instances that way we would be paying for the machine rather than the compute time which would be cheaper each ec2 instance could house multiple containers although we would still need to provision and manage virtual machines we could use those resources more efficiently in this way we solved the problem of parallel testing overall the solution is as follows we hosted multiple instances of collection runners in Docker containers housed in ec2 instances and used a load balancer to direct traffic to different containers at this point we had both the definition and the execution component in the cloud now that the collection Runner was set up to execute multi-step tests in parallel we moved to implementing a scheduling for which we considered two options we looked at cron jobs and AWS eventbrids the first option was to run cronzobs from a node.js process this approach would not require any additional infrastructure because the console logic could be co-located with the back end this approach however would have significant drawback coupling functionality like this introduces a significant vulnerability if the node storing the crown jobs were to go down schedule tasks stored on that node would be lost since crons are introduced vulnerability we then considered AWS eventbrids eventbrids is a serverless event bus that can receive and Route events based on user-defined rules and these rules can include chronic expressions we could use cron Expressions to trigger events on schedule although using eventbrids as a component to our architecture it decouples a scheduling functionality from existing processes and prevents a loss of data from potential failures because our aim with scopos was to provide a reliable API monitoring tool we decided to use eventbrids at this point we started implementing the scheduling functionality with eventbrids but we found that the eventbridge communicates over https The Collection Runner however communicates over http this meant either we had to acquire and manage SSL certificate for the collection Runner or use an intermediary between the eventbrids and the collection Runner we opted to use a Lambda function as an intermediary this way the event Breeze communicates with the Lambda which then sends a request to the collection Runner at this point scopus was able to Define test and execute them on schedule for notification we chose to use AWS simple notification service or SNS SNS has an effective setup for sending notifications to various services such as Pacer Duty and email when setting up a schedule back-end would create a topic and subscribers for the schedule pays a duty and email links would be subscribers to a topic corresponding to the schedule when some of the tests fail the execution component would publish message to the SNS topic and the subscribers would receive the message we also wanted the scopos to send notifications through slack but sending notifications to slack requires a specific data payload that SNS could not accommodate so we chose to send notifications to slack directly from the collection Runner when the execution component publicizes a message to SNS it would also send a notification to Slack at this point scopus was able to create collection of tests execute those collections on a schedule and notify the interested parties when some of the tests fail let us now do a quick overview of the AWS infrastructure let us focus on the box that the arrow points to the front-end build on react is hosted in an S3 bucket it communicates with the back end with a polo server hosted on a Docker container and managed by ECS the backend communicates with the postgres database hosted on RDS The Collection Runner which you can see at the bottom of the diagram is hosted on Docker containers running on ec2 instances and man is by ECS front end communicates with the collection Runner to execute tests on demand collection Runner communicates with the back end to fetch the collection data next is the scheduling hosted on AWS eventbrids backend server communicates with the eventbridge to create rules for each schedule the eventbridge triggers the Lambda that communicates with the collection Runner in this way collection of tests are executed on a schedule and finally notification is implemented with SNS when creating a schedule the backend also communicates with the SNS to create topic and subscribers when some of the tests fail during execution collection Runner publishes a message on the topic and the users are notified of the failure this completes the architecture of scopos next we will briefly discuss how we would like to develop scopos in the future we'd like to do three main things first we'd like to implement a Sandbox environment to run API requests in isolation for troubleshooting second we'd like to Generate random data for different inputs to test edge cases and finally we'd like to implement smart notifications to prevent repeat notifications for the same issue with that we end our presentation thank you for attending the session we would like to now open the discussion for questions all right so the first question is Hospital choose SNS some function yes so um the question is asking if it's possible to use SNS to send slack notifications by modifying it with a function so another approach that we considered that's a common workaround is to use the Lambda function as sort of an intermediary between the between SNS and slack so the Lambda function would be a subscriber to the SNS topic and then you would send a post request to slack that way we decided to not go with this approach just because at our scale of our project we thought that it would be more efficient to send a request directly from the collection Runner but that's definitely something that would be helpful to decouple if you were to scale our project in the future let's see any other questions okay so there's a question that came in were there any other things you implemented in scopost that you would like to talk about um I think Let's see we implemented scopos using the AWS cdk and that brought about a lot of challenges like um circular dependencies that we ran into and things like that that were really interesting challenges to kind of tackle as we were moving to the cloud um does anybody else in the group have um different pieces they'd like to speak about I think the the main other challenge like you said was working with the cdk and initially getting everything up and running on the cloud uh lots of time spent on that what was your favorite part of the project um I think the state machine was my favorite part of the project um I think that was really interesting um to learn about and try and implement but let's see does anybody else have their favorite parts for the project machine was my favorite part I just like the whole the whole project like to combine just like just like the interaction of all the different components together and just brought about about a bunch of like fun like debugging problems I don't know if some people can say that fun but I do but um just like all the pieces working together and trying to figure out how to get them all talk to each other and whatnot which is really fun for me yeah that was mostly like the overall project was pretty amazing in terms of the journey learning so many things on the way especially working with a state machine and trying to puzzle through the logic of it and although it was a nightmare kind of getting it working working with AWS cdk was also pretty much pretty fun because it was kind of challenging and yet rewarding when it worked yeah let's see how did you decide on what you would make together so we were really interested in microservices and kind of how the different pieces of those architectures fit together um especially whenever they're so complex just making sure that everything's running smoothly and we're kind of looking at observability and then we were pointed towards testfully which is one of the API monitoring tools that we looked at and from there started diving a little bit deeper on API monitoring tools you know what they're used for and everything like that so that's kind of how we got started so through the course of working with API monitoring and parallelization were there any takeaways regarding test optimization that you'll apply to your future work um this was something that was really interesting we um worked on trying to have tests run in parallel for a little while and considering different options um I think that it was interesting because we needed to decide how to group test together because we really like the idea of multi-step tests you know simulating those workflows that might require um request to multiple API endpoints but then we also didn't want everything to be running you know one at a time one after the other so it was interesting to think about how to group things that weren't dependent on one another and um which aspects we did want to be dependent on one another with the multi-steps so yeah I think being able to visualize those two differences was really interesting that hopefully we'll be able to take into into the future yeah and I think it was uh a great experience to get to work with um the different AWS container services I feel like that will be applicable in the future to different sorts of projects you know ECS and fargate and those sorts of services and really diving in there and figuring out what would be the best solution for running those tests in parallel yeah to add to the optimization we could we were considering adding casting to the collection Runner so that if a collection is scheduled to be monitored every minute it would be kind of Wishful to fetch data every minute so we're thinking of adding casting at that level but the problem was with cash invalidation if the user modifies the collection in between after scheduling and then goes on to modify the collection the casting would kind of create problem for the updated the updated data we wanted so we kind of postpone that discussion for later we're like okay once we figure this out we'll implement this later on but that's another one that we could consider in the future yeah all right next question was what was it like working on a team hopefully an answer from each of you uh personally it was I had a great experience working on this team I'm thankful for the hard work put in by everybody up here with me um so yeah I had a I had a great experience working on this team anybody else want to yeah it was great it was great I mean the best thing was like people kind of jumping in to help each other when things kind of Life happened you know like I had I had to take one day off to celebrate one of my in the festival and then kind of my teammates pick up the slack and that was amazing having your teammates kind of have your back that was nice yeah I was it was a fun experience it was definitely new it's very different from you know working individually uh we started out uh you know pretty pretty rough on planning and coordinating the work but as we went we we got pretty comfortable with that and by the end it felt like we you know were kind of a finely tuned machine in that regard so it was an interesting growth experience yeah I also really enjoyed working on this team um everyone was very um just very supportive and um very helpful we all had it seemed like we had our different strengths that um just really made the team work well together so I thought we did I thought we all worked really well together and it was a good experience yeah um let's see did you consider any other Cloud providers versus AWS um we didn't I think it would probably be a good idea to try and look into the different pros and cons um of other Cloud providers as well but we we kind of focused on AWS we briefly looked for like 10 minutes at Google Cloud but yeah that'll own AWS pretty quickly what was the time breakdown between working on the front end and back end um I think so the back end had a lot of graphql that we were working on um because that was kind of our gateway to the database and so we definitely spent quite a bit of time on um working on graphql queries and everything that communicate to the front end um I know that Kathy spent a lot of time on the front end and adding some additional features to make it more user-friendly and then we also had we spent a pretty good amount of time on the collection learner as well so I think between those three was that definitely a couple of weeks um it was probably pretty even though I don't know does anybody else have remember more of an accurate breakdown of those I feel like once we had the back end kind of its base functionality down all we had to do going forward as we worked on different parts was kind of tweak different queries or tweak parts of it um I know the front end we had as we um we got further in the project we had a number of irritating bugs that we had to keep tracking down in the front end I think overall we probably spent more time working in the front end and the back end was kind of once we had it set up and mostly ready to go it was able to chug along uh so probably more on the front end yeah yeah and I think we had some unexpected database challenges too just coming up with our you know energy relation diagram and everything um getting that set so then we had to go back and you know once we changed the database um we'd have to you know do the changes to the front end back end all of that to to make them all work together so yeah just to add for like the front and the front end part um I know we use graphql in the front end we used Apollo client and they um it allows you to do like caching and that saves you from having to make extra requests to the database so you know just trying to get that all coordinated so everything was caching appropriately and you know that was a little bit of a that was a little bit of a challenge to learn how to do and make it work right advice you would give to Future Capstone students um I think just keep going you know trust the process um it's really interesting working on a project and learning so much new information so quickly and um I feel like it was a lot easier to piece together different components um whenever you're figuring out how to actually use them rather than just you know reading up on reading up on them or looking at docs so it's really cool to actually apply those so yeah I think just just keep on going and be open to learning a lot um it's pretty cool anybody else have advice yeah I'd say I'd say you know trust your team uh uh it's it's a big shift working with a group of people on a project like this and you know you gotta you gotta get to know people and learn to work together in a way that you kind of don't when you're going through the Core Curriculum so the sooner you're able to kind of put your ego aside and like work together efficiently I think that'll make all the difference how did you distribute the work um I think something that might have been unique about our team is that we all worked on a lot of different aspects together so um we did have people work on different pieces you know whether it was x-state front end database notification so we kind of divided those key components um but I think something that worked really well for our group and hopefully is going to be beneficial with just our overall understanding of the project is if there was a bug or something that we needed help figuring out then everyone was really willing to jump in and you know tackle a piece of the project they may not have been the main person working on but to still try and figure it out and help each other so um I feel like that's how it worked from my perspective did anybody else have a different experience or want to speak on their portion that sounds right yeah I'd say that sounds about right yeah initially initially the work we kind of broke up the work um based on those initial components of like front-end back-end database collection Runner and we each worked on kind of a different one of those until it was mostly up and running and then going forward from there when we ran into a bug or um you know someone needed some help with something while like one person might have had kind of a deeper understanding of that component and kind of like felt more ownership over it we invited like other team members to come over and help us figure out what's going on not just because we needed help but because we thought it'd be like they said you know good for everyone to kind of understand every part of the process all right thank you everyone for that you can wrap it up 