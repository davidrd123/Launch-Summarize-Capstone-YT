Welcome to our presentation on scopos, a free and open source API monitoring tool. We are a team of four: Hans, Ellie, Catherine, Gagan, and Michaela. Today, we'll be introducing scopos, discussing API monitoring and its consequences, exploring different approaches to API monitoring, looking at existing API monitoring solutions, demonstrating how to use scopost, explaining how we built it, and discussing future work. Let's get started.

APIs are the connective tissue that binds together modern software applications. They allow different services to communicate and exchange data. When an API fails, it can have cascading effects on other dependent APIs, leading to unfortunate consequences. For example, in 2019, an API failure with ubereats's payment service led to customers being able to order large amounts of free food.

To detect API failures, companies invest in API monitoring tools. These tools track the performance of API endpoints and look for signs of failure, such as unexpected response data, unacceptable response times, or incorrect HTTP status codes. API monitoring tools have core functionalities, including definition, execution, scheduling, and notification.

In the definition step, users provide information about the API endpoint, such as the HTTP method, URL, headers, and request body. They also define assertions to compare expected and actual responses. API monitoring tools execute tests by sending requests to the API endpoint, receiving responses, and checking assertions. Tests can be scheduled to run at specified intervals or triggered by CI/CD pipelines. When an API fails, the monitoring tool alerts interested parties via notifications.

There are different approaches to API monitoring, including multi-step tests, parallel tests, geolocation-based tests, and CI/CD integrated tests. Multi-step tests simulate complex workflows that involve multiple API calls. Parallel tests execute multiple tests simultaneously, saving time and enabling load testing. Geolocation-based tests assess API performance from various locations. CI/CD integrated tests integrate with CI/CD pipelines to ensure API integrations don't break during code changes.

For our use case, we wanted an API monitoring tool that supports multi-step tests and parallel test execution. We evaluated existing tools like Newman, Checkley, and Tetsfully, but none fully met our requirements. So we decided to build our own tool called scopos. Scopos is an open-source tool that provides a user-friendly GUI, multi-step test functionality, and parallel test execution.

Now, let's do a quick demo of how to use scopost. To define multi-step tests, users can create collections and add tests to them. Each test includes details about the API endpoint and assertions for the response. Tests can refer to values from previous tests in the same collection. Once the tests are defined, the collection can be executed immediately or scheduled to run periodically. Users can choose to be notified by email, Slack, or PagerDuty when a test fails.

Now, let's talk about how we implemented the core functionality of scopos. We faced challenges in referencing values from previous tests and storing/retrieving a large amount of data. To tackle the first challenge, we grouped tests that referenced previous values into collections. In a collection, values from previous tests are available for interpolation in subsequent tests using a reference flag. For data storage and retrieval, we initially used REST endpoints but encountered limitations. So we switched to using GraphQL, which allowed us to retrieve precise data and avoid over-fetching or under-fetching. We used Prisma, an ORM, to interact with the database and handle schema updates efficiently.

In conclusion, scopos is a powerful API monitoring tool that supports multi-step tests and parallel test execution. It provides a user-friendly GUI and integrates with notifications. We built scopos to address our specific use case, as existing tools did not fully meet our requirements. With scopos, users can effectively monitor the performance of their APIs and detect failures early. In the future, we plan to enhance scopos by adding CI/CD integration and geolocation-based testing capabilities. Thank you for watching, and we are now open to questions. The project involved building the definition functionality for a coding Capstone project. One of the key challenges was implementing multi-step tests that required referencing values from previous tests. To address this challenge, the team decided to group tests that referenced previous values into collections. Collections are groups of tests where values needed downstream are made available. For example, a value from the response of test 1 can be accessed by test 2 or 3 in the same collection. However, values cannot be accessed by tests in different collections. To identify where values should be interpolated, the team created a reference flag using an at symbol followed by curly braces. This allowed users to easily access previous values for interpolation.

Another challenge the team faced was working with a large amount of data. Storing and retrieving data quickly became complex. To overcome this challenge, the team initially worked with REST endpoints but found them limiting. They were either fetching insufficient data or over-fetching. To retrieve precise data, the team decided to use GraphQL. By using GraphQL, they were able to target specific data without unnecessary overhead. They added a policy server to the backend and used Apollo client to communicate with the database. This architecture allowed Apollo server to act as a single gateway to the database.

To handle the execution functionality, the team created the "Collection Runner." This component was responsible for making API calls and executing tests. The Collection Runner was divided into several states to handle the complexity of the process. Initially, a post request was sent to an Express endpoint on the Collection Runner. Data for the tests, including requests and assertions, were fetched from the database. The requests were then processed, and values were interpolated using the reference flag. The first request was then sent to the specified API endpoint, and assertions were checked. If the assertions passed, the process repeated for the next request. If the assertions failed, interested parties were notified. This process continued until all the requests in the collection were completed.

To manage the complexities of the execution process, the team implemented a state machine using the x-state library. The Collection Runner machine had six states, including idle, querying, and initializing. The requests were processed by another state machine, the request processor machine, which handled request interpolation. After interpolation, the requests were sent to the API endpoints using the request runner machine. Responses were then saved, and assertions were checked using the assertion vendor machine. If an assertion failed or an error occurred, the Collection Runner machine entered the failed state. This process continued until all the tests in a collection were completed.

With the definition and execution components implemented, the team turned their attention to the cloud infrastructure. They wanted to implement parallel testing scheduling and notification capabilities. To move the definition component to the cloud, the team stored the front-end build file in an S3 bucket and hosted the back-end on a Docker container on an EC2 instance. An elastic load balancer was used to distribute the traffic. They used RDS for the database. To achieve parallel testing, the team considered a single-tenant solution and a multi-tenant solution. They chose the multi-tenant solution, provisioning multiple EC2 instances, each housing multiple Docker containers. A load balancer directed traffic to different containers, enabling parallel testing and optimal resource utilization.

To implement scheduling, the team examined cron jobs and AWS EventBridge. They initially considered running cron jobs from a Node.js process within the back-end. However, this approach introduced vulnerabilities and potential data loss. Instead, they opted to use AWS EventBridge as a separate component. EventBridge allowed them to schedule events based on cron expressions, decoupling scheduling functionality from existing processes and ensuring data integrity. To bridge the communication gap between EventBridge and the Collection Runner, a Lambda function was used as an intermediary.

With the cloud infrastructure in place, they were able to define test collections, execute tests in parallel, schedule tests using EventBridge, and orchestrate communication between components. This infrastructure formed the foundation for a reliable API monitoring tool. In this video, we will discuss the different ways in which we can utilize AWS Fargate, a serverless container service managed by AWS, and Elastic Container Service (ECS) with EC2. Specifically, we will focus on Fargate. Fargate is designed to simplify the management of virtual machines by abstracting away the complexity. It dynamically scales Fargate instances based on demand. However, we encountered two main drawbacks with Fargate. The first drawback is that auto scaling can take up to 15 seconds to spin up new containers, which increases compute time. The second drawback is that Fargate is a pay-as-you-go service, which means we would have to pay for the time waiting for responses during our testing.

To overcome these challenges, we decided to run containers on EC2 instances instead. Using EC2 instances allows us to pay for the machine rather than compute time, making it a more cost-effective solution for our needs. Each EC2 instance can house multiple containers, allowing us to utilize resources more efficiently. This approach addresses the problem of parallel testing.

Our solution is as follows: we hosted multiple instances of collection runners in Docker containers on EC2 instances and used a load balancer to direct traffic to different containers. With this setup, we had both the definition and execution components in the cloud. By configuring the collection runner to execute multi-step tests in parallel, we were able to optimize our testing process.

Now let's discuss the implementation of scheduling. We considered two options: cron jobs and AWS EventBridge. The first option involved running cron jobs from a Node.js process. This approach would not require any additional infrastructure as the console logic could be co-located with the backend. However, it had a significant drawback. If the node storing the cron jobs were to go down, schedule tasks stored on that node would be lost. This introduced a major vulnerability.

To address this, we turned to AWS EventBridge. EventBridge is a serverless event bus that can receive and route events based on user-defined rules, including cron expressions. By using cron expressions, we could trigger events on a schedule. Using EventBridge as a component in our architecture decouples the scheduling functionality from existing processes and prevents data loss from potential failures. Given our goal to provide a reliable API monitoring tool, we decided to use EventBridge for scheduling.

To implement scheduling with EventBridge, we encountered a challenge. EventBridge communicates over HTTPS, while the Collection Runner communicates over HTTP. This presented a dilemma: either we needed to acquire and manage an SSL certificate for the Collection Runner or use an intermediary between EventBridge and the Collection Runner. We decided to use a Lambda function as an intermediary. This way, EventBridge communicates with the Lambda function, which then sends a request to the Collection Runner.

With this setup, Scopos was able to define tests and execute them on schedule. To handle notifications, we chose to use AWS Simple Notification Service (SNS). SNS provides an effective setup for sending notifications to various services, such as PagerDuty and email. When setting up a schedule, the backend creates a topic and subscribers for PagerDuty and email, with each subscriber corresponding to a specific schedule.

In the event of test failures, the execution component publishes a message to the SNS topic, and the subscribers receive the message. However, we also wanted to send notifications through Slack. Since SNS couldn't accommodate the necessary data payload for Slack notifications, we decided to send them directly from the Collection Runner. So, when the execution component publishes a message to SNS, it also sends a notification to Slack.

With these implementation decisions, Scopos was able to create collections of tests, execute them on a schedule, and notify interested parties of any test failures. 

Now, let's take a quick overview of the AWS infrastructure. For the purpose of this discussion, we will focus on the specific box indicated by the arrow. The front-end, built on React, is hosted in an S3 bucket. It communicates with the backend via a Pollo server hosted on a Docker container managed by ECS. The backend communicates with a Postgres database hosted on RDS. The Collection Runner, shown at the bottom of the diagram, is hosted on Docker containers running on EC2 instances and managed by ECS. The frontend communicates with the Collection Runner to execute tests on demand, and the Collection Runner communicates with the backend to fetch collection data. 

Moving on to scheduling, it is hosted on AWS EventBridge. The backend server communicates with EventBridge to create rules for each schedule. When triggered, EventBridge activates a Lambda function, which in turn communicates with the Collection Runner. This way, collections of tests are executed on a schedule. Finally, notification is implemented using SNS. When creating a schedule, the backend also communicates with SNS to create a topic and subscribers. In the event of test failures during execution, the Collection Runner publishes a message on the topic, and the users are notified of the failure. 

This completes the architecture of Scopos. Next, let's briefly discuss our plans for future development. There are three main areas we would like to focus on. First, we want to implement a sandbox environment to run API requests in isolation for troubleshooting purposes. Second, we aim to generate random data for different inputs to test edge cases. Lastly, we plan to implement smart notifications to prevent repeat notifications for the same issue.

Thank you for attending this session. We are now open to any questions you may have.

Q: How did you decide to use SNS for notifications?
A: We considered using a Lambda function as an intermediary between SNS and Slack, but based on our project's scale, we found it more efficient to send notifications directly from the Collection Runner. This way, we can send messages to SNS for other types of notifications while simultaneously sending messages to Slack.

Q: Were there any other notable features you implemented in Scopos?
A: Apart from the mentioned features, we built Scopos using the AWS CDK (Cloud Development Kit). While this presented challenges such as circular dependencies, it was an interesting learning experience. We also faced unexpected database challenges while designing the entity relation diagram.

Q: Did you learn anything about test optimization while working on parallelization and API monitoring?
A: Yes, optimizing tests was an interesting aspect of our project. We spent time deciding how to group tests together and wanted to simulate multi-step tests while still running them in parallel. Visualizing these differences helped us make informed decisions for future test optimization.

Q: What other potential optimizations could you consider for Scopos in the future?
A: One potential optimization we considered was adding caching to the Collection Runner so that if a collection is scheduled to be monitored frequently, we could fetch the data less frequently. However, we did not implement this due to the challenge of cache invalidation when a collection is modified after scheduling. This is a future consideration.

Q: How was it working on a team for this project?
A: Working as a team was a great experience for all of us. We appreciate the hard work and support from each team member. It was a learning process, and while there were initial challenges in planning and coordination, we eventually became a well-coordinated team. It was rewarding to see all the different components come together smoothly.

Q: Did you consider other cloud providers besides AWS?
A: We primarily focused on AWS for this project. However, it would certainly be beneficial to evaluate the pros and cons of other cloud providers in future projects.

Q: Can you provide a breakdown of the time spent on the front-end and back-end?
A: We spent considerable time on the back-end, working on GraphQL queries and configuring the database. The front-end also received attention, with additional features added to enhance user-friendliness. The collection runner was another significant component that required attention. Overall, we spent more time on the front-end, but the back-end work was crucial in setting up the foundation.

Q: How did you choose the technologies and services used in Scopos?
A: Our interest in microservices and their complex architecture led us to explore API monitoring tools. During our research, we discovered Testfully, which inspired us to dive deeper into API monitoring tools. This exploration ultimately led us to choose the specific technologies and services used in Scopos.

Q: Were there any lessons learned about AWS through this project?
A: Working with various AWS container services, such as ECS and Fargate, was a valuable learning experience. This knowledge can be applied to future projects that involve similar services. The overall journey of implementing Scopos and overcoming challenges helped us enhance our understanding of AWS cloud services. In this coding Capstone project, our team encountered some challenges initially when it came to planning and coordinating the work. However, as we progressed, we became more comfortable with our roles and responsibilities and started functioning like a well-oiled machine. It was a new and exciting experience for all of us to work in a team setting.

One aspect that stood out was the incredible support and assistance we received from our teammates. It was amazing to have them pick up the slack and have our backs whenever needed. Each team member brought their own unique strengths to the table, which greatly contributed to the overall success of our project. We worked seamlessly together, making it a highly positive and enjoyable experience.

Although we primarily focused on AWS for our cloud provider, it would have been beneficial to explore other options as well. We briefly considered Google Cloud but ultimately decided to stick with AWS due to its superior capabilities and familiarity.

When it comes to the time breakdown, a significant portion of our development efforts was devoted to the back end. We spent considerable time dealing with GraphQL queries, as it served as our gateway to the database. Additionally, Kathy put in a lot of effort to enhance the front-end and make it more user-friendly. We also dedicated a substantial amount of time to the collection learner feature. Overall, the time spent on these three components was spread over a couple of weeks, with a fairly even distribution among them.

Once we established the essential functionality of the back end, our focus shifted towards refining and tweaking specific aspects rather than starting from scratch. The front end, on the other hand, posed some challenges with the discovery of bugs that needed constant troubleshooting. Consequently, we spent more time working on the front-end aspect, while the back end provided a smoother workflow once it was initially set up.

One unexpected obstacle we encountered was related to the database. We had to create an entity relationship diagram and make sure it aligned with our requirements. Adjustments made to the database structure necessitated corresponding changes in both the front-end and back-end components, which added an extra layer of complexity to the project.

In terms of front-end development, we utilized GraphQL in conjunction with Apollo Client. Apollo Client's caching feature allowed us to reduce the number of database requests, streamlining our application's performance. However, configuring and ensuring the proper functioning of the caching feature presented its own set of challenges.

For future Capstone students, we recommend embracing the process and being open to learning new concepts. Working on a real project and applying theoretical knowledge in practice offers an invaluable learning experience. Hands-on experience fosters a deeper understanding of the underlying technologies compared to merely reading documentation or studying theory.

Additionally, trust and collaboration among team members are vital to success. Shifting from individual assignments to working as a team requires getting to know your colleagues and finding ways to effectively work together. Setting aside personal ego and focusing on efficient collaboration will make a significant difference in the outcome.

In terms of task distribution, our team had a unique approach. Although we divided certain key components such as the front end, back end, and database, every team member participated to some extent in each area. This approach allowed us to gain a broader understanding of the entire project and improved our overall knowledge.

Moreover, whenever we encountered bugs or faced difficulties, we came together as a team to solve the issues. Everyone was willing to step in and contribute, even if it wasn't their primary area of responsibility. This collaborative effort was beneficial not only in resolving immediate challenges but also in promoting a better understanding of the project as a whole.

To conclude our project discussion, we would like to express our gratitude for the opportunity to work together and learn from one another. The experience was rewarding, and we all grew personally and professionally. We are confident that the skills we developed throughout this Capstone project will serve us well in our future endeavors.