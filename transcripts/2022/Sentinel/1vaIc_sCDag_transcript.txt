all right let's get started um hello everyone my name is brendan and with me today is sam drew and michael and together we developed sentinel a platform as a service with canary deployments and we're excited to tell you all about it sentinel is an open source cloud agnostic platform as a service that can easily deploy multiple postgres database backed applications with built-in support for upgrading them via canary deployments so let's jump right into it and break down what that all means we'll start by defining what exactly is a platform as a service now there are many components that have to work together to deploy an application to the internet which we collectively refer to as infrastructure any infrastructure is going to be comprised of many things such as servers operating systems hard drives just to name a few of them now you have the option to physically manage these things but that would take up a lot of space that would require you to learn how all these pieces work together and maintain them you could use something like infrastructure as a service which abstracts away the infrastructure details into lines of code but even then that could still prove to be a daunting task to learn and get right many developers simply just want to deploy their applications and not be bogged down by all these complex details and this is exactly what a platform as a service addresses it takes an application and manages all the infrastructure configuration details to make deploying applications easy so now that we know what a platform as a service is let's take a look at where canary deployments fit into the picture in general when we're talking about deploying an application we're typically trying to solve the problem of how to deploy an update to an application that's already in production now to do this effectively there are two general rules to keep in mind first we want to minimize the amount of time the application is offline when making the updates and second we want a plan in place to quickly and easily revert back to the previous version if for any reason the new version is not performing as expected now there are many strategies available to accomplish this but one effective and cautious way is to use canary deployments now the main idea behind canary deployments is to take a portion of the user's traffic and divert it towards the new version of the application one advantage we gain from canary deployments is the fine grain control over which users see the new updated application and we can be very creative in how we go about this as an example we could direct users to the new version based on their geographic region but in this diagram we're depicting a simple rule that defines 20 of all requests to be diverted to the new version of the application now when we first introduce the canary we want to start by routing only a small percentage of users to the new version now this is a cautious approach that mitigates any adverse consequences that may occur as a result of introducing unforeseen bugs now in this situation only a small percentage of users would be affected now if we're faced with a bad deployment we simply stop routing traffic to the new application and let the current version handle 100 of the requests but in the event the canary is performing well we can incrementally increase the amount of traffic to the new version of the application which will bolster our confidence that the new version is ready for production and when we're completely confident that the canary is ready we simply let it take over as a new production instance of the application and take down the old version as an added bonus applications can remain running while the update is being pushed out meaning there's no reason to take the application offline to make the update and lastly canary deployments allow development teams to rapidly push out updates and get immediate feedback on how their changes perform in a production environment with real users now one of the big challenges with canary deployments is that rather complex to implement this is because they require some mechanism in place to monitor and keep track of both versions of the applications in addition these monitoring tools have to be configured to provide clear feedback about how the canary is performing against the current version by collecting the right data to make this decision of whether or not to promote the canary or roll it back is a large task in and of itself but on top of that you have now have two versions of the application in production at the same time as a consequence this means the new version of the application should be compatible with any other existing systems that it may interact with such as a database now to keep things short you'll be adding additional complexity to your system to keep the new version backwards compatible so to summarize everything we just discussed about canary deployments if they're done correctly they can prove to be a powerful tool to deploy updates for your application and we believe their benefits outweigh their downsides to justify their use so now we have a better understanding of what sentinel is we'd like to introduce you to apps rs apps rs is a web development consultancy where anyone can hire them to build their web apps on any given day they would be responsible for developing new applications or updating existing ones for example they could be making great tracking software for a local school district while simultaneously making updates to a registration portal for a local sports league as you can see app service is responsible for many applications at different stages of development and they're in need of some platform as a service that can manage the infrastructure when deploying applications as well as updating existing ones this allows this will allow the development team to focus on what matters most developing the applications now appsource faces a very common problem and illustrates our motivation behind why we built sentinel so let's now take a look at how easy it is for appserrats to deploy applications and make canary deployments with sentinel sentinel is available to download as a global npm package and once installed they can run the sentinel init command to provision all the necessary infrastructure in the cloud as you can see there are many steps involved to making this happen but once they have successfully completed they're ready to deploy their application by simply running the sentinel deploy command here we see the apps rust team deploying a to-do list application now with our command line interface we collect some basic information about the application such as the domain name [Music] and the postgres database configuration now without getting into too much details all that's happening here is that we're setting up the to-do list application and associating it to its dedicated database you also have the option to upload an sql file for the database if you want to specify its schema before the application starts and once all the configuration details are confirmed it's just a few short seconds before apps rs can show off their brand new application so heading on over to the url of the application let's check out what they just deployed [Music] looks like the apps rs team has successfully deployed their first application with sentinel so let's add a quick item to the list here and make sure everything is looking great now when the apps rs team is ready to deploy an update to any of their applications they run the sentinel canary deploy command and choose from a list of currently deployed applications in the platform once again they run through the deployment process but notice now that sentinel is prompting for additional information about the canary version now this time we'll quickly run through the database configuration once again but coming up shortly is a very important step where they specify a percentage of traffic to route the users to the new version of the application which we'll see here is going to be 30 percent after confirming everything they entered is correct they can deploy the canary and once again in the browser we see our application looks the same but pay attention to the title here we'll quickly clear our cookies in order for us to land on the canary version once we refresh we see that our application has changed validating that we have two versions of the application in production at the same time so it seems like everything is working well with the canary version and for the apps arrest team so sentinel comes with a complete command line interface to help anyone deploy their applications and push out their updates with ease so now let's take a moment to compare how sentinel stacks up against the current players in the platform as a service space on one end of the spectrum you have options like heroku and elastic bean stock which are closed source these options tend to have many features but they don't necessarily mean they're going to support canary deployments out of the box additionally these options will most likely come at a monthly cost just to use them now the big cloud providers tend to offer their own platforms as a service that are tightly integrated with their cloud infrastructure but if they decide to raise the price make significant changes or depreciate support for something you're going to have to weigh the options of whether or not it's worth the time effort and money to make the switch to the new platform as a service now with the closed source option you'll always be at the mercy of win or if they make any changes to the infrastructure to really highlight that last point since heroku started back in 2007 they've been using aws ec2 classic instances at that time they're the latest and greatest compute instances available at aws but in 2013 aws launched a more performant and efficient vpc architecture only recently in october 2021 did heroku finally make the switch to the new model that aws offers so there would have been an eight year period of time where you've been left out from taking advantage of a more performant and efficient architecture because heroku and the other aforementioned platforms as a services are closed source there's no way to update the source code for any reason even if you wanted to on the other end of the spectrum you have open source platforms as a services such as cap rover with the open source option development teams will know will have more control because there's no longer a third party managing the platform the main benefit is the ability to control how the tool is used because access to the source code is unimpeded for the developers and that means they have the opportunity to shape it to their needs now the big trade-off is that you'll give up many of the support and features that a closed source option offers now in summary the benefits of an open source platform as a service offers more flexibility and is cheaper but sacrifices many of the features and support you get from a closed source alternative and sentinel aims to address these shortcomings now when we set out to build sentinel we had a few things in mind you wanted to provide an open source platform as a service that can easily update applications with canary deployments we wanted to provide the tools to monitor the canary in order to give valuable insight into its performance additionally we needed to support web developers and the applications they create as broadly as possible no matter what programming language runtime environment or other tools they may use and to do so sentinel supports postgres database backed applications that have been packaged as docker images and lastly we wanted to automatically provision ssl encryption for all applications deployed with sentinel now while we currently only support applications on aws's infrastructure we wanted to give developers the option to use sentinel with any cloud provider and allow them to make that decision as easy as possible to implement and that has informed many of our design decisions so to talk about how we went about designing sentinel to achieve these goals and more i'd like to turn things over to the rest of the sentinel team starting with michael all right so the first stop on the sentinel design tour is infrastructure sentinel takes care of the infrastructure necessary for hosting applications for the developers using our platform which is a big part of what makes us a platform as a service so when talking about sentinels infrastructure we're essentially talking about the infrastructure we provide to our users since we wanted to be a very flexible platform we had to consider this question how can sentinel's infrastructure provide its users with flexibility in terms of where their apps are hosted and what sorts of apps they deploy to address that first question where do we host our users applications the first way we provide flexible infrastructure to our users is by being cloud provider agnostic every cloud provider has plenty of custom services to simplify running applications in their cloud but because they aren't compatible between cloud providers using them locks you in for example if you use aws's elastic container service you're going to have a hard time if you decide you'd like to switch to google cloud platform or digitalocean but all cloud providers have simple compute instances and they're all the same linux virtual servers by limiting ourselves to simple compute instances we ensure that sentinel is deployable in any cloud provider so as for the resources we need to accomplish our goals it's just compute instances and of course we need some consistent cloud agnostic way to perform key actions on any given cloud provider actions such as scaling compute instances up and down installing and scaling applications and managing permissions and firewalls for these things we wouldn't want to use say aws's cloud formation to do this on amazon's ec2 instances because that would again tie us more tightly to a particular provider so to provision compute instances and interact with them we use a combination of terraform and ansible with these two tools basically anything you can do on your command line you can do programmatically on a remote compute instance now let's see how this infrastructure looks at a high level remember that awesome sentinel init command brendan showed you that magically created our platform in a user's cloud account here's what was going on there the first thing to note in this diagram is the sentinel cli on the left hand side the command line interface this is on the developer's computer so it's outside of the dotted line outside of the cloud infrastructure when a developer runs sentinel init it triggers some initial terraform and ansible code to create the sentinel control plane the sentinel control plane in the middle where the ball from the cli is headed off to is the first cloud compute instance that's a part of sentinel's infrastructure and it's the brains of the operation the server that's responsible for deploying any further servers our applications will run on and for deploying the applications themselves to those servers so if you look where the ball is headed now it's going from terraform on the control plane to the sentinel cluster the cluster is comprised of the servers that actually host our users applications when a user needs a new compute instance they just run a command from the cli and the control plane creates it using terraform and now we're looking at ansible in action when a user wants to deploy an application they again run a command from the cli and the control plane takes care of deploying it into the cluster via ansible the final moving ball here just shows that the control plane also sends requests to the cluster and gets information back about the current state of our applications here it's communicating with the docker demon why dr demon you ask well that leads to the second aspect of our flexible infrastructure we didn't want to limit the sorts of applications our users could deploy and we wanted to maximize the space available on any given compute instance but hosting multiple apps in multiple runtimes on the same machine raises some concerns among them conflicting dependencies one app might need an older version of python and one might need a newer version for security reasons you also may not want every app potentially accessible to every other app some isolation would be desirable there are basically two possible solutions to these issues one is to use virtual machines where you emulate multiple operating systems on the same server this does achieve the sort of isolation we're after but it's isolation at the machine or operating system level and it's a very resource-heavy way of achieving that isolation a better solution sentinel solution is to use containerized applications with containerized applications you still achieve isolation but at the process level so this is a much less resource intensive way of isolating than using a whole emulated os we show this in our diagram by putting six containers on the right hand side in that server as opposed to only three virtual machines on the left-hand server and so as you can see while this diagram may not be to scale containers certainly are and as for dependencies even if your host operating system doesn't have all the dependencies necessary for running an application if it has docker installed it can run a container with those dependencies without installing them on your computer this allows the developers using our our platform to deploy any app they like as long as it's containerized just to illustrate how this looks on sentinel's architecture we can zoom in on one of the nodes in the cluster and see that it has many containerized applications on it not just the applications our users deploy but also their databases and our monitoring tools there are other ways of containerizing applications of course but we went with docker due to its widespread use as a way of increasing the accessibility of our platform so now let's catch up with apps rs in another quick demo since it looks like they're ready to deploy another application into their sentinel cluster once again we'll go quickly through the deployment process here and collect some information about the new app they want to deploy what we're looking for is to see that this is deployed on the same node as the prior application we can see that the to do app is still up and running let's see if cats r us is up and running oh boy there it is let's just hope there's room for us at that ipo now we can go back to the command line and confirm that there's only one node in the cluster on which both of these applications are running so containers are great but no matter how nice they are when you have a whole lot of them things can still get complicated and to talk about how we address those challenges here's sam okay so now we need to consider what types of things are involved in managing multiple containers across one or more compute instances and how we can go about doing that some things we need to consider are distribution here our servers are scaled to two instances and we have three applications that need to be deployed one with a database what server should we deploy them on and how do we then keep track of where each application is running scaling and load balancing here we have the to do zap scales with three instances and its database to two across two servers how can we handle easily scaling applications up and down and once scaled how do we handle load balancing between them what happens when we scale our servers up and down how do we redistribute the existing containers networking the to do zap needs a way to communicate with its database but they're not only in separate containers they're on separate servers how do we handle communication between containers restart what happens when a server or a container crashes here our second server crashed and the to use app can no longer communicate with its database how do we ensure the container is restarted on a healthy server with minimal downtime managing all this manually for multiple applications would be significantly complex to do that's where container orchestrator comes in a container orchestrator solves all the problems mentioned in the last slide by automating the deployment management scaling and networking of containers let's talk about some of the key benefits of using an orchestrator scheduling refers to strategies a container orchestrator uses to determine which server to put a container on this feature automatically handles distribution and redistribution of containers across servers we can designate a restart policy for our containers so that if a container crashes our orchestrator will automatically attempt to restart it in the same way if a server crashes the orchestrator will detect this and schedule the containers to run on an available server orchestrators make it easy to scale containers up and down as needed once scaled load balancing between the application instances is a built-in feature when a container is created you provide a name for it and it gets assigned its own ip address now containers are ephemeral meaning they can stop and restart at any time so this also means their container ip address changes when they're restarted to counter this orchestrators use service discovery which uses an embedded dns server to automatically resolve that given name to the ip address and port of the associated container if a container is restarted and receives a new ip address the internal dns record will be updated service discovery is imperative for communication between containers as well as routing external requests which we'll see later on lastly container orchestrators provide overlay networks which enable containers to communicate with each other across a virtual layer 2 network that allows physically separated servers to communicate as though they were physically connected via lan while we briefly considered orchestrators like aws elastic container service and kubernetes docker swore mode was the clear choice it's built into any docker deployment it seamlessly integrates with other docker tools like the docker cli and docker compose which we can use to deploy and interact with our docker applications it provides us with the ability to interact with the swarm via docker api calls it has robust documentation with the benefit of ease of use and overall provides the appropriate functionality for our use case let's talk more about how exactly docker store mode works and how we use it to deploy an application a swarm is made up of docker hosts running in swarm mode that act as managers and workers nodes are servers that have docker installed and are part of the swarm manager knows handle delegation of swarm services and management of membership to the swarm a service is what we use to deploy applications in the swarm we define a service with the image we want to deploy and how many instances of the application we want to run known as task replicas a task is the actual running container of the service we defined services are deployed to the manager node which then schedules a service on nodes in the swarm as one or more replica tasks which run independently of each other looking at the diagram let's walk through an example with the catnip application here we have three servers one manager node and two worker nodes we define the catnip application as a service and submit the service definition to the manager node as part of the service definition we indicated the catnip image and defined three replicas to run for catnip meaning upon deployment catnip should be scaled to three instances the manager node then schedules the catnip service to run as three tasks on the available nodes as seen here the worker and manager nodes can all run tasks we can easily scale our applications up or down at any time by changing the number of tasks running via a simple docker command on the manager node the to-do's application that appsoros deployed earlier is getting a lot of traffic let's see how sentinel works with docker score mode to easily scale their production application as well as their cluster sentinel inspect app provides application specific information for the to-do's app as you can see there are three to-do services running production canary in the database and just one task replica running for the to-do's production service apps rust uses sentinel scale app to scale the to-do's production service up to four tasks in order to deal with the increase in traffic once that's complete running sentinel inspect app again for to-do's will show the production application has been scaled to four task replicas this command also shows appstars detailed information on each task their status and metrics related to the node they're running on both the canary and database services remain at one task since after us has not yet scaled their cluster all these tasks are running on the manager node as we can see when they run sentinel cluster inspect now that for now that four tasks are running for the to do production service app cerros decides to add another server to the cluster by running sentinel cluster scale running sentinel cluster inspect again will show we now have a second server a worker node has joined the swarm docker swarm mode will automatically redistribute all of our tasks for all of our services across both of these nodes since sentinel supports applications with a postgres database another thing we need to consider is docker's one process principle which states that each container should do one thing and do it well in the last demo we saw that the to-do's database exists as a separate service from this dues application while technically a database can run in the same container as the application ideally we want to keep them decoupled to allow us to scale applications and databases independently of each other and also easily version our applications we know that containers by default run in isolation and don't know anything about other processes or containers in the swarm or even on the same machine so if the application and database are running as separate services in the swarm let's talk about how communication between the application service and the database service actually works when describing container orchestrators we touched on their container networking capabilities by way of overlay networks this is how we can provide a way for our application and database service to communicate securely this type of virtual networking minimizes security risks as there's no need to expose the ports of our containers to the internet for our services to communicate keeping them isolated the important thing to remember with overlay networks is that no matter where the services are located in the swarm as long as they're on the same overlay network they can talk to each other upon deployment of the to-dos app we created a service for the application a separate service for the database as well as an application specific overlay network and connected both services to it because no other applications will be connected to this particular network the database remains isolated from other non-related applications providing us with the security we want to maintain for the database remember that container orchestrators use service discovery by way of dns to map service names to container ip addresses thanks to this feature our to to-do service just needs to know the service name of the database in order to connect to it over the network if you want to scale up our to-do's app as we did in the last demo we simply increase the number of tasks running for the to-do service and the new task will automatically connect to the to do's overlay network in the earlier demo we also saw absaros deploy the canary version of the to-do's application here's how the new version integrates into the last diagram version two of the to-do's application is deployed as a new service and connected to the same application specific overlay network that today's version one and the database are already on to-do's version two is communicating with the same existing database illustrating how following the one process principle and keeping our database decoupled from the application helps us in implementing new versions of our applications to see how a container orchestrator works with our current architecture we can zoom in on our sentinel cluster in the diagram we have two servers a manager and a worker node and multiple containerized applications deployed across both of them docker s4 mode is managing all the containerized applications for example it's handling the load balancing between the two instances of app one we can also see app2 has both a production and canary version spread across both nodes that are both communicating with the app2 database across an overlay network so at this point in our design process we understand the role of the container orchestrator and how our applications can use it to communicate internally but we still need to understand how our end users can access their applications externally and for that i'll pass it to drew okay thank you sam so let's uh talk about request routing obviously hosting the developers applications is only useful if their end users have a way to access those apps so we needed a way for a cluster to accept requests and route them to the appropriate application and to do all this securely now docker does publish a port for each service so we could expose these ports and have our users access the applications via these ports however this is obviously undesirable for a number of reasons first of all it is unintuitive and awkward for end users to have to include a port number in the url when trying to get to an application after all the average person on the internet probably doesn't even know what a port is secondly from a security perspective exposing all these ports greatly increases our vulnerability to malicious entities on the internet a better solution would be to use a reverse proxy to accept all requests and forward them to the appropriate application with the reverse proxy in place we can hide all of our servers in the cluster behind one ip address which protects the identity of those servers rather than opening up a different port for each docker service we can just open ports for http and https and let the reverse proxy route the request to the different docker services this greatly reduces our vulnerability to malicious entities on the internet speaking of security a reverse proxy can handle encryption for https which relieves our users applications of this responsibility this frees up their apps web servers to focus on serving their content rather than spending resources on encryption having reverse proxy in place also allows us to map our applications to distinct host names when choosing a specific reverse proxy we mainly consider nginx aj proxy and traffic given the maturity and popularity but as we researched these options we found that the best one for us was clearly traffic traffic was the best reverse proxy for our use case for a number of reasons first and foremost it was specifically designed to integrate with container orchestrators it also has excellent service discovery and its routing rules can be updated dynamically this means that the new services can be discovered and traffic routed to them without having to restart the reverse proxy these two points are especially important in regards to our canary deployment feature which i'll talk about in just a bit traffic also provides some support for integrating let's encrypt including automatic certificate renewal which helped us with our goal of providing https to users apps out of the box we could simply install traffic on its own server and have it route requests to the applications using the aforementioned ports that docker publishes for each service this would require us to create security rules where our cluster servers would only accept connections over these ports if the request was coming from the ip address of the server hosting the reverse proxy however this is not fully secure as it leaves the system vulnerable to ip spoofing attacks so we found what we really needed was a private network to connect traffic to the applications that need to receive the forwarded requests so how can we implement a private network well we can use another overlay network as sam already discussed an overly network is essentially a virtual private network and is exactly what we need in the scenario but how do we put traffic on an obviously network with our applications we've run us run the reverse proxy as a docker service docker makes it very easy to create overlay networks so all we need to do is run traffic as a docker service on our manager node create an overlay network and add all of our internet facing services to it you might be asking how can i see how can traffic actually know where to send the request for any given application well we use docker's built-in internal dns system when docker is in swarm mode each service is given in a virtual ip address for each network that it is connected to and the service name acts as an internal name that is resolved to this virtual ip address each task is also assigned a virtual ip address requests for a given services virtual ip address are routed to the different task virtual ip addresses and docker's internal load balancer determines which task should receive that request so now that we know how our end users will access the different applications let's turn our attention to how sentinel implements the various features involved with canary deployments first and foremost sentinel needs to allow a developer to deploy a canary this involves running the new version as a separate docker service and placing it on the reverse proxy overlay network we also need to update the routing rules so that requests for a given application are split between that app's production service and the canary service according to the prescribed traffic weights these routing rules also include the sticky sessions settings when sticky sessions are turned on the end user receives a cookie which ensures that they continue to see the same version of the application in their browser as they use the app and make requests signal also needs to allow a user to change the traffic splitting weights should they want to test the canary with a higher traffic load we also need to be able to provide metrics and logs to developers so they can determine the health and performance of their canaries and we need to allow the developers to promote the canary version as well as roll back should its performance be deemed unsatisfactory the first requirement is an easy ask just need to run the new version of the app as a separate docker service and place it on the overlay network with a reverse proxy however adding the new version to the overlay network isn't enough on its own as we still need to update the routing rules such that the requests for the game and app are split between production and canary services we largely accomplished this using traffic's routing abilities which can be adjusted dynamically this is done by first specifying the providers that we want to use in traffic's static configuration traffic will then watch these providers for any changes in their dynamic configuration the dynamic configuration for each provider varies the first provider we use is docker and the way we dynamically update its configuration is by adding labels to our docker services the second provider we use is the files provider this is exactly what it sounds like we add remove or edit certain configuration files and traffic adjusts accordingly so so to incorporate the traffic splitting behavior we need for our canaries we start by adding a configuration file that defines traffic splitting weights for some given services in this case the production and canary services for given application users can also change these weights while the canary is running using the sentinel canary traffic command we then add labels to the production and canary docker services to tell traffic to use the traffic splitting weights defined in the config file with getting a canary in place now covered let's discuss the ability to promote connects to start we update the configuration for the production service so that it uses a new image version this starts a rolling update process where one production task running the old version is taken down and replaced by a task running the new version if the new task is deemed healthy the process is repeated until all old tasks have been replaced by tasks running the new version if it if the new task fails for any reason the process is stopped and all tasks are reverted back to the original version once all tasks have been updated we remove the canary service and traffic splitting file configuration file so let's take a look at this process in action app service wants to promote their to-do's app we'll start by inspecting it with the sentinel inspect app we see that we have three different services associated with this app production service the postgres database service and our canary service let's take a look at the app in the browser when we first load the app we were given the production service and since we have sticky sessions enabled when we click around the app we stay in the production version but we want to see the canary version so we can use a tool provided by chrome to clear out our cookies which will delete our sticky session and then reload the page we'll do this a few times and eventually yeah we get our canary version everything seems fine but let's take a quick look at our monitoring tools to make sure everything's okay here we can look at the metrics for both our production and canary services and we can see that there doesn't seem to be any glaring issues with the canary so let's go ahead and promote it we run the command select the to-do's app confirm and the canary version is promoted when we go back to the application in the browser we see that we get the new version and everything is functioning as expected since the new version is the only version running now even if we clear our cookies and reload we always get the new version lastly we need to be able to roll our canary back if it doesn't go according to plan this is very straightforward if the canary is deemed unhealthy we simply remove the traffic configuration file that determines our traffic splitting behavior which will revert 100 of the traffic back to the production service then all we need to do is remove the canary service so we talked about promoting and rolling back canaries but in order to do either of these things we need to be able to analyze our canary's version canary versions performance and health to determine if it is worthy of promotion or not for that we need monitoring tools to explain our approach in this area i'll pass it back over to michael okay thank you drew so now that we have all this functionality to scale compute instances scale apps promote and roll back canaries the final question we pose here about our design as a platform is how do we make it easy for our users to make decisions about these things and that's where we come to our monitoring architecture for this part of the sentinel design tour we'll be focusing in on these monitoring applications deployed into the sentinel cluster node exporter prometheus and grafana these three tools work together building off one another to deliver metrics to the developers using sentinel as for node exporter unlike prometheus and grafana which only run on the manager node nodeexporter is deployed onto every node in the cluster the reason for this is that it provides system level metrics like disk space available and cpu usage on the right you can see system level metrics for the two nodes deployed in the cluster so node exporter helps deliver one kind of metrics prometheus on the other hand scrapes or reads metrics from two sources for our purposes here first it scrapes traffic for application level metrics the sort of metrics our users will primarily care about include things like request response time and response status codes we didn't want to ask our users to manually instrument their applications but we were able to capture these metrics at the level of our reverse proxy prometheus also scrapes node exporter for those system level metrics once prometheus has these metrics you can analyze that data with prometheus's query language promql we'll see an example of that on the next page but essentially it allows you to do things like say all right i don't want to see status codes for all applications just this application and only 404s and only for the last five minutes once you have some metrics and analysis using prometheus you can display that in a friendly manner to your users with grafana grafana is primarily a tool for visualizing data sources such as prometheus we provide our developers using our platform access to prometheus's default dashboard and here you can see them some metrics that have come in from traffic traffic is labeled as the scrape job and each application's metrics have been configured to include labels which allows us to sort by application there's also some prom ql codes surrounding these metrics to further interpret the data developers who know how to can use this dashboard if they want or they could head over to their custom sentinel grafana dashboard here's sentinel's custom grafana dashboard a user selects the application they want to know about from a drop-down and from there they can see how many requests it's received in the last five minutes the average response time for those requests and data about the status code issues issued from this application over the last five minutes and the last hour both of these dashboards are readily available to developers using our platform in their browser behind a password of their choosing that we configure for them so putting that all together again traffic captures application level data and then node exporter and prometheus or node exporter and traffic pass that on to prometheus and prometheus after doing some analysis passed this on to grafana for visualization so applications to traffic node exporter and traffic to prometheus and prometheus to grafana a developer can put all these tools together to help them decide when to promote or roll back a canary in three ways first they can compare the production and canary side by side in their grafana dashboard again as soon as you deploy a canary we've configured things so that it's automatically visible to these tools here we have the production version on the left and the canary on the right and initially it seemed like the canary was doing well flying high but then you can see under the sentinel canary there a growing red pie chart in a nasty upward tick red line telling us some responses are sending back 404s so so long canary it's been fun while it lasted if you want to dig deeper to figure out what specifically was leading to those 404s there's yet another metrics relevant cli command a user can run sentinel inspect logs then they'll be prompted to select the name of your application and immediately get logs from that application nothing very unusual here but the nice thing is that you don't have to go hunting for each of your logs you just enter the name of the application in the cli and there they are finally maybe your app isn't failing in any way but you want to know if it's a resource hog just type sentinel inspect app select the app you're interested in and you'll get a report about your application along with metrics about the node on which it's hosted disk space memory space and cpu usage as a bonus of course you can use these same commands to tell you when you need to scale a compute instance or scale a non-canary standard production app that nearly brings us to the end of this presentation and we wanted to conclude by mentioning some of our upcoming work we're going to add some support for other cloud providers and because we provision compute instances using terraform rather than a specific cloud provider's interface all it's going to take is a bit of elbow grease on our part to create these new terraform files for these different cloud providers the rest of our control plane logic will be able to remain the same we're also going to allow developers to deploy their canary and a baseline version of the production app each on their own compute instances this isn't essential but some currently consider it best practice as it allows you to isolate any differences between the canary and the production app more precisely again we're already splitting traffic and creating new instances so this is just going to require a little more of that we're going to add support for build packs to further increase the accessibility of our platform this way users don't have to know how to use docker even they can just give us a zip file or a link to their code on github and the build pack will take care of containerizing it for them and finally we're going to add support for other databases now that does bring us to the end of our presentation and so on behalf of brandon sam drew and myself we'd like to thank you for taking the time to learn about sentinel and we'd love to field any questions you might be curious about okay looks like we have a quest uh question here um what was the most satisfying part of building this what was the most challenging uh interesting question um sure we're all gonna have slightly different answers for this so anyone want to jump in you know i i think initially figuring out um how to use docker compose in a kind of repeatable way on ansible was a very challenging part that was one of the things we needed to figure out first to make sure that this would all work um because what we do under the hood is uh we've turned those docker compose files into templates and then each time a user runs one of these commands on the cli we uh populate that with the information they provide and deploy it onto the servers and that it took a while to figure out a good streamline way of doing that and once we did we kind of knew that some of our major functionality was within reach yeah i think the satisfying part for me at least was just getting the command line interface and the api like fully synced up and just being able to you know start start sentinel and launch deploy an app and deploy canary and just you know get all the seeing uh everything seeing it you know launch and just getting it everything to work is really the most satisfying part of building it yeah i would just add that getting the traffic slitting right for the canary was a little challenging to figure out but also obviously the most satisfying because it's kind of the shining star sentinel so uh yeah for me and for myself i'd say i agree with drew we spent a lot of time on that init initialization command and just getting it to finally work was like it's like awesome like you don't have that working nothing else will so felt good okay we have another question here um how did you how did you divide the labor on this project did you all focus on separate pieces or were you all involved in the whole project yeah so when we were when we were kind of thinking about this we kind of separated the whole problem into two parts like the api server side and the command line server side and and so we had like two people each of us working on each part and then eventually you know it was important to have someone like bridge that gap i think sam was like one of the first people to kind of jump back and forth between what's happening on the cli and on the api and then kind of have everyone catch up to speed and like okay now we can see how everything's working together yeah although the first week or two when we were just trying to figure out docker swarm and traffic and ansible and terraform we're just all kind of madly madly rushing trying to see who would have breakthroughs and then we eventually got to that place where we knew what the things were that could be divided great um we have another question here um about canary deployments with databases in general so how are schema challenges handled when you have two versions running at the same time yeah so um right now sentinel doesn't have support for canaries that actually change the scheme of the existing database the canaries just support changes to the application side um but that is definitely something we would like to add it's a very complex topic that we looked into um but yeah definitely something we can add one more question so is there anything you would have done differently knowing what you know now [Music] that was a tough one um would have spent more time on like the cli and refining like you know the commands and and like the vernacular surrounding it would have been nice to have kind of like a nice um clear and concise way of specifying the commands yeah i think uh one other thing that we went back and forth about a lot was whether to have a database to keep the state uh of all the applications on the sentinel control plane right now we're getting all that information directly from the swarm like uh you know how many replicas there are of a service and um whether there's a canary version or not and that makes a lot of sense but there were some situations where we wished we didn't have to go to the cluster for information it would have been nice to maybe also be keeping some things uh up to date some state up to date on the control plane itself separate from swarm api itself yeah terraform also has its own state that it's that it keeps track of so um since you know terraform and docker are both stateful um applications we didn't initially feel the need uh to have our own database to maintain state but i agree in retrospect there were certain things that would have been a bit easier to implement if we if we had uh included our own database so don't see any more questions so yeah i think we'd just like to say uh thank you to everyone for uh coming out to check out sentinel and uh thank you to launch school for giving us this platform and um that's it anybody else want to say anything thank you thank you you 