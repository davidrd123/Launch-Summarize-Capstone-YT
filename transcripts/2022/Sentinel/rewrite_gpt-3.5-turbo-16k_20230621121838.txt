Hello everyone, my name is Brendan and I'm here with Sam, Drew, and Michael. Together, we have developed Sentinel, an open-source, cloud-agnostic platform-as-a-service (PaaS) with canary deployments. Today, we are excited to share all the details with you.

So, let's start by understanding what a platform-as-a-service (PaaS) actually is. When we talk about deploying an application to the internet, there are multiple components that need to work together, which we refer to as infrastructure. This includes servers, operating systems, and hard drives, among other things. Managing all these components can be complex and time-consuming. That's where PaaS comes in. It takes care of all the infrastructure configuration details, making it easy for developers to deploy their applications without having to worry about the complexities.

Now, let's discuss canary deployments and how they fit into the picture. When deploying updates to an application that's already in production, there are two key rules to keep in mind: minimize downtime and have a plan to revert back if needed. Canary deployments are a cautious strategy to achieve this. They involve diverting a portion of user traffic to the new version of the application. This gives developers fine-grained control over which users see the new version. For example, we can direct users based on their geographic region. By gradually increasing the traffic to the new version, we gain confidence that it's ready for production. If any issues arise, we can easily revert back to the previous version. Canary deployments allow for rapid updates and immediate feedback from real users.

However, implementing canary deployments can be complex. It requires monitoring and managing both versions of the application and ensuring compatibility with existing systems. That's where Sentinel comes in. Sentinel is a complete PaaS that simplifies canary deployments. It provides the necessary mechanisms to monitor and track both versions of the application. It collects data to make informed decisions on promoting or rolling back the canary. It also ensures backward compatibility with existing systems, like a database.

Let's now talk about AppsRS, a web development consultancy. They develop and update multiple applications for their clients. However, managing the infrastructure for all these applications can be a challenge. This is where Sentinel comes in. It offers a solution for managing infrastructure while allowing the development team to focus on application development. AppsRS can easily deploy their applications and make canary deployments with Sentinel.

To deploy applications with Sentinel, AppsRS simply needs to install the global npm package and run the "sentinel init" command. This provisions all the necessary infrastructure in the cloud. After successful provisioning, they can deploy their applications using the "sentinel deploy" command. During deployment, they provide basic information about the application, such as the domain name and Postgres database configuration. Once confirmed, the application is deployed within seconds.

Let's take a look at the application AppsRS just deployed. It's a to-do list application. We can see that the deployment was successful, and we can interact with the application. Now, if AppsRS wants to deploy an update to the application, they would use the "sentinel canary deploy" command. They select the application they want to update and provide additional information about the canary version, including the database configuration. They can also specify the percentage of traffic to route to the new version. Once confirmed, the canary is deployed.

Now, we can see both versions of the application in production. By clearing our cookies, we can access the canary version. This demonstrates that we have two versions running simultaneously. In this case, the canary version appears to be functioning well. With Sentinel's command-line interface, anyone can easily deploy applications and push updates with confidence.

Let's compare Sentinel to other platform-as-a-service options in the market. On one end, we have closed-source options like Heroku and Elastic Beanstalk. While these options offer many features, they may not support canary deployments out of the box. They often come at a monthly cost, and switching providers can be challenging. Moreover, as closed-source platforms, they can limit access to updates and new infrastructure advancements.

On the other end, we have open-source options like CapRover. These options provide more control and flexibility since developers have access to the source code. However, they lack the support and features offered by closed-source alternatives.

Sentinel aims to bridge this gap by offering an open-source platform-as-a-service with canary deployments. It provides flexibility, supports PostgreSQL-backed applications packaged as Docker images, and automatically provisions SSL encryption.

When designing Sentinel, we prioritized flexibility, ease of use, and compatibility with multiple cloud providers. By adopting a cloud provider-agnostic approach, we ensure that developers can deploy their applications in any cloud environment. We use tools like Terraform and Ansible to provision and manage compute instances programmatically, allowing users to scale resources, install applications, and manage permissions.

In summary, Sentinel is an open-source, cloud-agnostic platform-as-a-service with canary deployments. It simplifies the deployment and update process for applications, allowing developers to focus on their core tasks. Whether you're a web development consultancy like AppsRS or an individual developer, Sentinel provides the tools you need to deploy and update applications with ease. It offers flexibility, compatibility, and valuable insights into the performance of canary deployments. With Sentinel, you can confidently push updates and gather real-time feedback from users. Sentinel is a platform that aims to support applications created using various programming languages, runtime environments, and tools. It specifically caters to Postgres database-backed applications that are packaged as Docker images. One of our main objectives is to automatically provision SSL encryption for all applications deployed with Sentinel.

While our current support is limited to applications on AWS's infrastructure, we wanted to extend the option for developers to use Sentinel with any cloud provider. We wanted to make this decision as easy as possible to implement, and it has influenced many of our design decisions.

To achieve these goals, we designed Sentinel to provide flexible infrastructure for hosting applications. We wanted to ensure that our users have the freedom to choose where their apps are hosted and what types of apps they deploy. So, we made sure that Sentinel's infrastructure is cloud provider agnostic.

Every cloud provider offers custom services that simplify running applications on their platform. However, these services are not compatible across different providers. Using these services could potentially lock users into a specific cloud provider. To avoid this, we decided to focus on simple compute instances that are available in all cloud providers. By doing so, we ensure that Sentinel can be deployed in any cloud environment.

To achieve consistent cloud-agnostic actions such as scaling compute instances, installing and scaling applications, and managing permissions and firewalls, we use a combination of Terraform and Ansible. These tools allow us to programmatically perform actions on remote compute instances, providing a consistent and flexible approach.

Let's take a high-level look at how this infrastructure is structured. When a developer runs the "sentinel init" command on their computer, it triggers Terraform and Ansible code to create the Sentinel control plane, which is the brains of the operation. This control plane is the first cloud compute instance that is a part of Sentinel's infrastructure. It is responsible for deploying further servers and applications. The control plane uses Terraform to provision new compute instances and Ansible to deploy applications to those instances.

The control plane communicates with the Sentinel cluster, which consists of the servers hosting the users' applications. When a user needs a new compute instance, they can simply run a command from the command line interface, and the control plane will create it using Terraform. Similarly, when a user wants to deploy an application, they run a command from the command line, and the control plane deploys it into the cluster using Ansible. The control plane also sends requests to the Docker daemon to manage the current state of the applications.

To maximize the flexibility of the applications our users can deploy, we decided to use containerized applications. Containerization provides isolation at the process level, allowing multiple apps to coexist on the same machine without conflicting dependencies. Unlike virtual machines, containers are less resource-intensive and offer a more lightweight approach to isolation.

By using Docker containers, we can host multiple applications, databases, and monitoring tools on the same server. This allows developers to deploy any containerized app they choose. Docker's widespread use and accessibility made it the ideal choice for containerizing applications on the Sentinel platform.

Now, let's join Apps R Us as they deploy another application into their Sentinel cluster. This deployment process involves gathering information about the new app and ensuring that it is deployed on the same node as the previous application. In this case, both the To-Do app and the Cats R Us app are up and running on the same node, validating the successful deployment.

While containers offer numerous benefits, managing a large number of them can become complicated. We faced challenges related to distribution, scaling and load balancing, networking, and container restarts. To address these challenges, we introduced container orchestrators.

A container orchestrator handles the deployment, management, scaling, and networking of containers across multiple compute instances. It automates tasks like distributing containers among servers, handling container restarts, scaling applications up and down, load balancing, and ensuring communication between containers.

One key benefit of using a container orchestrator is its scheduling capability, which determines where containers are placed. Restart policies ensure that failing containers are automatically restarted, while service discovery enables containers to communicate with each other using embedded DNS servers. Overlay networks allow containers on physically separated servers to communicate as if they were connected via a LAN.

Although there are various orchestrators available, such as AWS Elastic Container Service and Kubernetes, we chose Docker Swarm mode due to its seamless integration with Docker tools and its robust documentation. Swarm mode consists of Docker hosts running as manager and worker nodes, with services being the units used to deploy applications. Tasks represent the actual running containers, and the manager node schedules and distributes tasks across the available nodes.

In the example of the Catnip application, the service is defined with a specified image and the desired number of replicas. The manager node schedules the service on available nodes, and the replicas run independently. Scaling applications up and down is as simple as issuing a Docker command on the manager node, and Docker Swarm automatically redistributes tasks across the available nodes.

In our example, App Rs decide to scale their production application in response to increased traffic. By issuing a command to scale the replica tasks, they easily handle the increased demand. Docker Swarm ensures that all tasks remain distributed across the available nodes, maintaining scalability and load balancing.

As Sentinel also supports applications with a Postgres database, we must consider the one process principle in Docker, which suggests that each container should focus on doing one thing well. In this case, we keep the database as a separate service from the application to allow for independent scaling and versioning.

Overall, Sentinel's combination of infrastructure flexibility, containerization, and container orchestration using Docker Swarm mode enables developers to deploy and manage their applications with ease. By providing a cloud provider-agnostic approach and leveraging industry-standard tools, we empower developers to focus on creating and deploying their applications while simplifying the complexities of managing infrastructure and containers. In this coding Capstone project, we will discuss the deployment and scaling of services in a Docker swarm. Services are deployed to the manager node, which then schedules the services on worker nodes as replica tasks. These replica tasks run independently of each other.

Let's walk through an example using the "Catnip" application. In this example, we have three servers - one manager node and two worker nodes. We define the "Catnip" application as a service and submit the service definition to the manager node. The service definition includes the "Catnip" image and specifies that we want three replicas of the service.

The manager node then schedules the "Catnip" service to run as three tasks on the available worker nodes. All nodes in the swarm, both manager and worker, can run tasks. Scaling the application is as simple as changing the number of tasks running using a Docker command on the manager node.

Now, let's look at another application called "To-do's" deployed by a company called AppSoros. The "To-do's" application is receiving a lot of traffic, so AppSoros needs to scale it. By using the "sentinel scale app" command, AppSoros scales the "To-do's" production service up to four tasks to handle the increased traffic. After scaling, the "sentinel inspect app" command shows that the production application now has four task replicas.

In the "To-do's" application, there are also other services, such as the canary and the database, which remain at one task since the company has not scaled those services yet. All these tasks are currently running on the manager node.

To add another server to the cluster, AppSoros runs the "sentinel cluster scale" command. Now, with two servers, a manager and a worker node, the tasks for all services are automatically distributed across both nodes. The cluster inspect command confirms the addition of the second worker node.

In container orchestrators, such as Docker, it is recommended to keep the application and the database decoupled. Each container should do one thing and do it well. By running the application and database as separate services, we gain the ability to scale them independently, easily version our applications, and increase security. Containers by default run in isolation, so if the application and database are running as separate services in the swarm, communication between them is achieved through overlay networks.

Overlay networks in Docker allow services, regardless of their location in the swarm, to communicate securely as long as they are on the same overlay network. When deploying the "To-do's" app, an overlay network is created specifically for it. The application and database services are connected to this network, ensuring secure communication between them. Container orchestrators use DNS service discovery to map service names to container IP addresses, enabling the "To-do's" application to connect to the database using its service name.

With the architecture and communication mechanisms in place, we can focus on request routing for end users' access to applications. Instead of exposing all the application ports to the internet, a reverse proxy can accept requests and forward them to the appropriate application. This not only simplifies the access URL for end users but also enhances security by reducing vulnerabilities.

Choosing the right reverse proxy is essential. After considering options like Nginx, HAProxy, and Traefik, we selected Traffic as the best reverse proxy for our use case. Traffic is specifically designed to integrate with container orchestrators, provides excellent service discovery, and allows dynamic routing updates.

To implement a private network for the reverse proxy and applications, an overlay network is created. Traffic is run as a Docker service on the manager node and connected to this overlay network. All the internet-facing services are also added to this network, ensuring that the requests are routed securely.

Now, let's delve into canary deployments. A canary deployment involves running a new version of the application as a separate Docker service and updating the routing rules to split traffic between the production and canary services. This allows developers to test new versions of the application with a subset of users while minimizing risk.

To achieve this, we utilize Traffic's dynamic routing capabilities. By configuring traffic splitting weights in a configuration file, we can update the routing rules on the fly. Docker labels are added to the production and canary services, instructing Traffic to use the traffic splitting weights defined in the configuration file. These weights determine the proportion of traffic each service receives.

Promoting a canary version involves updating the configuration for the production service to use a new image version. Then, a rolling update process is initiated, replacing the old version tasks with new version tasks. This process continues until all tasks have been updated. If any new tasks fail, the process is stopped and all tasks are reverted back to the original version.

In the case of the "To-do's" app, the canary version is deployed as a new service connected to the same overlay network as the production service and the database. This allows them to communicate with each other seamlessly. By adjusting the traffic splitting weights, we can control the proportion of traffic directed to each version.

In conclusion, using Docker swarm mode and Traffic as the reverse proxy, we have created a robust and scalable system for managing containerized applications. Through canary deployments and dynamic routing updates, we can test and release new versions of our applications with ease. The private overlay network ensures secure communication between services, and the reverse proxy enhances accessibility and security for end users. The first requirement is to run the new version of the app as a separate docker service and place it on the overlay network with a reverse proxy. However, simply adding the new version to the overlay network is not enough; we also need to update the routing rules to split the requests for the game and app between the production and canary services. This is achieved using traffic's routing abilities, which can be adjusted dynamically.

To accomplish this, we specify the providers we want to use in traffic's static configuration. Traffic then monitors these providers for any changes in their dynamic configuration. The dynamic configuration for each provider varies. For the docker provider, we update its configuration dynamically by adding labels to our docker services. For the files provider, we make changes to certain configuration files, and traffic adjusts accordingly.

To incorporate the traffic splitting behavior for our canaries, we start by adding a configuration file that defines traffic splitting weights for the production and canary services. Users can also change these weights while the canary is running using the sentinel canary traffic command. We then add labels to the production and canary docker services, instructing traffic to use the traffic splitting weights defined in the config file.

Now that we have the canary in place, let's discuss how we can promote it. We update the configuration for the production service to use a new image version. This initiates a rolling update process, where one production task running the old version is replaced by a task running the new version. If the new task is healthy, the process is repeated until all old tasks have been replaced. If the new task fails for any reason, the process is stopped, and all tasks are reverted back to the original version. Once all tasks have been updated, we can remove the canary service and traffic splitting configuration file.

Let's take a look at this process in action. Suppose we want to promote the To-Do's app. We start by inspecting it using the sentinel inspect app command. We can see that there are three services associated with this app: the production service, the Postgres database service, and the canary service. When we load the app in the browser, we are initially directed to the production service. However, we want to see the canary version, so we clear our cookies to remove the sticky session and reload the page. By doing this multiple times, we eventually get the canary version.

To ensure everything is functioning correctly, we check our monitoring tools. We can view the metrics for both the production and canary services and see if there are any issues. Once we are satisfied with the canary's performance, we can promote it. We run the necessary command, confirm the promotion, and the canary version is promoted. When we return to the application in the browser, we see that we now receive the new version and everything is functioning as expected.

To roll back the canary, we simply remove the traffic configuration file that determines the traffic splitting behavior. This reverts 100% of the traffic back to the production service. We also need to remove the canary service.

Now let's discuss how we monitor the performance and health of the canary version. We use a monitoring architecture consisting of three tools: Node Exporter, Prometheus, and Grafana. Node Exporter runs on every node in the cluster and provides system-level metrics such as disk space and CPU usage. Prometheus collects metrics from two sources: traffic for application-level metrics and Node Exporter for system-level metrics. Prometheus's query language, PromQL, allows us to analyze the data and extract specific metrics.

Grafana is used to visualize the metrics collected by Prometheus. Developers have access to both Prometheus's default dashboard and a custom Grafana dashboard provided by Sentinel. These dashboards display metrics such as request response time, response status codes, and other relevant information for each application. By comparing the metrics of the production and canary versions side by side, developers can make informed decisions about promoting or rolling back the canary.

In addition to these dashboards, developers can use the sentinel inspect logs command to retrieve application-specific logs and the sentinel inspect app command to get a report on the application's performance and metrics about the node on which it is hosted. These commands provide insights into any issues or resource usage of the application.

Throughout the project, we divided the labor by focusing on different aspects. Some team members worked on the API server side, while others worked on the command line server side. We also had a team member who bridged the gap between the two parts, ensuring everything worked together seamlessly.

In terms of satisfaction, each team member found different parts of the project fulfilling. For some, getting the Docker Compose and Ansible integration to work smoothly was challenging but satisfying. For others, seeing the command line interface and API fully synced up and being able to deploy and manage apps successfully was the most fulfilling aspect. Additionally, figuring out how to configure traffic splitting for the canary was initially challenging, but once accomplished, it became a highlight of the project.

Regarding challenges, working with Docker Swarm, traffic, Ansible, and Terraform in the early stages proved to be demanding. However, with perseverance, breakthroughs were made, and the team ultimately achieved a streamlined and functional solution.

The division of labor allowed us to make progress in parallel and ensure all aspects of the project received attention. Communication and collaboration were key in bridging the different components together. Overall, the project was a collaborative effort, with all team members involved in the different pieces while also having individual areas of focus.

Looking ahead, we have plans to add support for other cloud providers. By using Terraform for compute instance provisioning, we can easily create new Terraform files for different cloud providers. This will allow developers to deploy apps on a variety of cloud platforms while retaining the existing control plane logic.

We also intend to allow developers to deploy the canary and baseline versions of the production app on separate compute instances. This will provide better isolation of differences between the canary and production app, following best practices.

To increase the accessibility of our platform, we aim to add support for build packs. This will eliminate the need for users to have knowledge of Docker, as they can simply provide a zip file or a link to their code on GitHub, and the build pack will handle containerization.

Furthermore, we plan to enhance our database support by adding compatibility with other databases. This will provide users with more options when deploying apps with different database requirements.

In conclusion, the team would like to express gratitude for taking the time to learn about Sentinel. If you have any questions, we are happy to address them. During the development of our project, we established a system where each time a user runs a command on the command line interface (CLI), we populate it with the provided information and deploy it onto the servers. It took us some time to figure out an efficient way to streamline this process. Once we found a solution, we knew that we were close to achieving our main functionality. Personally, I found it very satisfying to have the CLI and the API fully synchronized. Being able to start Sentinel, launch and deploy an app, and implement canary deployments, and seeing everything work seamlessly was the most fulfilling part of building this project.

I would also like to mention that configuring the traffic splitting for the canary deployment was a bit challenging, but it was also the most satisfying aspect. It is the shining star of Sentinel. Additionally, I agree with Drew that we spent a lot of time perfecting the initialization (init) command. Finally getting it to work was an awesome feeling because without that working, everything else would have been futile.

Now, let's move on to how we divided the labor for this project. We initially split the problem into two parts: the API server-side and the command line server-side. Each of us had two people working on each part. Eventually, we realized the need for someone to bridge the gap between the CLI and the API. Sam was one of the first people to do that, and then we all caught up and understood how everything was working together.

In the first couple of weeks, we were all trying to figure out Docker Swarm, Traffic, Ansible, and Terraform. We were rushing to have breakthroughs and eventually reached a point where we had a clear understanding of how things could be divided.

Let's address a question about canary deployments with databases. Currently, Sentinel does not support canaries that change the schema of an existing database. Canaries only support changes on the application side. However, adding support for schema changes is something we would like to incorporate in the future. It is a complex topic that we have researched but have yet to implement.

Now, moving on to whether there is anything we would have done differently with the knowledge we have now. This is a tough question. One aspect we would have focused more on is refining the CLI commands and the surrounding vernacular. It would have been beneficial to have a clear and concise way of specifying commands.

Another topic we often deliberated on was whether or not to have a database to store the state of all the applications on the Sentinel control plane. Currently, we gather information directly from the Swarm, such as the number of replicas and whether there is a canary version. While this approach makes sense, there were situations where having our own database would have been advantageous. It would have been helpful to have some state information updated on the control plane itself, separate from the Swarm API.

We don't see any more questions, so we want to take this opportunity to thank everyone for attending this presentation of Sentinel. We would also like to express our gratitude to Launch School for providing us with this platform to showcase our project. Does anyone else have anything they would like to add? Thank you and thank you all.