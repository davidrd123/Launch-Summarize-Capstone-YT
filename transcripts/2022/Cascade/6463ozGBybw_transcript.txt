foreign thank you for joining us today and Welcome to our presentation of cascade an open source containerized application deployment solution my name is Rona and I'm here today with my colleagues Ewan Natalie and Ann and we are very excited to introduce you to the application we worked on for the past several months as a fully remote team spread over three time zones so some background knowledge in the challenges related to deploying software is necessary for understanding Cascade so first on the agenda I'm going to walk you through the problem space next Ewan is going to explain the current Solutions then and will walk us through a demo of cascade after that Natalie will address the three final topics including explaining Cascades architecture what challenges did Cascade encounter while building the application and finally where should Cascade go from here please hold any questions till the end but we'll have a q a thank you everyone for coming and showing us all your support so what are some of the challenges of deploying software applications are reliant on a specific environment in order to run examples of environments include development staging and production possibly and probably more this requires multiple manual processes such as installing the necessary operating system as well as the tools and dependencies of the software configuring the environment manually for each application is time consuming and error prone which leads to longer deployment Cycles to show the struggle that different environments can have on deploying software here is a quote from the creator of Docker Solomon hikes you're going to test using Python 2.7 and then it's going to run on Python 3 in production and something weird will happen or you'll rely on the behavior of a certain version of an SSL library and another one will be installed you'll run your tests on Debian and production is on red hat and all sorts of weird things happen so clearly to reliably deploy software across various platforms and expect consistent Behavior developers needed a way to package application code with its various dependencies what is the solution containers containers are standardized software packages containing all of the necessary elements to run an application regardless of the underlying environment this reduces errors that arise when the application isn't compatible with the new environment containerized applications can deploy on multiple rating multiple operating systems consistently and can scale horizontally when demands increase this is because each container is a completely isolated unit independent of other containers additionally since a container isn't executable for the entire runtime environment it is faster to build and start thereby improving the software development life cycle once the application is packaged into a container and ready to run on any environment the next challenge is the deployment and management of these containers what if a container fails in the middle of the night how does that container get restarted what if there are multiple containers the complexity only grows the answer to this is contain our orchestration container orchestration manages the complexity of running containers on the cloud it allows developers to deploy scale and secure containers with minimal effort container orchestration automates the various tasks required in the container life cycle including provisioning networking configuration and scheduling as well as allocating resources across containers there are many container orchestration Solutions including open source kubernetes and Amazon elastic container service ECS although a container orchestration service like ECS automatically attempts to mitigate issues when it detects failures with the underlying host or containers insight into the system is still required to allow developers to determine when and where problems occur observability aims to answer both questions observability is the idea that your application and Hardware emit Telemetry data that can be used to pinpoint where a problem originated what is telemetry data well there's logs metrics and traces most applications produce logs that an engineer can reference to shed lights on how a system got to its current state for applications that have more than one container sending all logs to a centralized application could make the process of understanding logs less tedious metrics are numeric values that represent and describe the overall behavior of a component measured over time metrics give a big picture overview so that you can better identify which component of your application is malfunctioning traces represent a single request journey through different services within the entire application traces are composed of spans which encapsulate the start and finish timestamps of each step as well as preferences to related spans along the way observability tools allow for improved visibility improved alerting and better problem workflow management among these tools the cloud native Computing foundation's open Telemetry otel project aims to standardize collecting and transmitting Telemetry data the data is processed through the otel collector which is composed of three main components receivers to translate the data to a specified format processors to filter or retry data exporters to send data to various backends where data can be visualized and analyzed so far we have talked about the need for containerized applications to be deployed with observability so that developers have better insights to the parts of where their application is crashing we have talked about how using container orchestration tool reduces the manual timing of a person having to restart the application in the case of a crash containerizing the application instead of deploying directly on a virtual machine reduces dependency issues and ensures that deployment will be faster the last layer we have yet to talk about is the infrastructure that an application lives on let's look at what infrastructure is necessary for a containerized application to be deployed on the cloud with AWS as an example for AWS the virtual private cloud is a logically isolated section of the cloud in which applications run in a controlled private and secure manner in order to provide High availability redundancy is required on AWS that can be seen as two availability zones in case of a failure in one the other can still meet demand inside each availability zone are subnets which provide a range of IP addresses whether it's public or private depends on its ability to send outbound traffic directly to the internet and I will hand it off to Ewan who will now discuss current Solutions thanks Verona so far we've looked at what's needed for deploying a containerized application such as high availability Cloud infrastructure container orchestration and observability let's look at some of the current Solutions available to achieve this on AWS elastic container service or ECS the AWS Management console provides comprehensive tooling needed to deploy an application into a cloud Network Engineers interacting with the console are provided everything they need to manage their Cloud infrastructure with granular control but it comes with trade-offs first of all this process can be tedious it involves navigating through different interfaces for each service with each needing its own distinct steps for configuration and deployment also tear down will be a similar prolonged process as each resource usually has dependencies requiring the deletion of components in a specific order another downside of using the console is that often there isn't a record of exactly how the infrastructure and the services within were configured this makes it difficult to accurately duplicate as needed and can slow down the development workflow as we discussed manually deploying to the the AWS cloud with the console can be air prone and complex the next solution we'll be looking at aims to mitigate these issues by providing a central place for infrastructure configurations the purpose of infrastructure's code or IAC is to enable setting up a complete infrastructure by writing a script eliminating the manual effort of building an entire infrastructure piece by piece with an IAC tool users can automate the laborious tasks of managing Cloud resources let's look at terraform as an example terraform allows users to describe the complete infrastructure for a cloud provider in HCL as shown on the left this config file is all terraform needs to deploy and provision the resources onto a cloud provider such as AWS while IAC tools are useful for automating the process of infrastructure Creation in a more consistent and repeatable way learning planning and implementing this method can be time consuming there are solutions that attempt to automate using IAC tools AWS co-pilot is an opinionated ECS CLI that removes the need for the user to know anything about the underlying AWS network resources or container orchestration for application deployment instead copilot uses cloudformation when AWS IAC tool to deploy AWS Cloud resources also it provides those cloud formation files for the engineer to build up if they want to expand any resources each solution we just looked at incrementally automates part of the deployment process but there is no solution that also automates observability and that's why we built Cascade Cascade is an open source containerized application deployment solution Cascade shares similarities with AWS co-pilot in that it uses an IAC tool underneath to automate the deployment process but there are notable differences between the two solutions first Cascade provides an instrumentation agent and sets of logs and Traces by default also Cascade provides a GUI in which users can deploy their containers application and view logs and trace this with a few clicks finally unlike copilot Cascade uses terraform as an IAC tool let's look at how Cascade compares to all other current Solutions both co-pilot and Cascade generate scripts based on user input and automate deployment with an IAC tool similar to AWS Management console Cascade provides a GUI where users can plan deploy and Destroy resources by default Cascade boot infrastructure is equipped with a sidecar collector enabled for tracing of each container this means that users can start viewing traces on AWS x-ray as soon as the deployment is finished without setting it up themselves finally Cascade adds flexibility and options by generating terraform templates even though cloud formation is AWS in-house IEC tool terraform being open source and popular especially for the AWS AWS provider makes it a better option when it comes to support for new features for example terraform developers quickly jump in to add the functionality that they want generally terraform is known to support newly available resources months earlier than they are made available in cloud formation that being said Cascade is not for everyone Cascade was built for users who wish to deploy their containers applications onto esys fargate and be able to see logs and traces without manual effort Cascades approach enhances similar Solutions such as copilot by providing a GUI as well as built-in observability so that users don't have to navigate through the overwhelming AWS console next Ann will walk us through how to deploy using Cascade thanks you and now that we've seen how Cascade compares to other options let's run to how it works before using the Cascade GUI you must first install the Cascade agent and require it at the top of each application you want to instrument for traces you must also configure your AWS CLI credentials which will be used later now that you containerized application is instrumented you can visit the Cascade GUI which will take you through a three-step setup process for deployment when you first launch Cascade it detects whether your account has an existing Cascade built S3 bucket assuming you're a first-time user no bucket has been created yet so you will be taken to the create an application page where you input your application name after hitting submit Cascade will create that S3 bucket for you while you're directed to the next step the credentials page is where you enter your environment name as well as the AWS credentials you set up previously these credentials are needed so that the collector knows which AWS x-ray account to send traces to the third step is to add the information of the containerized applications you want to deploy you'll need a container name the image link the port and if your app requires them you can add any necessary environment variables these containers will be added to the generated terraform Json config files each will be deployed alongside a collector container which will collect and Export the applications Telemetry data to X-ray and cloudwatch finally you're directed to the application dashboard which consists of four main components the button menu the status bar the stack section and the container section once you see that your Apple your container has been added this means that the backend has generated the terraform config files and you're ready to deploy clicking deploy stock will trigger the deployment process and each AWS resource will be created for you the backend server sends messages to the front end to indicate the creation progress of each resource within the stacks section each AWS resource will be checked off when done and the deployment status will update to deployed once all resources have finished being created you'll be able to click visit site where you'll be taken to your deployed application you can also view logs and traces emitted from your application via the view Cloud logs and view x-ray traces buttons which will direct you to the AWS console to see the generated Json config files you can either view them directly on Cascade or visit your S3 bucket and download the files from there these files contain the infrastructure that Cascade builds and can be used as a starting point to build upon destroying the resources is just as easy as deploying them with a click of a button the teardown process begins and once it finishes the only thing remaining on your AWS account will be the S3 bucket now Natalie is going to take us through their design decisions and architecture that made this all happen thank you Ann how did we accomplish automating the deployment of users containerized applications with observability the first piece of our architecture is our graphical user interface or GUI that we just saw demoed in line with our main goal to make the deployment process simple we wanted to provide an intuitive GUI that serves as a central place for users to prepare deploy View and destroy their containerized applications the dashboard of the GUI is where users can view each step of the deployment process all in one view eliminating the need to visit various interfaces on AWS to achieve the same the next component is the Cascade agent the Cascade agent is an npm package for instrumenting users applications to allow distributed tracing it is compatible with AWS distro for open Telemetry or ADOT an upstream distribution of Oto which has stable support for tracing with various backend options the next component we will discuss is the Cascade back end the back end is responsible for creating a central storage for users as well as building and destroying AWS resources via terraform cdk where and what information is being stored all information is stored within the user's AWS account in an S3 bucket to ensure the security of user data Cascade created S3 buckets are configured to be only accessible by the user files seen in the S3 bucket include credentials for the Cascade agent a list of users deployed containers any Associated environment variables and terraform cdk generated Json files that are used to deploy the necessary resources onto AWS based on the necessary infrastructure components Rona discussed earlier Cascade creates a new VPC component for the user in their preferred region inside the region Cascade sets up two availability zones to add the redundancy necessary for high availability this ensures that even if a failure occurs in one availability Zone that the application running in the other availability Zone can still fulfill the demand Cascade hosts the user's application on ECS with fargate since we want the user to be able to visit the hosted application with one link from our GUI and add a layer of protection between the public internet and our application instances Cascade places and application load balancer between the application instances and the public internet the Cascade back end sets up all this infrastructure for the user with one API request what else does cascade's back end automate for the user to deploy an application to ECS Cascade creates a task definition service and cluster a task definition is a blueprint that includes each container's launch configuration such as a Docker image port mappings and environment files in order to spin up the containers an ECS service must instantiate the associated task definition the Cascade ECS service is configured to be placed behind the load balancer we created earlier the Cascade ECS cluster is linked to the aforementioned networking pieces including the VPC so that all containers within the cluster run on the same environment let's take a step back and look at the components of ECS without Cascade when using ECS with ec2 virtual machine instances what does the user need to manage to deploy a containerized application the user needs to start by providing their containerized application to create instances of your container via a task the user needs to create a task definition ECS abstracts the ECS control plane so the user has to deploy and manage an ECS agent to detail what containers are a part of what cluster the ECS agent also communicates any updates that need to be applied to the virtual machine ec2 instances through a worker node the user needs to set up and connect the network infrastructure and set up IEM policies for hosting their application the user does have full control over their data overall the underlying design decisions that went into Cascade have the user in mind focusing on decisions that remove underlying complexity and move that management away from the user to AWS Cascade takes advantage of ECS as an underlying container orchestration tool what is ECS doing for the user AWS manages the underlying ECS control plane and any foundation services that their user application communicates with some IEM policies are already set up and managed by AWS Cascade takes advantage of fargate a serverless option on top of ECS to remove some of the responsibility from the user the user does not need to provision resources for virtual machines or worry about the management of the ECS agent the Cascade backend automates the setup of ECS on fargate by generating the task definition based on the user's provided container information since Cascade creates the task definition we can include the adopt collector as a sidecar to the user's other containers this means that by providing the configuration logic within the task definition resource The Collector can receive traces from the application running in the main container and Export them to the X-ray backend for visualization Cascade then adds the necessary IEM policies for exporting traces and logs from the user application the automation of the task definition linking with the ADOT collector a creation of IAM rules for X-ray and cloudwatch and creating law groups tackles the observability problem of containerized applications lastly Cascade removes the need for the user to set up their own network infrastructure by providing the load balance infrastructure I talked about earlier meaning with Cascade the only pieces the user has to worry about managing are their containerized application and its Associated data now that we've looked at the components that make up Cascades architecture let's look at some of the challenges we encountered along the way while trying out other deployment options we realized that there was no automated solution that included distributed traces this problem was one of the initial sources of inspiration for the creation of cascade as a full containerized deployment solution to solve the lack of observability resources we investigated two ways to add traces through AWS we initially considered the X-ray Daemon as our instrument agent however this limits our Telemetry data to traces and the back into x-ray meaning users would have to instrument their applications again if they wanted other Telemetry data and or other observability back-ends we chose to support ADOT to give Cascades growth the flexibility to export to various back-ends and so we have the option to add metrics when they become more stable the first step for automating observability was creating a one-line command for instrumenting users applications without our instrument agent users looking to instrument their application to work with the ADOT collector have to install numerous dependencies and follow multiple steps to get the connection working to solve this we wrote a Cascade agent in JavaScript and published it as an npm package as you saw in the demo users simply require Cascade agent at the top of their code base so that their app is compatible with the ADOT collector we also connect the agent and The Collector by specifying them as a sidecar in the task definition so that upon deployment users can see traces on AWS x-ray foreign the next challenge we encountered was dealing with real-time data while connecting the front and the back end we needed a unidirectional communication that alerts the front end of AWS resource creation so that it can be displayed real time on our GUI we first considered making requests to AWS via the AWS SDK using a polling approach to get each resources creation status however this would have been resource intensive for the server it would require making several requests at Short intervals to stimulate near real-time updates so instead of having our application waste resources on requests we considered web hooks but this method would have required us to set a webhook for each AWS resource and would have had an unnecessary complexity to our back end since our backend was already outputting logs of the creation and destruction process we decided to leverage that data via server sent events we created an Event Source instance on the client side to persist a connection between the client and server during the build and tear down process this enabled the server to send the text data to the client as it's generated with the received data the client matches two keywords the resource name and terraform action and displays real-time updates on the GUI although Cascade fulfills the need to quickly deploy containerized applications there's always more that can be added to an open source project currently Cascade creates a new VPC and subnets when a user first deploys their infrastructure using our GUI in the future we would like to add functionality to support the user's already existing VPC and subnets another Improvement would be adding support for uploading a terraform config file so that users can redeploy Stacks with their custom configurations finally we plan to provide an interactive infrastructure map on the Cascade dashboard to help users understand the relationships between resources thank you all again for coming and supporting us today we are now going to open up for Q a so we have a question just to clarify is the Json generated by Cascade or by terraform or another service um the ter terraform is generated by a terraform cdk um that we invoke using our backend API and I guess just to give a little more clarity on that is when they input information in the front end um that information has to be given to the terraform cdk and to do that the back end saves that information to S3 and then um terraform the terraform cdk request is made and terraform has to get that information back from the S3 bucket okay we have another question was there another option to make Cascade work without using a sidecar approach or would it be too complicated there was an option to this is still it's a different variation of the sidecar approach um but you could use a service sidecar instead of um a collector sidecar like within the same task definition so um but the perk of using it within the task definition is that you can get more metadata from your AWS Services um whereas you lose that metadata whenever it's in the service Sidecar and it would have been a little bit more complex because um if it was running in a different service then we would have to configure something called service Discovery so that the applications would be able to talk to each other okay um what was your favorite challenge to overcome during development um okay so my favorite challenge to overcome I think was the API part and connecting that terraform cdk to the front end it was challenging and um fun because it brought the team together because we all had to talk about what we were doing what we needed the API to do what information we didn't have like the terraform cdk didn't have the same information as the front end and how are we going to get that information there and like we it led to us creating like a middleware to fix it and it was pretty fun my favorite part of the project was delving into AWS it was my first experience doing that I was pretty intimidated going into it um it kind of felt like walking into Home Depot and you know you can get a nail but can you build a house and actually it was every bit as intimidating as it looked but at least I was forced to go in there and start looking and understanding how things work and I really appreciated that process and um we have another question similar to that one is what did you enjoy most I think uh the part I enjoyed the most was um seeing the front end and like it work like when you press the deploy button and you see all the spinning wheels come up and then at the end you log into your AWS account and it's there like that feels really good so I think that was the most enjoyable part yeah I think it was really fun to just um start with something small and then um like with just a Docker image and then building it up and bringing it to like different environments and seeing like each piece start working and really coming together so we have a question uh why do you call it Cascade so initially our project was gonna focus on traces and then there's something called like a kind of like a waterfall graph for each of the spans I think we saw it earlier when rono's talking and then it kind of looked like a a waterfall so we end up just calling it Cascade and then we kept it although it started to centralize on um deployment because we have something called Stacks which is basically um templates for the deployment of each AWS resource I think that might be all the questions thank you guys so much for coming yeah thank you everyone thank you 