foreign thank you for coming to our presentation on BART an open source session replay tool for viewing and analyzing user sessions we are excited to show you what we have been working on over these last few months first I will introduce the team we develop Bart as a fully remote team in three different time zones Morrison is in Vancouver Nino is in Boston Jean is in Tulsa and I am Aaron and I am in Austin today is going to be broken up into six sections first we will explore the problem domain Bart is intended to address second we will answer the question what is Bard and demonstrate its use third we will describe Bart's architecture fourth we will talk about the major design decisions we made while developing bard fifth we will describe some of the challenges we ran into during the development process and lastly we will discuss the different deployment options for Bart let's start by exploring the problem we set out to help solve before we can really dig into the problem Barton tends to solve there are a few Concepts you will need to understand first a user session is basically a sustained interaction of a user with a web application key idea here is that a session begins when the user starts interacting with an application and ends when a user leaves the page or after a certain amount of inactivity second a user use user conversion is an action the developers want a user to complete this is you usually La The Last Action on a path of events that developers designed to lead to this conversion examples of these are things like subscribing to an email or purchasing a product developers focus a lot on the user's experience and how they interact with an application they create paths for the user to follow hopefully ending in some sort of conversion but how developers design the app and how it is actually used and experienced by the users can be drastically different developers have very little insight into how the user experiences their application therefore it can be difficult to discover the reasons users do not convert was there a broken button did the page take too long to load or did the website crash it is important to find out where and why users are strained from the conversion path to discover some of these issues we need to gather not only only quantitative data like conversion rate and time to load but also qualitative data like videos and images to gain the full context of a user's session specifically what caused users or groups of users to drop off the conversion pack while looking at this problem we will use a simple e-commerce business better brew a site that focuses on specialty coffee equipment their user interface is shown here this is an example of a company that may be trying to improve their conversion rate on purchasing products so let's look at some possible solutions to this parabola the first possible solution is to use heat Maps this is a visual representation of how often users Click on each element on a website's user interface this can be useful for a website like better Brew it's a great tool to gain insight into where the user's attention is what are they focusing on and clicking on like add to cart for better Brew it looks like many people added the Chemex Coffee Maker to their cart as seen on the heat map to the right heat Maps also can show what users are missing on the application but heat Maps do not show the full context of why users are or are not following the intended conversion path we can see what users are clicking which is useful but it cannot show other issues with your application maybe the page is taking forever to load maybe an input box is broken causing an error the developers of better Brew have no insight into these problems also it lacks the ability to look at individual sessions usually heat Maps only show an aggregate of all the users sessions so while this is good at giving an overview of qualitative data for users interactions it lacks content and individual sessions that would allow for conversion analysis to see where users are leaving the conversion path it's another possible solution to our problem is to collect and analyze metrics metrics are excellent tools at Gathering high level information about a website's performance these metrics can show data about site traffic top viewed pages and even conversion rate on the left we can see a dashboard used to collect metrics for better as you can see the data is in aggregate form which is great for the overall health of an application but it's too general for our problem domain these metrics lack meaningful context on individual user interactions and at best they only offer a percentage of successful conversions so you cannot see why individuals are failing to convert for example metrics could tell you that your website has fast load times but they cannot tell you ninety percent of your users don't purchase your product because after the adding adding the item to their cart they cannot find the checkout button this leads us to a solution that helps give context through qualitative and quantitative data session replay session replay is a reproduction of a user session exactly how the user experienced it you can think of this as a recording of a user's interactions with your application while while also Gathering some metrics about each session session replay gives us a lot of the quantitative and qualitative data that we are looking for when investigating why users leave a conversion path you using session replay gives you the ability to watch exactly how the user interacted with a site and shows any difficulties experienced when using the application as the session replay on the right demonstrates also session replay can gather some of the key metrics that we could need when looking at successful versus unsuccessful conversions things like error count conversion rate and so on foreign replay can answer many of the questions needed to analyze a user experience and conversion success first session replay can show how a specific user interacts with an application second session replay shows what parts of the application did not work well for users this can guide changes to the application to help people stay on the conversion path lastly and most importantly for our problem domain it is a great way to deduce why users do not convert but session replay cannot answer all of the questions for example session replay will not show the overall health of the application as a full metric Gathering tool like Prometheus with grafana can it also will not show why crashes happen why Pages load slowly or why a button on a form is not working also session replay does not help with aggregate data it is more useful when looking at individual users and their sessions let's look at some existing session replay tools and their benefits and drawbacks first you have Enterprise application like post hog or open replay these applications will record this session and provide many features like metrics heat maps and conversion analysis this gives a lot more data for the user but can be overwhelming when you're specifically looking at conversions also with these two tools you lose ownership of your data and are required to pay for the storage of that data another option is to use a DIY solution like our our web this DIY solution allows for the recording and replaying of sessions but lacks any back end to store and parse the session metadata or events using this tool requires a lot of effort time and expertise to build the back end and storage for this to be useful but a positive of this approach is the flexibility provided by RR web developers can design the back end and storage in any way they please this is actually what we did at Bart we wrapped over our web and implemented a back end with conversion analysis Bard focuses on Simplicity it allows you to keep ownership of your data deploy easily and do quick simple conversion analysis this focus on Simplicity allows barred users to develop quick deploy quickly and view conversion data without a lot of complexity like a DIY option and without giving up data ownership as an Enterprise application requires now I'm going to hand it off to Gene to go more in depth about Bard and its use thank you Aaron so next we're going to dive a bit deeper into what Bard is so Bart it's an open source session replay and conversion analysis tool it enables the review and Analysis of user sessions an application owner can filter a list of recorded sessions view sessions as if they were videos and perform conversion analysis using conversion funnels our user interface lists available session recordings which can be filtered by date duration application name and if the session contains errors a user can select a session to view and play back a specific user's experience here we see a user shopping on the better Brew app and checking their cart so while a session is playing the progress bar highlights custom events and application errors logged to the console some sessions can be lengthy so this lets a Bard user quickly find interesting moments in a user session although what you just saw played like a video recording it's really a stream of timestamp data that data details Dom mutations and browser events generated by a user's interaction with a web application when a session is replayed this data is used to reconstruct the initial Dom in the player then play out the recorded mutations and events just like the user originally experienced each data point recorded is transformed into an event here we see an example Mouse click event each event has a type indicating the event category a timestamp of when the event was recorded and the actual event data for a mouse click event we see an Event Source type the ID is the Dom element the event occurred on and the coordinates for the mouse's location along with session replay we also implemented conversion analysis we see we use funnels to analyze sessions for user conversion so as a reminder a user conversion is when a user completes unintended and beneficial action on a web application like placing an order so a funnel is a set of predefined steps that results in a conversion user sessions are evaluated to see which steps they complete and where along the conversion path they dropped for example we can check to see what percentage of users visiting a web store end up completing a purchase here we start with 500 user sessions that visit the web store we notice only 50 of those users add an item to their cart we see that 30 percent of the original 500 user sessions move on to the checkout process and finally twenty percent or a hundred user sessions complete an order this funnel gives us a quick overview of where users drop off along the conversion path we might be surprised to find out that 50 users started the checkout process but never placed their order so these funnels they're easy to create in The Bard user interface so in Bard a funnel is made up of three components a funnel name session filters and an event sequence the session filters will Define the subset of sessions analyzed against the event sequence session filters include the application name session length and if the session has errors after defining the session filters a Bard user can specify the conversion funnels event sequence here we Define the steps for purchasing a specific product a Chemex Coffee Brewer the first step is adding the Chemex to a user's cart the second is clicking the place order button and the third step is our conversion indicating the Chemex was included in the order so let's move on to viewing an existing funnel a funnel's results will show how many user sessions entered how many users completed or dropped at each step and the completion percentage of each step based on the initial number of sessions here we see a funnel for user sessions of the better Brew app with the user conversion defined as completing an order looking at the last step we can see that 55 of sessions that entered the funnel successfully placed an order interestingly three of the nine sessions that completed step three clicking the checkout button did not end up placing an order Bard lets you view these user sessions from the funnel page giving context to why users are having trouble placing order so if we select one of the dropped user sessions we can see the checkout form will go blank on the user twice after he fills it out this user most likely got irritated and took their business elsewhere not telling anyone of the issues they faced putting this session in the context of the conversion funnel gives a quick way for an application owner to find where issues occur on their web applications this can be a great starting point for debugging and the user doesn't need to be tracked down to gain context for the problem now that we know a little bit more about Bard let's go level lower and let's take a look at the architecture so Bard comprises eight components that roughly divide into three groups collection data processing and storage and analyze and replay let's take a look at what each group entails we'll start with collection which only has one component the agent the agent is contained inside an npm package named Bard RR which stands for Bard record and replace we'll talk later about how to instrument a web application with this package Bard R is responsible for collecting session data from web applications the npm package contains an agent that monitors the browser and collects the data needed to reconstruct user sessions then send that data back to Bard's backend some of the things the agent records are Dom mutations like adding or removing elements in user events like Mouse movement clicks and scrolling the agent wraps around RR web which functions as the session recording engine and the agent expands upon RR web to enable features like data authentication session persistence and custom event tracking so an agent instance will then accompany your web application with session data being sent back to the agent API this brings us to the data processing and storage part of Bard's system the data processing and storage group contains five components this group of components is dedicated to receiving data from web applications processing that data so it's ready for replay and conversion analysis and storing it so let's look at how each component contributes to these objectives the first is the agent API which receives and processes the data sent from instances of the agent the agent API is responsible for processing tasks like authenticating data identifying conversion events tracking session metadata and catching all event data are our web recorded in the agent the first component the agent API sends data to is a rabbitmq message queue the individual event data points for session replay and conversion analysis are sent here rabbitmq supports a high throughput of event data and complements our column oriented database clickhouse which it integrates directly with our clickhouse database stores event data from our our web conversion event data and metadata for finalized sessions clickhouse is an analytical column oriented database and it's optimized to handle complex queries over really large volumes of data in addition to clickhouse we have a postgres database that stores metadata for active user sessions and the data that defines all of a Bard user's conversion funnels this data is stored separately in postgres because we treat it as mutable whereas the data in our clickhouse database is treated as immutable the final component of the data processing and storage group is the session Ender it's a Cron job that scans the active user sessions in the postgres database and finalizes any sessions based on the time since the last recorded activity when the session Ender detects an expired session that sessions data is pulled out of postgres it moved into its final resting place inside quick house later when we discuss implementation challenges Marson will unpack the process we went through when determining how we could use end user session the third and final group of components in Bard's architecture is analyze and replay this group is made up of two components the first component is the replayer API which connects Bard's user interface to The Click house and postgres databases this API handles listing user sessions retrieving session replay data and all crud operations for conversion funnels the second component is the replayer user interface which is built with react and it enables barred users to easily filter through sessions replay sessions Define and view conversion funnels so now that we've looked at each component of Bard's architecture let's put it back together we started with three primary groups of responsibility collection data processing and storage and analyze and replay and we discussed each component of that system and how it comes together to create Bart so next Anthony is going to talk about the design decisions we made while building bard about it thanks Gene so Gene just gave a high level overview of Bard's architecture and now I'm going to talk a little bit about why we built bar the way we did and what it really all comes down to is data management and how we were going to build a back end that could handle all the data coming in from our our web first thing to know is that Bard works with three different data types and each data type has unique characteristics that make working with them challenging I'm going to start my section off by giving an overview of the three different kinds of data and then I'll move into how we built our back end in order to handle them first we have session data this is the metadata we attach to each user session so that Bard users can quickly find the ones they're interested in as mentioned earlier this metadata has information like the length of the session the app it came from and if it has any errors the central challenge with the session metadata was that it has two states mutable and immutable when a session's active it's mutable and we want to be able to update it whenever it updates needed but when a sessions ended we don't ever want to update it again and it becomes immutable and last this kind of data has a fairly low volume next up we've got funnel data this data corresponds to the funnels that Bard users create in order to perform conversion analysis funnels always need to be mutable we wanted barred users to be able to create and edit their funnels anytime they wanted to potentially iterating over a funnel until they found exactly what they needed and lastly this kind of data would also come in at a low volume finally we have event data this corresponds to the data that Gene showed earlier that corresponds to a user click it's the data coming from RR web that contains information about what a user is actually doing within the browser it's used for conversion analysis and it's also used in replaying user sessions this data is always immutable once we have an event we would never want to change it we would just want to be able to query and return it but most importantly event data comes in at a very high volume RR web records about 50 events per second so that means that if Bard is tracking just 200 users we're receiving 10 000 events per second on our backend so if we weren't careful our application was going to end up like this now this is a quick overview of the three kinds of data that we're working with and what's important to note is that each one has a unique combination of mutability and volume we needed to pick a starting place so we chose event data because it's high volume represented the most significant challenge out of all the three types and this kicked off a database Goldilocks process for our team we started trying out different databases until we found one that was just what we needed well but we needed a database that could handle a high amount of Rights and also handle complicated queries involving a large amount of data in order to do conversion analysis Marson will take a deep dive into how conversion analysis Works later on but for now I'm going to focus on how we built our back end in order to make it happen first we looked at postgres which is a typical relational database but we quickly turned it down it couldn't handle the throughput of rights that we needed in order to support 10 000 events per second it would have gotten swamped by the volume of event data so we started looking at nosql databases instead document databases like were more attractive because they had higher right through output than postgres did however the document data structure wasn't a great fit for conversion analysis mango made it really easy for us to get our data into the database but it would have gotten tangled up when we actually needed to use it time series databases were also attractive to us because they also had a much higher right throughput than postgres did however these databases are optimized so that the most recently collected data is the most available accordingly older data is compressed to save space here's a quick chart that illustrates how efficient compression is in time scale which is a popular time series database we start with 34 terabytes of data and compress it to two or three terabytes of data and that's great for saving space but the problem is that the compressed data is less available and you can't query it immediately now that trade-off wasn't acceptable for our team because we wanted barred users to be able to query data over all time not just the data that was collected recently so finally we turn to column oriented databases which ended up being what we needed these take the familiar row oriented schema of relational databases and flip it on its head here we've got two members of The Beatles represented in a familiar row oriented schema and here's the same data represented in a column schema the key difference here is that row oriented schemas emphasize each row as a single and complete entity of information this means that the columns are all connected in a in a row oriented schema you can't find out that John Lennon died in 1980 without also finding out he was born in 1940 in England to contrast this column oriented databases emphasize the individual pieces of data stored in the columns as complete entities of information so that means you can find out that John Lennon died in 1980 without needing to read all of the other information stored about him in the database this change of schema allows column oriented databases to have a high write throughput and also support complex queries we looked at two column oriented databases Cassandra and clickhouse but even though these were both column oriented databases it soon became really clear that we weren't comparing Apples to Apples Cassandra is a transactional database when you think of a database you're probably thinking of a transactional one this just means that it's built to support many simultaneous operations transactional databases are able to handle many users Reading Writing and updating the data within them without causing any problems but the trade-off is that these databases are not optimized for complex queries involving large data sets in contrast to this clickhouse is an analytical database this means that it is optimized for complex queries over large data sets but it doesn't support many simultaneous operations out of the box click house only supports a hundred to a thousand simultaneous queries so if we were to send all of our event data to clickhouse by itself it would be overwhelmed very quickly to recap we needed a database that could handle complicated queries in a high write throughput and it looked like out of the two column oriented options we explored we weren't going to find exactly what we needed at least until a little rabbit entered from stage left it turns out that clickhouse integrates with the rabbitmq message queue message cues are components that are built to ingest data at tremendously High rates since clickhouse integrated with the queue we could send our fire hose of event data to the queue instead of directly to the database the queue would be able to handle the high volume of data and it would come to rest in clickhouse without any issues so by itself clickhouse didn't give us what we needed but when we integrated clickhouse with rabbitmq we got exactly what we needed rabbitmq would be able to handle the high right throughput necessary to work with our event data and click House's native functionality made it great for working with complicated queries over that very large amount of event data we were receiving as you can see when we have rabbitmq clickhouse is able to handle the high volume of a vet data from multiple recorded applications so this solved our problems for event data but we still needed to think about session and funnel data and we couldn't just store that in Click house 2. this is because as an analytical database clickhouse isn't optimized for updating data within it it's an expensive operation that isn't intended to be done often since session of final data needed to be updated a lot we couldn't store them in Click house because the operation was too heavy luckily these data types were low volume which made them much simpler to work with we decided to use postgres to handle these mutable data types session and funnel data both fit well within the relational model and had a much lower volume than our event data did funnel data always lives at postgres because it's always mutable session metadata lives in postgres while a session is active but once a session expires and the metadata becomes immutable the metadata is moved into click house Marson will discuss this process in detail shortly but for now the important idea is that the low volume Obsession and funnel data made them much easier to work with than our event data so where postgres would have gotten swamped by a high volume of event data it's able to surf the wave perfectly for funnel data in active session data so now I'm going to pass things off to Marcin and he's going to talk about some of the challenges we faced once we began implementing Bard's key features it's him okay on to implementation challenges the first significant implementation challenge we encountered was organizing the data generated by our web into sessions to accomplish this we decided to generate a universally unique identifier for each session and tag every piece of ROM data with the appropriate one the main challenge with this approach was persisting a session ID through Page reloads and navigation to different pages of the application fortunately the session storage browser API presented a nearly ready-made solution to this problem this mechanism provides a key value store for a browser tab that persists while the user keeps the tab open and stays at the same origin that is the same combination of steam hostname and port by choosing to Define our sessions as ending when the user either navigates to a different origin closes the tab or is inactive for some configurable amount of time we're able to generate session IDs appropriately as follows whenever our web generates data we check whether or not session storage contains a session ID if it does we tag the r web data with a session ID set a timeout to replace the existing session ID with a new one after a configurable amount of time and clear the previous timeout if one exists if session storage does not contain a session ID then we generate one put it in session storage and proceed just as in the other case well this process took care of starting and ending sessions on the client-side agent that left the server-side components of barred with no well-defined way of doing the same initially we considered having the agent communicate the start and end of a session to our backend by means of explicit HTTP requests as Illustrated here however we ultimately rejected this idea due to inherent Network on reliability there would always be a possibility that these requests would not be delivered and hence the and hence that a session would never be marked as started or ended instead we decided to start and end sessions independently of the agent specifically we decided to have the agent API defined the start of a session as that session's earliest event and we created a separate server-side component called the session enter to take care of ending sessions as discussed earlier bars stores active session metadata in a postgres database and ended session metadata in a clickhouse database as such to Performance tasks the session enter must first identify expired sessions and then move each expired session from postgres to clickhouse identifying expired sessions is straightforward the active session metadata stored in postgres includes a session's maximum idle time and the timestamp of the session's most recent event if it's now past the most recent event time plus the maximum idle time then the session has expired moving the expired sessions is more involved since a move consists of transactions on separate data stores namely a deletion from postgres and an insertion into clickhouse the atomicity of the move as a whole is not automatically guaranteed as Illustrated here our implementation does not ensure atomicity but does conduct the move in such a way that there are no adverse consequences if the move is not atomic specifically expired sessions are moved as follows for each expired session we check if the session is already in Clickers if it is then we go ahead and delete the session from postgres otherwise we attempt to insert the session into clickhouse only if the insertion is successful do we proceed to delete the session from postgres in the case of an unsuccessful insertion we simply continue on to the next expired session as such the only way for a move to not be Atomic is for the insertion into clickhouse to be successful and the deletion from postgres to be unsuccessful the harmless side effect of this scenario is a stray row and postgres that will be safely eliminated during a future run of the session our next significant challenge was implementing conversion analysis to tackle this we decided to use funnels which required us to First determine an appropriate structure for a funnel and then figure out how to execute it first let's talk about structure as previously mentioned each Bard funnel has three components a name session filters and an event sequence let's create an example funnel together recall the online store better brew that we saw earlier in the presentation say that we'd like to perform conversion analysis on the store's previous week's sales the name is self-explanatory the session filters Define the subset of sessions that we'd like to analyze we're interested in sessions from better Brew that occurred last week from November 1st to 7th the event sequence defines the conversion path in our case the conversion is a sale and the conversion path consists of a click on add to cart followed by a click on checkout followed by a custom event called order placed and that's our funnel it's important to note here that there's no limit on the number of events in the event sequence supporting this functionality proved to be the central challenge in implementing funnel execution to execute a funnel Bart creates and executes a sequence of queries first Bart creates and executes a query based on the session filters to obtain the desired subset of sessions for analysis then for each event in the event sequence Bart creates and executes a query that uses the results of the previous query as its base so for example to get the sessions that made it through all three events in the event sequence Bard would create and execute a sequence of queries corresponding to this pseudo query once we add the code to dynamically create and execute the appropriate sequence of queries we could easily execute any file finally a security measure we incorporated into Bard was authentication of requests sent to the agent API the agent API exposes two endpoints tokens and events when an agent instance is started it first makes a get request to the token's endpoint an in response receives assigned Json web token the agent then includes this token in the headers of all subsequent requests made to the events endpoint which handles processing of event data the agent API authenticates all requests made to this endpoint before it takes any other action in this way we protect our application from processing illegitimate requests which at work which at best would waste resources and at worst expose the application to malicious actions and that wraps things up for implementation challenges if you like what you've heard and would like to take part out for a spin just clone the deploy repo and then proceed with one of two options the first option is to run Docker compose up which will create a local containerized version of each of bars components along with a network for them to communicate the second option is to run a script that will do essentially the same thing except on Amazon web services using elastic container service for orchestration our gate for management of ec2 instances and elastic file system for data persistence regardless of which option you choose the final step is to instrument your application that is to insert Bard's agent into your front-end application code to do so first install The Bard RR npm package as a dependency then in the entry point of your application import the agent class from Bard RR create an Asian instance and invoke its start method the start method accepts an object with three properties the name of the application being instrumented the origin at which the agent API is listening and the amount of inactivity in milliseconds that causes the session to end and that's all we've got thank you for attending our presentation and we hope that you enjoyed learning about bart well uh we'll take a few minutes here to see if any questions uh filter in and um right I see one question in the chat is the source available on GitHub yep it is um I believe the repo is bar Dash RR yeah you can take a look look at all of it up there now I'm dropping a URL to the organization on GitHub now all of our code for each of the components is up there awesome thanks Nino okay we got uh one question here in the chat how did you come up with this idea if you have mentioned this already my bad as I joined a few minutes later no problem um does any uh anyone want to take that or I guess uh I can take a stab at it and uh my teammates can can follow up if they have anything to add I think the way we came across it was uh we were inspired by another uh Capstone project uh retrospect um that uh kind of took a look at this issue but maybe not not as at that problem space but not as uh as deeply as we ended up going into it um so yeah that inspired us to kind of take a deeper dive especially into the conversion analysis part and uh yeah it just seemed kind of like a cool uh fun thing it's really cool to see your session replayed after after you do it so that kind of uh provided us with some inspiration as well um yeah and you know go ahead yeah just to kind of build on what you were saying we started off by looking at like at the highest level observability um between like typically when you think of observability you'd think of interactions between different microservices on the back end but then we started to foray into more of a front end type of observability and that's where we kind of fell into session replay and conversion analysis and it was something that we all found interesting so we dove into it from there awesome thanks Nino all right we've got another question in the Q a this time uh from Jason is analysis only performed on predefined funnel sequences or is there some sort of exploratory feature that searches all possible permutations of files uh so yeah the the analysis is only performed on redefined funnel sequences but um yeah there's not kind of any kind of AI thing that makes funnels for you but maybe um yeah another another aspect is all all the events are stored there so if you want to create a funnel like after the fact it'll have all all previous um session recording data available for analysis it's not like uh it's only once you create the funnel that that the events pertaining to that funnel um are collected uh Nino did you say you wanted to answer that one a bit more or I was kind of just playing around with the the Q a functionality I think you hit the nail on the head with that one all right uh yeah sorry uh guys just jumped in if I'm uh taking all the questions here if you want to say something don't be shy um got a question the Q a from Michael I'd love to hear more about how you guys came across this problem domain was there team interest in conversion data uh that's kind of similar to the other question of how we we came across this topic um unless anyone has anything further to add okay I'll just say we answered that one already sorry Michael um got another question uh what was the most challenging problem to solve D I wonder if this is one where like all the teammates would have a different perspective um I mean for me it was kind of for me the most challenging part was what I talked about just the process of juggling databases and designing the system until we found something that we think would work that we thought would work just because that involved learning how to at least that involved learning how to use so many different new tools and new kinds of databases just because in order to figure out whether or not something is going to work or not you know you want to dive into play with it a little bit so for me personally that part was the most challenging I think another big challenge we had was dynamically making these uh funnels for the SQL queries um on click house that was quite difficult for us for a while took us a day or two yep uh for me similarly um going through the process of finding a database that was supported that would behave the way we wanted to with the various types of data um that that took a good chunk of work and us going out and uh setting up a lot of different kinds of databases and testing them to really understand that and it uh was really educational but at the same time it was a challenge to kind of parse different types of documentation and understand different ways these databases functioned yeah I agree with what uh Gina Nino said it's easy just to kind of go and pick any old database and that'll do the trick when you're just recording your own sessions and testing things out but uh once you get to scale it's a it's a different story and it's good to kind of anticipate that and built the application to be able to handle that uh from uh from the ground up um got another question the Q a how did you implement the front end so to talk about the front end we used the rat react uh to build it so it's JavaScript with the react framework and then we use the material UI library that goes well with react to uh be able to use those pre-book components which can have some built-in features that you would have to add manually yourself if you weren't using that Library yeah so just to build off of what Gene was saying one of the components that was a huge Time Saver for us on the front end was that uh material UI had a table component that automatically built in pagination and also the ability to sort by the different columns in the table so just having material UI was a pretty pretty awesome tool for us to use all right next question from Joey were there any features of session replay tools that you would have loved to implement given more time I think one of them was a click rage so you can actually catch when people are clicking many times very quickly and you can mark that on the timeline of the video so you can see that um that would have been great to add into our uh project yeah one other thing that we would have liked to implement too is just a bit more flexibility when you're creating funnels so for example the only kind of logic that you can have between different events in an event sequence with Bard right now is and logic so it's a user clicked on the checkout button and then clicked on the place order button but it would have been cool to have a bit more complicated logic there so you could do things like user clicked on the checkout button and then did not click on the payment button or something like that yeah one uh one uh feature that we came across when looking at um kind of Enterprise applications that do session replay is this thing where you can view live sessions in progress and you can also um uh uh kind of uh request uh control from the person on the sessions so that you can actually help them in real time that was pretty cool and creepy but uh it was kind of not yeah we decided not not to go down that route and focus more on conversion analysis uh I've got a question is switching between session replay tools and easy tasks is there a learning curve or is there a lot of overlap between other options foreign [Music] I guess one they're all you know pretty similar Sometimes some personally I found some to be a little bit overwhelming like there's so many different bells and whistles that you could play with that it's kind of maybe detract from focusing on the on the most meaty and like important um features [Music] um if you are not self-hosting or I guess even if you are self-hosting it's probably tricky to move all of your data from one provider to another I imagine they probably don't make that particularly easy just to protect their own interests um yeah so for using it I think maybe a little bit overwhelming but eventually it's not a big deal but yeah for if you pick one and then want to move your data over that might be a bit tricky um another question from Michael may have covered this but how do you convert raw session data into a replayable video so I'd be happy to talk about this a little bit so one of the first uh event data points that is recorded on a web application is an entire snapshot of the document object model and that is sent back as an initial snapshot of the entire structure of the web page and uh then when we're recording things like Dom mutations or browser events that are initiated by users usually or any JavaScript code in the web application uh those things are recorded with those timestamps so when you see that read player that looks like a video player it's actually you could think of a second Dom structure that's sitting inside of that player and when the replayer gets those events back it is actually um doing those events it's replaying those events uh based on their time stamps to that Dom structure so if you went into the replay and you looked at the HTML of barge user interface you would notice that it's called an iframe element and within that iframe is an entire separate Dom structure that is uh the recorded application yeah pretty neat and um one last question in the chat here any browser apis you wish you had to make building this easier I have one thing um there doesn't seem to be a good browser event for detecting the the tab being closed which kind of uh surprised me um because initially we were thinking that we'd get the uh the client-side agent to Signal the end of a session um to the back end by sending an explicit request uh and we wanted to to do that when a tab closed obviously but it wasn't there wasn't any like reliable event for that uh ultimately it wasn't a problem because we ended up deciding to end sessions a different way but um yeah kind of surprising that there's not an event for that and I I can imagine being handy in uh other situations all right I think that uh wraps up all the questions uh thanks so much everyone for attending we hope you enjoyed our presentation learning about Barb thanks everybody thank you 