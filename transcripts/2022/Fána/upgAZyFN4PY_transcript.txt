welcome everybody thank you for joining us today on this wonderful thursday today's presentation we will be introducing fauna fauna is an open source feature flag management platform that allows developers to test new features in production confidently first i'd like to introduce the team behind fauna urim rob juan and myself audrey and we've worked extremely hard on this project and are quite proud of what we've been able to accomplish these last few months and you'll get to hear from each of them throughout the presentation so before talking about fauna we're going to talk about the problem space that inspired fauna where fauna fits in as a solution to the problem and the engineering decisions we made while building that application we'll also have a brief discussion about some potential future work at the end so first i'll dive into the problem exploration and talk some of the about some of the challenges of testing new features in production and why testing is important for building confidence in future releases when releasing a new feature to end users developers want to be able to release that feature with confidence this means bug-free with a minimal risk of unexpected behavior one way to increase confidence is to use automated testing suites in a deployment pipeline testing helps verify that an application as a whole behaves as intended this is especially crucial as new code is being contributed the code base on potentially a daily basis automated testing can help prevent regressions or break things that previously worked another way that developers can increase confidence when releasing a new feature is to use staging environments staging environments are meant to be a near replica of production environments to ensure quality under a production-like system the main benefit of testing and staging is that if something breaks doesn't affect any real users so in summary testing is the foundation for developers to be able to move fast with confidence despite extensive testing in pre-production environments things can still go wrong in production why is that well testing is about reducing uncertainty by checking for your known failures your path to failures or predictable failures these are your known unknowns things that you can write tests for what you can't test for are your unknown unknowns race conditions network unreliability and most importantly it's really hard to predict how your users may behave so the combination of production system infrastructure and unpredictable user behavior makes it extremely difficult to test and guard against all possibilities in a controlled environment like staging so here i have a quote from charity majors she's the co-founder of honeycomb and she thumbs up this idea pretty well she says once you deploy a feature you aren't testing code anymore you're testing systems a system's resilience is not defined by lack of errors it's defined by its ability to survive many many many errors so if testing in pre-production systems doesn't guarantee bug-free feature releases and errors and bugs and production systems are inevitable how can devs build confidence in their applications resilience in other words how can developers responsibly test a new feature in production with real users so developers need a way to minimize the impact on end users if something goes wrong there are three key strategies to enable this first release new code to a small group of users first and then isolate that feature causing the issue and third roll back the application to a stable state let's dissect this first criteria a little bit further by releasing a feature to a small group of users first then gradually increasing the number of users exposed over time developers can limit the total number of users impacted if any problems arise to take this strategy a step further developers can target specific types of users starting with the most bug tolerant subset of the user base examples of this include users internal to the organization also known as dog fooding and then releasing the feature to beta users or early adopters and eventually all users to make this idea of testing production a little bit more concrete i'll briefly talk about a case study the engineering team at vmware a cloud computing company wanted to rewrite an important highly visible feature of one of their applications their team had three goals in mind for deployment first investigate real user issues in production second evaluate feedback from those users and third limits the impact of any bugs on the user base so to achieve this vmware first released that feature to only the developer team to test it in the live production environment next they released the feature to external beta users they developed and tested new iterations of the feature based on those users feedback only after several iteration cycles did they finally release it to the general audience so vmware's engineering team used a progressive rollout strategy to target specific users to facilitate testing and production using this strategy they were able to meet their goals and deploy the new feature with confidence now that we've discussed how we can test in production responsibly we'll explore two common approaches to accomplish this multiple deployments and feature flags a multiple deployment strategy also known as canary deployments utilizes a second production environment that hosts the application with the new feature and uses a load balancer to route traffic between those two production versions so in this visual we have about 95 percent of users being brought to prod version one and five percent of users being ready to prod version two or the environment that's hosting that experimental or new feature so what are some of the benefits of this strategy first a cleaner code base routing logic is abstracted to the load balancer so each environment is solely responsible for serving one version of the application this ensures new versions won't contain code references to older versions next there's no downtime to users if a rollback is needed developers can reroute all traffic from prod version 2 back to the stable quad version 1. last developers can configure the routing logic in the application load balancer to route users to specific experiences so let's consider some of the trade-offs developers cannot selectively roll back a specific feature in a given deployment for example if three new features are included in the prod version 2 deployment but one of those features is not performing as expected the entire application including those two satisfactory features would need to be rolled back a second trade up to consider is the additional infrastructure resources needed since there is more than one production environment online this has cost implications and adds complexity in order to monitor and maintain that additional infrastructure furthermore if a need arises for more versions of the deployment developers will need to deploy more environments to test the different variants of the application this can compound the cost complexity and overhead of maintaining multiple production environments next we'll take a look at what feature blogs are and how they can be used at the tool for testing in production a feature block strategy at the lowest level consists of conditional statements that determine whether or not a given block of code should be executed this can be used in a front-end or back-end application so in this code snippet example we have an if-else statement that controls the logic flow of the app if the evaluate function for the new feature evaluates to true then it will serve the new feature otherwise it'll serve the old feature so what are some benefits of using feature flags feature flags allow selective feature rollbacks each new feature is tied to a single flag going back to our previous example on the last slide if one of the three features in a deployment is buggy the developer can simply toggle off that one feature while the others stay on next zero downtime future flags rely on the conditional flow in the source code itself which allows them to be configurable remotely code does not need to be redeployed to change the routing or evaluation logic third feature flag can target users using logic or data accessible in the application itself such as what users logged in and using the inputs to the evaluate flag function and determine the flow of the conditional block and of course we have to consider the trade-off here so first potential trade-off is technical debt because the conditional logic lives in the application code the extra code adds maintenance complexity to the code base this necessitates increased coordination between teams including managing the status of active or inactive flag understanding when old flag can be removed from the code base and who has ownership over the governance of different flags another tradeoff to consider is that future flags can add to page load times feature flags like the evaluate black function and our snippet are remotely controlled over the network network latency and fault can affect the loading time of the application relying on the results of that feature flag so which approach best fits our use case as a reminder our criteria for pr for testing and production responsibly is giving developers the ability to target user groups roll back the application to a stable state and isolate the buggy feature so let's compare the two strategies so for targeting users in both strategies we're able to do this just in different ways multiple deployments that have then at the infrastructure level whereas for feature flag this is done at the application code level next rolling back the application state again both are able to achieve this no downtime to users multiple deployments does this by rerouting 100 of traffic back to the stable production version while feature flags you simply toggle off the flag to change the application logic third isolating the feature that needs to be fixed in a multiple deployment strategy this requires a full rollback of the deployed version of the application that contains the buggy feature in a feature flag strategy you can selectively disable a specific feature flag because each feature is wrapped in its own flag so for this criteria using multiple deployments does not allow developers to solely isolate the problematic feature since an issue with one feature impacts the deployment of other features so because feature flag meets all three of our criteria feature flag is a better fit for a use case of testing in production now i'm going to hand it off to one who's going to take us on a more in-depth look at feature flags and how they can be used to responsibly test in production thanks audrey so we'll be exploring feature flags as a solution next one of the defining abilities of feature flags is the ability to toggle functionality most implementations allow this to be reflected in real time allowing for any updates to instantly be reflected on the client or server side in this visual you can see that when the google pay option flag is toggled on the page displays the google pay button and when it's off it displays the standard credit card payment option so toggling is a key and useful feature of future flags but it alone isn't enough to safely test in production because it indiscriminately affects all end users rather than just a bug tolerant subset feature flag implementations also commonly allow targeting specific users based on attributes attributes are made up of user traits like state of residence and whether they are a student like you can see on the left side for example if we wanted to target any users who are students on the us west coast we can set up a feature flag to target users with a student attribute value of true and a state attribute value of california washington or oregon and you can see these targeting rules in the middle the feature flat code would then compare the user attributes against the targeting rules and determine whether to evaluate to true or false this evaluation would then be used by the developer to determine what experience to serve so in this example audrey is a student in washington so she passes a targeting rule and she gets served the new feature juan is in california so he fulfills the state requirement but he is not a student so he could serve the old feature to take attributes one step further some future flagging platforms also allow the creation of audiences which are essentially bundles of reusable conditions they functionally work the same as general attribute targeting but audiences provide the convenience of reusability and they allow for idiomatic naming which makes it easier for other developers to understand who the flag is for in this visual we have three users being evaluated to see if they're part of the west coast student's audience this audience is just an abstraction of the conditions we saw in the previous slide but other developers can easily take a look at the audience being targeted to see what the intent is next let's take a look at some of the options developers have to start making use of feature flags one option to implement a feature flag platform is to do it yourself and build an in-house solution to do this the team would need would need to build out three components at the bare minimum they need some way to manage the flags like a command line interface or a dashboard if you expect that developers will be the primary users of this a cli would definitely be easier to implement they would also need a database to hold persistent flag data and a code package or an sdk that they can embed in their application to communicate with that database as an aside teams can also leverage open source solutions as a starting point for their future flagging platform but this will likely still require time and resources to configure to the team's requirements so diy can be doable for a small team but as needs start to grow it can be difficult to ramp up the development to accommodate evolving needs like in this case study so atlassian build its own feature flagging system primarily to control feature releases but as more teams wanted to use the in-house solution developing and maintaining the solution became unwieldy because there weren't sdks available for every language in addition the system was not designed for users outside the development team without a ui teams outside of engineering like product management they had to rely on engineering teams to manage flags and run beta tests atlassian actually ended up migrating to launch darkly which is a paid solution and i'll be getting into that next so the resource overhead and lead time of diy solution may be too prohibitive for some companies so they may look towards commercial feature flagging solutions on the market these tend to save time and resources by providing rich and convenient functionality out of the box like support for experimentation workflow integrations audience targeting and more two of the biggest names out there are launched darkly and optimizely so let's compare the options we just covered diy solutions are self-hosted which means you don't have to worry about issues with sharing data with third parties but while you can configure them however you wish diy solutions take time and resources to set up especially to build out more advanced features like audience targeting and experimentation paid solutions on the other hand are designed to be easy to set up and their feature rich out of the box but some functionalities may be gated behind higher payment tiers they also tend to be third-party hosted which can be an issue for companies handling sensitive information so as you can see there is a need for an affordable feature flagging platform that is able to accommodate testing and production via audience targeting and here's where fauna comes in i'll start by comparing fauna to the previous two options and then we'll go a bit into what we call fun as entities vana is open source so it's a free alternative that provides the ability to target audiences out of the box however since the primary goal of fauna is to facilitate testing and production it doesn't include other features like experimentation and a b testing it's entirely self-hosted so your data stays with you and is not shared with any third parties it's also very easy to get started which we will talk about a bit later in the presentation next we'll be demonstrating how to create feature flags but to understand how to do this we'll need to take a closer look at fonda's entities these enable final users to target intentional segments within their application this is a screenshot of the find a dashboard where you can create and manage your entities we're currently looking at the flags where you can see all of your created flags and their status which can be toggled on the left navigation pane you can see that there are audience and attributes sections as well which looks similar to this view to get us started let's talk about attributes first as i briefly covered earlier an attribute is a category that describes a user and it can also be thought of as a trait or characteristic about a particular user fauna currently supports string number and boolean type attributes and in the developer's code base itself each unique user should have what's called a user context object generated by the application it consists of attribute value pairs that are used when evaluating that user's eligibility for an audience so below you can see that user context object using the three attributes above each with an assigned value and here's a short clip of how to create attributes within finest dashboard you can click on create attribute which will bring up the creation model you can pick a key and pick a data type and then hit create you can also click into the attribute to see details about it like what audiences it's being used in these attributes will come in handy when we create audiences which i'll talk about next so audiences are collections of logical conditions and they're reusable but what are conditions they're essentially the targeting rules that define whether a user is eligible for an audience and conditions are made up of three parts an attribute an operator and a target value in this example of a condition any user with a state attribute set to either california washington or oregon will pass for this particular condition audiences can actually have multiple conditions which means we need to specify how to evaluate them in aggregate so when creating audiences finite users can specify a combination indicator which can be set to either any or all if the combination is any it means the user only needs to fulfill one of the conditions to be eligible for that audience and if the combination is all it means the user needs to fulfill all conditions to be eligible now let's take a look at how to create an audience within the fauna dashboards audiences page clicking on the create audience in the corner brings up the creation model where you can specify a display name and a key next you can start adding your conditions we're going to create two conditions here one to pick out the west coast states so we pick the state attribute the is in operator and our condition values will be c-a-w-a-o-r and we save it and then we also want to pick out the students so we add another condition picking the attribute student the operator is equal to and the boolean value true since we want both of these conditions to be fulfilled we're going to set the combination indicator to all and then we click create audience and you can also click into your newly created audience to see and edit the details you can also see a list of flags that are using this audience since we just created this one there aren't any but we'll get there next flags are the core entity of fauna each flag is meant to represent a specific feature that the developer wishes to test they consist of four properties first is a title which helps the final user easily distinguish between flags a key which is an identifier string used during evaluation within the application a toggle which indicates whether the flag is enabled or disabled and a list of targeted audiences a user must be eligible for at least one of the targeted audiences to evaluate to true for this flag here's how to create a flag within the fonda dashboard in the flags view you can click on create flag to bring up the creation model fill in your title and your key this flag that we're making is going to represent a banner for west coast students next you can pick out the audiences you want to target where you can see our newly created west coast student and then click on create and then you can click into the flag to take a look at the details and make any edits if you need to you can actually also click into your targeted audiences in case you may have forgotten what conditions it consists of and you can see our related flag there our west coast student banner and to toggle the flag on you just gonna hit the switch and a toggle it off you do the same now let's take a look at this real time toggling in action here we can see the final dashboard and a simple react shopping application side by side our west coast student banner flag is enabled on the left side and you can see that this is reflected on the right side where i'm visiting the page as a supposed west coast student but suppose that something were to go wrong maybe this experimental banner is causing some sort of system breaking bug well there's no need to redeploy a generic version thanks to feature flags you can simply toggle it off and you can see that the banner reverts back to the generic banner in real time so now that we've covered fina's entities i will hand it off to urim for a technical deep dive into finest architecture thank you juan so in this technical deep dive i will discuss fauna's overall architecture the responsibilities of each part and trade-offs we've considered in making those decisions in this diagram the components shown in orange are part of fana's event-driven architecture where events like changes or updates to flags trigger communication between the components the first component is the manager platform this consists of our ui dashboard that one walked us through earlier where developers can create and manage a set of attributes audiences and flags it also has the manager api backend which processes incoming data and writes to the postgres database when the manager back-end starts up or when any changes are made to the flac data the backend sends a copy of the flag data to our data store layer this data store layer also serves as a pub sub service where the manager can publish those flag updates to the data store layer is also connected to our flag bearer our flag bearer is responsible for managing connections from the client application as well as evaluating flag data it also subscribes to different channels from the pops up and routes or directs the event notification to its destination which are our client and server side software development kits or sdks for short you can see here that fana's sdk shown in orange sits within the flag consumer shown in purple this is because the sdks are embedded into the developer's application and this allows that application to communicate with finance platform enable flag evaluation at the application's runtime and consume event notifications in real time fauna provides sdks for the cloud for a client-side react app and a server-side new js application and we will discuss some of the differences later in the presentation so now that we have a birth eye view of all our components let's discuss why we have these different components separated out in our first iteration of the fauna platform we had the one component to perform all tasks like serving the dashboard making queries to a database managing validating sdk connections and pushing event notifications to sdks in real time and while we were working with this architecture we noticed a few things first because the manager is handling different types of requests like requests from the ui dashboard and requests coming from various sdks we had to think about engineering decisions for when the application reaches the limit of the server due to increased traffic and even concurrency secondly having all functions in one component introduces a major single point of failure making our system not a very fault tolerant one and lastly the way we were organizing the functionality in our copay started to organically separate concerns into different modules so ultimately applying that principle of separation of concerns would allow our application to be more cohesive and reduce tightly coupled components that depend on each other a natural breakout point then was to have one component handle requests coming from the developer dashboard and another component to handle requests from the sdks so our evolved architecture was born now we have the manager platform serving dashboard and making queries to a persistent database and a flat bearer to act as a simple reverse proxy for the manager and this abstracts away some of the sdk concerns from our backend so like managing connections and sending updates and having this flag there broken out we now have to decide how will this uh flag bearer access the flag data should the flag bearer reach out to the manager to um to get the flag data every time request comes in if we were to do this then we no longer solve the problem of reducing traffic load to the manager another option would be to store a copy of the flag data in memory of the flag bearer and while this was a viable option for us we wanted a strategy that would allow us to continue to maintain separation of concerns where the flack bearer wouldn't have the additional responsibility of managing data and its consistency so ultimately our solution was to decouple the data storage with a redis data store layer as you can see in this diagram we now have the redis data layer sitting in between the manager platform and the flat bearer the manager will write the most current flag data to the data store when any changes are made to the data and the flat bearer can now just access data through this redis cluster instead of requesting them from the manager every time but aside from responding to sdk's initial requests we also had to address how the flag notifications will be communicated out to the sdks as they happen and for our use case we needed a way to communicate different types of updates as well for example flag toggles for when we are enabling or disabling flags and also general flag configuration updates and since pop sub patterns enable the creation of publisher and subscribers with message channels we chose to integrate this communication pattern in our architecture so here's our final evolution diagram shown here we can see that the redis serves as a data store and a message broker and here if we can follow the red dot we see that when manager receives the update it writes to the data store and publishes messages to the pub sub and it doesn't need to concern about the consumers who are receiving these messages it's the flag bearer that subscribes the sdks to the appropriate channels so that they can receive the messages the added benefit to this model is that it gives us the flexibility and option in the future to scale the flat bearer layer without impacting how the manager will communicate its updates to the flag bearer nodes okay so we've discussed how fauna is architecturally set up let's talk about how we can deploy fana in this section i'll discuss our docker compose and aws deployment options and the discussions we decisions we made and choosing them we have used docker files to containerize fana's manager and flag bearer component separately and containerizing our application allows us to achieve process level isolation this helps us to reduce any risk of issues like ones arising from version dependencies for example it also allows us allows flexibility of deploying and managing finer components in the environment of developers choosing so developers can then deploy fada using a docker compose file which references fauna's latest images for each component as well as the redis and postgres images as well docker compose is helpful here for two main reasons so by design containers run in complete isolation and they don't know anything about each other's processes so we need a way to facilitate that communication secondly we need our containers or services to start up in a specific order for example the manager needs to connect to postgres first before being able to serve the dashboard and it also needs to send data to our redis cluster and the flag there depends on being able to connect with redis and the manager as well in case the data store is not filled yet by using docker compose a docker network is automatically created for us and its built-in dns resolutions allows our containers to communicate with each other the docker network also allows us to control our services exposure to the broader internet so for the redis and postgres database for example that shouldn't be accessed by anything outside of the network we can also define which containers depend on one another so that the containers can start up in their respective dependency order and once the developer configure necessary environment variables finite platform can be launched for use alternatively developers who want to leverage awss managed services have the option of using fana's cloud development kit or cdk for short and cdk is a framework that allows developers to specify their infrastructure as code and deploy their application fano's aw stack would be something like as shown we have our finite manager and bearer container managed by the elastic container service or ecs for sure the containers will then be connected to their respective rds and elastic databases and they will all communicate with each other through the virtual network called virtual private cloud or vpc for short by creating a vpc for fauna and adding appropriate security groups we can customize the network configuration to allow appropriate traffic through our fana platform and finally we chose to include a load balancer to help us route external internet traffic so how does our load balancer know where to send the traffic to to configure our load balancer a listener was added to handle routes for specific path patterns when a developer wants to access the dashboard they can make a request using the root path and the load balancer will route that request to our dashboard and when the sdk is making requests using path starting with slash connect or a stream the load balancer will forward those requests to the flag bearer to handle them appropriately before we move forward there was one additional decision to make to finalize our aws stack aws ecs is a container orchestration platform that is it manages the life cycle and placement of the task or the instances of our docker images but we need a service that can actually run our containers as well the options we have are ec2 or fargate with ec2 the developer will need to deploy and manage a cluster of ec2 instances to run the relevant containers and while this allows for greater control over the machines it increases developers responsibility to provision scale and secure these ac2 instances with fargate though there's no longer the need to manually provision the compute instances developers can simply create a cluster and specify resources like cpu and memory and when the containers are deployed fargate will run and manage the servers to meet the necessary requirements as one discussed earlier during future flag implementation options it was important for the fana platform to be a easily maintainable solution so that the engineering team can focus more on the business needs rather than the task of managing a feature flagging platform therefore hosting the containers using the fargate service was more appropriate for our use case so now that we've established our stack and the decisions that led them let's walk through how we can use cdk to deploy this architecture on aws there are two different stacks defined in our cdk the fauna shared resources which pertain the vpc a new instance of air a rds as well as a elastic redis cluster and these shared resources are referenced in the finance platform stack which contain our containers the load balancer and a security group is defined here as well and we can deploy both the stacks using cdk deploy all and once deployed we can reference the output for the load balancer dns address to access our application here we can see that our load balancer directs the root path to our ui dashboard and we're also able to provide this url as the bare endpoint for the sdks and the load balancer will listen for those appropriate routes and direct them to the flag flag bearer and now that we see how fauna is set up and deployed rob will discuss some additional engineering decisions and challenges we've encountered in arriving at our solution thanks erin i'm going to talk about some of the technical directions we took specifically how we tailored our respective sdk implementations why we pivoted from our initial database choice and how we went about choosing appropriate communication patterns for fona's purpose so first i'm going to talk about why we have distinct sdk types and specifically how they work within different types of applications and how those distinct contexts influence their respective design requirements we started with just a server sdk meant for back-end applications within your architecture because that application code executes within your system we could get away with a more straightforward approach to flag evaluation it worked this way the sdk would receive a full rule set object composed of all flags in their associated audience conditions and then individual flags could be evaluated on demand however there's an issue with this when we move to user facing client applications like websites the problem is that rules can very easily contain sensitive information if you're targeting by emails if you're targeting by names if you're targeting by other personally identifying account information all of those values are exposed in that rule set object that's delivered to the sdk so of course it's not acceptable to expose that to every user visiting your website our solution in the case of client-side sdks was to evaluate our flags before delivering them so that process looks like this the flag bearer would receive the user context object from the sdk it would then request the full rule set object from the redis store and then it would evaluate that role set against the user context ultimately the client sdk initialization concludes with the sdk receiving an object composed only at flag keys and their boolean values this flag evaluates true this flag evaluates false etc so this alleviates our privacy concerns but this pattern requires upfront evaluation of all flags in the application depending on your app it's very likely this includes flags won't ultimately be executed so this pre-evaluation imposes an additional load on the flag bearer but since it also helps secure that potentially sensitive information it was a worthwhile trade-off to reinforce the security we use sdk keys to verify requests sdk keys serve to authenticate those sdk initialization requests they ensure those requests originated from an appropriate source because the client sdk key is visible in the request any user can see it in their browser if there was only a single key for both client and server any request through the server endpoint using that publicly visible key could retrieve the full rule set with all its sensitive information having separate fck keys helps avoid this by denying requests that don't match the proper key type and next i'll talk about the database conversation the first iterations of fauna were built on for two primary reasons first without schema constraints the structure of our data could change freely as the approaches evolved in early development phase and second mongo's atlas platform allowed us to host our data online and that was very convenient accessibility for collaboration but as our data model took shape we realized it needed to express relationships between dynamic data nearly all of the operations on our data leverage those relationships between flags audiences attributes etc we needed to maintain consistency within relationships between changing data it made sense to move to a database optimized for aggregating and extracting data across tables it was more clearly a sql story and ultimately we chose postgres for its familiarity as an open source solution last challenge i'll talk about was the communication between our architecture's components one of the primary requirements driving that decision was evaluating our data in real time the reason real-time updates are critical the fauna goes back to the core dynamic for choosing a feature flag platform for testing and production in the first place part of that is that time is critical specifically if something goes wrong that issue's impact on users is a function of the response time to that issue in other words rolling back back code as soon as possible is essential and perhaps less critically but still important flags ideally should be evaluated against the most recent data so there's two steps on our data's distribution journey the first step was getting the flag rules from our database to the flag bearers and the second hop from there was getting those flag rules out to the sdks so the first decision we needed to make was with regards to how distributing flag rules to the bearer would be handled as i just mentioned it was fundamental that the flag bearer received data updates in real time we considered three options to get there api requests were a familiar pattern a consumer makes a request a provider responds with the requested data but this pattern is conceptually backwards for our needs the bearer initializing communications means the origin of the request is not where the data changes the only way for a consumer to assure its data is fresh is to make a new request in other words a request is made every time the rule set is served to an sdk and we realized we could do better so web hooks were the first event driven model we considered and it met a lot of our requirements updates pushed to the flag bearer when they happened and because it's in real time the bearer no longer has to make requests to the manager it already has the fresh data set and that assurance allows us to cache that data set so we can get all the benefits of that too however as the bearer scales so too do the web hook consumers registered with the manager and this strongly reinforces coupling of those components which is what we're trying to avoid ultimately of course we went with a different event-driven approach the pub sub as uranium mentioned with this model the manager now published to a single message broker a distributed messages to all flag bearer instances flag bearers subscribe to topics or you could think of them as channels published by the message broker and they would receive messages along those channels in real time so now both the manager and flag bearers were concerned only with this single pub sub service and this represented the level of decoupling we were looking for in order to accommodate the scaling needs we anticipated the second half of the real-time update story was the communication from flag bearer to the sdks we needed to ensure flags were evaluated at the application level with the most recent data again we considered three options polling was one naive way by which to approximate real-time communications sdks make requests to the flag bearer at specified time intervals but this really just presented a choice between compromises how close to real time do you want this to get versus how many requests you want to hit your flag bearer with web sockets on the other hand were an authentic real-time solution they offer two-way event-driven communications between the flag bearer and sdks however they were a little more complex pattern than we needed we really only needed to go one way from the bear to the sdk and a more critical issue is that websockets will not automatically attempt to reconnect if the connection is dropped a better option was sses ssc has offered a one-way real-time channel from flag bearer to sdk what this meant for fauna specifically was that when a flag is toggled off the flag bearer can forward that updated flag status to the sdks by the way of the currently open and ongoing sse connections this ensures that a feature that needed to be rolled back are is disabled as quickly as possible additionally sscs have built-in recovery with regards to dropped connections so ultimately because this was simpler and more resilient this was the best fit for our needs to close things out here's a brief look at what future work on fauna might be right now the biggest opportunity to reduce latency at runtime for apps using fauna is within the communication between the application and the flag bearer so we could cut that significantly by hosting flag bearer instances on the edge closer to users launch darkly executed this for 10 times faster sdk initializations we could also utilize sdks to relay metadata back to the flag bearer so we could profile data like flag evaluation frequency and result values and help alleviate tech debt by a way of identifying stale flags and that's fauna thanks for listening we'll open the floor to questions what did you find most interesting when building this out uh audrey do you want to start us off on that one sure um i'll actually kind of continue on the one of the points in the future work section that rob was talking about so he mentioned that launch directly was able to reduce their sdk initialization time significantly so in our research of launch directly they did this by moving more of their functionality to their content delivery networks vendors so for better global reach and like reduce that latency time so we actually did go through a couple different diagramming iterations of our architecture to figure out what this might look like for fauna so the flag bearer being that reverse proxy component responsible for managing those streaming connections with sdks um we would have been able to move that logic that function that evaluates the flag data to something like a serverless function on the edge on an edge node like on aws or a cloudflare worker and potentially use some sort of data store at the edge like a cloudflare kv to hold the flag data um we were really close to implementing this i think like this this definitely would have been a real possibility for us if we had um a little bit more time but this eventually would have kind of helped separate concerns a little bit better so that um the processing logic is fully taken out of the flag bearer and then its only role is just managing those those sdk connections so that was something that we were very keen on i'm very interested in implementing and there was a lot of um there's a lot of evidence based on some of our research that that that definitely would have been possible and fit our use case really well i will let uh one do you want to just pipe in really quick on this question uh sure so for me it was definitely the dashboard um really enjoyed working with react um and working on it in like a real project like intended for developers was really great um and i also like put a lot of effort into um making it uh good for non-developers as well so like adding a bunch of little convenience features uh it was just a lot of fun to work on and um also being able to communicate with the manager um just like kind of figuring out like the front and the back end like designing the api endpoints and like the shape of the data that we passed back and forth like that was a it was it was like a lot it was really fun to work on actually so definitely really enjoyed that part uh you're what about you yeah i most enjoyed having engineering trade-off conversations as we're building fauna especially the heated ones that really forced us to consider you know the strengths and weaknesses of each solution i think the most memorable one we had were related to something rob had mentioned about sql and nosql it was an important conversation to talk through the implication of going with you know one database uh versus the other and thinking through what type of data and what shape is optimal for a dashboard and bearer not only for the current system that we were building but being flexible for future evolution of our flat platform as well so how about you rob uh yeah so mine's a little bit off of what juan was talking about with regards to developing the api between the dashboard and the manager uh so we built the manager and go which i'm personally in love with that language but one of the interesting choices there was whether or not to use an orm uh appropriately named gorm so because we only have a single application accessing the database it was really convenient to also control the structure the schema from that same source you know exactly at the application level what you're working with because that's where it's declared uh but it gets it gets like more interesting when you move to more distributed data access like larger systems meaning to maintain consistency and um potentially disparate data models right there can be conflicts um and there's there's a lot of very strong voices in the go community who work with their data on a much lower level including straight up raw sequel so that was an interesting pace space to visit for sure uh we have another question was it obvious to you immediately that flag feature flagging would be worthy of an entire project or did it happen as a result of exploring the space audrey you want to take that one sure i think like we were definitely we did spend a fair amount of time just researching different kind of areas of interest and different problem domains but if you even google like feature flags in the search bar and just the number of different vendors and solutions that come up and if you dig a little bit deeper even to that like the different many different use cases of how future flags can be used and we only really covered one of those very many use cases um you can see that there's a lot of there's a lot of different ways that you can architect a feature flag platform you can optimize it for certain things and there's certainly a need for feature flags um i mean in our presentation alone we had two case studies there was definitely a handful more that we came across in our research so it definitely seems like feature using feature flags is almost i wouldn't say standard but it seems very common for developer teams to be utilizing and sort of their workflows yeah great um what was your favorite part of working as a team working with a mentor and working on this project that we just mentioned that so what was your favorite part working as a team and working with a mentor i can try to tackle this one i think my favorite part of working as a team uh i think it was very evident that whenever we approached a problem um we each kind of uh approached it with a different lens and so we're able to consider the different ideas or things that we like i if something i couldn't see maybe you know audrey or rob or juan would be able to point out um and you know having it forces us to have you know conversations about trade-offs more as well um and thinking through thinking through more holistically about you know what what's the best solution and so for me that was uh that was the most my favorite part i think it also uh is similar to like parent pairing and code code coding as a pair it's difficult at first but as you get used to it you know you really see a lot of benefit to to doing that and and having having an extra pair of eye extra pair of perspective to to add benefit to your project and working with a mentor i mean so there's so many great things emotional support providing really industry level sort of uh perspective on on different approaches as well some things that you there's just some things that are hard to research where you know you need that insider knowledge to get a perspective on things yeah definitely echo everything you're um just said like it's really like working on a team it's like uh like you just get the advantage because uh i know like everyone has a tendency to like tunnel vision in on like a specific solution or or a specific implementation and then like you have other people to like you know offer different perspectives like uh like they challenge you you know which is a good thing because it forces you to kind of like think about what you're saying and like a different from a different perspective and like working with a mentor is like similarly like like it's a similar benefit to that because like you have an industry professional who's like telling you like hey like this is like this is a real thing in the like in tech world or it's not a real thing so having like that expertise like was completely invaluable and really appreciate uh rodney's help there can you expand a bit more on how you used redis in your implementation it sounds as if it was used as both a cache and a pub sub yeah i can take this one thanks elaine for your question um you're correct we used it both at the cache and a pub sub um so i'm not sure if it's very well known that redis had like a pub sub uh i guess like api interface that you can use i think it's most commonly ubiquit ubiquitously thought of as like a caching store um but since we had already kind of made that design decision that we wanted to extract at that data layer it seemed really convenient to also use that same sort of component of infrastructure to also handle the message brokering so that certainly simplified it when it came to deployment so we only needed one redis image instead of maybe a redis image and something else that would that would be the message broker and definitely in our aws cloud deployment as well we can just utilize that same elastic redis cluster for both both uses um instead of having to manage and spin up another separate piece of infrastructure why did you choose golang for the manager platform was there any language features that made it the right choice for that feature well i can take this a lot of the a lot of things we were seeing as we were building up to this project was that a lot of um it it's used a lot in the web development space for back-end services and because the manager was holding a lot of responsibilities um or our back end the performance was not negligible we it was we we built a rudimentary version in node first and um it was definitely quicker but also um just the fact that it is such a it is so common in the space um you know it was just as arbitrary to build it node as going so you know why not get experience with that um and i think ultimately allowed a slightly simpler slightly more straightforward um management of our data with its static typing and uh that coupled with the orm so i think those are the main reasons we decided to go with that we had one more question come in and we'll answer this one before we wrap up there was a question about the load balancer for the cloud deployed application can you let us know more about how you used the load balancer urm do you want to take this one sure um so for the load balancer we have used it uh by adding listeners that will listen to different rules for the path um and so you know i think in one of the slides we had shown that you know with the dns load balancer the root path when it sees that a request is made with a with the root path it'll direct it to our ui dashboard which the manager serves and then there we have specified uh paths for the sdks to make certain requests to connect or to make a you know sse connection and those requests are routed by the load balancer to the flag bearer to handle them appropriately as well i hope that helped answer your question okay well if there's any more questions you can find any of us on github and drop us a message but thank you all for tuning in today 