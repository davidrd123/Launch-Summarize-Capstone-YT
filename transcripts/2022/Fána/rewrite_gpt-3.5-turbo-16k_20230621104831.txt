Welcome, everyone! Thank you for joining us today on this Thursday. In today's presentation, we will be introducing Fauna, an open-source feature flag management platform. Fauna allows developers to confidently test new features in production. Let's start by introducing the team behind Fauna - Urim, Rob, Juan, and myself, Audrey. We have worked hard on this project and are proud of our achievements over the last few months. Each team member will share their insights throughout the presentation.

Before diving into Fauna, let's discuss the problem space that inspired its creation. We will explore the challenges of testing new features in production and highlight the importance of testing to build confidence in future releases. Developers want to release new features with minimal risk and ensure they are bug-free. Automated testing plays a crucial role in this process, as it helps verify that the application behaves as intended. However, testing in pre-production environments doesn't always guarantee bug-free releases in the production environment. Unpredictable user behavior and the complexity of production systems make it challenging to guard against all possibilities in a controlled environment.

To address this problem, developers need a way to minimize the impact on end users if something goes wrong during feature testing. Three key strategies enable this: releasing new code to a small group of users, isolating the problematic feature, and rolling back the application to a stable state. By gradually increasing the number of exposed users over time and targeting specific subsets of users, developers can limit the impact of problems and gain confidence in their feature releases. 

One case study that illustrates this strategy is VMware, a cloud computing company. They successfully deployed a new feature by releasing it to their developer team first and then to external beta users. This progressive rollout allowed them to investigate real user issues, evaluate feedback, and limit the impact of any bugs. 

Now, let's explore two common approaches to testing in production: multiple deployments and feature flags. Multiple deployments involve using a second production environment to host the new feature and routing traffic between two versions using load balancers. This strategy offers benefits such as cleaner code bases, no downtime for users during rollbacks, and the ability to configure routing logic for specific user experiences. However, it also comes with trade-offs like additional infrastructure, increased complexity, and potential cost implications.

Feature flags, on the other hand, provide a way to toggle functionality based on conditional statements. By using if-else statements in the application code, developers can control the logic flow and choose which feature to serve. Feature flags offer benefits such as selective feature rollbacks, zero downtime, and the ability to target users based on attributes or audiences. However, they also introduce technical debt and may impact page load times.

Considering our criteria for testing in production responsibly - targeting specific user groups, rolling back the application, and isolating buggy features - feature flags are a better fit than multiple deployments. Feature flags allow developers to fulfill all three criteria effectively by selectively enabling or disabling specific features without affecting the entire deployment.

Now, I'll pass it on to Juan for a more in-depth look at feature flags and how they can be used for responsible testing in production.

Great, thanks Audrey! Let's dive deeper into feature flags as a solution. One key feature of feature flags is the ability to toggle functionality in real-time. This means that any updates to the flags are instantly reflected on the client or server side. For example, toggling a flag for the "Google Pay" option can dynamically display or hide the relevant payment button on a webpage.

Toggling functionality alone is not sufficient for safe testing in production because it affects all end users. To address this, feature flag implementations often incorporate targeting specific users based on attributes. These attributes can include user traits like residence or employment status. By defining targeting rules based on these attributes, developers can determine which users should be exposed to a particular feature. This allows for testing with a more bug-tolerant subset of users.

To further enhance targeting capabilities, some feature flagging platforms also offer the creation of audiences. Audiences are bundles of reusable conditions that target specific groups of users. These conditions are evaluated against user attributes to determine if they are part of the targeted audience. Using audiences makes it easier for developers to understand the intent of a feature flag and facilitates reuse across different components or teams.

Now let's explore some options for implementing feature flags. One option is to build an in-house solution from scratch. This requires developing three main components: a flag management system, a database to store flag data, and a code package or SDK for communication between the application and the flag system. While this approach is feasible for small teams, it can become challenging to maintain and scale as requirements evolve. Atlassian, for example, initially built its own feature flagging system but faced difficulties with its scalability and lack of language-specific SDKs.

Another option is to leverage existing open-source solutions as a starting point. However, configuring these solutions to meet specific requirements often requires additional time and resources. As needs grow, it may become necessary to consider paid platforms like LaunchDarkly, which offer more comprehensive features, support, and language-specific SDKs. Migrating from an in-house solution to a paid platform can help alleviate maintenance overhead and enable teams outside of engineering to participate in managing flags and running tests.

In conclusion, feature flags provide a powerful solution for responsible testing in production. They allow developers to toggle functionality, target specific user groups, and minimize the impact of bugs by selectively enabling or disabling features. While there are trade-offs to consider, such as technical debt and potential impact on page load times, feature flags prove to be a better fit for our criteria. By leveraging feature flags, developers can confidently test new features in production and release them with minimal risk.

Thank you for your attention, and now I'll pass it back to Audrey for closing remarks and a brief discussion on potential future work.

 Oregon has targeting rules in the middle, which the feature flag code uses to compare user attributes and determine if they evaluate to true or false. This determines the experience that gets served. For example, Audrey is a student in Washington, so she passes the targeting rule and gets served the new feature. Juan is in California and fulfills the state requirement but is not a student, so he gets served the old feature. 

To go a step further with user attributes, some feature flagging platforms allow the creation of audiences. Audiences are bundles of reusable conditions that work the same way as general attribute targeting but offer the convenience of reusability and idiomatic naming, making it easier for other developers to understand who the flag is for. The visual representation shows three users being evaluated for inclusion in the West Coast Students audience, which is an abstraction of the conditions seen earlier. This makes it easier for developers to understand the intent of the audience.

Now let's look at options for implementing feature flags. One option is to build an in-house solution, where the team would need to develop three main components: a way to manage the flags (like a command line interface or a dashboard), a database to store persistent flag data, and an SDK to communicate with the database. While open source solutions can be leveraged, they still require configuration to meet the team's specific requirements. Atlassian, for example, built its own feature flagging system initially for controlling feature releases, but it became unwieldy as more teams wanted to use it and lacked language support. Atlassian ended up migrating to LaunchDarkly, a paid solution.

Building and maintaining a DIY solution can be challenging as needs grow, so many companies turn to commercial feature flagging solutions. These solutions save time and resources by offering rich functionality out of the box, including support for experimentation workflow integrations and audience targeting. Launched Darkly and Optimizely are two popular names in the market. However, DIY solutions allow for more control and self-hosting, while paid solutions may have limitations on functionality and raise concerns about handling sensitive information.

Therefore, there is a need for an affordable feature flagging platform that accommodates testing and production through audience targeting. Enter Fauna. Fauna is an open-source alternative that provides the ability to target audiences out of the box. It's self-hosted, ensuring data privacy, and is easy to get started with. While Fauna focuses on testing and production, it lacks features like experimentation and A/B testing that commercial solutions offer.

Now, let's dive into Fauna's entities. Entities allow users to target specific segments within their applications. The Fauna dashboard allows users to create and manage entities, such as flags, attributes, and audiences. Flags represent specific features to be tested and have properties like a title, key, toggle (enabled/disabled), and targeted audiences. Attributes are user traits or characteristics used in targeting, while audiences are collections of logical conditions defining user eligibility. Conditions consist of an attribute, operator, and target value. Audiences can have multiple conditions, and the combination indicator specifies how conditions are evaluated (any or all).

Creating entities in Fauna is straightforward. Attributes are created by specifying a key and data type. Audiences are created by specifying a display name, key, and conditions. Conditions are defined by selecting an attribute, operator, and target value. Flags are created by providing a title, key, and selecting target audiences. They can be toggled on or off in real-time.

Moving on to the technical deep dive, Fauna's architecture consists of several components. The Manager Platform includes a UI dashboard for creating and managing attributes, audiences, and flags. It also has a Manager API backend that writes data to a Postgres database. The Data Store Layer acts as a pub-sub service, receiving flag updates from the Manager Platform and storing the data. The Flag Bearer manages client application connections, evaluates flag data, and delivers event notifications to the client and server-side SDKs.

The decision to separate components was motivated by scalability and fault tolerance. Initially, all functionality was in one component, leading to potential performance limitations and a single point of failure. Separating functionality improved system cohesion, reducing dependencies. The Flag Bearer component abstracts SDK concerns from the backend, managing connections and event notifications.

To address data storage, a Redis data store layer was introduced, decoupling data storage from the Flag Bearer. This allows the Flag Bearer to retrieve flag data without relying on the Manager Platform, reducing traffic load. Fauna's SDKs, available for client-side React apps and server-side Node.js applications, are embedded in the developer's application to enable communication with the Fauna platform, flag evaluation, and real-time event notification handling.

In conclusion, Fauna provides an open-source, self-hosted alternative for feature flagging. While it lacks certain advanced features, it offers easy audience targeting and is ideal for testing and production environments. The architectural design in Fauna ensures scalability, fault tolerance, and separation of concerns, improving system reliability and performance. Fauna's entities, including flags, attributes, and audiences, allow developers to create and manage targeted feature releases. The Fauna dashboard provides an intuitive interface for managing entities, while the Flag Bearer and SDKs handle flag evaluation and real-time communication. Kits or SDKs, short for software development kits, are tools that developers can use to embed functionality into their applications. Fauna's SDK, shown in orange, is embedded within the Flag Bearer, shown in purple. This allows the application to communicate with the Fauna platform, enabling flag evaluation at runtime and real-time event notifications. Fauna provides SDKs for the cloud, client-side React applications, and server-side Node.js applications. The differences between these SDKs will be discussed later in the presentation.

Initially, the Fauna platform had one component that performed all tasks, including serving the dashboard, making queries to the database, managing and validating SDK connections, and pushing event notifications. However, this architecture had some drawbacks. First, handling different types of requests in one component made it difficult to handle increased traffic and concurrency. Second, having all functions in one component created a single point of failure. And lastly, the functionality was organically separating into different modules, which indicated a need for a separation of concerns.

To address these issues, the architecture was evolved to have separate components. One component handles requests from the developer dashboard, while another component handles requests from the SDKs. This separation allows for a more cohesive application and reduces the dependency between components. The Manager Platform serves the dashboard and makes queries to the database, while the Flag Bearer acts as a reverse proxy for the Manager, abstracting away some of the SDK concerns.

To determine how the Flag Bearer should access the flag data, two options were considered. The first option was to have the Flag Bearer reach out to the Manager every time a request comes in to get the flag data. However, this would increase the traffic load on the Manager. The second option was to store a copy of the flag data in memory on the Flag Bearer. While this was a viable option, it would introduce additional responsibilities for the Flag Bearer, which goes against the principle of separation of concerns. The chosen solution was to decouple the data storage by using a Redis data store layer. This allows the Manager to write the most current flag data to the data store when changes are made, and the Flag Bearer can access the data directly from the Redis cluster instead of requesting it from the Manager every time.

In addition to responding to SDK's initial requests, the platform also needed a way to communicate flag notifications to the SDKs as they happen. This required a communication pattern that could handle different types of updates, such as flag toggles and general flag configuration updates. To address this, a publish-subscribe pattern was integrated into the architecture. The Manager writes flag updates to the data store and publishes messages to the pub-sub. The Flag Bearer subscribes the SDKs to the appropriate channels so they can receive the messages. This pattern provides flexibility and scalability for the platform, allowing the Flag Bearer layer to be scaled independently without impacting the communication between the Manager and the Flag Bearer nodes.

Moving on to deployment options, Fauna can be deployed using Docker Compose or AWS. Docker Compose allows developers to deploy Fauna using a configuration file that references the latest images for each component, as well as the Redis and PostgreSQL images. Docker Compose is useful because it provides isolation between containers and allows for communication between them. It also ensures that the containers start up in the correct order, as some dependencies need to be established before certain components can be fully operational. Once the necessary environment variables are configured, Fauna can be launched for use.

Alternatively, developers can use AWS's managed services by leveraging Fauna's Cloud Development Kit (CDK). CDK allows developers to specify their infrastructure as code and deploy their application. In Fauna's AWS stack, the Manager and Flag Bearer containers are managed by the Elastic Container Service (ECS). They are connected to their respective RDS and Elastic databases and communicate with each other through a Virtual Private Cloud (VPC). By customizing the network configuration and adding appropriate security groups, developers can control the traffic flow through the Fauna platform. A load balancer is also included to route external internet traffic.

To configure the load balancer, a listener is added to handle specific path patterns. Requests made to the root path are routed to the UI dashboard, while requests starting with '/connect' or '/stream' are forwarded to the Flag Bearer. This ensures that the requests are handled by the appropriate component. Fauna's AWS stack can be deployed using either EC2 or Fargate as the container orchestration platform. EC2 requires developers to manage a cluster of instances for running the containers, while Fargate allows for easier deployment and management of containers by handling the provisioning and scaling of compute instances.

When tailoring the SDK implementations, different types of applications needed to be taken into account. Server SDKs, which are meant for back-end applications, can receive a full rule set object and evaluate flags on demand. However, for client-facing client-side applications, privacy concerns arise. To address this, client SDKs evaluate the flags before delivering them to the application, ensuring that sensitive information is not exposed. SDK keys are used to authenticate requests and prevent unauthorized access to sensitive information.

Regarding the choice of database, Fauna initially used MongoDB as it allowed for flexibility during the early stages of development. However, as the data model took shape and relationships between dynamic data became important, a database optimized for aggregating and extracting data across tables was needed. PostgreSQL was chosen for its ability to maintain consistency within relationships and its familiarity as an open-source solution.

Communicating between the components of the architecture required real-time updates. This is crucial for the feature flag platform as it allows for the quick response to issues and ensures that flags are evaluated against the most recent data. The distribution of flag rules involves two steps: getting the flag rules from the database to the Flag Bearers, and then getting the flag rules out to the SDKs. To achieve this, communication patterns were chosen that support real-time updates. The chosen solution involved using Redis as both a data store and a message broker, allowing for efficient distribution of flag rules to the Flag Bearers and the SDKs.

This concludes the discussion on how Fauna is set up architecturally and how it can be deployed. The technical decisions and challenges addressed in the project include the separation of components, the use of Docker Compose and AWS, the tailoring of SDK implementations, the choice of database, and the communication patterns employed for real-time updates. These decisions were made with careful consideration of the requirements and objectives of the Fauna platform. The user context for our coding Capstone project involves the client SDK initialization, which concludes with the SDK receiving an object composed only of flag keys and their boolean values. This flag is then evaluated as either true or false. This approach alleviates privacy concerns, but it does require upfront evaluation of all flags in the application. Depending on the app, it's likely that some flags won't be executed, resulting in an additional load on the flag bearer. However, this trade-off is worthwhile as it reinforces security.

To authenticate and verify requests, we use SDK keys. These keys ensure that the SDK initialization requests originate from an appropriate source. If we had a single key for both client and server, any request through the server endpoint using the publicly visible key could retrieve the full rule set with sensitive information. By using separate keys for the client (FCK keys), we avoid this issue by denying requests that don't match the proper key type.

Moving on to the database conversation, the first iterations of Fauna were built on MongoDB for two primary reasons. First, without schema constraints, we had the flexibility to change the structure of our data as our approaches evolved. Second, the MongoDB Atlas platform allowed us to conveniently host our data online for collaboration. However, as our data model took shape, we realized the need to express relationships between dynamic data, such as flags, audiences, and attributes. To maintain consistency within these relationships, we decided to move to a database optimized for aggregating and extracting data across tables. PostgreSQL was chosen for its familiarity and open-source nature.

The communication between our architecture's components was another challenge we encountered. Real-time updates were critical for Fauna since it was designed as a feature flag platform for testing and production. This meant that time was essential, as the impact of any issues on users depended on the response time. Rolling back code as soon as possible was crucial, and flags ideally should be evaluated against the most recent data.

To achieve real-time updates, we needed to tackle two steps in the data's distribution journey. The first step involved getting the flag rules from the database to the flag bearers, while the second step required getting those flag rules out to the SDKs. To handle the distribution of flag rules to the bearers, we considered three options. API requests were initially considered, but they were conceptually backwards for our needs. Webhooks were also explored, but they involved strong coupling between components as the bearers scaled. Eventually, we decided on a pub-sub approach where the manager published to a message broker, and flag bearers subscribed to topics/channels to receive real-time updates.

The second half of the real-time update story involved communication from the flag bearers to the SDKs. We needed to ensure flags were evaluated at the application level with the most recent data. We considered three options: polling, websockets, and SSE (Server-Sent Events). Polling was a compromise and didn't offer true real-time updates. Websockets were more complex than what we needed, and they didn't automatically reconnect if the connection was dropped. SSE, with its one-way real-time channel, emerged as the best fit. It allowed the flag bearer to forward updated flag status to the SDKs through open SSE connections, ensuring quick disabling of features that needed to be rolled back. SSEs also had built-in recovery for dropped connections.

In terms of future work, there are opportunities to reduce latency by hosting flag bearer instances on the edge closer to the users. This would greatly improve runtime for apps using Fauna. Additionally, utilizing SDks to relay metadata back to the flag bearer could help profile data like flag evaluation frequency and identify stale flags, thereby alleviating technical debt.

During the Q&A session, a question was asked about the most interesting aspect of building this project. Audrey pointed out the possibility of reducing latency by hosting flag bearer instances on the edge and mentioned how LaunchDarkly achieved 10 times faster SDK initialization by implementing this approach. Juan talked about enjoying working on the dashboard, making it convenient for both developers and non-developers. Rob highlighted the interesting engineering trade-off conversations that were crucial in the decision-making process, particularly regarding the choice between SQL and NoSQL databases.

Another question inquired about whether it was evident from the start that feature flagging would be the focus of the project. Audrey shared that their research into feature flags revealed a wide range of use cases and the abundance of vendors and solutions in the market. This, coupled with the need for feature flags in developer workflows, made it clear that feature flagging was a worthy focus for the project.

The final question asked about the favorite part of working as a team and with a mentor. The team members appreciated the diverse perspectives and insights that working together brought. They found value in challenging each other's ideas and discussing trade-offs, ultimately leading to a stronger solution. Working with a mentor provided emotional support and industry-level insights that helped them navigate complex decisions. The collaboration between team members and mentor was highly beneficial to the project's success.

In summary, the Capstone project focused on the user context, database conversation, and communication between the architecture's components. It addressed privacy concerns, ensured authentication and verification of requests, and optimized the database for aggregating and extracting data. Real-time updates were achieved through a pub-sub approach and SSE for communication between flag bearers and SDKs. Future work included reducing latency, utilizing metadata, and addressing technical debt. The team found the project's diverse perspectives, trade-off discussions, and mentorship invaluable to their collective success. The video discussed the presence of feature flags in the search bar, highlighting the abundance of vendors and solutions available. Delving deeper, it became apparent that feature flags have various use cases, further emphasizing the multitude of ways a feature flag platform can be designed and optimized. The relevance of feature flags was supported by multiple case studies presented in the video, indicating that it is a common practice for developer teams to incorporate feature flags into their workflows.

When asked about their favorite part of working on the project as a team and with a mentor, the team members expressed their appreciation for the diverse perspectives brought to the table. Each member approached problems with a different lens, resulting in holistic and comprehensive solutions. They commended the value of conversations about trade-offs and the benefits of having an extra set of eyes and perspectives. Additionally, working with a mentor provided emotional support and industry-level guidance, offering valuable insights that can't always be found through research alone.

The team made use of Redis in their implementation, utilizing it both as a cache and a pub-sub system. While Redis is commonly recognized as a caching store, it also has a pub-sub API interface that proved convenient for the project. This decision simplified deployment since only one Redis image was necessary, eliminating the need for another separate piece of infrastructure specifically for message brokering. In their AWS cloud deployment, they were able to utilize the same elastic Redis cluster for both caching and message brokering purposes.

The team chose Go (Golang) as the language for their manager platform. Go is widely used in the web development space for backend services, and this aligned with the manager's role as a backend component carrying significant responsibilities. The team initially built a basic version in Node, which provided faster performance, but they ultimately decided to use Go due to its prevalence in the field. The language's static typing and the availability of object-relational mapping (ORM) facilitated a simpler and more straightforward management of their data.

Another question focused on the load balancer for the cloud-deployed application. The team explained that they used listeners to monitor different rules for the path. For example, a request made to the root path would be directed to the UI dashboard served by the manager via the DNS load balancer. They also specified paths for the SDKs to make specific requests, which would then be routed by the load balancer to the flag bearer for appropriate handling.

The team concluded the video by encouraging viewers to reach out to them on GitHub if they had any further questions. They expressed gratitude for the audience's participation in the session.