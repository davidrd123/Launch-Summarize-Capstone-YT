foreign thank you for attending our presentation today's talk is about health life an open source speech per flag framework designed for easier code deployment with automated Builder protection my name is Steven I'm here with my colleague's friend Jordan and Elaine we're a group of remote developers who have been working on sales life for the past couple months for today's talk we'll first introduce the problem domain that Health life focuses on then describe what feature flags are and how it helps like fits into its competitive landscape followed by explanation of parallel architecture and demonstration of its feature flag and circuit breaking capabilities engineering decisions we made future work for test light finally we'll end the presentation with q a so at a higher level Health light is the future black solution that entangles code deployment and feature release deployment is one of the last stats in a software release cycle is the process of moving code from development environment to the production environment on the other hand future release is the action that makes new or change functionalities available to the user here a picture is something that satisfies the needs of customers it can be an addition of functionalities improvement in performance or availability for application historically feature released was often tied with deployment since code in applications is active by default this means once code is deployed all users received a new feature the downside of traditional deployment is that when features break developers must crumble to their feed and deployed how to fix it on top of the problematic deployment or roll back the problematic deployments all together this leads to unpredictable repair wait time for extended service downtime this type coupling nature between deployment and future release means that once code is in production developers have little control as to whether features should be active or not tell slide provides a server-side feature flag solution that enables developers to safely Deploy on demand instantly Taco featured on or off at runtime test code in production environment with so-called Dart launches control the release of features to selected users monitor features performance and automatically shut off poor performing features with recovery options for the next couple minutes a hypothetical startup health bar will be used to illustrate how future flag can simplify deployment and feature release for microservice oriented companies health bar is a startup that provides platform as the service related to information technology for Hospital medical and healthcare companies deployment at healthpart is often a major event because it impacts its entire user digital experience nature of patient data avoiding service downtime is crucial for its clients this means that before any code can enter production it must go through rigorous automated tests rounds of approvals among stakeholders at least typically takes health bar over a month to deploy new code into production which is sloped by industry tender recently management has asked development teams to implement more features to expand health bars customer base however due to low deployment frequency each deployment at health bar involves many new features which makes developers anxious before deployment due date the developers would like to be able to gradually introduce the new features to their clients so that they can gauge the performance and reliability of the new features before affecting all of their users the developers found out that they can achieve gradual run out of their new features with Canary deployment Canary deployment requires setting up a separate infrastructure that holds the new features alongside the existing infrastructure a law about their Islam placed before the two infrastructure to control user traffic to either version this allows developers to initially trickle users to the new version and gradually dial up user flow turreted over time if the new infrastructure encounters an issue the users are redirected back to the original version plus minimizing potential service outage with Canary deployment the teams are able to increase their deployment frequency and quickly deliver load features for customers upon seeing a team success and how the new features have matured enough the management wants to integrate those new features into help our Core Business offerings to stay competitive developers were confident deploying new features with Canary deployment because the new features were isolated they had little worry about interactions for conflicts among the teachers but integrating those new features into existing Services posted new challenges integration of features introduced dependency of featured amount team in addition individual features may work in production alone but may have conflicts when run together developers also want to test different combinations of features for the overall integration for instance a potential pollution may involve any combinations of features a b and c ult ultimately testing the Synergy of those features in production environment is the only way to ensure they function correctly on the real world conditions such as high traffic load infrastructure problems and uncountered client use cases to experiment with the possible combinations that requires multiple deployment and infrastructure for hosting those deployments which increases deployment cost and complexity with help buyers limited resources the developers need another deployment strategies to integrate those features with single deployment they found feature Flags at the simplest future black impact conditional logic into applications that allows developers to dynamically control the on or update of features at runtime foreign fundamentally future Flags allowed developers to decouple deployment from feature release such that deployment is now only about moving code into production and future release totally depends on runtime logic with picture Flags developers can dynamically toggle features on or off in production as well as relief features to selected users learning from their deployment experience the developers determined that the ability to monitor the performance of new features and automatically turn off the problematic features will be very helpful in reducing service downtime it prevents required from hitting Surplus that letter down and provides those Services recovery periods with limited traffic it also gives developers peace of mind for instance if they release new features on Friday they don't have to manually check or respond to whether the new features are functioning normally on the weekend the teams decide on using feature flags for its ability to test different combinations of features in production with a single deployment the ease of tackling features on or off without requiring expertise and the ability to reliability and responsively handle fallback strategies that developers specify while researching for future flags as a deployment and feature release strategy the developers find various existing solutions that range from Full Enterprise solution with observability and automatic circuit breaking to open source feature flag Frameworks launch Darkly is the most prominent Enterprise Solutions and the offered and easy to set up and feature Rich platform with structure breaking that health bar one however it comes with high service costs and required sensitive patient information to be stored on the cloud both are not an option for health bar furthermore since it is not open source the developers may not be able to configure it exactly the way they want at a later time for open source Frameworks such as unleash they typically offer either tough hosting of their framework or using their cloud services they offer many features just like launch directly but are harder to set up and don't come with circuit breaking finally they found test flight and open source framework that is fully self-hosted and easy to set up health bar only needs to pay for infrastructure costs well it only comes with essential feature flag options it does include the circuit breaking that developers need since tells light satisfied mode if not all of the requirements the developers at helpbar agree that it is their best option next Jordan will walk through Tesla architecture with demonstration thanks Steven hi I'm Jordan Welcome to our project tail slide this is a diagram of tailslide's overall architecture the tail slide consists of four main pieces the first of these is a full stack application called the tower it handles the management of feature flag rule set data and storage circuit breaking configuration information the second key piece of our system is the nas jet stream a message streaming system that facilitates communication between the other tail side components the third key part of tail side are the software development kits that are embedded in each individual user microservice these sdkas incorporate into the microservices the code that is required to react to flag State and circuit status the final part of our project is the aeroblast this application handles keeping track of success and failure rates generated via our sdks and determining when circuits need to be tripped now that we've gone over the overall architecture of tail side we're going to take a look at the deployment process tailside is a self-hosted open source feature flag framework since tailside is not a managed solution all data such as flag rule set data and user application information is stored on the user's own infrastructure and is not shared with any external parties tailsite provides a simple out-of-the-box Diplomat strategy which will spin up all components of tail side on a single server using docker depending on a user's needs tailside also offers individual components as separate repositories on GitHub these can each be hosted on individual machines or with any hosting service this provides the flexibility to scale and use different components as needed to deploy tail side with Docker a user needs to first clone the docker Branch from GitHub then ensure the appropriate configuration files are present and finally pass in an SDK key argument along with the docker compose up command now we're going to focus in on just the feature flag functionality in detail later on in the presentation Trent will cover the circuit breaking side of the functionality there are three components core to the feature flag architecture the tower application the natsjetstream and the software development kits one thing to note is that the blue components are tail side specific they're intended to support the user-provided microservices highlighted in Orange the software development kits shown in blue with the flags are a library intended to be incorporated in a user's microservice now we're going to zoom into the tower portion of tailsight this is a full stock application that handles the functionality related to feature Flag Management it consists of a react front-end user interface that allows users to do things such as create an app in a flag make edits to Flags toggle a flag on and off and view logs related to those flags all of the feature flag data is stored on a postgres database and whenever A change is made to the flag data the backend Express server publishes the full set of new flag-related information onto the natsjet stream where it is then dispersed to the rest of the project before we take a more detailed look at the other components the Nats jet stream and the sdks I'm going to show some examples of how to use the front-end interface if a user would like to create a new app they will want to navigate into the dashboard this brings the user to a page that shows all apps available in the project as you can see in the example health bar already has three in order to add an app the user will just need to provide an app name once the app has been created we can see it populated in the app screen as shown in the demonstration if the user would like to add a flag to an app they can click into that app this brings up a screen showing all of the feature Flags related to that particular app to add one we can bring up the ADD and edit flag model the user can then enter a name and a description of the new flag there are several other features that can be managed via this modal but for now we're just going to create this new flag to toggle a flag on or off a user can simply click the toggle button to the right of the feature flag in question if the user would like to see logs related to a specific flag they can navigate into that flag from the main flag screen that will bring up the flag information screen from there they can select the second logs tab at the top this will bring up the history of log information related to that particular flag if a user would like to see logs related to All Flags they can use the logs button on the left menu to see all logs related to every flag in every app on their deployment of tailside now we're going to move on to another main component of the tailsite architecture that is the next jet stream this is a third-party asynchronous message streaming service it's a key piece of tail side because we envision the system as being able to accommodate many different user microservices as the number of microservices plugged into tail slide increases so does the traffic between those microservices and the feature flag information stored in the tower we need Nets as a way to decouple the microservices from the tower application to keep Tails Library scalable another benefit of using mats is that it allows for real-time communication this means that if a user were to make a change such as toggling a flag on or off via the front end there would be no delay in that logic being available to the microservices for use in rendering feature Flags also knots offers guaranteed message delivery this means that if for some reason the tail side Tower work to go down or if a user's microservice went offline and needed to be spun back up after the latest feature flag data has been published to the jet stream there's no issue at all the not jet stream always stores a record of the last full set of feature flag data so when a microservice is brought online for the first time we're back online it will immediately have access to the latest information whenever there's an update to the feature flag data the tower publishes the full set of flag logic to the Nats jet stream the tailside software development kit embedded in each user microservice contains a Nats consumer that is listening for updated flag data once the consumer receives a new set of rules the SDK will store that information and can use the logic immediately when evaluating whether or not a new feature will be displayed going back to Steven's discussion of how useful it is for engineers to be able to deploy many new features at once and use the functionality of each independently the ability of this system to support events such as the user toggling a flag on or off from the front end without requiring a redeployment for application Behavior to change a response is a crucial benefit of tail slide and feature flags as a whole the final feature flag related component of the tailslide architecture is a software development kit embedded into each user microservice this SDK is responsible for consuming the most recently Pub flag data from an apps jet stream storing that information and using it for flag evaluation at runtime currently tailside provides sdks designed to be used in server-side applications to use the SDK a user will need to First install the SDK and provide a configuration object that includes an SDK key and app ID which is a number representing the application the user's microservice belongs to and a Nat server URL to connect into the jet stream each flag into microservice will require its own instantiation of the Toggler this Toggler will be used to evaluate whether the new or existing code will be run the code on the right here shows an example of a configuration object and the instantiation of a new Toggler the if else conditional statement at the bottom of the code represents the heart of tailsite's flag toggling logic we'll get into how this works a little more over the next couple of slides but first I want to talk about two key drivers for whether a flag gets toggled on or off the first is rollout percentage this is a number that represents the percentage of end users that a developer intends for the new feature to be enabled for the second term is whitelisted users these are users that are intended to always be shown a new feature these could be developers or other people on the product team who need to test the feature out within the tailside tower database whitelisted users are kept track of as a comma separated string of uu IDs as you can see from these images the tailslide user interface allows developers to update whitelisted users and change the rollout percentage for any flag as they see fit that if else statement that I pointed out earlier as the heart of whether or not a new feature is called uses the is flag active function picture here this function first checks if a particular flag is set to active if the flag is set to inactive the flag will evaluate to false and fall back to the pre-existing code within the else statement if the flag is set to active the function will then check if the user making a request to the microservice has a uuid that is white listed if their uuid is a match for any of the uuo IDS stored in that particular flag's information is whitelisted the flag will evaluate to true this means that the new feature code in the If part of the statement will be evaluated if the flag is set to active and the requester is not whitelisted the function will make the final determination based on the rollout percentage this involves hashing the uuid to a number between 1 and 100. if that hash number turns out to be greater than the rollout percentage the flag will evaluate to false if the hash number turns out to be less than or equal to the rollout percentage the flag will evaluate to true and now Trent is going to explain tailside circuit breaking functionality thanks so much Jordan let's now take a look at the second portion of tail slides architecture circuit breaking calls to a new feature over a network introduces an additional point of failure whether the new feature itself fails or the connection between service to grade this can lead to a poor end user experience for further adages within an application tailslide has implemented the circuit breaker stability pattern to protect against these failures tailsite has taken a unique approach to implementing this pattern within its architecture in order to serve the deployment of new features as well as integration with its future Flag Management System the circuit breaking architecture integrates with feature Flags by observing the health of connections to new services and automatically closing an octave connection when failure occurs connections that Services can then be re-established through an automated recovery process tail slide circuit breaking implementation can be broken down into four stages versus the monitoring and emission of success and failure data related to the new feature deployed behind a feature flag second is the storage of this data in a redis Time series cache next is the evaluation of error rate and circuits that they handled by tail a tail slide application named Arab lastly is the propagation of any change in circuit debate back to tower via natural jet stream which is then published back out to all sdks completing a full cycle of messaging Within tailslide the first stage of circuit breaking is the emission of success and failure data the sixth place for the microservices themselves by monitoring the successive calls to a new feature using the tailslide SDK Jordan has already covered the import and configuration of the tailslide SDK as well as instantiating a new Toggler object or a flag once instantiated the object not only exposes the flag evaluation method but also an emit success and emit failure method called directly on the object these methods are configured to emit a success or failure data point by writing directly to a redis Time series cache and should signal a successful or failed operation as related to the new feature as opposed to a traditional circuit breaker function wrapper these methods are exposed in order to allow the developer to explicitly call as they see fit based on their own success or failure conditions redis time series is a Time series module built on top of a redis cache the tail slide SDK for each slide microservice is configured to connect directly to the cache for writing via the emit methods each right contains the flag ID the success or failure status as well as the app ID as metadata each data point written to the cache is stored under a key defined by flag ID and the status of the operation each key holds all data points associated with its flag thanks to the time series data structure of the cache at times table that added to each data point correlating with the time at which the success or failure took place this is an important aspect of tailed slide circuit breaking capabilities and allows for querying aggregate data within a specified time range this is the key to evaluating real-time error rates and also enables tailslide to provide graph visualizations of circuit health slide is also built to handle scaled microservice applications and thus evaluate circuits in aggregate all instances of a new feature deployed via feature flag even if scaled horizontally many times are stored and analyzed as a single circuit ensuring the consistent application State for the entire user base the next stage in tail slide circuit breaking architecture is Tangled by arabat aerobat is a lightweight node application responsible for querying redis and evaluating the current error rate of flags aerobat fetches data by pulling redis at a configurable interval querying for some success and failure counts submitted by application microservices each pull queries for some success and failure counts over a developer defined window of time this window effectively acts as a sliding window constantly checking the thumbned values starting at the time of the query this ensures that any sudden spikes and data are smoothed over and gives a more accurate view of the current error rate all flag data is efficiently fetched in the single query through the use of an app ID metadata tag arabat is responsible for parsing of data for each flag before evaluation with these summed values airbat is able to calculate the current error rate for each flag within a specified window in real time each polar bread that's updated to the values allowing aerobat to constantly monitor the health of deployed features in connection to them in order to evaluate circuits aerobat must be aware of the developer configurations cut for each flag aerobat receives these circuit rule sets by subscribing the netjet stream consuming updated rule sets as they're published by Tower anytime an update is made this lets their about know what flags are active the developer set error threshold and any automated recovery configurations I heard that story these rules internally for evaluation updating the state any time any rules that is received each time redis is pulled an error rate is calculated aerobat will then evaluate the error rate against the error threshold to determine if the circuit should be tripped open if the error rate surpasses the error threshold the circuit is tripped open at this point aerobat must then publish a message to Nat the notified Tower of this change in circuit state Tower is then responsible for updating the circuit to date by toggling the appropriate flag off and propagating this change to the SDK instances dnf effectively cutting off all traffic directed to the new feature while the circuit is in an open state the circuit will remain in an open State until a developer toggles the flag back on or the circuit is automatically recovered while tail slide's default behavior of tripping open a circuit protects against the failing feature it still requires an engineer to respond to the failure in order to turn the flag back on and close the circuit this time to recovery can increase that the engineer is unavailable especially during weekends or nights leading to additional downtime for a feature and a degraded user experience tailsite is implemented an automated recovery system that follows the principles of a circuit breaker half open state which Tails fly refers to as in recovery the display is shown here is a view of the Tails applied dashboard showing the circuit monitoring view available for each flag this display gives developers a real-time visualization of the help of the circuit at the top is a graph displaying error rate against error threshold with an additional view of request counts over time the scrap is updated in real time by pulling the redis cache for current data over the selected window below that is a live view of all circuit recovery configurations including the current state as well as the current health of the circuit itself through the use of math websocket integration this display also updates in real time giving developers a powerful yet concise view of a circuit walking through the stages of recovery the display to the right will first show a circuit exceeding of error threshold and entering an open state when initially tripped open a recovery delay period will begin during which the circuit will remain open and the flag will continue to continue evaluating faults preventing any traffic to the new future and hopefully allowing time for the future or connection to recover after this delay period the circuit will enter into recovery while in recovery the circuit will reclose turning the flag back on but only directing A reduced portion of users to the new feature determined by the initial recovery percentage during this recovery period aerobat will continue to monitor the error rate of the circuit and if the threshold is surpassed reopen the circuit and start the recovery process again arabat will incrementally increase the percentage of users directed to the new feature configured via the recovery rate and Recovery increment percentage the circuit Health display shows current percentage being directed to the Future if the help is able to reach 100 arabat will send a message that this Cricket has recovered and the circuit will return to a fully closed State directing the full rollout percentage of users to the Future although the circuit monitoring dashboard page allows real-time monitoring it's also important developers can be aware of changes in circuit State when not actively monitoring whether to reduce mean time to recovery or to debug a circuit that continues to trip open tail slide allows developers to configure slack Channel alerts for any changes in the circuit State this configuration is as simple as entering the Web book URL into the flag setup card tail slide will then send a Web book request with details of the flag and its updated state anytime there's a change giving developers a peace of mind knowing they will be immediately alerted of any circuit changes I'll now pass it off to Elaine who will discuss some of the engineering and trade-off decisions made while developing and building tail slide thanks Trent I'm Elaine and I'll first walk us through the various engineering trade-offs we made as we built tailslide one of the major decisions we had to make was around communication how were we going to deliver flag information to the sdks and to aerobat our solution needed the following features it needed to be able to support a published subscribe messaging pattern which is an asynchronous services communication pattern in this messaging pattern Publishers publish messages to a subject and fan out those messages to subscribers in real time in our case upon any change to the flag rule set our server would need to publish the updated flag rule set to a subject where it would then be fanned out to all subscribers which would be the user microservices this ensures that all user microservices receive the latest flag rule set it also needed to be fault tolerant meaning it could provide guaranteed delivery due to the fact that networks and distributed systems are inherently unreliable if a user microservice went down or became unavailable due to a temporary Network partition our solution would still need some mechanism to guarantee delivery to the service once it was up again it also needed some way to persistent messages and provide message replay of the last sent message message replay is a feature of some message Brokers which allows them to resend messages to new or existing subscribers regardless of when those messages were first received by the message broker by having message replay when a new user microservice comes online they can immediately be pushed the last sent message representing the latest flag rule set also needed to be able to authenticate both Publishers and subscribers securing the flag rule set data and ensuring that only applications that should have access to the flag rule set get that access lastly it needed to be easily Deployable and relatively lightweight with these requirements in mind we consider different Alternatives or one we considered utilizing our server to directly serve flag for the user microservices via Service Center event the solution is easy to deploy as the server is already part of our existing infrastructure however the solution would be entirely DIY and would result in us having to engineer all of the desired features from scratch which although provides us with maximum customizability would be a complex task another key issue with the solution is that It suffers from a lack of separation of concerns as our server would now need to service requests from endpoints communicate with the database and also be responsible for sending updated flag rule set to every instance of a user microservice we also considered various message Brokers such as rabbit mq which is a lightweight open source message broker but unfortunately it did not support message replay we looked into Kafka as well and although it has all the capabilities we required it is quite a complex and feature-rich tool and is primarily meant to be used for very high volume throughput event-driven architectures due to the fact that we don't anticipate flag rule set data being changed on a very frequent basis we opted not to pursue usage of Kafka finally finally we landed on Match jet stream which is an open source message broker that met all of our requirements including supporting Pub sub communication being fault tolerant and guaranteeing delivery providing message replay and authentication and being a lightweight and easy to deploy message broker other decisions however needed to be made particularly around how we were going to architect our circuit breaking functionality our proposed solution needed to implement the following it needed to emit Telemetry data specifically the data needed to differentiate between a feature of flag that is operating as expected or failing once emitted we would need an appropriate data store in order to hold that data lastly we will need to query that data store at a periodic interval in order to assess the current error threshold and compare that to the configured error threshold as defined by the user in order to assess if a circuit was tripped if so we would then need a way to communicate that information to propagate that change across all user Medical Services with these high-level problems in mind I'll now discuss the different Alternatives we considered when it came to emitting success and failure data we had different proposals here including emitting observability Telemetry data or emitting based on user-defined success and failure events emitting observability Telemetry data consists of enriched data that includes things such as logs and metrics although in Rich the data could require additional infrastructure for instance a Telemetry collector could be needed in order to receive process and Export the data ultimately the solution came with increased complexity and because we are looking for a lightweight and simple solution we opted not to pursue this the other option we considered was to give users the ability to specify the conditions of a failure or success directly in their code using the emission methods defined in our SDK this approach differs from traditional circuit breakers which can be more rigid in their evaluation criteria these are often implemented by wrapping HTTP requests between synchronous service to service communication and predefining success and failure criteria based on response status codes our approach which allows the user to decide for themselves what their specific success and for your failure criteria would be gives flexibility to the user and is also simple to use the second decision we had to make was around the storage of this data and what datastore would be suitable our data store would have to support a high volume right throughput because every user microservice will be writing to the data store when it encountered the Omission methods the data store would also need to support reads that would occur on a periodic basis this happens when aerobac queries the data store to a set if a circuit has stripped and lastly the data we were storing was ephemeral in nature and was very simple it would just consist of an identifier for the flag a timestamp that indicated when the write occurred and if it was a failure or success event to leverage our existing infrastructure one option we considered was having the sdks published directly to a success or failure message log topic within Nets we would then query that message log topic periodically although feasible consistent time bucketed querying of persisted messages in a message broker is more complex and evolved process and is not natively supported by Nets and would require custom code due to the time series like nature of our data we also considered using a feature-rich Time series database that should in fact DB if we had decided to pursue a full-fledged observability approach with Rich Telemetry data the complexity of our data increases and could benefit from the additional features provided by a Time series database however due to the Simplicity of our data and our specific use case as we didn't require these robust features we chose not to pursue this instead we ended up using an implementation of redis that added a Time series data structure to Revis redis is an open source lightweight in-memory data store which was simple to deploy simple to use and allowed for high volume throughput and performant read and write capabilities using redis time series allowed us to easily group the time series data into buckets for assessment by Errol back and allowed us to set a time to live on that data so stale data could be cleared out periodically an important feature due to the fact that high volume of data needed to be ingested time bucket of data could then be sent to the front end for graphing and visualization purposes lastly a third decision we had to make was around how we wanted to communicate to the server if the error threshold exceeded the configured threshold one option would involve setting up an API endpoint on the server and having aerobac communicate directly with the server using HTTP requests sent over the network although feasible this approach is not fault tolerant and could be susceptible to drop messages if there was a network partition instead to improve reliability and guarantee message delivery we opted to have aerobat public published to Nets and we configured the server to consume for nets for the appropriate circuit breaking subjects overall our circuit breaking architecture resulted in successive failure emissions being flexibly defined by the user Telemetry data being stored in a redis Time series datastore and when circuits were tripped or recovered messaging being sent via Nets to notify the server of changes that concludes our engineering trade-off discussion and I now I'd like to take you through some future work we have in store future work we are considering includes having user accounts authenticate before being granted access to the front-end UI in addition we would like to build out the ability to see the detailed live change history such as who toggle the flag on or off or who edited the circuit breaking options for a flag we also looking towards adding fdks and additional languages to support our users currently the client Library sdks we support include python golang JavaScript and Ruby in the future we would like to add additional popular back-end languages such as c-sharp and Java lastly we would like to add the ability for users to Circuit very based on other trigger criteria such as user-defined HTTP response codes and user configured response time thresholds specifically when the circuit breaker is used for synchronous service to service communication all that concludes all presentation I'd like to thank everyone for coming and I'd like to open up the floor to any questions anyone might have all right uh I can read the question I'm curious to hear more about what you did to explore Kafka and at what point you decided not to pursue it further uh okay so I can take this question thanks for the question miles we considered Kafka early on in our architecture as we thought through what pieces we'd like to use in order to construct our feature flag functionality uh ultimately we decided not to pursue it further just due to the types of use cases you would normally see Kafka used in it's normally meant for very high volume throughput event-driven scenarios where you're streaming um thousands of requests a second and for our specific use case since we the flag rule set data being toggled on or off on not as frequent of a basis we opted not to pursue YouTube pasta just to avoid the complexity of that tool if anybody in the group would like to add on feel free okay second question which pieces were the most interesting to work on I guess I can answer from a personal standpoint here uh just because I specifically covered a lot of the circuit breaking um I'm sure everybody kind of found different portions most interesting but I thought it was really fun to kind of figure out how to integrate that um within kind of this feature deployment we're going over in the future flag system um it kind of offered up a lot of interesting challenges just because it's a fairly established uh stability pattern it's been around for a while it's been implemented by some pretty big names but each implementation kind of works differently whether it's at the single process level whether it's looking at an application more holistically and the Integrations we've kind of seen with feature plug systems out there were pretty specific to very large Enterprise Solutions a lot of integration scandaling you know these these applications that maybe had a very big full stack observability tool already entered so kind of our problem space of approaching this from a smaller company that kind of wanted this functionality built in to Future deployment uh just offered up so many kind of different challenges you know that Elaine kind of went over uh and it was just a lot of fun kind of uh deciding how we were going to implement that and and kind of just finding all the little uh trade-offs on the way um and sorry I see the next yeah go ahead no I was just gonna say I was gonna throw in my own um like I thought it was pretty interesting to learn a little more about like Nets and uh just like the idea of how microservices communicate with one another as a whole and kind of like thinking through that problem um I don't know if Elaine Stephen do you guys want to answer your favorite too yeah I can uh um pop up with Ben and Jordan have mentioned I think personally it was really rewarding when we took the time in the beginning to plan out our architecture or how all the pieces communicate with each other and it was very satisfying that because we took the extra time to make sure that the communications between the components are solid when we implemented all those components it was really amazing to see how all the pieces fall together and if they want to add a link uh not too much I feel as if most of my teammates covered uh most of the interesting things I also found it really rewarding just to be able to work in a environment with uh other Engineers um including some group and pair programming it's just really nice to see how other people approach a problem and how we can talk about things whenever we come across uh issues or discussions that we'd like to have uh I'm just going to move on on to Missy's question uh and she'd like to know exactly how does a user currently specify what constitutes success or failure for a particular feature yes I can answer that one um that was kind of covered with the concept of that amid success sentiment failure method that's exposed in the fdk and we did that very intentionally to give some flexibility to the developer because if you take a look at a lot of the traditional circuit breaking libraries um what they'll do is they'll have you pass in an async request function to a Constructor and they'll return an object that wraps that function and internally within their their SDK uh they'll monitor essentially the response coming back from that request and then they're usually going to be taking a look at the exact status code so then they'll have some sort of like a pretty strict definition of what's success or failure so they'll say you know anything in a 200 range is a success anything outside for that is a failure uh and we felt that that was a little bit constricting especially in the sense of you know leading up to how the developer might want to deploy this new feature through various means so we decided to expose those methods directly in the sense that they could simply just call the amid success stay within a try catch block or if they wanted to kind of build it in any way they could then kind of Define those conditions that they thought determined whether it was a success or a failure which could be you know maybe checking that code and maybe you're open to some other status codes not throwing an error so we kind of just did that for giving the developer a little more flexibility and control over over the implementation um while I'm at it I guess I'll just read the next question what which is uh can you talk a little bit more about the incorporation of Reddit and at what point in the design process was that considered and integrated Audrey's question so originally we had three different ideas of how we want to distort that success or failure information one solution as mentioned in the presentation was using natsjetstream to store those information and then we can grab the information from the next jet stream and used in our own logic to calculate the error rate for say the past 10 minutes but we found out that redis had this time series data structure implementation that we can utilize since we're only interested in aggregate better rate for a period of time we can use read this time series to make a simple query to get that error rate information we need so after considering the trade-off we decided that using redis time series was much simpler implementation also we were originally considering storing other observability information like logs or metrics for the minimal implementation we decided not to store those information so we disregarded the option of having a full open Telemetry observability option there so yeah that's how we ended up with the British time series approach yeah and I'll quickly add on to that it had some other kind of very powerful properties that ended up uh making the best option including that you can essentially set like a full time to live on your entire time series cache if we could that about an hour and all of our data would be expired on a rolling basis um making it super super lightweight uh and then it also has very kind of like powerful uh query language for time series even allow allowing us to uh create kind of buckets within say a time window range so we kind of receive a data response with play a 10 second window we could Buck it within that uh each second and then receive the stun value of each second uh which was really useful for doing some visualization on the front end okay uh next question did the entirety of your collaboration occur remotely and yes it did uh next question uh Trent mentioned observing other solutions that offer circuit breaking logic are there any recognized standards protocols or best practices that you were able to follow or was it mostly taking leads from existing Solutions um did a good amount of research into this uh a lot of other kind of like the base references you'll see kind of with any looking into circuit breaking is the an older Martin Fowler article where he then references a book by uh Michael Nygaard called release it where he kind of discusses a lot of stability patterns uh related to distributed applications um so that's kind of where the whole concept was I guess first laid out uh within this context as far as implementation um Netflix's uh District circuit breaker was like a big one that's now been deprecated um and now there's like some kind of major language specific libraries out there so we kind of took a look at that um most of them are quite configurable because realistically application the application you're going to have kind of different needs in terms of uh what you kind of Define as a condition for breaking up in a circuit uh as well as that kind of concept of half open recovery um so that's why we tried to really allow a pretty granular amount of control over that recovery and that circuit breaking process but generally speaking as far as standards usually based on error rate when looking at requests to a service um based on a uh then an error threshold that's been surpassed there are some libraries out there that handle it based on maybe like a timeout condition but uh there weren't really you know a ton of best practices in the sense that it's very specific to the actual implementation uh in the application I would like to add that circuit breaking logic is taken from electrical and Engineering concept I guess the logic itself is just a fundamental um circuit breaking pattern there but what makes it interesting is depends on the architecture of your microservices or applications the interactions among those moving Parts can very greatly yeah that's an interesting part of implementation of the circuit breaking logic all right uh I think at this point if there are no more questions we will conclude our presentation uh we'd like to thank everyone for attending and for your questions and for your attention throughout our presentation uh and thanks so much enjoy your Wednesday everyone bye 