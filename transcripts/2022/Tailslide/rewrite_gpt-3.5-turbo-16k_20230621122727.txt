Today's presentation is about HealthLife, an open-source speech per flag framework designed for easier code deployment with automated Builder protection. Our team consists of remote developers who have been working on HealthLife for the past few months. In this talk, we will introduce the problem domain that HealthLife focuses on, explain what feature flags are and how HealthLife fits into the competitive landscape. We will also discuss the parallel architecture and demonstrate its feature flag and circuit breaking capabilities. We will cover the engineering decisions we made, future work for HealthLife, and end with a Q&A session.

At a high level, HealthLife is a feature flag solution that simplifies code deployment and feature release. Deployment is the process of moving code from development to production, while feature release makes new or changed functionalities available to users. Traditional deployment methods often result in unpredictable repair wait times and extended service downtime. With HealthLife, developers can safely deploy on demand, toggle features on or off at runtime, test code in the production environment, control the release of features to selected users, monitor feature performance, and automatically shut off poor performing features.

To illustrate how HealthLife simplifies deployment and feature release for microservice-oriented companies, let's consider a hypothetical startup called HealthBar. HealthBar provides a platform as a service for hospital, medical, and healthcare companies. Deployment at HealthBar is a major event, impacting the user experience and the security of patient data. To avoid service downtime, code must go through rigorous automated testing and stakeholder approval. It often takes over a month for HealthBar to deploy new code into production. This slow deployment frequency makes developers anxious, especially when new features need to be implemented.

Developers at HealthBar want to gradually introduce new features to their clients to gauge their performance and reliability before affecting all users. They found that Canary deployment, which involves setting up a separate infrastructure to hold new features alongside the existing infrastructure, allows them to gradually redirect user traffic to the new features. This approach minimizes potential service outage and enables the teams to increase their deployment frequency.

However, integrating those new features into HealthBar's core business offerings poses new challenges. Integrating features introduces dependencies and requires testing the synergy of different combinations of features in a production environment. Traditional deployment methods, which involve multiple deployments and infrastructure, increase deployment costs and complexity. With limited resources, HealthBar's developers need a deployment strategy that allows them to integrate features with a single deployment.

They found that feature flags provide a solution. Feature flags add conditional logic to applications, allowing developers to dynamically control the on or off state of features at runtime. Feature flags decouple deployment from feature release, enabling developers to deploy code into production and release features based on runtime logic. This approach allows testing of different combinations of features in production with less cost and complexity.

During their research, the developers at HealthBar came across various existing solutions for feature flags. LaunchDarkly is a prominent enterprise solution that provides observability and automatic circuit breaking. It is easy to set up and offers many features but comes with high service costs and requires storing sensitive patient information on the cloud. Unleash is an open-source feature flag framework that offers many features similar to LaunchDarkly but is harder to set up and lacks circuit breaking capabilities. Finally, they found TestLife, an open-source framework that is fully self-hosted, easy to set up, and includes circuit breaking capabilities. While TestLife may not have all the features of LaunchDarkly, it satisfies most of HealthBar's requirements and is the best option for them.

Next, let's discuss TestLife's architecture. TestLife consists of four main components: the Tower, Nats JetStream, software development kits (SDKs), and Aeroblast. The Tower is a full-stack application that manages feature flag rule set data, storage, and circuit breaking configuration information. Nats JetStream is a message streaming system that facilitates communication between the components. The SDKs are embedded in each user microservice and handle flag state and circuit status reactions. Aeroblast tracks success and failure rates generated by the SDKs and triggers circuit breaking when necessary.

TestLife can be deployed using Docker. The process involves cloning the Docker branch from GitHub, ensuring the appropriate configuration files are present, and passing in an SDK key argument with the Docker compose up command. The user's data, including flag rule set data and application information, is stored on their own infrastructure and not shared with external parties.

Now let's focus on the feature flag functionality in more detail. The Tower application is the core component responsible for feature flag management. It has a React front-end user interface that enables users to create and edit apps, toggle flags on and off, and view flag-related logs. All flag data is stored in a PostgreSQL database. When changes are made to the flag data, the backend Express server publishes the updated flag information onto the Nats JetStream, which is then distributed throughout the project.

The Nats JetStream is a third-party asynchronous message streaming service used by TestLife to decouple microservices from the Tower application. It enables real-time communication and ensures guaranteed message delivery. When a change is made to the feature flag data in the Tower, the updated flag logic is published to the JetStream. The TestLife SDKs, embedded in user microservices, listen for updated flag data and store it for immediate use when evaluating feature availability.

In conclusion, HealthLife is an open-source feature flag framework that simplifies code deployment and feature release. By decoupling deployment from feature release, HealthLife allows developers to toggle features on or off, test code in production, control feature releases, and monitor performance. It provides a self-hosted, easy-to-set-up solution with circuit breaking capabilities. With its parallel architecture and feature-rich functionality, HealthLife is a valuable tool for microservice-oriented companies like HealthBar. Title: Tailsite Coding Capstone Project

Introduction:
In this video, we will discuss the coding Capstone project known as Tailsite. We will provide a detailed overview of its key components, including the flag management system and the circuit-breaking functionality. Let's begin!

Flag Management System:
The flag management system in Tailsite allows users to create and toggle feature flags. Users can enter a name and description for each flag. Although there are other features available in this system, we will focus on creating new flags in this discussion.

To toggle a flag on or off, users can simply click the toggle button next to the desired feature flag. If users want to view logs related to a specific flag, they can navigate to that flag's information screen from the main flag screen. From there, they can select the logs tab to see the history of log information associated with that flag. For logs related to all flags, users can use the logs button on the left menu to view logs for every flag across all apps in their Tailsite deployment.

Nats Jet Stream:
Another crucial component of the Tailsite architecture is the Nats Jet Stream. This third-party asynchronous message streaming service is essential for accommodating multiple user microservices and managing the increasing traffic between them and the feature flag information stored in the tower. Nats enables the decoupling of microservices from the tower application, ensuring scalability.

Real-time communication is a significant advantage of using Nats. Any changes made, such as toggling a flag on or off via the front end, are immediately available to microservices without any delay. Additionally, Nats provides guaranteed message delivery. If the Tailsite tower or a user's microservice goes offline and needs to be brought back online, the Nats Jet Stream stores the last full set of feature flag data. This ensures that the microservice can access the latest information without any issues.

Tailsite Software Development Kit (SDK):
The Tailsite SDK is embedded into each user microservice and plays a crucial role in flag evaluation at runtime. To use the SDK, users need to install it and provide a configuration object containing the SDK key, app ID, and Nats server URL for connecting to the Jet Stream. Each flag and microservice requires its own instantiation of the Toggler. This Toggler evaluates whether new or existing code will be executed.

The heart of Tailsite's flag toggling logic lies in the if-else conditional statement. The is_flag_active function determines whether a flag is active or inactive. If the flag is set to inactive, the code falls back to the pre-existing code inside the else statement. For an active flag, the function checks if the user making the request has a UUID on the whitelist. If the UUID is whitelisted, the new feature code is evaluated. If not, the function determines whether the rollout percentage permits displaying the new feature by hashing the UUID. If the hash number is greater than the rollout percentage, the flag evaluates to false; otherwise, it evaluates to true.

Circuit Breaking Functionality:
Tailsite employs circuit breaking to handle network calls to new features and alleviate potential failures. This architecture protects against poor user experiences caused by feature or service failures. Tailsite's circuit-breaking implementation comprises four stages, and let's dive into them now.

Stage 1: Emitting Success and Failure Data
Microservices emit success and failure data by monitoring calls to new features using the Tailsite SDK. The SDK features methods for emitting success and failure data, which writes directly to a Redis Time series cache. These methods allow developers to explicitly signal whether an operation succeeded or failed. Each data point includes the flag ID, success/failure status, and app ID as metadata. The Redis Time series cache stores the data points under a key defined by flag ID and operation status, enabling querying and visualization of aggregate data.

Stage 2: Redis Time Series Cache
The emitted success and failure data is stored in a Redis Time series cache. This cache provides a Time series data structure, allowing efficient querying and visualization of aggregate data within a specified time range. Each data point contains metadata such as flag ID, success/failure status, and timestamp. The Redis cache ensures consistent application state across multiple instances of a new feature deployed via feature flags.

Stage 3: Arabat Application
Arabat, a lightweight Node.js application, retrieves data from Redis and evaluates error rates for each flagged feature. It periodically pulls the Redis cache for success and failure counts within a specified time window. By parsing this data, Arabat calculates the error rate for each flag and compares it to the error threshold set by developers. Arabat subscribes to updates from the Nats Jet Stream, enabling it to receive circuit rule sets for evaluation, including flag activity, error threshold, and recovery configurations.

Stage 4: Propagation and Recovery
When the error threshold is surpassed, Arabat triggers the opening of the circuit and notifies the Tailsite tower through Nat's Jet Stream. The Tailsite tower updates the circuit state, toggling the appropriate flag off and propagating this change to all SDK instances. When a circuit is open, no traffic is directed to the new feature. Tailsite offers a circuit monitoring view in its dashboard, providing real-time insights into circuit health.

To safeguard the system's availability, Tailsite includes an automated recovery system. After an initial circuit is opened, the recovery process begins. Gradually, the circuit closes again, allowing a reduced percentage of users to access the new feature. This recovery process is repeated, incrementally increasing the user percentage until the circuit fully recovers. Once the circuit recovers, Arabat sends a recovery message to the Tailsite tower, ensuring the SDK instances reconnect and traffic is restored to the new feature.

Conclusion:
In conclusion, Tailsite's flag management system and circuit-breaking functionality make it a powerful tool for managing feature flags and handling potential failures. The Nats Jet Stream facilitates decoupling of microservices and ensures real-time communication. The Tailsite SDK enables flag evaluation at runtime, providing flexibility in feature deployment. With the circuit-breaking architecture, Tailsite protects against poor user experiences and offers automated recovery. By combining these components, Tailsite empowers developers to efficiently manage and monitor feature flags. The circuit is tripped open, and Aerobat must publish a message to the notified Tower about this change. The Tower is responsible for updating the circuit by toggling the appropriate flag off and propagating this change to the SDK instances. This effectively cuts off all traffic directed to the new feature while the circuit is in an open state. The circuit will remain open until a developer toggles the flag back on or the circuit automatically recovers.

Tail Slide's default behavior of tripping open a circuit protects against failing features, but it still requires an engineer to respond and close the circuit. This could lead to increased downtime and a degraded user experience, especially if the engineer is unavailable during weekends or nights. To address this, Tail Slide has implemented an automated recovery system called "In Recovery." It follows the principles of a circuit breaker's half-open state.

The Tail Slide dashboard provides developers with real-time visualization of the circuit. It includes a graph displaying error rate against the error threshold, as well as a view of request counts over time. The display is updated in real time by pulling data from the Redis cache. Below the graph, there is a live view of all circuit recovery configurations, including the current state and health of the circuit.

Through the use of WebSocket integration, the display updates in real time, giving developers a concise view of the circuit's stages of recovery. When a circuit exceeds the error threshold and enters an open state, a recovery delay period begins. During this period, the circuit remains open, and the flag continues to evaluate faults, preventing traffic to the new feature. After the delay period, the circuit enters recovery and recloses, turning the flag back on but directing a reduced portion of users to the new feature based on the initial recovery percentage.

Aerobat continues to monitor the error rate of the circuit during recovery. If the threshold is surpassed, the circuit reopens, and the recovery process starts again. Each time, the percentage of users directed to the new feature increases incrementally. The circuit health display shows the current percentage being directed to the feature, and if it reaches 100, Aerobat sends a message that the circuit has recovered, and it returns to a fully closed state, directing the full rollout percentage of users to the feature.

While the circuit monitoring dashboard allows real-time monitoring, it's also important for developers to be aware of circuit state changes when not actively monitoring. To reduce mean time to recovery and debug a circuit that continues to trip open, Tail Slide allows developers to configure Slack channel alerts for any changes in circuit state. This configuration is as simple as entering the Webhook URL into the flag setup card. Tail Slide will send a Webhook request with details of the flag and its updated state, immediately alerting developers of any circuit changes.

Moving on to the engineering and trade-off decisions made during the development of Tail Slide, one major decision was around communication. The solution needed to support a publish-subscribe messaging pattern, provide fault tolerance, support persistence, and enable authentication. After considering various alternatives, we chose Match Jetstream, an open-source lightweight message broker that met all our requirements.

Another decision was around the storage of circuit data. We needed a data store that could handle high volume read and write throughput, support periodic querying, and store simple ephemeral data. Redis, an in-memory data store, proved to be a suitable choice with its simplicity, high throughput, and performant read/write capabilities. We utilized Redis Time Series, an implementation of Redis that added time series data structure, allowing us to group data into buckets and set a time to live for stale data.

The third decision revolved around communicating circuit changes to the server. Rather than setting up an API endpoint, we chose to have Aerobat publish to Nets, a messaging system, to ensure reliable message delivery. This decision improved the overall reliability of the system.

As for future work, we are considering adding user account authentication for accessing the front-end UI and adding detailed live change history to track flag toggles and circuit breaking edits. Additionally, we plan to expand SDK support to include popular backend languages such as C# and Java. We also aim to enhance circuit breaking capabilities based on other trigger criteria, such as user-defined response codes and response time thresholds.

Thank you for attending the presentation, and we will now open the floor to questions. One question asked about exploring Kafka: we did consider Kafka as a possible solution but decided against it due to its complexity and mainly being designed for high volume throughput event-driven scenarios that didn't align with our flag rule set data changes. Another question asked about the interesting aspects of the project: from a personal standpoint, working on the circuit breaking functionality and engineering the emission of telemetry data were particularly interesting. In this Capstone project, we set up an API endpoint on the server for aerobac to communicate directly with the server using HTTP requests over the network. However, we realized that this approach was not fault-tolerant and could lead to dropped messages in case of a network partition. To improve the reliability and guarantee message delivery, we decided to publish aerobat to Nets and configured the server to conser for Nets for the appropriate circuit breaking subjects.

Our circuit breaking architecture allowed the user to flexibly define successive failure emissions. Telemetry data was stored in a Redis Time series datastore, and messaging via Nets was used to notify the server of circuit trips or recoveries. This approach ensured better reliability and message delivery.

For future work, we plan to implement user account authentication to grant access to the front-end UI. Additionally, we aim to provide detailed live change history, including information about who toggled flags on or off and who edited circuit breaking options for a flag. We also plan to expand our support by adding SDKs and additional languages. Currently, we support Python, Golang, JavaScript, and Ruby, but we plan to include languages like C# and Java.

Lastly, we aim to enhance the circuit breaking functionality by allowing users to circuit break based on other trigger criteria, such as user-defined HTTP response codes and user-configured response time thresholds, particularly for synchronous service-to-service communication.

Thank you everyone for attending the presentation. We now open the floor for any questions.

Q: Can you share more about your exploration of Kafka and why you decided not to pursue it further?

We considered using Kafka early on in our architecture planning phase to build our feature flag functionality. However, we ultimately decided not to pursue it further due to the specific use cases where Kafka is typically employed. Kafka is ideal for high-volume, event-driven scenarios where thousands of requests are streamed per second. Since our use case involved toggling flag rule set data, which doesn't happen frequently, we decided to avoid the complexity associated with Kafka.

Q: Which part of the project was the most interesting to work on?

Answering from a personal standpoint, working on the circuit breaking aspect was particularly intriguing. Integrating circuit breaking into the feature deployment offered interesting challenges. It was fascinating to explore different implementations and trade-offs considering the established nature of circuit breaking as a stability pattern. Typically, circuit breaking is implemented at either the single process level or the application level, and we had to find the best approach for our smaller company's needs.

Q: Can you provide more details about the incorporation of Redis and when it was considered during the design process?

During the design process, we initially brainstormed three different ideas for storing success or failure information. One option was to use Nats JetStream to store this information, but we found that Redis had a time series data structure implementation that suited our needs. Since we only needed aggregate error rates for specific periods, Redis' time series allowed us to make simple queries to obtain the required error rate information. Considering the trade-offs, we decided that using Redis for time series was a simpler implementation. We also chose not to store additional observability information like logs or metrics for the minimal implementation.

Q: Was your collaboration entirely remote?

Yes, our entire collaboration took place remotely.

Q: Did you follow recognized standards, protocols, or best practices for implementing circuit breaking, or did you rely on existing solutions?

We conducted extensive research on circuit breaking and referred to well-known references such as an older Martin Fowler article and Michael Nygaard's book, "Release It." These resources laid the groundwork for understanding stability patterns in distributed applications. While there are language-specific circuit breaking libraries available, we found that most implementations are highly configurable. We aimed to provide developers with flexibility and control over determining success or failure by exposing methods that allow them to define their own conditions. While there are no strict standards or best practices for all scenarios, determining success or failure generally revolves around considering error rates and thresholds.

If there are no further questions, we conclude our presentation. Thank you all for attending and for your attention throughout the session. Have a great day! Thank you all for attending our presentation and for your engagement throughout. We appreciate the questions that were raised during the session. As we have addressed all inquiries, we will now conclude our presentation. We want to express our gratitude for your presence and participation. We hope you found this session informative and insightful. 

Once again, we want to extend our thanks to each and every one of you for joining us today. We value your time and attention and are grateful for your interest in our project. It is our sincere hope that you found this Capstone project presentation helpful. 

As we bring this session to a close, we wish you all a wonderful and productive Wednesday. We hope you have a great day ahead. Thank you for joining us, and until next time, take care and goodbye.